{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load env\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import os\n",
    "# The location of my CA File\n",
    "cert_file = '/Users/noble/Downloads/mitmproxy-ca-cert.pem' \n",
    "os.environ['REQUESTS_CA_BUNDLE'] = cert_file\n",
    "os.environ['SSL_CERT_FILE'] = cert_file\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:8080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package llama-index is installed but has a version conflict:\n",
      "\t(llama-index 0.9.21 (/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages), Requirement.parse('llama-index>=0.9.26'))\n",
      "\n",
      "This package is optional for trulens_eval so this may not be a problem but if\n",
      "you need to use the related optional features and find there are errors, you\n",
      "will need to resolve the conflict:\n",
      "\n",
      "    ```bash\n",
      "    pip install 'llama-index>=0.9.26'\n",
      "    ```\n",
      "\n",
      "If you are running trulens_eval in a notebook, you may need to restart the\n",
      "kernel after resolving the conflict. If your distribution is in a bad place\n",
      "beyond this package, you may need to reinstall trulens_eval so that all of the\n",
      "dependencies get installed and hopefully corrected:\n",
      "    \n",
      "    ```bash\n",
      "    pip uninstall -y trulens_eval\n",
      "    pip install trulens_eval\n",
      "    ```\n",
      "\n",
      "Package chromadb is installed but has a version conflict:\n",
      "\t(chromadb 0.4.14 (/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages), Requirement.parse('chromadb>=0.4.18'))\n",
      "\n",
      "This package is optional for trulens_eval so this may not be a problem but if\n",
      "you need to use the related optional features and find there are errors, you\n",
      "will need to resolve the conflict:\n",
      "\n",
      "    ```bash\n",
      "    pip install 'chromadb>=0.4.18'\n",
      "    ```\n",
      "\n",
      "If you are running trulens_eval in a notebook, you may need to restart the\n",
      "kernel after resolving the conflict. If your distribution is in a bad place\n",
      "beyond this package, you may need to reinstall trulens_eval so that all of the\n",
      "dependencies get installed and hopefully corrected:\n",
      "    \n",
      "    ```bash\n",
      "    pip uninstall -y trulens_eval\n",
      "    pip install trulens_eval\n",
      "    ```\n",
      "\n",
      "Using legacy llama_index version 0.9.21. Consider upgrading to 0.10.0 or later.\n"
     ]
    }
   ],
   "source": [
    "virtual_app = dict(\n",
    "    llm=dict(\n",
    "        modelname=\"some llm component model name\"\n",
    "    ),\n",
    "    template=\"information about the template I used in my app\",\n",
    "    debug=\"all of these fields are completely optional\"\n",
    ")\n",
    "from trulens_eval import Select\n",
    "from trulens_eval.tru_virtual import VirtualApp\n",
    "\n",
    "virtual_app = VirtualApp(virtual_app) # can start with the prior dictionary\n",
    "virtual_app[Select.RecordCalls.llm.maxtokens] = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Select\n",
    "retriever = Select.RecordCalls.retriever\n",
    "synthesizer = Select.RecordCalls.synthesizer\n",
    "\n",
    "virtual_app[retriever] = \"retriever\"\n",
    "virtual_app[synthesizer] = \"synthesizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.tru_virtual import VirtualRecord\n",
    "\n",
    "# The selector for a presumed context retrieval component's call to\n",
    "# `get_context`. The names are arbitrary but may be useful for readability on\n",
    "# your end.\n",
    "context_call = retriever.get_context\n",
    "generation = synthesizer.generate\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"./data/test.json\", \"r\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "data = []\n",
    "task_instances = []\n",
    "for issue_details in ground_truth:\n",
    "    with open(f\"./data/v1/{issue_details['instance_id']}.json\", \"r\") as f:\n",
    "        generated_details = json.load(f)\n",
    "    problem_statement = issue_details[\"problem_statement\"]\n",
    "    mayil_response = generated_details[\"mayil_response\"]\n",
    "    rets = [f\"Filename: {c['filename']} | (Lines: {c['start_line']} to {c['end_line']})\\nCode Snippet:\\n{c['code']}\" for c in generated_details[\"mayil_collected_data\"][\"relevant_snippets\"]]\n",
    "    if not mayil_response:\n",
    "        continue\n",
    "\n",
    "    rec = VirtualRecord(\n",
    "        main_input=problem_statement,\n",
    "        main_output=mayil_response,\n",
    "        calls=\n",
    "            {\n",
    "                context_call: dict(\n",
    "                    args=[problem_statement],\n",
    "                    rets=rets\n",
    "                ),\n",
    "                generation: dict(\n",
    "                    args=[\"\"\"\n",
    "                        We have provided the below context: \\n\n",
    "                        ---------------------\\n\n",
    "                        \"\"\"+ \"\\n\".join(rets) +\n",
    "                        \"\"\"\n",
    "                        ---------------------\\n\n",
    "                        Given this information, please create a response to guide the developer in solving the issue:\n",
    "                        \"\"\"+ problem_statement],\n",
    "                    rets=[mayil_response]\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "    data.append(rec)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In qs_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In qs_relevance, input context will be set to __record__.app.retriever.get_context.rets[:] .\n",
      "âœ… In Groundedness, input source will be set to __record__.app.retriever.get_context.rets[:].collect() .\n",
      "âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.feedback.provider import OpenAI\n",
    "from trulens_eval.feedback.feedback import Feedback\n",
    "\n",
    "# Initialize provider class\n",
    "openai = OpenAI()\n",
    "\n",
    "# Select context to be used in feedback. We select the return values of the\n",
    "# virtual `get_context` call in the virtual `retriever` component. Names are\n",
    "# arbitrary except for `rets`.\n",
    "context = context_call.rets[:]\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(openai.context_relevance)\n",
    "    .on_input()\n",
    "    .on(context)\n",
    ")\n",
    "\n",
    "from trulens_eval.feedback import Groundedness\n",
    "grounded = Groundedness(groundedness_provider=openai)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name = \"Groundedness\")\n",
    "    .on(context.collect())\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = (\n",
    "    Feedback(openai.relevance_with_cot_reasons, name = \"Answer Relevance\")\n",
    "    .on_input_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.tru_virtual import TruVirtual\n",
    "\n",
    "virtual_recorder = TruVirtual(\n",
    "    app_id=\"Mayil\",\n",
    "    app=virtual_app,\n",
    "    feedbacks=[f_context_relevance, f_groundedness, f_qa_relevance],\n",
    "    feedback_mode = \"deferred\" # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in data:\n",
    "    virtual_recorder.add_record(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "\n",
    "# tru.run_dashboard(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will keep max of 32 feedback(s) running."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Thread(Thread-4 (_future_target_wrapper), started daemon 11509198848)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tasks are spread among max of 128 thread(s).\n",
      "Will rerun running feedbacks after a minute.\n",
      "Will rerun failed feedbacks after 5 minutes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f786e09f1f046c789adb58a47d11eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feedback Status:   0%|          | 0/3 [00:00<?, ?feedbacks/s, NONE=3]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c06660e66b4528aa9de8873e3661eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Done Runs: 0runs [00:00, ?runs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147cadb61e064566b1baaaced3e99594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Waiting for Runs: 0runs [00:00, ?runs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6aa79a5d6944b90ae8a453821323b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 55425, Requested 8173. Please try again in 3.598s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f51281046c143e8b49a6dee0a87792a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7377ea162bca47d2a02fec5ec81977c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59130, Requested 8185. Please try again in 7.315s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59183, Requested 8174. Please try again in 7.357s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58763, Requested 8257. Please try again in 7.02s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58847, Requested 8257. Please try again in 7.104s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 56565, Requested 8222. Please try again in 4.787s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n"
     ]
    }
   ],
   "source": [
    "tru.start_evaluator()\n",
    "\n",
    "# tru.stop_evaluator() # stop if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mayil_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
