{
    "id": "pydata__xarray-7203",
    "repo_name": "testbed/pydata__xarray__2022.09",
    "task_id": 3674726787559760554,
    "title": "Avoid loading any data for reprs",
    "body": "### What happened?\r\n\r\nFor \"small\" datasets, we load in to memory when displaying the repr. For cloud backed datasets with large number of \"small\" variables, this can use a lot of time sequentially loading O(100) variables just for a repr.\r\n\r\nhttps://github.com/pydata/xarray/blob/6c8db5ed005e000b35ad8b6ea9080105e608e976/xarray/core/formatting.py#L548-L549\r\n\r\n### What did you expect to happen?\r\n\r\nFast reprs!\r\n\r\n### Minimal Complete Verifiable Example\r\n\r\nThis dataset has 48 \"small\" variables\r\n```Python\r\nimport xarray as xr\r\n\r\ndc1 = xr.open_dataset('s3://its-live-data/datacubes/v02/N40E080/ITS_LIVE_vel_EPSG32645_G0120_X250000_Y4750000.zarr', engine= 'zarr', storage_options = {'anon':True})\r\ndc1._repr_html_()\r\n```\r\n\r\nOn `2022.03.0` this repr takes 36.4s\r\nIf I comment the `array.size` condition I get 6\u03bcs.\r\n\r\n\r\n### MVCE confirmation\r\n\r\n- [x] Minimal example \u2014 the example is as focused as reasonably possible to demonstrate the underlying issue in xarray.\r\n- [x] Complete example \u2014 the example is self-contained, including all data and the text of any traceback.\r\n- [x] Verifiable example \u2014 the example copy & pastes into an IPython prompt or [Binder notebook](https://mybinder.org/v2/gh/pydata/xarray/main?urlpath=lab/tree/doc/examples/blank_template.ipynb), returning the result.\r\n- [x] New issue \u2014 a search of GitHub Issues suggests this is not a duplicate.\r\n\r\n### Relevant log output\r\n\r\n_No response_\r\n\r\n### Anything else we need to know?\r\n\r\n_No response_\r\n\r\n### Environment\r\n\r\n<details>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.10.4 | packaged by conda-forge | (main, Mar 24 2022, 17:43:32) [Clang 12.0.1 ]\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 21.5.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: ('en_US', 'UTF-8')\r\nlibhdf5: None\r\nlibnetcdf: None\r\n\r\nxarray: 2022.3.0\r\npandas: 1.4.2\r\nnumpy: 1.22.4\r\nscipy: 1.8.1\r\nnetCDF4: None\r\npydap: None\r\nh5netcdf: None\r\nh5py: None\r\nNio: None\r\nzarr: 2.11.3\r\ncftime: None\r\nnc_time_axis: None\r\nPseudoNetCDF: None\r\nrasterio: 1.2.10\r\ncfgrib: None\r\niris: None\r\nbottleneck: None\r\ndask: 2022.05.2\r\ndistributed: None\r\nmatplotlib: 3.5.2\r\ncartopy: 0.20.2\r\nseaborn: 0.11.2\r\nnumbagg: None\r\nfsspec: 2022.5.0\r\ncupy: None\r\npint: None\r\nsparse: None\r\nsetuptools: 62.3.2\r\npip: 22.1.2\r\nconda: None\r\npytest: None\r\nIPython: 8.4.0\r\nsphinx: 4.5.0\r\n\r\n\r\n</details>\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/pydata__xarray__2022.09.git",
    "requested_commit": "",
    "mayil_response": "To address the performance issue with loading \"small\" datasets into memory for repr in xarray version 2022.3.0, consider the following solution:\n\n- Modify the `dataset_repr` function in `xarray/core/formatting.py` to conditionally load data into memory based on dataset size and whether it is cloud-backed.\n- Introduce a helper function to determine if a dataset is small and not cloud-backed.\n- Use the `array.size` condition to decide whether to load the data for repr.\n- Since the issue occurs when accessing cloud-backed datasets, ensure that datasets from cloud storage (e.g., S3) are not loaded into memory for repr.\n\nHere is a code snippet that could be added or modified in `xarray/core/formatting.py`:\n\n```python\ndef is_small_and_local(ds):\n    # Define what constitutes a \"small\" dataset, e.g., total size in bytes or number of elements\n    small_size_threshold = ...  # Define an appropriate threshold\n    # Check if the dataset is small and not cloud-backed\n    return ds.nbytes < small_size_threshold and not ds._is_remote\n\n@recursive_repr(\"<recursive Dataset>\")\ndef dataset_repr(ds):\n    summary = [f\"<xarray.{type(ds).__name__}>\"]\n    col_width = _calculate_col_width(ds.variables)\n    max_rows = OPTIONS[\"display_max_rows\"]\n\n    # ... existing code ...\n\n    # Modify the condition to check if the dataset is small and local\n    if is_small_and_local(ds):\n        summary.append(data_vars_repr(ds.data_vars, col_width=col_width, max_rows=max_rows))\n    else:\n        summary.append(\"Data variables not loaded into memory for performance reasons\")\n\n    # ... existing code ...\n\n    return \"\\n\".join(summary)\n```\n\n- Implement the `is_small_and_local` function to determine if the dataset is small and not cloud-backed.\n- Modify the `dataset_repr` function to use the `is_small_and_local` function when deciding whether to load data into memory for repr.\n- Ensure that the `ds._is_remote` attribute or an equivalent check is implemented to determine if the dataset is cloud-backed.\n- Adjust the `small_size_threshold` to a value that balances performance with the need for detailed repr output.\n\n### Code snippets to check\n\n-  \u2192 [xarray](https://github.com/testbed/pydata__xarray__2022.09/tree//xarray) \u2192 [core](https://github.com/testbed/pydata__xarray__2022.09/tree//xarray/core)- [Lines 626 - 877](https://github.com/testbed/pydata__xarray__2022.09/blob//xarray/core/formatting.py#L626-L877)\n\n This snippet contains the 'array_repr' function which is likely involved in generating the representation of arrays, and may need to be modified to avoid loading data for reprs. \n\n\n   - [Lines 1 - 77](https://github.com/testbed/pydata__xarray__2022.09/blob//xarray/core/formatting.py#L1-L77)\n\n This snippet includes utility functions for string formatting in '__repr__' which might be relevant when adjusting how data is loaded for reprs. \n\n\n   - [Lines 240 - 316](https://github.com/testbed/pydata__xarray__2022.09/blob//xarray/core/formatting.py#L240-L316)\n\n This snippet contains functions for inline representation of arrays, which could be relevant if the issue involves how data is displayed in reprs.\n-  \u2192 [xarray](https://github.com/testbed/pydata__xarray__2022.09/tree//xarray) \u2192 [core](https://github.com/testbed/pydata__xarray__2022.09/tree//xarray/core)- [Lines 254 - 342](https://github.com/testbed/pydata__xarray__2022.09/blob//xarray/core/formatting_html.py#L254-L342)\n\n This snippet includes HTML formatting functions for array representations, which might be relevant for the issue if HTML reprs are also affected. \n\n\n   - [Lines 1 - 296](https://github.com/testbed/pydata__xarray__2022.09/blob//xarray/core/formatting_html.py#L1-L296)\n\n This snippet is related to HTML formatting and might be relevant if the issue affects HTML reprs of datasets.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0017299999999999998,
        "snippet_processor": 0.06601,
        "issue_star_creation": 0.0274,
        "issue_star_solver": 0.08095000000000001,
        "bouncer": 0.03254
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711751597.71012,
        "relevant_snippets": [
            {
                "code": "import numpy as np\nimport pandas as pd\n\nimport xarray as xr\n\n\nclass Repr:\n    def setup(self):\n        a = np.arange(0, 100)\n        data_vars = dict()\n        for i in a:\n            data_vars[f\"long_variable_name_{i}\"] = xr.DataArray(\n                name=f\"long_variable_name_{i}\",\n                data=np.arange(0, 20),\n                dims=[f\"long_coord_name_{i}_x\"],\n                coords={f\"long_coord_name_{i}_x\": np.arange(0, 20) * 2},\n            )\n        self.ds = xr.Dataset(data_vars)\n        self.ds.attrs = {f\"attr_{k}\": 2 for k in a}\n\n    def time_repr(self):\n        repr(self.ds)\n\n    def time_repr_html(self):\n        self.ds._repr_html_()\n\n\nclass ReprMultiIndex:\n    def setup(self):\n        index = pd.MultiIndex.from_product(\n            [range(1000), range(1000)], names=(\"level_0\", \"level_1\")\n        )\n        series = pd.Series(range(1000 * 1000), index=index)\n        self.da = xr.DataArray(series)\n\n    def time_repr(self):\n        repr(self.da)\n\n    def time_repr_html(self):\n        self.da._repr_html_()",
                "filename": "asv_bench/benchmarks/repr.py",
                "start_index": 0,
                "end_index": 1036,
                "start_line": 1,
                "end_line": 40,
                "max_line": 40,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.09",
                "sha": ""
            },
            {
                "code": "cursive_repr(\"<recursive array>\")\ndef array_repr(arr):\n    from xarray.core.variable import Variable\n\n    max_rows = OPTIONS[\"display_max_rows\"]\n\n    # used for DataArray, Variable and IndexVariable\n    if hasattr(arr, \"name\") and arr.name is not None:\n        name_str = f\"{arr.name!r} \"\n    else:\n        name_str = \"\"\n\n    if (\n        isinstance(arr, Variable)\n        or _get_boolean_with_default(\"display_expand_data\", default=True)\n        or isinstance(arr.variable._data, MemoryCachedArray)\n    ):\n        data_repr = short_data_repr(arr)\n    else:\n        data_repr = inline_variable_array_repr(arr.variable, OPTIONS[\"display_width\"])\n\n    start = f\"<xarray.{type(arr).__name__} {name_str}\"\n    dims = dim_summary_limited(arr, col_width=len(start) + 1, max_rows=max_rows)\n    summary = [\n        f\"{start}({dims})>\",\n        data_repr,\n    ]\n\n    if hasattr(arr, \"coords\"):\n        if arr.coords:\n            col_width = _calculate_col_width(arr.coords)\n            summary.append(\n                coords_repr(arr.coords, col_width=col_width, max_rows=max_rows)\n            )\n\n        unindexed_dims_str = unindexed_dims_repr(\n            arr.dims, arr.coords, max_rows=max_rows\n        )\n        if unindexed_dims_str:\n            summary.append(unindexed_dims_str)\n\n        display_default_indexes = _get_boolean_with_default(\n            \"display_default_indexes\", False\n        )\n\n        xindexes = filter_nondefault_indexes(\n            _get_indexes_dict(arr.xindexes), not display_default_indexes\n        )\n\n        if xindexes:\n            summary.append(indexes_repr(xindexes, max_rows=max_rows))\n\n    if arr.attrs:\n        summary.append(attrs_repr(arr.attrs, max_rows=max_rows))\n\n    return \"\\n\".join(summary)\n\n\n@recursive_repr(\"<recursive Dataset>\")\ndef dataset_repr(ds):\n    summary = [f\"<xarray.{type(ds).__name__}>\"]\n\n    col_width = _calculate_col_width(ds.variables)\n    max_rows = OPTIONS[\"display_max_rows\"]\n\n    dims_start = pretty_print(\"Dimensions:\", col_width)\n    dims_values = dim_summary_limited(ds, col_width=col_width + 1, max_rows=max_rows)\n    summary.append(f\"{dims_start}({dims_values})\")\n\n    if ds.coords:\n        summary.append(coords_repr(ds.coords, col_width=col_width, max_rows=max_rows))\n\n    unindexed_dims_str = unindexed_dims_repr(ds.dims, ds.coords, max_rows=max_rows)\n    if unindexed_dims_str:\n        summary.append(unindexed_dims_str)\n\n    summary.append(data_vars_repr(ds.data_vars, col_width=col_width, max_rows=max_rows))\n\n    display_default_indexes = _get_boolean_with_default(\n        \"display_default_indexes\", False\n    )\n    xindexes = filter_nondefault_indexes(\n        _get_indexes_dict(ds.xindexes), not display_default_indexes\n    )\n    if xindexes:\n        summary.append(indexes_repr(xindexes, max_rows=max_rows))\n\n    if ds.attrs:\n        summary.append(attrs_repr(ds.attrs, max_rows=max_rows))\n\n    return \"\\n\".join(summary)\n\n\ndef",
                "filename": "xarray/core/formatting.py",
                "start_index": 20303,
                "end_index": 23207,
                "start_line": 626,
                "end_line": 877,
                "max_line": 893,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.09",
                "sha": ""
            },
            {
                "code": "def inline_dask_repr(array):\n    \"\"\"Similar to dask.array.DataArray.__repr__, but without\n    redundant information that's already printed by the repr\n    function of the xarray wrapper.\n    \"\"\"\n    assert isinstance(array, array_type(\"dask\")), array\n\n    chunksize = tuple(c[0] for c in array.chunks)\n\n    if hasattr(array, \"_meta\"):\n        meta = array._meta\n        identifier = (type(meta).__module__, type(meta).__name__)\n        meta_repr = _KNOWN_TYPE_REPRS.get(identifier, \".\".join(identifier))\n        meta_string = f\", meta={meta_repr}\"\n    else:\n        meta_string = \"\"\n\n    return f\"dask.array<chunksize={chunksize}{meta_string}>\"\n\n\ndef inline_sparse_repr(array):\n    \"\"\"Similar to sparse.COO.__repr__, but without the redundant shape/dtype.\"\"\"\n    sparse_array_type = array_type(\"sparse\")\n    assert isinstance(array, sparse_array_type), array\n    return \"<{}: nnz={:d}, fill_value={!s}>\".format(\n        type(array).__name__, array.nnz, array.fill_value\n    )\n\n\ndef inline_variable_array_repr(var, max_width):\n    \"\"\"Build a one-line summary of a variable's data.\"\"\"\n    if hasattr(var._data, \"_repr_inline_\"):\n        return var._data._repr_inline_(max_width)\n    if var._in_memory:\n        return format_array_flat(var, max_width)\n    dask_array_type = array_type(\"dask\")\n    if isinstance(var._data, dask_array_type):\n        return inline_dask_repr(var.data)\n    sparse_array_type = array_type(\"sparse\")\n    if isinstance(var._data, sparse_array_type):\n        return inline_sparse_repr(var.data)\n    if hasattr(var._data, \"__array_function__\"):\n        return maybe_truncate(repr(var._data).replace(\"\\n\", \" \"), max_width)\n    # internal xarray array type\n    return \"...\"\n\n\ndef summarize_variable(\n    name: Hashable,\n    var,\n    col_width: int,\n    max_width: int | None = None,\n    is_index: bool = False,\n):\n    \"\"\"Summarize a variable in one line, e.g., for the Dataset.__repr__.\"\"\"\n    variable = getattr(var, \"variable\", var)\n\n    if max_width is None:\n        max_width_options = OPTIONS[\"display_width\"]\n        if not isinstance(max_width_options, int):\n            raise TypeError(f\"`max_width` value of `{max_width}` is not a valid int\")\n        else:\n            max_width = max_width_options\n\n    marker = \"*\" if is_index else \" \"\n    first_col = pretty_print(f\"  {marker} {name} \", col_width)\n\n    if variable.dims:\n        dims_str = \"({}) \".format(\", \".join(map(str, variable.dims)))\n    else:\n        dims_str = \"\"\n    front_str = f\"{first_col}{dims_str}{variable.dtype} \"\n\n    values_width = max_width - len(front_str)\n    values_str = inline_variable_array_repr(variable, values_width)\n\n    return front_str + values_str",
                "filename": "xarray/core/formatting.py",
                "start_index": 8342,
                "end_index": 11003,
                "start_line": 240,
                "end_line": 316,
                "max_line": 893,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.09",
                "sha": ""
            },
            {
                "code": "index_section = partial(\n    _mapping_section,\n    name=\"Indexes\",\n    details_func=summarize_indexes,\n    max_items_collapse=0,\n    expand_option_name=\"display_expand_indexes\",\n)\n\nattr_section = partial(\n    _mapping_section,\n    name=\"Attributes\",\n    details_func=summarize_attrs,\n    max_items_collapse=10,\n    expand_option_name=\"display_expand_attrs\",\n)\n\n\ndef _get_indexes_dict(indexes):\n    return {\n        tuple(index_vars.keys()): idx for idx, index_vars in indexes.group_by_index()\n    }\n\n\ndef _obj_repr(obj, header_components, sections):\n    \"\"\"Return HTML repr of an xarray object.\n\n    If CSS is not injected (untrusted notebook), fallback to the plain text repr.\n\n    \"\"\"\n    header = f\"<div class='xr-header'>{''.join(h for h in header_components)}</div>\"\n    sections = \"\".join(f\"<li class='xr-section-item'>{s}</li>\" for s in sections)\n\n    icons_svg, css_style = _load_static_files()\n    return (\n        \"<div>\"\n        f\"{icons_svg}<style>{css_style}</style>\"\n        f\"<pre class='xr-text-repr-fallback'>{escape(repr(obj))}</pre>\"\n        \"<div class='xr-wrap' style='display:none'>\"\n        f\"{header}\"\n        f\"<ul class='xr-sections'>{sections}</ul>\"\n        \"</div>\"\n        \"</div>\"\n    )\n\n\ndef array_repr(arr):\n    dims = OrderedDict((k, v) for k, v in zip(arr.dims, arr.shape))\n    if hasattr(arr, \"xindexes\"):\n        indexed_dims = arr.xindexes.dims\n    else:\n        indexed_dims = {}\n\n    obj_type = f\"xarray.{type(arr).__name__}\"\n    arr_name = f\"'{arr.name}'\" if getattr(arr, \"name\", None) else \"\"\n\n    header_components = [\n        f\"<div class='xr-obj-type'>{obj_type}</div>\",\n        f\"<div class='xr-array-name'>{arr_name}</div>\",\n        format_dims(dims, indexed_dims),\n    ]\n\n    sections = [array_section(arr)]\n\n    if hasattr(arr, \"coords\"):\n        sections.append(coord_section(arr.coords))\n\n    if hasattr(arr, \"xindexes\"):\n        indexes = _get_indexes_dict(arr.xindexes)\n        sections.append(index_section(indexes))\n\n    sections.append(attr_section(arr.attrs))\n\n    return _obj_repr(arr, header_components, sections)\n\n\ndef dataset_repr(ds):\n    obj_type = f\"xarray.{type(ds).__name__}\"\n\n    header_components = [f\"<div class='xr-obj-type'>{escape(obj_type)}</div>\"]\n\n    sections = [\n        dim_section(ds),\n        coord_section(ds.coords),\n        datavar_section(ds.data_vars),\n        index_section(_get_indexes_dict(ds.xindexes)),\n        attr_section(ds.attrs),\n    ]\n\n    return _obj_repr(ds, header_components, sections)",
                "filename": "xarray/core/formatting_html.py",
                "start_index": 7729,
                "end_index": 10213,
                "start_line": 254,
                "end_line": 342,
                "max_line": 342,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.09",
                "sha": ""
            },
            {
                "code": "\"\"\"String formatting routines for __repr__.\n\"\"\"\nfrom __future__ import annotations\n\nimport contextlib\nimport functools\nimport math\nfrom collections import defaultdict\nfrom collections.abc import Collection, Hashable\nfrom datetime import datetime, timedelta\nfrom itertools import chain, zip_longest\nfrom reprlib import recursive_repr\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.errors import OutOfBoundsDatetime\n\nfrom xarray.core.duck_array_ops import array_equiv\nfrom xarray.core.indexing import ExplicitlyIndexed, MemoryCachedArray\nfrom xarray.core.options import OPTIONS, _get_boolean_with_default\nfrom xarray.core.pycompat import array_type\nfrom xarray.core.utils import is_duck_array\n\n\ndef pretty_print(x, numchars: int):\n    \"\"\"Given an object `x`, call `str(x)` and format the returned string so\n    that it is numchars long, padding with trailing spaces or truncating with\n    ellipses as necessary\n    \"\"\"\n    s = maybe_truncate(x, numchars)\n    return s + \" \" * max(numchars - len(s), 0)\n\n\ndef maybe_truncate(obj, maxlen=500):\n    s = str(obj)\n    if len(s) > maxlen:\n        s = s[: (maxlen - 3)] + \"...\"\n    return s\n\n\ndef wrap_indent(text, start=\"\", length=None):\n    if length is None:\n        length = len(start)\n    indent = \"\\n\" + \" \" * length\n    return start + indent.join(x for x in text.splitlines())\n\n\ndef _get_indexer_at_least_n_items(shape, n_desired, from_end):\n    assert 0 < n_desired <= math.prod(shape)\n    cum_items = np.cumprod(shape[::-1])\n    n_steps = np.argmax(cum_items >= n_desired)\n    stop = math.ceil(float(n_desired) / np.r_[1, cum_items][n_steps])\n    indexer = (\n        ((-1 if from_end else 0),) * (len(shape) - 1 - n_steps)\n        + ((slice(-stop, None) if from_end else slice(stop)),)\n        + (slice(None),) * n_steps\n    )\n    return indexer\n\n\ndef first_n_items(array, n_desired):\n    \"\"\"Returns the first n_desired items of an array\"\"\"\n    # Unfortunately, we can't just do array.flat[:n_desired] here because it\n    # might not be a numpy.ndarray. Moreover, access to elements of the array\n    # could be very expensive (e.g. if it's only available over DAP), so go out\n    # of our way to get them in a single call to __getitem__ using only slices.\n    if n_desired < 1:\n        raise ValueError(\"must request at least one item\")\n\n    if array.size == 0:\n        # work around for https://github.com/numpy/numpy/issues/5195\n        return []\n\n    if n_desired < array.size:\n        indexer = _get_indexer_at_least_n_items(array.shape, n_desired, from_end=False)\n        array = array[indexer]\n    return np.asarray(array).flat[:n_desired]",
                "filename": "xarray/core/formatting.py",
                "start_index": 0,
                "end_index": 2599,
                "start_line": 1,
                "end_line": 77,
                "max_line": 893,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.09",
                "sha": ""
            },
            {
                "code": "from __future__ import annotations\n\nimport uuid\nfrom collections import OrderedDict\nfrom functools import lru_cache, partial\nfrom html import escape\nfrom importlib.resources import files\n\nfrom xarray.core.formatting import (\n    inline_index_repr,\n    inline_variable_array_repr,\n    short_data_repr,\n)\nfrom xarray.core.options import _get_boolean_with_default\n\nSTATIC_FILES = (\n    (\"xarray.static.html\", \"icons-svg-inline.html\"),\n    (\"xarray.static.css\", \"style.css\"),\n)\n\n\n@lru_cache(None)\ndef _load_static_files():\n    \"\"\"Lazily load the resource files into memory the first time they are needed\"\"\"\n    return [\n        files(package).joinpath(resource).read_text(encoding=\"utf-8\")\n        for package, resource in STATIC_FILES\n    ]\n\n\ndef short_data_repr_html(array):\n    \"\"\"Format \"data\" for DataArray and Variable.\"\"\"\n    internal_data = getattr(array, \"variable\", array)._data\n    if hasattr(internal_data, \"_repr_html_\"):\n        return internal_data._repr_html_()\n    text = escape(short_data_repr(array))\n    return f\"<pre>{text}</pre>\"\n\n\ndef format_dims(dims, dims_with_index):\n    if not dims:\n        return \"\"\n\n    dim_css_map = {\n        dim: \" class='xr-has-index'\" if dim in dims_with_index else \"\" for dim in dims\n    }\n\n    dims_li = \"\".join(\n        f\"<li><span{dim_css_map[dim]}>\" f\"{escape(str(dim))}</span>: {size}</li>\"\n        for dim, size in dims.items()\n    )\n\n    return f\"<ul class='xr-dim-list'>{dims_li}</ul>\"\n\n\ndef summarize_attrs(attrs):\n    attrs_dl = \"\".join(\n        f\"<dt><span>{escape(str(k))} :</span></dt>\" f\"<dd>{escape(str(v))}</dd>\"\n        for k, v in attrs.items()\n    )\n\n    return f\"<dl class='xr-attrs'>{attrs_dl}</dl>\"\n\n\ndef _icon(icon_name):\n    # icon_name should be defined in xarray/static/html/icon-svg-inline.html\n    return (\n        \"<svg class='icon xr-{0}'>\"\n        \"<use xlink:href='#{0}'>\"\n        \"</use>\"\n        \"</svg>\".format(icon_name)\n    )",
                "filename": "xarray/core/formatting_html.py",
                "start_index": 0,
                "end_index": 1911,
                "start_line": 1,
                "end_line": 296,
                "max_line": 342,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.09",
                "sha": ""
            },
            {
                "code": "from __future__ import annotations\n\nimport os\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pandas as pd\n\nimport xarray as xr\n\nfrom . import _skip_slow, parameterized, randint, randn, requires_dask\n\ntry:\n    import dask\n    import dask.multiprocessing\nexcept ImportError:\n    pass\n\n\nos.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n\n_ENGINES = tuple(xr.backends.list_engines().keys() - {\"store\"})\n\n\nclass IOSingleNetCDF:\n    \"\"\"\n    A few examples that benchmark reading/writing a single netCDF file with\n    xarray\n    \"\"\"\n\n    timeout = 300.0\n    repeat = 1\n    number = 5\n\n    def make_ds(self):\n        # single Dataset\n        self.ds = xr.Dataset()\n        self.nt = 1000\n        self.nx = 90\n        self.ny = 45\n\n        self.block_chunks = {\n            \"time\": self.nt / 4,\n            \"lon\": self.nx / 3,\n            \"lat\": self.ny / 3,\n        }\n\n        self.time_chunks = {\"time\": int(self.nt / 36)}\n\n        times = pd.date_range(\"1970-01-01\", periods=self.nt, freq=\"D\")\n        lons = xr.DataArray(\n            np.linspace(0, 360, self.nx),\n            dims=(\"lon\",),\n            attrs={\"units\": \"degrees east\", \"long_name\": \"longitude\"},\n        )\n        lats = xr.DataArray(\n            np.linspace(-90, 90, self.ny),\n            dims=(\"lat\",),\n            attrs={\"units\": \"degrees north\", \"long_name\": \"latitude\"},\n        )\n        self.ds[\"foo\"] = xr.DataArray(\n            randn((self.nt, self.nx, self.ny), frac_nan=0.2),\n            coords={\"lon\": lons, \"lat\": lats, \"time\": times},\n            dims=(\"time\", \"lon\", \"lat\"),\n            name=\"foo\",\n            attrs={\"units\": \"foo units\", \"description\": \"a description\"},\n        )\n        self.ds[\"bar\"] = xr.DataArray(\n            randn((self.nt, self.nx, self.ny), frac_nan=0.2),\n            coords={\"lon\": lons, \"lat\": lats, \"time\": times},\n            dims=(\"time\", \"lon\", \"lat\"),\n            name=\"bar\",\n            attrs={\"units\": \"bar units\", \"description\": \"a description\"},\n        )\n        self.ds[\"baz\"] = xr.DataArray(\n            randn((self.nx, self.ny), frac_nan=0.2).astype(np.float32),\n            coords={\"lon\": lons, \"lat\": lats},\n            dims=(\"lon\", \"lat\"),\n            name=\"baz\",\n            attrs={\"units\": \"baz units\", \"description\": \"a description\"},\n        )\n\n        self.ds.attrs = {\"history\": \"created for xarray benchmarking\"}\n\n        self.oinds = {\n            \"time\": randint(0, self.nt, 120),\n            \"lon\": randint(0, self.nx, 20),\n            \"lat\": randint(0, self.ny, 10),\n        }\n        self.vinds = {\n            \"time\": xr.DataArray(randint(0, self.nt, 120), dims=\"x\"),\n            \"lon\": xr.DataArray(randint(0, self.nx, 120), dims=\"x\"),\n            \"lat\": slice(3, 20),\n        }",
                "filename": "asv_bench/benchmarks/dataset_io.py",
                "start_index": 0,
                "end_index": 2724,
                "start_line": 1,
                "end_line": 607,
                "max_line": 652,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.09",
                "sha": ""
            },
            {
                "code": "import numpy as np\n\nimport xarray as xr\n\nfrom . import requires_dask\n\nntime = 500\nnx = 50\nny = 50\n\n\nclass Reindex:\n    def setup(self):\n        data = np.random.RandomState(0).randn(ntime, nx, ny)\n        self.ds = xr.Dataset(\n            {\"temperature\": ((\"time\", \"x\", \"y\"), data)},\n            coords={\"time\": np.arange(ntime), \"x\": np.arange(nx), \"y\": np.arange(ny)},\n        )\n\n    def time_1d_coarse(self):\n        self.ds.reindex(time=np.arange(0, ntime, 5)).load()\n\n    def time_1d_fine_all_found(self):\n        self.ds.reindex(time=np.arange(0, ntime, 0.5), method=\"nearest\").load()\n\n    def time_1d_fine_some_missing(self):\n        self.ds.reindex(\n            time=np.arange(0, ntime, 0.5), method=\"nearest\", tolerance=0.1\n        ).load()\n\n    def time_2d_coarse(self):\n        self.ds.reindex(x=np.arange(0, nx, 2), y=np.arange(0, ny, 2)).load()\n\n    def time_2d_fine_all_found(self):\n        self.ds.reindex(\n            x=np.arange(0, nx, 0.5), y=np.arange(0, ny, 0.5), method=\"nearest\"\n        ).load()\n\n    def time_2d_fine_some_missing(self):\n        self.ds.reindex(\n            x=np.arange(0, nx, 0.5),\n            y=np.arange(0, ny, 0.5),\n            method=\"nearest\",\n            tolerance=0.1,\n        ).load()\n\n\nclass ReindexDask(Reindex):\n    def setup(self):\n        requires_dask()\n        super().setup()\n        self.ds = self.ds.chunk({\"time\": 100})",
                "filename": "asv_bench/benchmarks/reindexing.py",
                "start_index": 0,
                "end_index": 1378,
                "start_line": 1,
                "end_line": 52,
                "max_line": 52,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.09",
                "sha": ""
            },
            {
                "code": "class IOMultipleNetCDF:\n    \"\"\"\n    A few examples that benchmark reading/writing multiple netCDF files with\n    xarray\n    \"\"\"\n\n    timeout = 300.0\n    repeat = 1\n    number = 5\n\n    def make_ds(self, nfiles=10):\n        # multiple Dataset\n        self.ds = xr.Dataset()\n        self.nt = 1000\n        self.nx = 90\n        self.ny = 45\n        self.nfiles = nfiles\n\n        self.block_chunks = {\n            \"time\": self.nt / 4,\n            \"lon\": self.nx / 3,\n            \"lat\": self.ny / 3,\n        }\n\n        self.time_chunks = {\"time\": int(self.nt / 36)}\n\n        self.time_vars = np.split(\n            pd.date_range(\"1970-01-01\", periods=self.nt, freq=\"D\"), self.nfiles\n        )\n\n        self.ds_list = []\n        self.filenames_list = []\n        for i, times in enumerate(self.time_vars):\n            ds = xr.Dataset()\n            nt = len(times)\n            lons = xr.DataArray(\n                np.linspace(0, 360, self.nx),\n                dims=(\"lon\",),\n                attrs={\"units\": \"degrees east\", \"long_name\": \"longitude\"},\n            )\n            lats = xr.DataArray(\n                np.linspace(-90, 90, self.ny),\n                dims=(\"lat\",),\n                attrs={\"units\": \"degrees north\", \"long_name\": \"latitude\"},\n            )\n            ds[\"foo\"] = xr.DataArray(\n                randn((nt, self.nx, self.ny), frac_nan=0.2),\n                coords={\"lon\": lons, \"lat\": lats, \"time\": times},\n                dims=(\"time\", \"lon\", \"lat\"),\n                name=\"foo\",\n                attrs={\"units\": \"foo units\", \"description\": \"a description\"},\n            )\n            ds[\"bar\"] = xr.DataArray(\n                randn((nt, self.nx, self.ny), frac_nan=0.2),\n                coords={\"lon\": lons, \"lat\": lats, \"time\": times},\n                dims=(\"time\", \"lon\", \"lat\"),\n                name=\"bar\",\n                attrs={\"units\": \"bar units\", \"description\": \"a description\"},\n            )\n            ds[\"baz\"] = xr.DataArray(\n                randn((self.nx, self.ny), frac_nan=0.2).astype(np.float32),\n                coords={\"lon\": lons, \"lat\": lats},\n                dims=(\"lon\", \"lat\"),\n                name=\"baz\",\n                attrs={\"units\": \"baz units\", \"description\": \"a description\"},\n            )\n\n            ds.attrs = {\"history\": \"created for xarray benchmarking\"}\n\n            self.ds_list.append(ds)\n            self.filenames_list.append(\"test_netcdf_%i.nc\" % i)\n\n\nclass IOWriteMultipleNetCDF3(IOMultipleNetCDF):\n    def setup(self):\n        # TODO: Lazily skipped in CI as it is very demanding and slow.\n        # Improve times and remove errors.\n        _skip_slow()\n\n        self.make_ds()\n        self.format = \"NETCDF3_64BIT\"\n\n    def time_write_dataset_netcdf4(self):\n        xr.save_mfdataset(\n            self.ds_list, self.filenames_list, engine=\"netcdf4\", format=self.format\n        )\n\n    def time_write_dataset_scipy(self):\n        xr.save_mfdataset(\n            self.ds_list, self.filenames_list, engine=\"scipy\", format=self.format\n        )",
                "filename": "asv_bench/benchmarks/dataset_io.py",
                "start_index": 7697,
                "end_index": 10696,
                "start_line": 239,
                "end_line": 641,
                "max_line": 652,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.09",
                "sha": ""
            },
            {
                "code": "import numpy as np\n\nimport xarray as xr\n\n\nclass DatasetAddVariable:\n    param_names = [\"existing_elements\"]\n    params = [[0, 10, 100, 1000]]\n\n    def setup(self, existing_elements):\n        self.datasets = {}\n        # Dictionary insertion is fast(er) than xarray.Dataset insertion\n        d = {}\n        for i in range(existing_elements):\n            d[f\"var{i}\"] = i\n        self.dataset = xr.merge([d])\n\n        d = {f\"set_2_{i}\": i for i in range(existing_elements)}\n        self.dataset2 = xr.merge([d])\n\n    def time_variable_insertion(self, existing_elements):\n        dataset = self.dataset\n        dataset[\"new_var\"] = 0\n\n    def time_merge_two_datasets(self, existing_elements):\n        xr.merge([self.dataset, self.dataset2])\n\n\nclass DatasetCreation:\n    # The idea here is to time how long it takes to go from numpy\n    # and python data types, to a full dataset\n    # See discussion\n    # https://github.com/pydata/xarray/issues/7224#issuecomment-1292216344\n    param_names = [\"strategy\", \"count\"]\n    params = [\n        [\"dict_of_DataArrays\", \"dict_of_Variables\", \"dict_of_Tuples\"],\n        [0, 1, 10, 100, 1000],\n    ]\n\n    def setup(self, strategy, count):\n        data = np.array([\"0\", \"b\"], dtype=str)\n        self.dataset_coords = dict(time=np.array([0, 1]))\n        self.dataset_attrs = dict(description=\"Test data\")\n        attrs = dict(units=\"Celcius\")\n        if strategy == \"dict_of_DataArrays\":\n\n            def create_data_vars():\n                return {\n                    f\"long_variable_name_{i}\": xr.DataArray(\n                        data=data, dims=(\"time\"), attrs=attrs\n                    )\n                    for i in range(count)\n                }\n\n        elif strategy == \"dict_of_Variables\":\n\n            def create_data_vars():\n                return {\n                    f\"long_variable_name_{i}\": xr.Variable(\"time\", data, attrs=attrs)\n                    for i in range(count)\n                }\n\n        elif strategy == \"dict_of_Tuples\":\n\n            def create_data_vars():\n                return {\n                    f\"long_variable_name_{i}\": (\"time\", data, attrs)\n                    for i in range(count)\n                }\n\n        self.create_data_vars = create_data_vars\n\n    def time_dataset_creation(self, strategy, count):\n        data_vars = self.create_data_vars()\n        xr.Dataset(\n            data_vars=data_vars, coords=self.dataset_coords, attrs=self.dataset_attrs\n        )",
                "filename": "asv_bench/benchmarks/merge.py",
                "start_index": 0,
                "end_index": 2442,
                "start_line": 1,
                "end_line": 77,
                "max_line": 77,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.09",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "xarray/core/formatting.py": [
                {
                    "chunk": {
                        "code": "cursive_repr(\"<recursive array>\")\ndef array_repr(arr):\n    from xarray.core.variable import Variable\n\n    max_rows = OPTIONS[\"display_max_rows\"]\n\n    # used for DataArray, Variable and IndexVariable\n    if hasattr(arr, \"name\") and arr.name is not None:\n        name_str = f\"{arr.name!r} \"\n    else:\n        name_str = \"\"\n\n    if (\n        isinstance(arr, Variable)\n        or _get_boolean_with_default(\"display_expand_data\", default=True)\n        or isinstance(arr.variable._data, MemoryCachedArray)\n    ):\n        data_repr = short_data_repr(arr)\n    else:\n        data_repr = inline_variable_array_repr(arr.variable, OPTIONS[\"display_width\"])\n\n    start = f\"<xarray.{type(arr).__name__} {name_str}\"\n    dims = dim_summary_limited(arr, col_width=len(start) + 1, max_rows=max_rows)\n    summary = [\n        f\"{start}({dims})>\",\n        data_repr,\n    ]\n\n    if hasattr(arr, \"coords\"):\n        if arr.coords:\n            col_width = _calculate_col_width(arr.coords)\n            summary.append(\n                coords_repr(arr.coords, col_width=col_width, max_rows=max_rows)\n            )\n\n        unindexed_dims_str = unindexed_dims_repr(\n            arr.dims, arr.coords, max_rows=max_rows\n        )\n        if unindexed_dims_str:\n            summary.append(unindexed_dims_str)\n\n        display_default_indexes = _get_boolean_with_default(\n            \"display_default_indexes\", False\n        )\n\n        xindexes = filter_nondefault_indexes(\n            _get_indexes_dict(arr.xindexes), not display_default_indexes\n        )\n\n        if xindexes:\n            summary.append(indexes_repr(xindexes, max_rows=max_rows))\n\n    if arr.attrs:\n        summary.append(attrs_repr(arr.attrs, max_rows=max_rows))\n\n    return \"\\n\".join(summary)\n\n\n@recursive_repr(\"<recursive Dataset>\")\ndef dataset_repr(ds):\n    summary = [f\"<xarray.{type(ds).__name__}>\"]\n\n    col_width = _calculate_col_width(ds.variables)\n    max_rows = OPTIONS[\"display_max_rows\"]\n\n    dims_start = pretty_print(\"Dimensions:\", col_width)\n    dims_values = dim_summary_limited(ds, col_width=col_width + 1, max_rows=max_rows)\n    summary.append(f\"{dims_start}({dims_values})\")\n\n    if ds.coords:\n        summary.append(coords_repr(ds.coords, col_width=col_width, max_rows=max_rows))\n\n    unindexed_dims_str = unindexed_dims_repr(ds.dims, ds.coords, max_rows=max_rows)\n    if unindexed_dims_str:\n        summary.append(unindexed_dims_str)\n\n    summary.append(data_vars_repr(ds.data_vars, col_width=col_width, max_rows=max_rows))\n\n    display_default_indexes = _get_boolean_with_default(\n        \"display_default_indexes\", False\n    )\n    xindexes = filter_nondefault_indexes(\n        _get_indexes_dict(ds.xindexes), not display_default_indexes\n    )\n    if xindexes:\n        summary.append(indexes_repr(xindexes, max_rows=max_rows))\n\n    if ds.attrs:\n        summary.append(attrs_repr(ds.attrs, max_rows=max_rows))\n\n    return \"\\n\".join(summary)\n\n\ndef",
                        "filename": "xarray/core/formatting.py",
                        "start_index": 20303,
                        "end_index": 23207,
                        "start_line": 626,
                        "end_line": 877,
                        "max_line": 893,
                        "git_instance": "github",
                        "repo_name": "testbed/pydata__xarray__2022.09",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the 'array_repr' function which is likely involved in generating the representation of arrays, and may need to be modified to avoid loading data for reprs."
                },
                {
                    "chunk": {
                        "code": "\"\"\"String formatting routines for __repr__.\n\"\"\"\nfrom __future__ import annotations\n\nimport contextlib\nimport functools\nimport math\nfrom collections import defaultdict\nfrom collections.abc import Collection, Hashable\nfrom datetime import datetime, timedelta\nfrom itertools import chain, zip_longest\nfrom reprlib import recursive_repr\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.errors import OutOfBoundsDatetime\n\nfrom xarray.core.duck_array_ops import array_equiv\nfrom xarray.core.indexing import ExplicitlyIndexed, MemoryCachedArray\nfrom xarray.core.options import OPTIONS, _get_boolean_with_default\nfrom xarray.core.pycompat import array_type\nfrom xarray.core.utils import is_duck_array\n\n\ndef pretty_print(x, numchars: int):\n    \"\"\"Given an object `x`, call `str(x)` and format the returned string so\n    that it is numchars long, padding with trailing spaces or truncating with\n    ellipses as necessary\n    \"\"\"\n    s = maybe_truncate(x, numchars)\n    return s + \" \" * max(numchars - len(s), 0)\n\n\ndef maybe_truncate(obj, maxlen=500):\n    s = str(obj)\n    if len(s) > maxlen:\n        s = s[: (maxlen - 3)] + \"...\"\n    return s\n\n\ndef wrap_indent(text, start=\"\", length=None):\n    if length is None:\n        length = len(start)\n    indent = \"\\n\" + \" \" * length\n    return start + indent.join(x for x in text.splitlines())\n\n\ndef _get_indexer_at_least_n_items(shape, n_desired, from_end):\n    assert 0 < n_desired <= math.prod(shape)\n    cum_items = np.cumprod(shape[::-1])\n    n_steps = np.argmax(cum_items >= n_desired)\n    stop = math.ceil(float(n_desired) / np.r_[1, cum_items][n_steps])\n    indexer = (\n        ((-1 if from_end else 0),) * (len(shape) - 1 - n_steps)\n        + ((slice(-stop, None) if from_end else slice(stop)),)\n        + (slice(None),) * n_steps\n    )\n    return indexer\n\n\ndef first_n_items(array, n_desired):\n    \"\"\"Returns the first n_desired items of an array\"\"\"\n    # Unfortunately, we can't just do array.flat[:n_desired] here because it\n    # might not be a numpy.ndarray. Moreover, access to elements of the array\n    # could be very expensive (e.g. if it's only available over DAP), so go out\n    # of our way to get them in a single call to __getitem__ using only slices.\n    if n_desired < 1:\n        raise ValueError(\"must request at least one item\")\n\n    if array.size == 0:\n        # work around for https://github.com/numpy/numpy/issues/5195\n        return []\n\n    if n_desired < array.size:\n        indexer = _get_indexer_at_least_n_items(array.shape, n_desired, from_end=False)\n        array = array[indexer]\n    return np.asarray(array).flat[:n_desired]",
                        "filename": "xarray/core/formatting.py",
                        "start_index": 0,
                        "end_index": 2599,
                        "start_line": 1,
                        "end_line": 77,
                        "max_line": 893,
                        "git_instance": "github",
                        "repo_name": "testbed/pydata__xarray__2022.09",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes utility functions for string formatting in '__repr__' which might be relevant when adjusting how data is loaded for reprs."
                },
                {
                    "chunk": {
                        "code": "def inline_dask_repr(array):\n    \"\"\"Similar to dask.array.DataArray.__repr__, but without\n    redundant information that's already printed by the repr\n    function of the xarray wrapper.\n    \"\"\"\n    assert isinstance(array, array_type(\"dask\")), array\n\n    chunksize = tuple(c[0] for c in array.chunks)\n\n    if hasattr(array, \"_meta\"):\n        meta = array._meta\n        identifier = (type(meta).__module__, type(meta).__name__)\n        meta_repr = _KNOWN_TYPE_REPRS.get(identifier, \".\".join(identifier))\n        meta_string = f\", meta={meta_repr}\"\n    else:\n        meta_string = \"\"\n\n    return f\"dask.array<chunksize={chunksize}{meta_string}>\"\n\n\ndef inline_sparse_repr(array):\n    \"\"\"Similar to sparse.COO.__repr__, but without the redundant shape/dtype.\"\"\"\n    sparse_array_type = array_type(\"sparse\")\n    assert isinstance(array, sparse_array_type), array\n    return \"<{}: nnz={:d}, fill_value={!s}>\".format(\n        type(array).__name__, array.nnz, array.fill_value\n    )\n\n\ndef inline_variable_array_repr(var, max_width):\n    \"\"\"Build a one-line summary of a variable's data.\"\"\"\n    if hasattr(var._data, \"_repr_inline_\"):\n        return var._data._repr_inline_(max_width)\n    if var._in_memory:\n        return format_array_flat(var, max_width)\n    dask_array_type = array_type(\"dask\")\n    if isinstance(var._data, dask_array_type):\n        return inline_dask_repr(var.data)\n    sparse_array_type = array_type(\"sparse\")\n    if isinstance(var._data, sparse_array_type):\n        return inline_sparse_repr(var.data)\n    if hasattr(var._data, \"__array_function__\"):\n        return maybe_truncate(repr(var._data).replace(\"\\n\", \" \"), max_width)\n    # internal xarray array type\n    return \"...\"\n\n\ndef summarize_variable(\n    name: Hashable,\n    var,\n    col_width: int,\n    max_width: int | None = None,\n    is_index: bool = False,\n):\n    \"\"\"Summarize a variable in one line, e.g., for the Dataset.__repr__.\"\"\"\n    variable = getattr(var, \"variable\", var)\n\n    if max_width is None:\n        max_width_options = OPTIONS[\"display_width\"]\n        if not isinstance(max_width_options, int):\n            raise TypeError(f\"`max_width` value of `{max_width}` is not a valid int\")\n        else:\n            max_width = max_width_options\n\n    marker = \"*\" if is_index else \" \"\n    first_col = pretty_print(f\"  {marker} {name} \", col_width)\n\n    if variable.dims:\n        dims_str = \"({}) \".format(\", \".join(map(str, variable.dims)))\n    else:\n        dims_str = \"\"\n    front_str = f\"{first_col}{dims_str}{variable.dtype} \"\n\n    values_width = max_width - len(front_str)\n    values_str = inline_variable_array_repr(variable, values_width)\n\n    return front_str + values_str",
                        "filename": "xarray/core/formatting.py",
                        "start_index": 8342,
                        "end_index": 11003,
                        "start_line": 240,
                        "end_line": 316,
                        "max_line": 893,
                        "git_instance": "github",
                        "repo_name": "testbed/pydata__xarray__2022.09",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains functions for inline representation of arrays, which could be relevant if the issue involves how data is displayed in reprs."
                }
            ],
            "xarray/core/formatting_html.py": [
                {
                    "chunk": {
                        "code": "index_section = partial(\n    _mapping_section,\n    name=\"Indexes\",\n    details_func=summarize_indexes,\n    max_items_collapse=0,\n    expand_option_name=\"display_expand_indexes\",\n)\n\nattr_section = partial(\n    _mapping_section,\n    name=\"Attributes\",\n    details_func=summarize_attrs,\n    max_items_collapse=10,\n    expand_option_name=\"display_expand_attrs\",\n)\n\n\ndef _get_indexes_dict(indexes):\n    return {\n        tuple(index_vars.keys()): idx for idx, index_vars in indexes.group_by_index()\n    }\n\n\ndef _obj_repr(obj, header_components, sections):\n    \"\"\"Return HTML repr of an xarray object.\n\n    If CSS is not injected (untrusted notebook), fallback to the plain text repr.\n\n    \"\"\"\n    header = f\"<div class='xr-header'>{''.join(h for h in header_components)}</div>\"\n    sections = \"\".join(f\"<li class='xr-section-item'>{s}</li>\" for s in sections)\n\n    icons_svg, css_style = _load_static_files()\n    return (\n        \"<div>\"\n        f\"{icons_svg}<style>{css_style}</style>\"\n        f\"<pre class='xr-text-repr-fallback'>{escape(repr(obj))}</pre>\"\n        \"<div class='xr-wrap' style='display:none'>\"\n        f\"{header}\"\n        f\"<ul class='xr-sections'>{sections}</ul>\"\n        \"</div>\"\n        \"</div>\"\n    )\n\n\ndef array_repr(arr):\n    dims = OrderedDict((k, v) for k, v in zip(arr.dims, arr.shape))\n    if hasattr(arr, \"xindexes\"):\n        indexed_dims = arr.xindexes.dims\n    else:\n        indexed_dims = {}\n\n    obj_type = f\"xarray.{type(arr).__name__}\"\n    arr_name = f\"'{arr.name}'\" if getattr(arr, \"name\", None) else \"\"\n\n    header_components = [\n        f\"<div class='xr-obj-type'>{obj_type}</div>\",\n        f\"<div class='xr-array-name'>{arr_name}</div>\",\n        format_dims(dims, indexed_dims),\n    ]\n\n    sections = [array_section(arr)]\n\n    if hasattr(arr, \"coords\"):\n        sections.append(coord_section(arr.coords))\n\n    if hasattr(arr, \"xindexes\"):\n        indexes = _get_indexes_dict(arr.xindexes)\n        sections.append(index_section(indexes))\n\n    sections.append(attr_section(arr.attrs))\n\n    return _obj_repr(arr, header_components, sections)\n\n\ndef dataset_repr(ds):\n    obj_type = f\"xarray.{type(ds).__name__}\"\n\n    header_components = [f\"<div class='xr-obj-type'>{escape(obj_type)}</div>\"]\n\n    sections = [\n        dim_section(ds),\n        coord_section(ds.coords),\n        datavar_section(ds.data_vars),\n        index_section(_get_indexes_dict(ds.xindexes)),\n        attr_section(ds.attrs),\n    ]\n\n    return _obj_repr(ds, header_components, sections)",
                        "filename": "xarray/core/formatting_html.py",
                        "start_index": 7729,
                        "end_index": 10213,
                        "start_line": 254,
                        "end_line": 342,
                        "max_line": 342,
                        "git_instance": "github",
                        "repo_name": "testbed/pydata__xarray__2022.09",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes HTML formatting functions for array representations, which might be relevant for the issue if HTML reprs are also affected."
                },
                {
                    "chunk": {
                        "code": "from __future__ import annotations\n\nimport uuid\nfrom collections import OrderedDict\nfrom functools import lru_cache, partial\nfrom html import escape\nfrom importlib.resources import files\n\nfrom xarray.core.formatting import (\n    inline_index_repr,\n    inline_variable_array_repr,\n    short_data_repr,\n)\nfrom xarray.core.options import _get_boolean_with_default\n\nSTATIC_FILES = (\n    (\"xarray.static.html\", \"icons-svg-inline.html\"),\n    (\"xarray.static.css\", \"style.css\"),\n)\n\n\n@lru_cache(None)\ndef _load_static_files():\n    \"\"\"Lazily load the resource files into memory the first time they are needed\"\"\"\n    return [\n        files(package).joinpath(resource).read_text(encoding=\"utf-8\")\n        for package, resource in STATIC_FILES\n    ]\n\n\ndef short_data_repr_html(array):\n    \"\"\"Format \"data\" for DataArray and Variable.\"\"\"\n    internal_data = getattr(array, \"variable\", array)._data\n    if hasattr(internal_data, \"_repr_html_\"):\n        return internal_data._repr_html_()\n    text = escape(short_data_repr(array))\n    return f\"<pre>{text}</pre>\"\n\n\ndef format_dims(dims, dims_with_index):\n    if not dims:\n        return \"\"\n\n    dim_css_map = {\n        dim: \" class='xr-has-index'\" if dim in dims_with_index else \"\" for dim in dims\n    }\n\n    dims_li = \"\".join(\n        f\"<li><span{dim_css_map[dim]}>\" f\"{escape(str(dim))}</span>: {size}</li>\"\n        for dim, size in dims.items()\n    )\n\n    return f\"<ul class='xr-dim-list'>{dims_li}</ul>\"\n\n\ndef summarize_attrs(attrs):\n    attrs_dl = \"\".join(\n        f\"<dt><span>{escape(str(k))} :</span></dt>\" f\"<dd>{escape(str(v))}</dd>\"\n        for k, v in attrs.items()\n    )\n\n    return f\"<dl class='xr-attrs'>{attrs_dl}</dl>\"\n\n\ndef _icon(icon_name):\n    # icon_name should be defined in xarray/static/html/icon-svg-inline.html\n    return (\n        \"<svg class='icon xr-{0}'>\"\n        \"<use xlink:href='#{0}'>\"\n        \"</use>\"\n        \"</svg>\".format(icon_name)\n    )",
                        "filename": "xarray/core/formatting_html.py",
                        "start_index": 0,
                        "end_index": 1911,
                        "start_line": 1,
                        "end_line": 296,
                        "max_line": 342,
                        "git_instance": "github",
                        "repo_name": "testbed/pydata__xarray__2022.09",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet is related to HTML formatting and might be relevant if the issue affects HTML reprs of datasets."
                }
            ]
        },
        "solution_task": "To address the performance issue with loading \"small\" datasets into memory for repr in xarray version 2022.3.0, consider the following solution:\n\n- Modify the `dataset_repr` function in `xarray/core/formatting.py` to conditionally load data into memory based on dataset size and whether it is cloud-backed.\n- Introduce a helper function to determine if a dataset is small and not cloud-backed.\n- Use the `array.size` condition to decide whether to load the data for repr.\n- Since the issue occurs when accessing cloud-backed datasets, ensure that datasets from cloud storage (e.g., S3) are not loaded into memory for repr.\n\nHere is a code snippet that could be added or modified in `xarray/core/formatting.py`:\n\n```python\ndef is_small_and_local(ds):\n    # Define what constitutes a \"small\" dataset, e.g., total size in bytes or number of elements\n    small_size_threshold = ...  # Define an appropriate threshold\n    # Check if the dataset is small and not cloud-backed\n    return ds.nbytes < small_size_threshold and not ds._is_remote\n\n@recursive_repr(\"<recursive Dataset>\")\ndef dataset_repr(ds):\n    summary = [f\"<xarray.{type(ds).__name__}>\"]\n    col_width = _calculate_col_width(ds.variables)\n    max_rows = OPTIONS[\"display_max_rows\"]\n\n    # ... existing code ...\n\n    # Modify the condition to check if the dataset is small and local\n    if is_small_and_local(ds):\n        summary.append(data_vars_repr(ds.data_vars, col_width=col_width, max_rows=max_rows))\n    else:\n        summary.append(\"Data variables not loaded into memory for performance reasons\")\n\n    # ... existing code ...\n\n    return \"\\n\".join(summary)\n```\n\n- Implement the `is_small_and_local` function to determine if the dataset is small and not cloud-backed.\n- Modify the `dataset_repr` function to use the `is_small_and_local` function when deciding whether to load data into memory for repr.\n- Ensure that the `ds._is_remote` attribute or an equivalent check is implemented to determine if the dataset is cloud-backed.\n- Adjust the `small_size_threshold` to a value that balances performance with the need for detailed repr output.",
        "bouncer_response": {
            "score": 7,
            "criteria": "RESPONSES that are highly relevant, offering clear and comprehensive guidance or solutions for the described bug, should score between 7 and 9.",
            "supporting_evidence": "The RESPONSE provides a detailed and relevant solution to the issue of loading small datasets into memory for repr in xarray. It suggests modifying the 'dataset_repr' function and introducing a helper function to determine if a dataset is small and not cloud-backed. The RESPONSE also includes a code snippet that could be added or modified in 'xarray/core/formatting.py', which is directly related to the issue described in the PROMPT. The RESPONSE addresses the need to avoid loading data for reprs from cloud storage, which is a key aspect of the problem. However, it does not provide a complete solution as it leaves placeholders (e.g., 'small_size_threshold') for the user to define and assumes the existence of a '_is_remote' attribute without confirming its presence in the current xarray codebase. Therefore, while the RESPONSE is highly relevant and provides clear guidance, it is not entirely complete, which is why it does not score a 10.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711751755.768917,
        "result": "To address the performance issue with loading \"small\" datasets into memory for repr in xarray version 2022.3.0, consider the following solution:\n\n- Modify the `dataset_repr` function in `xarray/core/formatting.py` to conditionally load data into memory based on dataset size and whether it is cloud-backed.\n- Introduce a helper function to determine if a dataset is small and not cloud-backed.\n- Use the `array.size` condition to decide whether to load the data for repr.\n- Since the issue occurs when accessing cloud-backed datasets, ensure that datasets from cloud storage (e.g., S3) are not loaded into memory for repr.\n\nHere is a code snippet that could be added or modified in `xarray/core/formatting.py`:\n\n```python\ndef is_small_and_local(ds):\n    # Define what constitutes a \"small\" dataset, e.g., total size in bytes or number of elements\n    small_size_threshold = ...  # Define an appropriate threshold\n    # Check if the dataset is small and not cloud-backed\n    return ds.nbytes < small_size_threshold and not ds._is_remote\n\n@recursive_repr(\"<recursive Dataset>\")\ndef dataset_repr(ds):\n    summary = [f\"<xarray.{type(ds).__name__}>\"]\n    col_width = _calculate_col_width(ds.variables)\n    max_rows = OPTIONS[\"display_max_rows\"]\n\n    # ... existing code ...\n\n    # Modify the condition to check if the dataset is small and local\n    if is_small_and_local(ds):\n        summary.append(data_vars_repr(ds.data_vars, col_width=col_width, max_rows=max_rows))\n    else:\n        summary.append(\"Data variables not loaded into memory for performance reasons\")\n\n    # ... existing code ...\n\n    return \"\\n\".join(summary)\n```\n\n- Implement the `is_small_and_local` function to determine if the dataset is small and not cloud-backed.\n- Modify the `dataset_repr` function to use the `is_small_and_local` function when deciding whether to load data into memory for repr.\n- Ensure that the `ds._is_remote` attribute or an equivalent check is implemented to determine if the dataset is cloud-backed.\n- Adjust the `small_size_threshold` to a value that balances performance with the need for detailed repr output.\n\n### Code snippets to check\n\n-  \u2192 [xarray](https://github.com/testbed/pydata__xarray__2022.09/tree//xarray) \u2192 [core](https://github.com/testbed/pydata__xarray__2022.09/tree//xarray/core)- [Lines 626 - 877](https://github.com/testbed/pydata__xarray__2022.09/blob//xarray/core/formatting.py#L626-L877)\n\n This snippet contains the 'array_repr' function which is likely involved in generating the representation of arrays, and may need to be modified to avoid loading data for reprs. \n\n\n   - [Lines 1 - 77](https://github.com/testbed/pydata__xarray__2022.09/blob//xarray/core/formatting.py#L1-L77)\n\n This snippet includes utility functions for string formatting in '__repr__' which might be relevant when adjusting how data is loaded for reprs. \n\n\n   - [Lines 240 - 316](https://github.com/testbed/pydata__xarray__2022.09/blob//xarray/core/formatting.py#L240-L316)\n\n This snippet contains functions for inline representation of arrays, which could be relevant if the issue involves how data is displayed in reprs.\n-  \u2192 [xarray](https://github.com/testbed/pydata__xarray__2022.09/tree//xarray) \u2192 [core](https://github.com/testbed/pydata__xarray__2022.09/tree//xarray/core)- [Lines 254 - 342](https://github.com/testbed/pydata__xarray__2022.09/blob//xarray/core/formatting_html.py#L254-L342)\n\n This snippet includes HTML formatting functions for array representations, which might be relevant for the issue if HTML reprs are also affected. \n\n\n   - [Lines 1 - 296](https://github.com/testbed/pydata__xarray__2022.09/blob//xarray/core/formatting_html.py#L1-L296)\n\n This snippet is related to HTML formatting and might be relevant if the issue affects HTML reprs of datasets."
    }
}