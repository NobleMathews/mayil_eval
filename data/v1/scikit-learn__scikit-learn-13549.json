{
    "id": "scikit-learn__scikit-learn-13549",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
    "task_id": 5857033857529253190,
    "title": "Stratified subsampler utility?",
    "body": "I have some data `X` and `y` that I want to subsample (i.e. only keep a subset of the samples) in a stratified way.\r\n\r\n\r\nUsing something like \r\n\r\n```py\r\n_, X_sub, _, y_sub = train_test_split(\r\n    X, y, stratify=stratify, train_size=None, test_size=n_samples_sub)\r\n```\r\n\r\nis almost what I need. But that will error if:\r\n\r\n- I happen to want exactly `X.shape[0]` samples (`ValueError: test_size=60 should be either positive and smaller than the number of samples`)\r\n- I want something close to `X.shape[0]` which is not enough to have a stratified test or train set (`ValueError: The train_size = 1 should be greater or equal to the number of classes = 2`)\r\n\r\n----\r\n\r\n~~Would it make sense to add a `subsample()` util?\r\nAnother option would be to add a `bypass_checks` to `train_test_split`~~\r\n\r\nBasically what I need is a `stratify` option to `utils.resample`, that's probably the most appropriate place to introduce this.\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.21.git",
    "requested_commit": "",
    "mayil_response": "To implement a stratified subsampling utility, you can modify the existing `resample` function in `sklearn/utils/__init__.py` to handle cases where the number of samples requested is equal to or nearly equal to the number of samples in the input data `X`. Here's a high-level overview of the steps to implement this feature:\n\n- Modify the `resample` function to accept a new parameter that allows for stratified subsampling.\n- Handle edge cases where the number of requested samples is equal to or close to the number of samples in `X`.\n- Ensure that the stratified subsampling respects the class distribution of `y`.\n\nHere's a pseudo-code snippet to illustrate the changes:\n\n```python\nfrom sklearn.utils import check_random_state, check_array, _safe_indexing\nfrom sklearn.utils.validation import check_consistent_length, indexable\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\ndef resample(*arrays, replace=True, n_samples=None, random_state=None, stratify=None):\n    # Existing code to handle parameters and input validation...\n    \n    # New code to handle stratified subsampling\n    if stratify is not None:\n        # Ensure input is indexable\n        arrays = indexable(*arrays)\n        X, y = arrays[0], check_array(stratify, ensure_2d=False, dtype=None)\n        \n        # Create a StratifiedShuffleSplit instance\n        sss = StratifiedShuffleSplit(n_splits=1, test_size=n_samples, random_state=random_state)\n        \n        # Perform the split\n        train_index, test_index = next(sss.split(X, y))\n        \n        # If n_samples is None or equal to the number of samples, use all indices\n        if n_samples is None or n_samples >= X.shape[0]:\n            indices = train_index\n        else:\n            # Otherwise, use the test indices as the subsample\n            indices = test_index\n        \n        # Resample the arrays\n        resampled_arrays = [_safe_indexing(a, indices) for a in arrays]\n        \n        # Return the resampled arrays\n        if len(resampled_arrays) == 1:\n            return resampled_arrays[0]\n        else:\n            return resampled_arrays\n    else:\n        # Existing code for non-stratified resampling...\n```\n\nThis pseudo-code modifies the `resample` function to handle stratified subsampling by using `StratifiedShuffleSplit` when the `stratify` parameter is provided. It also handles the case where `n_samples` is `None` or equal to the number of samples in `X`.\n\nRemember to handle edge cases and input validation as per the existing `resample` function's implementation. This pseudo-code is a starting point and may need to be adjusted to fit into the existing codebase properly.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/utils)- [Lines 576 - 629](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/utils/__init__.py#L576-L629)\n\n This snippet contains the implementation of resampling with stratification, which is directly related to the issue of adding a stratify option to utils.resample. \n\n\n   - [Lines 483 - 574](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/utils/__init__.py#L483-L574)\n\n This snippet provides the documentation and parameters for the resample function, which is relevant for understanding how to add the stratify option. \n\n\n   - [Lines 115 - 1210](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/utils/__init__.py#L115-L1210)\n\n This snippet shows the parameter validation for the resample function, which will need to be updated to handle the new stratify option.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0008225,
        "snippet_processor": 0.051160000000000004,
        "issue_star_creation": 0.035809999999999995,
        "issue_star_solver": 0.07194,
        "bouncer": 0.02488
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711740446.619953,
        "relevant_snippets": [
            {
                "code": "if max_n_samples is None:\n        max_n_samples = n_samples\n    elif (max_n_samples > n_samples) and (not replace):\n        raise ValueError(\n            \"Cannot sample %d out of arrays with dim %d when replace is False\"\n            % (max_n_samples, n_samples)\n        )\n\n    check_consistent_length(*arrays)\n\n    if stratify is None:\n        if replace:\n            indices = random_state.randint(0, n_samples, size=(max_n_samples,))\n        else:\n            indices = np.arange(n_samples)\n            random_state.shuffle(indices)\n            indices = indices[:max_n_samples]\n    else:\n        # Code adapted from StratifiedShuffleSplit()\n        y = check_array(stratify, ensure_2d=False, dtype=None)\n        if y.ndim == 2:\n            # for multi-label y, map each distinct row to a string repr\n            # using join because str(row) uses an ellipsis if len(row) > 1000\n            y = np.array([\" \".join(row.astype(\"str\")) for row in y])\n\n        classes, y_indices = np.unique(y, return_inverse=True)\n        n_classes = classes.shape[0]\n\n        class_counts = np.bincount(y_indices)\n\n        # Find the sorted list of instances for each class:\n        # (np.unique above performs a sort, so code is O(n logn) already)\n        class_indices = np.split(\n            np.argsort(y_indices, kind=\"mergesort\"), np.cumsum(class_counts)[:-1]\n        )\n\n        n_i = _approximate_mode(class_counts, max_n_samples, random_state)\n\n        indices = []\n\n        for i in range(n_classes):\n            indices_i = random_state.choice(class_indices[i], n_i[i], replace=replace)\n            indices.extend(indices_i)\n\n        indices = random_state.permutation(indices)\n\n    # convert sparse matrices to CSR for row-based indexing\n    arrays = [a.tocsr() if issparse(a) else a for a in arrays]\n    resampled_arrays = [_safe_indexing(a, indices) for a in arrays]\n    if len(resampled_arrays) == 1:\n        # syntactic sugar for the unit argument case\n        return resampled_arrays[0]\n    else:\n        return resampled_arrays",
                "filename": "sklearn/utils/__init__.py",
                "start_index": 18643,
                "end_index": 20670,
                "start_line": 576,
                "end_line": 629,
                "max_line": 1210,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "class StratifiedShuffleSplit(BaseShuffleSplit):",
                "filename": "sklearn/model_selection/_split.py",
                "start_index": 71944,
                "end_index": 71991,
                "start_line": 2014,
                "end_line": 2014,
                "max_line": 2751,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "def _make_test_folds(self, X, y=None):\n        rng = check_random_state(self.random_state)\n        y = np.asarray(y)\n        type_of_target_y = type_of_target(y)\n        allowed_target_types = (\"binary\", \"multiclass\")\n        if type_of_target_y not in allowed_target_types:\n            raise ValueError(\n                \"Supported target types are: {}. Got {!r} instead.\".format(\n                    allowed_target_types, type_of_target_y\n                )\n            )\n\n        y = column_or_1d(y)\n\n        _, y_idx, y_inv = np.unique(y, return_index=True, return_inverse=True)\n        # y_inv encodes y according to lexicographic order. We invert y_idx to\n        # map the classes so that they are encoded by order of appearance:\n        # 0 represents the first label appearing in y, 1 the second, etc.\n        _, class_perm = np.unique(y_idx, return_inverse=True)\n        y_encoded = class_perm[y_inv]\n\n        n_classes = len(y_idx)\n        y_counts = np.bincount(y_encoded)\n        min_groups = np.min(y_counts)\n        if np.all(self.n_splits > y_counts):\n            raise ValueError(\n                \"n_splits=%d cannot be greater than the\"\n                \" number of members in each class.\" % (self.n_splits)\n            )\n        if self.n_splits > min_groups:\n            warnings.warn(\n                \"The least populated class in y has only %d\"\n                \" members, which is less than n_splits=%d.\"\n                % (min_groups, self.n_splits),\n                UserWarning,\n            )\n\n        # Determine the optimal number of samples from each class in each fold,\n        # using round robin over the sorted y. (This can be done direct from\n        # counts, but that code is unreadable.)\n        y_order = np.sort(y_encoded)\n        allocation = np.asarray(\n            [\n                np.bincount(y_order[i :: self.n_splits], minlength=n_classes)\n                for i in range(self.n_splits)\n            ]\n        )\n\n        # To maintain the data order dependencies as best as possible within\n        # the stratification constraint, we assign samples from each class in\n        # blocks (and then mess that up when shuffle=True).\n        test_folds = np.empty(len(y), dtype=\"i\")\n        for k in range(n_classes):\n            # since the kth column of allocation stores the number of samples\n            # of class k in each test set, this generates blocks of fold\n            # indices corresponding to the allocation for class k.\n            folds_for_class = np.arange(self.n_splits).repeat(allocation[:, k])\n            if self.shuffle:\n                rng.shuffle(folds_for_class)\n            test_folds[y_encoded == k] = folds_for_class\n        return test_folds\n\n    def _iter_test_masks(self, X, y=None, groups=None):\n        test_folds = self._make_test_folds(X, y)\n        for i in range(self.n_splits):\n            yield test_folds == i",
                "filename": "sklearn/model_selection/_split.py",
                "start_index": 23440,
                "end_index": 26325,
                "start_line": 695,
                "end_line": 760,
                "max_line": 2751,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "class BaseShuffleSplit(_MetadataRequester, metaclass=ABCMeta):\n    \"\"\"Base class for ShuffleSplit and StratifiedShuffleSplit\"\"\"\n\n    # This indicates that by default CV splitters don't have a \"groups\" kwarg,\n    # unless indicated by inheriting from ``GroupsConsumerMixin``.\n    # This also prevents ``set_split_request`` to be generated for splitters\n    # which don't support ``groups``.\n    __metadata_request__split = {\"groups\": metadata_routing.UNUSED}\n\n    def __init__(\n        self, n_splits=10, *, test_size=None, train_size=None, random_state=None\n    ):\n        self.n_splits = n_splits\n        self.test_size = test_size\n        self.train_size = train_size\n        self.random_state = random_state\n        self._default_test_size = 0.1\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            The target variable for supervised learning problems.\n\n        groups : array-like of shape (n_samples,), default=None\n            Group labels for the samples used while splitting the dataset into\n            train/test set.\n\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n\n        test : ndarray\n            The testing set indices for that split.\n\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting `random_state`\n        to an integer.\n        \"\"\"\n        X, y, groups = indexable(X, y, groups)\n        for train, test in self._iter_indices(X, y, groups):\n            yield train, test\n\n    @abstractmethod\n    def _iter_indices(self, X, y=None, groups=None):\n        \"\"\"Generate (train, test) indices\"\"\"\n\n    def get_n_splits(self, X=None, y=None, groups=None):\n        \"\"\"Returns the number of splitting iterations in the cross-validator\n\n        Parameters\n        ----------\n        X : object\n            Always ignored, exists for compatibility.\n\n        y : object\n            Always ignored, exists for compatibility.\n\n        groups : object\n            Always ignored, exists for compatibility.\n\n        Returns\n        -------\n        n_splits : int\n            Returns the number of splitting iterations in the cross-validator.\n        \"\"\"\n        return self.n_splits\n\n    def __repr__(self):\n        return _build_repr(self)",
                "filename": "sklearn/model_selection/_split.py",
                "start_index": 59727,
                "end_index": 62389,
                "start_line": 1677,
                "end_line": 1755,
                "max_line": 2751,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "if self.n_iter_no_change is not None:\n            stratify = y if is_classifier(self) else None\n            X, X_val, y, y_val, sample_weight, sample_weight_val = train_test_split(\n                X,\n                y,\n                sample_weight,\n                random_state=self.random_state,\n                test_size=self.validation_fraction,\n                stratify=stratify,\n            )\n            if is_classifier(self):\n                if self._n_classes != np.unique(y).shape[0]:\n                    # We choose to error here. The problem is that the init\n                    # estimator would be trained on y, which has some missing\n                    # classes now, so its predictions would not have the\n                    # correct shape.\n                    raise ValueError(\n                        \"The training data after the early stopping split \"\n                        \"is missing some classes. Try using another random \"\n                        \"seed.\"\n                    )\n        else:\n            X_val = y_val = sample_weight_val = None",
                "filename": "sklearn/ensemble/_gb.py",
                "start_index": 15757,
                "end_index": 16828,
                "start_line": 434,
                "end_line": 456,
                "max_line": 1828,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "@validate_params(\n    {\n        \"replace\": [\"boolean\"],\n        \"n_samples\": [Interval(numbers.Integral, 1, None, closed=\"left\"), None],\n        \"random_state\": [\"random_state\"],\n        \"stratify\": [\"array-like\", None],\n    },\n    prefer_skip_nested_validation=True,\n)",
                "filename": "sklearn/utils/__init__.py",
                "start_index": 15399,
                "end_index": 15668,
                "start_line": 115,
                "end_line": 1210,
                "max_line": 1210,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "from abc import abstractmethod\nfrom copy import deepcopy\nfrom math import ceil, floor, log\nfrom numbers import Integral, Real\n\nimport numpy as np\n\nfrom ..base import _fit_context, is_classifier\nfrom ..metrics._scorer import get_scorer_names\nfrom ..utils import resample\nfrom ..utils._param_validation import Interval, StrOptions\nfrom ..utils.multiclass import check_classification_targets\nfrom ..utils.validation import _num_samples\nfrom . import ParameterGrid, ParameterSampler\nfrom ._search import BaseSearchCV\nfrom ._split import _yields_constant_splits, check_cv\n\n__all__ = [\"HalvingGridSearchCV\", \"HalvingRandomSearchCV\"]\n\n\nclass _SubsampleMetaSplitter:\n    \"\"\"Splitter that subsamples a given fraction of the dataset\"\"\"\n\n    def __init__(self, *, base_cv, fraction, subsample_test, random_state):\n        self.base_cv = base_cv\n        self.fraction = fraction\n        self.subsample_test = subsample_test\n        self.random_state = random_state\n\n    def split(self, X, y, groups=None):\n        for train_idx, test_idx in self.base_cv.split(X, y, groups):\n            train_idx = resample(\n                train_idx,\n                replace=False,\n                random_state=self.random_state,\n                n_samples=int(self.fraction * train_idx.shape[0]),\n            )\n            if self.subsample_test:\n                test_idx = resample(\n                    test_idx,\n                    replace=False,\n                    random_state=self.random_state,\n                    n_samples=int(self.fraction * test_idx.shape[0]),\n                )\n            yield train_idx, test_idx\n\n\ndef _top_k(results, k, itr):\n    # Return the best candidates of a given iteration\n    iteration, mean_test_score, params = (\n        np.asarray(a)\n        for a in (results[\"iter\"], results[\"mean_test_score\"], results[\"params\"])\n    )\n    iter_indices = np.flatnonzero(iteration == itr)\n    scores = mean_test_score[iter_indices]\n    # argsort() places NaNs at the end of the array so we move NaNs to the\n    # front of the array so the last `k` items are the those with the\n    # highest scores.\n    sorted_indices = np.roll(np.argsort(scores), np.count_nonzero(np.isnan(scores)))\n    return np.array(params[iter_indices][sorted_indices[-k:]])",
                "filename": "sklearn/model_selection/_search_successive_halving.py",
                "start_index": 0,
                "end_index": 2247,
                "start_line": 1,
                "end_line": 60,
                "max_line": 1083,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"Resample arrays or sparse matrices in a consistent way.\n\n    The default strategy implements one step of the bootstrapping\n    procedure.\n\n    Parameters\n    ----------\n    *arrays : sequence of array-like of shape (n_samples,) or \\\n            (n_samples, n_outputs)\n        Indexable data-structures can be arrays, lists, dataframes or scipy\n        sparse matrices with consistent first dimension.\n\n    replace : bool, default=True\n        Implements resampling with replacement. If False, this will implement\n        (sliced) random permutations.\n\n    n_samples : int, default=None\n        Number of samples to generate. If left to None this is\n        automatically set to the first dimension of the arrays.\n        If replace is False it should not be larger than the length of\n        arrays.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for shuffling\n        the data.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    stratify : array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n            default=None\n        If not None, data is split in a stratified fashion, using this as\n        the class labels.\n\n    Returns\n    -------\n    resampled_arrays : sequence of array-like of shape (n_samples,) or \\\n            (n_samples, n_outputs)\n        Sequence of resampled copies of the collections. The original arrays\n        are not impacted.\n\n    See Also\n    --------\n    shuffle : Shuffle arrays or sparse matrices in a consistent way.\n\n    Examples\n    --------\n    It is possible to mix sparse and dense arrays in the same run::\n\n      >>> import numpy as np\n      >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n      >>> y = np.array([0, 1, 2])\n\n      >>> from scipy.sparse import coo_matrix\n      >>> X_sparse = coo_matrix(X)\n\n      >>> from sklearn.utils import resample\n      >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0)\n      >>> X\n      array([[1., 0.],\n             [2., 1.],\n             [1., 0.]])\n\n      >>> X_sparse\n      <3x2 sparse matrix of type '<... 'numpy.float64'>'\n          with 4 stored elements in Compressed Sparse Row format>\n\n      >>> X_sparse.toarray()\n      array([[1., 0.],\n             [2., 1.],\n             [1., 0.]])\n\n      >>> y\n      array([0, 1, 0])\n\n      >>> resample(y, n_samples=2, random_state=0)\n      array([0, 1])\n\n    Example using stratification::\n\n      >>> y = [0, 0, 1, 1, 1, 1, 1, 1, 1]\n      >>> resample(y, n_samples=5, replace=False, stratify=y,\n      ...          random_state=0)\n      [1, 1, 1, 0, 1]\n    \"\"\"\n    max_n_samples = n_samples\n    random_state = check_random_state(random_state)\n\n    if len(arrays) == 0:\n        return None\n\n    first = arrays[0]\n    n_samples = first.shape[0] if hasattr(first, \"shape\") else len(first)",
                "filename": "sklearn/utils/__init__.py",
                "start_index": 15760,
                "end_index": 18637,
                "start_line": 483,
                "end_line": 574,
                "max_line": 1210,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "def _apply_on_subsets(func, X):\n    # apply function on the whole set and on mini batches\n    result_full = func(X)\n    n_features = X.shape[1]\n    result_by_batch = [func(batch.reshape(1, n_features)) for batch in X]\n\n    # func can output tuple (e.g. score_samples)\n    if type(result_full) == tuple:\n        result_full = result_full[0]\n        result_by_batch = list(map(lambda x: x[0], result_by_batch))\n\n    if sparse.issparse(result_full):\n        result_full = result_full.A\n        result_by_batch = [x.A for x in result_by_batch]\n\n    return np.ravel(result_full), np.ravel(result_by_batch)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_methods_subset_invariance(name, estimator_orig):\n    # check that method gives invariant results if applied\n    # on mini batches or the whole set\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20, 3))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    y = X[:, 0].astype(int)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n    estimator.fit(X, y)\n\n    for method in [\n        \"predict\",\n        \"transform\",\n        \"decision_function\",\n        \"score_samples\",\n        \"predict_proba\",\n    ]:\n        msg = (\"{method} of {name} is not invariant when applied to a subset.\").format(\n            method=method, name=name\n        )\n\n        if hasattr(estimator, method):\n            result_full, result_by_batch = _apply_on_subsets(\n                getattr(estimator, method), X\n            )\n            assert_allclose(result_full, result_by_batch, atol=1e-7, err_msg=msg)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 51629,
                "end_index": 53408,
                "start_line": 1506,
                "end_line": 1558,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "class StratifiedKFold(_BaseKFold):",
                "filename": "sklearn/model_selection/_split.py",
                "start_index": 20604,
                "end_index": 20638,
                "start_line": 618,
                "end_line": 618,
                "max_line": 2751,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            }
        ],
        "classification": "feature request",
        "snippet_processor_task": {
            "sklearn/utils/__init__.py": [
                {
                    "chunk": {
                        "code": "if max_n_samples is None:\n        max_n_samples = n_samples\n    elif (max_n_samples > n_samples) and (not replace):\n        raise ValueError(\n            \"Cannot sample %d out of arrays with dim %d when replace is False\"\n            % (max_n_samples, n_samples)\n        )\n\n    check_consistent_length(*arrays)\n\n    if stratify is None:\n        if replace:\n            indices = random_state.randint(0, n_samples, size=(max_n_samples,))\n        else:\n            indices = np.arange(n_samples)\n            random_state.shuffle(indices)\n            indices = indices[:max_n_samples]\n    else:\n        # Code adapted from StratifiedShuffleSplit()\n        y = check_array(stratify, ensure_2d=False, dtype=None)\n        if y.ndim == 2:\n            # for multi-label y, map each distinct row to a string repr\n            # using join because str(row) uses an ellipsis if len(row) > 1000\n            y = np.array([\" \".join(row.astype(\"str\")) for row in y])\n\n        classes, y_indices = np.unique(y, return_inverse=True)\n        n_classes = classes.shape[0]\n\n        class_counts = np.bincount(y_indices)\n\n        # Find the sorted list of instances for each class:\n        # (np.unique above performs a sort, so code is O(n logn) already)\n        class_indices = np.split(\n            np.argsort(y_indices, kind=\"mergesort\"), np.cumsum(class_counts)[:-1]\n        )\n\n        n_i = _approximate_mode(class_counts, max_n_samples, random_state)\n\n        indices = []\n\n        for i in range(n_classes):\n            indices_i = random_state.choice(class_indices[i], n_i[i], replace=replace)\n            indices.extend(indices_i)\n\n        indices = random_state.permutation(indices)\n\n    # convert sparse matrices to CSR for row-based indexing\n    arrays = [a.tocsr() if issparse(a) else a for a in arrays]\n    resampled_arrays = [_safe_indexing(a, indices) for a in arrays]\n    if len(resampled_arrays) == 1:\n        # syntactic sugar for the unit argument case\n        return resampled_arrays[0]\n    else:\n        return resampled_arrays",
                        "filename": "sklearn/utils/__init__.py",
                        "start_index": 18643,
                        "end_index": 20670,
                        "start_line": 576,
                        "end_line": 629,
                        "max_line": 1210,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the implementation of resampling with stratification, which is directly related to the issue of adding a stratify option to utils.resample."
                },
                {
                    "chunk": {
                        "code": "\"\"\"Resample arrays or sparse matrices in a consistent way.\n\n    The default strategy implements one step of the bootstrapping\n    procedure.\n\n    Parameters\n    ----------\n    *arrays : sequence of array-like of shape (n_samples,) or \\\n            (n_samples, n_outputs)\n        Indexable data-structures can be arrays, lists, dataframes or scipy\n        sparse matrices with consistent first dimension.\n\n    replace : bool, default=True\n        Implements resampling with replacement. If False, this will implement\n        (sliced) random permutations.\n\n    n_samples : int, default=None\n        Number of samples to generate. If left to None this is\n        automatically set to the first dimension of the arrays.\n        If replace is False it should not be larger than the length of\n        arrays.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for shuffling\n        the data.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    stratify : array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n            default=None\n        If not None, data is split in a stratified fashion, using this as\n        the class labels.\n\n    Returns\n    -------\n    resampled_arrays : sequence of array-like of shape (n_samples,) or \\\n            (n_samples, n_outputs)\n        Sequence of resampled copies of the collections. The original arrays\n        are not impacted.\n\n    See Also\n    --------\n    shuffle : Shuffle arrays or sparse matrices in a consistent way.\n\n    Examples\n    --------\n    It is possible to mix sparse and dense arrays in the same run::\n\n      >>> import numpy as np\n      >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n      >>> y = np.array([0, 1, 2])\n\n      >>> from scipy.sparse import coo_matrix\n      >>> X_sparse = coo_matrix(X)\n\n      >>> from sklearn.utils import resample\n      >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0)\n      >>> X\n      array([[1., 0.],\n             [2., 1.],\n             [1., 0.]])\n\n      >>> X_sparse\n      <3x2 sparse matrix of type '<... 'numpy.float64'>'\n          with 4 stored elements in Compressed Sparse Row format>\n\n      >>> X_sparse.toarray()\n      array([[1., 0.],\n             [2., 1.],\n             [1., 0.]])\n\n      >>> y\n      array([0, 1, 0])\n\n      >>> resample(y, n_samples=2, random_state=0)\n      array([0, 1])\n\n    Example using stratification::\n\n      >>> y = [0, 0, 1, 1, 1, 1, 1, 1, 1]\n      >>> resample(y, n_samples=5, replace=False, stratify=y,\n      ...          random_state=0)\n      [1, 1, 1, 0, 1]\n    \"\"\"\n    max_n_samples = n_samples\n    random_state = check_random_state(random_state)\n\n    if len(arrays) == 0:\n        return None\n\n    first = arrays[0]\n    n_samples = first.shape[0] if hasattr(first, \"shape\") else len(first)",
                        "filename": "sklearn/utils/__init__.py",
                        "start_index": 15760,
                        "end_index": 18637,
                        "start_line": 483,
                        "end_line": 574,
                        "max_line": 1210,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet provides the documentation and parameters for the resample function, which is relevant for understanding how to add the stratify option."
                },
                {
                    "chunk": {
                        "code": "@validate_params(\n    {\n        \"replace\": [\"boolean\"],\n        \"n_samples\": [Interval(numbers.Integral, 1, None, closed=\"left\"), None],\n        \"random_state\": [\"random_state\"],\n        \"stratify\": [\"array-like\", None],\n    },\n    prefer_skip_nested_validation=True,\n)",
                        "filename": "sklearn/utils/__init__.py",
                        "start_index": 15399,
                        "end_index": 15668,
                        "start_line": 115,
                        "end_line": 1210,
                        "max_line": 1210,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the parameter validation for the resample function, which will need to be updated to handle the new stratify option."
                }
            ]
        },
        "solution_task": "To implement a stratified subsampling utility, you can modify the existing `resample` function in `sklearn/utils/__init__.py` to handle cases where the number of samples requested is equal to or nearly equal to the number of samples in the input data `X`. Here's a high-level overview of the steps to implement this feature:\n\n- Modify the `resample` function to accept a new parameter that allows for stratified subsampling.\n- Handle edge cases where the number of requested samples is equal to or close to the number of samples in `X`.\n- Ensure that the stratified subsampling respects the class distribution of `y`.\n\nHere's a pseudo-code snippet to illustrate the changes:\n\n```python\nfrom sklearn.utils import check_random_state, check_array, _safe_indexing\nfrom sklearn.utils.validation import check_consistent_length, indexable\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\ndef resample(*arrays, replace=True, n_samples=None, random_state=None, stratify=None):\n    # Existing code to handle parameters and input validation...\n    \n    # New code to handle stratified subsampling\n    if stratify is not None:\n        # Ensure input is indexable\n        arrays = indexable(*arrays)\n        X, y = arrays[0], check_array(stratify, ensure_2d=False, dtype=None)\n        \n        # Create a StratifiedShuffleSplit instance\n        sss = StratifiedShuffleSplit(n_splits=1, test_size=n_samples, random_state=random_state)\n        \n        # Perform the split\n        train_index, test_index = next(sss.split(X, y))\n        \n        # If n_samples is None or equal to the number of samples, use all indices\n        if n_samples is None or n_samples >= X.shape[0]:\n            indices = train_index\n        else:\n            # Otherwise, use the test indices as the subsample\n            indices = test_index\n        \n        # Resample the arrays\n        resampled_arrays = [_safe_indexing(a, indices) for a in arrays]\n        \n        # Return the resampled arrays\n        if len(resampled_arrays) == 1:\n            return resampled_arrays[0]\n        else:\n            return resampled_arrays\n    else:\n        # Existing code for non-stratified resampling...\n```\n\nThis pseudo-code modifies the `resample` function to handle stratified subsampling by using `StratifiedShuffleSplit` when the `stratify` parameter is provided. It also handles the case where `n_samples` is `None` or equal to the number of samples in `X`.\n\nRemember to handle edge cases and input validation as per the existing `resample` function's implementation. This pseudo-code is a starting point and may need to be adjusted to fit into the existing codebase properly.",
        "bouncer_response": {
            "score": 7,
            "criteria": "RESPONSES that are highly relevant, offering clear and comprehensive suggestions or solutions for the described feature request, should score between 7 and 9.",
            "supporting_evidence": "The RESPONSE provides a detailed and relevant suggestion for implementing a stratified subsampling utility by modifying the existing `resample` function in scikit-learn. It includes a high-level overview of the steps needed, handles edge cases, and respects the class distribution of `y`. The RESPONSE also provides pseudo-code to illustrate the proposed changes, which is actionable and directly addresses the feature request. The inclusion of code snippets to check from the scikit-learn repository adds value by guiding where the changes should be made. However, the RESPONSE does not fully address the potential complexities of integrating the new feature into the existing codebase, such as compatibility with other parts of the library or potential performance implications, which prevents it from scoring a 9 or 10.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711740584.738315,
        "result": "To implement a stratified subsampling utility, you can modify the existing `resample` function in `sklearn/utils/__init__.py` to handle cases where the number of samples requested is equal to or nearly equal to the number of samples in the input data `X`. Here's a high-level overview of the steps to implement this feature:\n\n- Modify the `resample` function to accept a new parameter that allows for stratified subsampling.\n- Handle edge cases where the number of requested samples is equal to or close to the number of samples in `X`.\n- Ensure that the stratified subsampling respects the class distribution of `y`.\n\nHere's a pseudo-code snippet to illustrate the changes:\n\n```python\nfrom sklearn.utils import check_random_state, check_array, _safe_indexing\nfrom sklearn.utils.validation import check_consistent_length, indexable\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\ndef resample(*arrays, replace=True, n_samples=None, random_state=None, stratify=None):\n    # Existing code to handle parameters and input validation...\n    \n    # New code to handle stratified subsampling\n    if stratify is not None:\n        # Ensure input is indexable\n        arrays = indexable(*arrays)\n        X, y = arrays[0], check_array(stratify, ensure_2d=False, dtype=None)\n        \n        # Create a StratifiedShuffleSplit instance\n        sss = StratifiedShuffleSplit(n_splits=1, test_size=n_samples, random_state=random_state)\n        \n        # Perform the split\n        train_index, test_index = next(sss.split(X, y))\n        \n        # If n_samples is None or equal to the number of samples, use all indices\n        if n_samples is None or n_samples >= X.shape[0]:\n            indices = train_index\n        else:\n            # Otherwise, use the test indices as the subsample\n            indices = test_index\n        \n        # Resample the arrays\n        resampled_arrays = [_safe_indexing(a, indices) for a in arrays]\n        \n        # Return the resampled arrays\n        if len(resampled_arrays) == 1:\n            return resampled_arrays[0]\n        else:\n            return resampled_arrays\n    else:\n        # Existing code for non-stratified resampling...\n```\n\nThis pseudo-code modifies the `resample` function to handle stratified subsampling by using `StratifiedShuffleSplit` when the `stratify` parameter is provided. It also handles the case where `n_samples` is `None` or equal to the number of samples in `X`.\n\nRemember to handle edge cases and input validation as per the existing `resample` function's implementation. This pseudo-code is a starting point and may need to be adjusted to fit into the existing codebase properly.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/utils)- [Lines 576 - 629](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/utils/__init__.py#L576-L629)\n\n This snippet contains the implementation of resampling with stratification, which is directly related to the issue of adding a stratify option to utils.resample. \n\n\n   - [Lines 483 - 574](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/utils/__init__.py#L483-L574)\n\n This snippet provides the documentation and parameters for the resample function, which is relevant for understanding how to add the stratify option. \n\n\n   - [Lines 115 - 1210](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/utils/__init__.py#L115-L1210)\n\n This snippet shows the parameter validation for the resample function, which will need to be updated to handle the new stratify option."
    }
}