{
    "id": "scikit-learn__scikit-learn-13496",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
    "task_id": 5475147428835062199,
    "title": "Expose warm_start in Isolation forest",
    "body": "It seems to me that `sklearn.ensemble.IsolationForest` supports incremental addition of new trees with the `warm_start` parameter of its parent class, `sklearn.ensemble.BaseBagging`.\r\n\r\nEven though this parameter is not exposed in `__init__()` , it gets inherited from `BaseBagging` and one can use it by changing it to `True` after initialization. To make it work, you have to also increment `n_estimators` on every iteration. \r\n\r\nIt took me a while to notice that it actually works, and I had to inspect the source code of both `IsolationForest` and `BaseBagging`. Also, it looks to me that the behavior is in-line with `sklearn.ensemble.BaseForest` that is behind e.g. `sklearn.ensemble.RandomForestClassifier`.\r\n\r\nTo make it more easier to use, I'd suggest to:\r\n* expose `warm_start` in `IsolationForest.__init__()`, default `False`;\r\n* document it in the same way as it is documented for `RandomForestClassifier`, i.e. say:\r\n```py\r\n    warm_start : bool, optional (default=False)\r\n        When set to ``True``, reuse the solution of the previous call to fit\r\n        and add more estimators to the ensemble, otherwise, just fit a whole\r\n        new forest. See :term:`the Glossary <warm_start>`.\r\n```\r\n* add a test to make sure it works properly;\r\n* possibly also mention in the \"IsolationForest example\" documentation entry;\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.21.git",
    "requested_commit": "",
    "mayil_response": "To implement the feature request described in the ticket, follow these steps:\n\n- Expose the `warm_start` parameter in the `IsolationForest` constructor.\n- Document the `warm_start` parameter in the class docstring.\n- Add a test to ensure that the `warm_start` functionality works as expected.\n- Update the \"IsolationForest example\" documentation to mention the `warm_start` parameter.\n\nHere is a high-level overview of the changes to be made in the codebase:\n\n1. Modify the `IsolationForest` class definition in `_iforest.py` to include the `warm_start` parameter:\n\n```python\nclass IsolationForest(OutlierMixin, BaseBagging):\n    def __init__(self, n_estimators=100, max_samples=\"auto\", contamination=\"auto\",\n                 max_features=1.0, bootstrap=False, n_jobs=None, random_state=None,\n                 verbose=0, warm_start=False):  # Add warm_start parameter here\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(max_features=1,\n                                              splitter='random',\n                                              random_state=random_state),\n            # ... other parameters remain unchanged ...\n            warm_start=warm_start,  # Pass warm_start to the superclass\n        )\n        # ... rest of the constructor remains unchanged ...\n```\n\n2. Update the class docstring to document the `warm_start` parameter:\n\n```python\n\"\"\"\n    Isolation Forest Algorithm.\n    ...\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n    ...\n\"\"\"\n```\n\n3. Add a test for the `warm_start` functionality in `sklearn/ensemble/tests/test_iforest.py`:\n\n```python\ndef test_warm_start():\n    rng = np.random.RandomState(42)\n    X = 0.3 * rng.randn(100, 2)\n    clf = IsolationForest(n_estimators=100, warm_start=True, random_state=rng)\n    clf.fit(X)\n    n_estimators_first_fit = len(clf.estimators_)\n    \n    clf.set_params(n_estimators=110)\n    clf.fit(X)\n    n_estimators_second_fit = len(clf.estimators_)\n    \n    assert n_estimators_second_fit == n_estimators_first_fit + 10\n```\n\n4. Update the \"IsolationForest example\" documentation in `examples/ensemble/plot_isolation_forest.py` to mention the `warm_start` parameter:\n\n```python\n# ...\nclf = IsolationForest(max_samples=100, random_state=0, warm_start=True)\n# ...\n```\n\nMake sure to run the test suite to ensure that the new test passes and that no other parts of the codebase are affected by this change. Additionally, ensure that the documentation builds correctly and that the `warm_start` parameter is properly documented.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [ensemble](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/ensemble)- [Lines 1 - 25](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/ensemble/_iforest.py#L1-L25)\n\n This snippet shows the import of BaseBagging in the IsolationForest implementation file, indicating that IsolationForest inherits from BaseBagging, which is relevant to the issue. \n\n\n   - [Lines 28 - 28](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/ensemble/_iforest.py#L28-L28)\n\n This snippet defines the IsolationForest class and shows that it inherits from BaseBagging, which is directly related to the issue of exposing the warm_start parameter. \n\n\n   - [Lines 29 - 545](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/ensemble/_iforest.py#L29-L545)\n\n This snippet contains the IsolationForest class definition with its parameters, which is where the warm_start parameter should be exposed as per the issue description.\n-  \u2192 [examples](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//examples) \u2192 [ensemble](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//examples/ensemble)- [Lines 1 - 120](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//examples/ensemble/plot_isolation_forest.py#L1-L120)\n\n This snippet is from the IsolationForest example documentation, which should mention the warm_start parameter as suggested in the issue.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0008225,
        "snippet_processor": 0.051160000000000004,
        "issue_star_creation": 0.035809999999999995,
        "issue_star_solver": 0.07194,
        "bouncer": 0.02488
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711740446.639704,
        "relevant_snippets": [
            {
                "code": "if n_more_estimators < 0:\n            raise ValueError(\n                \"n_estimators=%d must be larger or equal to \"\n                \"len(estimators_)=%d when warm_start==True\"\n                % (self.n_estimators, len(self.estimators_))\n            )\n\n        elif n_more_estimators == 0:\n            warn(\n                \"Warm-start fitting without increasing n_estimators does not \"\n                \"fit new trees.\"\n            )\n        else:\n            if self.warm_start and len(self.estimators_) > 0:\n                # We draw from the random state to get the random state we\n                # would have got if we hadn't used a warm_start.\n                random_state.randint(MAX_INT, size=len(self.estimators_))\n\n            trees = [\n                self._make_estimator(append=False, random_state=random_state)\n                for i in range(n_more_estimators)\n            ]\n\n            # Parallel loop: we prefer the threading backend as the Cython code\n            # for fitting the trees is internally releasing the Python GIL\n            # making threading more efficient than multiprocessing in\n            # that case. However, for joblib 0.12+ we respect any\n            # parallel_backend contexts set at a higher level,\n            # since correctness does not rely on using threads.\n            trees = Parallel(\n                n_jobs=self.n_jobs,\n                verbose=self.verbose,\n                prefer=\"threads\",\n            )(\n                delayed(_parallel_build_trees)(\n                    t,\n                    self.bootstrap,\n                    X,\n                    y,\n                    sample_weight,\n                    i,\n                    len(trees),\n                    verbose=self.verbose,\n                    class_weight=self.class_weight,\n                    n_samples_bootstrap=n_samples_bootstrap,\n                )\n                for i, t in enumerate(trees)\n            )\n\n            # Collect newly grown trees\n            self.estimators_.extend(trees)",
                "filename": "sklearn/ensemble/_forest.py",
                "start_index": 14294,
                "end_index": 16314,
                "start_line": 427,
                "end_line": 477,
                "max_line": 2908,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "if hasattr(self, \"oob_score_\") and self.warm_start:\n            del self.oob_score_\n\n        if not self.warm_start or not hasattr(self, \"estimators_\"):\n            # Free allocated memory, if any\n            self.estimators_ = []\n            self.estimators_features_ = []\n\n        n_more_estimators = self.n_estimators - len(self.estimators_)\n\n        if n_more_estimators < 0:\n            raise ValueError(\n                \"n_estimators=%d must be larger or equal to \"\n                \"len(estimators_)=%d when warm_start==True\"\n                % (self.n_estimators, len(self.estimators_))\n            )\n\n        elif n_more_estimators == 0:\n            warn(\n                \"Warm-start fitting without increasing n_estimators does not \"\n                \"fit new trees.\"\n            )\n            return self\n\n        # Parallel loop\n        n_jobs, n_estimators, starts = _partition_estimators(\n            n_more_estimators, self.n_jobs\n        )\n        total_n_estimators = sum(n_estimators)\n\n        # Advance random state to state after training\n        # the first n_estimators\n        if self.warm_start and len(self.estimators_) > 0:\n            random_state.randint(MAX_INT, size=len(self.estimators_))\n\n        seeds = random_state.randint(MAX_INT, size=n_more_estimators)\n        self._seeds = seeds\n\n        all_results = Parallel(\n            n_jobs=n_jobs, verbose=self.verbose, **self._parallel_args()\n        )(\n            delayed(_parallel_build_estimators)(\n                n_estimators[i],\n                self,\n                X,\n                y,\n                sample_weight,\n                seeds[starts[i] : starts[i + 1]],\n                total_n_estimators,\n                verbose=self.verbose,\n                check_input=check_input,\n            )\n            for i in range(n_jobs)\n        )\n\n        # Reduce\n        self.estimators_ += list(\n            itertools.chain.from_iterable(t[0] for t in all_results)\n        )\n        self.estimators_features_ += list(\n            itertools.chain.from_iterable(t[1] for t in all_results)\n        )\n\n        if self.oob_score:\n            self._set_oob_score(X, y)\n\n        return self",
                "filename": "sklearn/ensemble/_bagging.py",
                "start_index": 14235,
                "end_index": 16404,
                "start_line": 435,
                "end_line": 828,
                "max_line": 1278,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "# Authors: Nicolas Goix <nicolas.goix@telecom-paristech.fr>\n#          Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n# License: BSD 3 clause\n\nimport numbers\nfrom numbers import Integral, Real\nfrom warnings import warn\n\nimport numpy as np\nfrom scipy.sparse import issparse\n\nfrom ..base import OutlierMixin, _fit_context\nfrom ..tree import ExtraTreeRegressor\nfrom ..tree._tree import DTYPE as tree_dtype\nfrom ..utils import (\n    check_array,\n    check_random_state,\n    gen_batches,\n    get_chunk_n_rows,\n)\nfrom ..utils._param_validation import Interval, RealNotInt, StrOptions\nfrom ..utils.validation import _num_samples, check_is_fitted\nfrom ._bagging import BaseBagging\n\n__all__ = [\"IsolationForest\"]",
                "filename": "sklearn/ensemble/_iforest.py",
                "start_index": 0,
                "end_index": 717,
                "start_line": 1,
                "end_line": 25,
                "max_line": 564,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n=======================\nIsolationForest example\n=======================\n\nAn example using :class:`~sklearn.ensemble.IsolationForest` for anomaly\ndetection.\n\nThe :ref:`isolation_forest` is an ensemble of \"Isolation Trees\" that \"isolate\"\nobservations by recursive random partitioning, which can be represented by a\ntree structure. The number of splittings required to isolate a sample is lower\nfor outliers and higher for inliers.\n\nIn the present example we demo two ways to visualize the decision boundary of an\nIsolation Forest trained on a toy dataset.\n\n\"\"\"\n\n# %%\n# Data generation\n# ---------------\n#\n# We generate two clusters (each one containing `n_samples`) by randomly\n# sampling the standard normal distribution as returned by\n# :func:`numpy.random.randn`. One of them is spherical and the other one is\n# slightly deformed.\n#\n# For consistency with the :class:`~sklearn.ensemble.IsolationForest` notation,\n# the inliers (i.e. the gaussian clusters) are assigned a ground truth label `1`\n# whereas the outliers (created with :func:`numpy.random.uniform`) are assigned\n# the label `-1`.\n\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nn_samples, n_outliers = 120, 40\nrng = np.random.RandomState(0)\ncovariance = np.array([[0.5, -0.1], [0.7, 0.4]])\ncluster_1 = 0.4 * rng.randn(n_samples, 2) @ covariance + np.array([2, 2])  # general\ncluster_2 = 0.3 * rng.randn(n_samples, 2) + np.array([-2, -2])  # spherical\noutliers = rng.uniform(low=-4, high=4, size=(n_outliers, 2))\n\nX = np.concatenate([cluster_1, cluster_2, outliers])\ny = np.concatenate(\n    [np.ones((2 * n_samples), dtype=int), -np.ones((n_outliers), dtype=int)]\n)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n# %%\n# We can visualize the resulting clusters:\n\nimport matplotlib.pyplot as plt\n\nscatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\nhandles, labels = scatter.legend_elements()\nplt.axis(\"square\")\nplt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\nplt.title(\"Gaussian inliers with \\nuniformly distributed outliers\")\nplt.show()\n\n# %%\n# Training of the model\n# ---------------------\n\nfrom sklearn.ensemble import IsolationForest\n\nclf = IsolationForest(max_samples=100, random_state=0)\nclf.fit(X_train)\n\n# %%\n# Plot discrete decision boundary\n# -------------------------------\n#\n# We use the class :class:`~sklearn.inspection.DecisionBoundaryDisplay` to\n# visualize a discrete decision boundary. The background color represents\n# whether a sample in that given area is predicted to be an outlier\n# or not. The scatter plot displays the true labels.\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\ndisp = DecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    response_method=\"predict\",\n    alpha=0.5,\n)\ndisp.ax_.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\ndisp.ax_.set_title(\"Binary decision boundary \\nof IsolationForest\")\nplt.axis(\"square\")",
                "filename": "examples/ensemble/plot_isolation_forest.py",
                "start_index": 0,
                "end_index": 2985,
                "start_line": 1,
                "end_line": 120,
                "max_line": 123,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "# warm start: this is not the first time fit was called\n        else:\n            # Check that the maximum number of iterations is not smaller\n            # than the number of iterations from the previous fit\n            if self.max_iter < self.n_iter_:\n                raise ValueError(\n                    \"max_iter=%d must be larger than or equal to \"\n                    \"n_iter_=%d when warm_start==True\" % (self.max_iter, self.n_iter_)\n                )\n\n            # Convert array attributes to lists\n            self.train_score_ = self.train_score_.tolist()\n            self.validation_score_ = self.validation_score_.tolist()\n\n            # Compute raw predictions\n            raw_predictions = self._raw_predict(X_binned_train, n_threads=n_threads)\n            if self.do_early_stopping_ and self._use_validation_data:\n                raw_predictions_val = self._raw_predict(\n                    X_binned_val, n_threads=n_threads\n                )\n            else:\n                raw_predictions_val = None\n\n            if self.do_early_stopping_ and self.scoring != \"loss\":\n                # Compute the subsample set\n                (\n                    X_binned_small_train,\n                    y_small_train,\n                    sample_weight_small_train,\n                ) = self._get_small_trainset(\n                    X_binned_train, y_train, sample_weight_train, self._random_seed\n                )\n\n            # Get the predictors from the previous fit\n            predictors = self._predictors\n\n            begin_at_stage = self.n_iter_",
                "filename": "sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py",
                "start_index": 23720,
                "end_index": 25283,
                "start_line": 593,
                "end_line": 629,
                "max_line": 2009,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "class IsolationForest(OutlierMixin, BaseBagging):",
                "filename": "sklearn/ensemble/_iforest.py",
                "start_index": 720,
                "end_index": 769,
                "start_line": 28,
                "end_line": 28,
                "max_line": 564,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n    An extra-trees classifier.\n\n    This class implements a meta estimator that fits a number of\n    randomized decision trees (a.k.a. extra-trees) on various sub-samples\n    of the dataset and uses averaging to improve the predictive accuracy\n    and control over-fitting.\n\n    Read more in the :ref:`User Guide <forest>`.\n\n    Parameters\n    ----------\n    n_estimators : int, default=100\n        The number of trees in the forest.\n\n        .. versionchanged:: 0.22\n           The default value of ``n_estimators`` changed from 10 to 100\n           in 0.22.\n\n    criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n        The function to measure the quality of a split. Supported criteria are\n        \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n        Shannon information gain, see :ref:`tree_mathematical_formulation`.\n        Note: This parameter is tree-specific.\n\n    max_depth : int, default=None\n        The maximum depth of the tree. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, then consider `min_samples_split` as the minimum number.\n        - If float, then `min_samples_split` is a fraction and\n          `ceil(min_samples_split * n_samples)` are the minimum\n          number of samples for each split.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, then consider `min_samples_leaf` as the minimum number.\n        - If float, then `min_samples_leaf` is a fraction and\n          `ceil(min_samples_leaf * n_samples)` are the minimum\n          number of samples for each node.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n\n    max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n        The number of features to consider when looking for the best split:\n\n        - If int, then consider `max_features` features at each split.\n        - If float, then `max_features` is a fraction and\n          `max(1, int(max_features * n_features_in_))` features are considered at each\n          split.\n        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n        - If \"log2\", then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        .. versionchanged:: 1.1\n            The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        If None then unlimited number of leaf nodes.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    bootstrap : bool, default=False\n        Whether bootstrap samples are used when building trees. If False, the\n        whole dataset is used to build each tree.\n\n    oob_score : bool or callable, default=False\n        Whether to use out-of-bag samples to estimate the generalization score.\n        By default, :func:`~sklearn.metrics.accuracy_score` is used.\n        Provide a callable with signature `metric(y_true, y_pred)` to use a\n        custom metric. Only available if `bootstrap=True`.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors. See :term:`Glossary\n        <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls 3 sources of randomness:\n\n        - the bootstrapping of the samples used when building trees\n          (if ``bootstrap=True``)\n        - the sampling of the features to consider when looking for the best\n          split at each node (if ``max_features < n_features``)\n        - the draw of the splits for each of the `max_features`\n\n        See :term:`Glossary <random_state>` for details.\n\n    verbose : int, default=0\n        Controls the verbosity when fitting and predicting.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`Glossary <warm_start>` and\n        :ref:`gradient_boosting_warm_start` for details.\n\n    class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts, \\",
                "filename": "sklearn/ensemble/_forest.py",
                "start_index": 68621,
                "end_index": 74820,
                "start_line": 1,
                "end_line": 1974,
                "max_line": 2908,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"Gradient Boosting for classification.\n\n    This algorithm builds an additive model in a forward stage-wise fashion; it\n    allows for the optimization of arbitrary differentiable loss functions. In\n    each stage ``n_classes_`` regression trees are fit on the negative gradient\n    of the loss function, e.g. binary or multiclass log loss. Binary\n    classification is a special case where only a single regression tree is\n    induced.\n\n    :class:`sklearn.ensemble.HistGradientBoostingClassifier` is a much faster\n    variant of this algorithm for intermediate datasets (`n_samples >= 10_000`).\n\n    Read more in the :ref:`User Guide <gradient_boosting>`.\n\n    Parameters\n    ----------\n    loss : {'log_loss', 'exponential'}, default='log_loss'\n        The loss function to be optimized. 'log_loss' refers to binomial and\n        multinomial deviance, the same as used in logistic regression.\n        It is a good choice for classification with probabilistic outputs.\n        For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.\n\n    learning_rate : float, default=0.1\n        Learning rate shrinks the contribution of each tree by `learning_rate`.\n        There is a trade-off between learning_rate and n_estimators.\n        Values must be in the range `[0.0, inf)`.\n\n    n_estimators : int, default=100\n        The number of boosting stages to perform. Gradient boosting\n        is fairly robust to over-fitting so a large number usually\n        results in better performance.\n        Values must be in the range `[1, inf)`.\n\n    subsample : float, default=1.0\n        The fraction of samples to be used for fitting the individual base\n        learners. If smaller than 1.0 this results in Stochastic Gradient\n        Boosting. `subsample` interacts with the parameter `n_estimators`.\n        Choosing `subsample < 1.0` leads to a reduction of variance\n        and an increase in bias.\n        Values must be in the range `(0.0, 1.0]`.\n\n    criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'\n        The function to measure the quality of a split. Supported criteria are\n        'friedman_mse' for the mean squared error with improvement score by\n        Friedman, 'squared_error' for mean squared error. The default value of\n        'friedman_mse' is generally the best as it can provide a better\n        approximation in some cases.\n\n        .. versionadded:: 0.18\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, values must be in the range `[2, inf)`.\n        - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`\n          will be `ceil(min_samples_split * n_samples)`.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, values must be in the range `[1, inf)`.\n        - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`\n          will be `ceil(min_samples_leaf * n_samples)`.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n        Values must be in the range `[0.0, 0.5]`.\n\n    max_depth : int or None, default=3\n        Maximum depth of the individual regression estimators. The maximum\n        depth limits the number of nodes in the tree. Tune this parameter\n        for best performance; the best value depends on the interaction\n        of the input variables. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n        If int, values must be in the range `[1, inf)`.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n        Values must be in the range `[0.0, inf)`.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    init : estimator or 'zero', default=None\n        An estimator object that is used to compute the initial predictions.\n        ``init`` has to provide :term:`fit` and :term:`predict_proba`. If\n        'zero', the initial raw predictions are set to zero. By default, a\n        ``DummyEstimator`` predicting the classes priors is used.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given to each Tree estimator at each\n        boosting iteration.\n        In addition, it controls the random permutation of the features at\n        each split (see Notes for more details).\n        It also controls the random splitting of the training data to obtain a\n        validation set if `n_iter_no_change` is not None.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    max_features : {'sqrt', 'log2'}, int or float, default=None\n        The number of features to consider when looking for the best split:\n\n        - If int, values must be in the range `[1, inf)`.\n        - If float, values must be in the range `(0.0, 1.0]` and the features\n          considered at each split will be `max(1, int(max_features * n_features_in_))`.\n        - If 'sqrt', then `max_features=sqrt(n_features)`.\n        - If 'log2', then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Choosing `max_features < n_features` leads to a reduction of variance\n        and an increase in bias.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    verbose : int, default=0\n        Enable verbose output. If 1 then it prints progress and performance\n        once in a while (the more trees the lower the frequency). If greater\n        than 1 then it prints progress and performance for every tree.\n        Values must be in the range `[0, inf)`.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        Values must be in the range `[2, inf)`.\n        If `None`, then unlimited number of leaf nodes.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just erase the\n        previous solution. See :term:`the Glossary <warm_start>`.\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Values must be in the range `(0.0, 1.0)`.\n        Only used if ``n_iter_no_change`` is set to an integer.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=None\n        ``n_iter_no_change`` is used to decide if early stopping will be used\n        to terminate training when validation score is not improving. By\n        default it is set to None to disable early stopping. If set to a\n        number, it will set aside ``validation_fraction`` size of the training\n        data as validation and terminate training when validation score is not\n        improving in all of the previous ``n_iter_no_change`` numbers of\n        iterations. The split is stratified.\n        Values must be in the range `[1, inf)`.\n\n        .. versionadded:: 0.20\n\n    tol : float, default=1e-4\n        Tolerance for the early stopping. When the loss is not improving\n        by at least tol for ``n_iter_no_change`` iterations (if set to a\n        number), the training stops.\n        Values must be in the range `[0.0, inf)`.\n\n        .. versionadded:: 0.20\n\n    ccp_alpha : non-negative float, default=0.0\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n        Values must be in the range `[0.0, inf)`.\n        See :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    n_estimators_ : int\n        The number of estimators as selected by early stopping (if\n        ``n_iter_no_change`` is specified). Otherwise it is set to\n        ``n_estimators``.\n\n        .. versionadded:: 0.20\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    oob_improvement_ : ndarray of shape (n_estimators,)\n        The improvement in loss on the out-of-bag samples\n        relative to the previous iteration.\n        ``oob_improvement_[0]`` is the improvement in\n        loss of the first stage over the ``init`` estimator.\n        Only available if ``subsample < 1.0``.\n\n    oob_scores_ : ndarray of shape (n_estimators,)\n        The full history of the loss values on the out-of-bag\n        samples. Only available if `subsample < 1.0`.\n\n        .. versionadded:: 1.3\n\n    oob_score_ : float\n        The last value of the loss on the out-of-bag samples. It is\n        the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.\n\n        .. versionadded:: 1.3\n\n    train_score_ : ndarray of shape (n_estimators,)\n        The i-th score ``train_score_[i]`` is the loss of the\n        model at iteration ``i`` on the in-bag sample.\n        If ``subsample == 1`` this is the loss on the training data.\n\n    init_ : estimator\n        The estimator that provides the initial predictions.\n        Set via the ``init`` argument or ``loss.init_estimator``.\n\n    estimators_ : ndarray of DecisionTreeRegressor of \\",
                "filename": "sklearn/ensemble/_gb.py",
                "start_index": 31581,
                "end_index": 42812,
                "start_line": 835,
                "end_line": 1077,
                "max_line": 1828,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\n\nX, y = load_iris(return_X_y=True)\nestimators = [\n    (\"rf\", RandomForestClassifier(n_estimators=10, random_state=42)),\n    (\"svr\", make_pipeline(StandardScaler(), LinearSVC(random_state=42))),\n]\nclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\nclf.fit(X_train, y_train).score(X_test, y_test)\n\n# %%\n# Permutation-based feature importance\n# ------------------------------------\n#\n# The :func:`inspection.permutation_importance` can be used to get an\n# estimate of the importance of each feature, for any fitted estimator:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\n\nX, y = make_classification(random_state=0, n_features=5, n_informative=3)\nfeature_names = np.array([f\"x_{i}\" for i in range(X.shape[1])])\n\nrf = RandomForestClassifier(random_state=0).fit(X, y)\nresult = permutation_importance(rf, X, y, n_repeats=10, random_state=0, n_jobs=2)\n\nfig, ax = plt.subplots()\nsorted_idx = result.importances_mean.argsort()\nax.boxplot(\n    result.importances[sorted_idx].T, vert=False, labels=feature_names[sorted_idx]\n)\nax.set_title(\"Permutation Importance of each feature\")\nax.set_ylabel(\"Features\")\nfig.tight_layout()\nplt.show()\n\n# %%\n# Native support for missing values for gradient boosting\n# -------------------------------------------------------\n#\n# The :class:`ensemble.HistGradientBoostingClassifier`\n# and :class:`ensemble.HistGradientBoostingRegressor` now have native\n# support for missing values (NaNs). This means that there is no need for\n# imputing data when training or predicting.\n\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nX = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\ny = [0, 0, 1, 1]\n\ngbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\nprint(gbdt.predict(X))\n\n# %%\n# Precomputed sparse nearest neighbors graph\n# ------------------------------------------\n# Most estimators based on nearest neighbors graphs now accept precomputed\n# sparse graphs as input, to reuse the same graph for multiple estimator fits.\n# To use this feature in a pipeline, one can use the `memory` parameter, along\n# with one of the two new transformers,\n# :class:`neighbors.KNeighborsTransformer` and\n# :class:`neighbors.RadiusNeighborsTransformer`. The precomputation\n# can also be performed by custom estimators to use alternative\n# implementations, such as approximate nearest neighbors methods.\n# See more details in the :ref:`User Guide <neighbors_transformer>`.\n\nfrom tempfile import TemporaryDirectory\n\nfrom sklearn.manifold import Isomap\nfrom sklearn.neighbors import KNeighborsTransformer",
                "filename": "examples/release_highlights/plot_release_highlights_0_22_0.py",
                "start_index": 2957,
                "end_index": 5944,
                "start_line": 44,
                "end_line": 161,
                "max_line": 282,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n    Isolation Forest Algorithm.\n\n    Return the anomaly score of each sample using the IsolationForest algorithm\n\n    The IsolationForest 'isolates' observations by randomly selecting a feature\n    and then randomly selecting a split value between the maximum and minimum\n    values of the selected feature.\n\n    Since recursive partitioning can be represented by a tree structure, the\n    number of splittings required to isolate a sample is equivalent to the path\n    length from the root node to the terminating node.\n\n    This path length, averaged over a forest of such random trees, is a\n    measure of normality and our decision function.\n\n    Random partitioning produces noticeably shorter paths for anomalies.\n    Hence, when a forest of random trees collectively produce shorter path\n    lengths for particular samples, they are highly likely to be anomalies.\n\n    Read more in the :ref:`User Guide <isolation_forest>`.\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    n_estimators : int, default=100\n        The number of base estimators in the ensemble.\n\n    max_samples : \"auto\", int or float, default=\"auto\"\n        The number of samples to draw from X to train each base estimator.\n            - If int, then draw `max_samples` samples.\n            - If float, then draw `max_samples * X.shape[0]` samples.\n            - If \"auto\", then `max_samples=min(256, n_samples)`.\n\n        If max_samples is larger than the number of samples provided,\n        all samples will be used for all trees (no sampling).\n\n    contamination : 'auto' or float, default='auto'\n        The amount of contamination of the data set, i.e. the proportion\n        of outliers in the data set. Used when fitting to define the threshold\n        on the scores of the samples.\n\n            - If 'auto', the threshold is determined as in the\n              original paper.\n            - If float, the contamination should be in the range (0, 0.5].\n\n        .. versionchanged:: 0.22\n           The default value of ``contamination`` changed from 0.1\n           to ``'auto'``.\n\n    max_features : int or float, default=1.0\n        The number of features to draw from X to train each base estimator.\n\n            - If int, then draw `max_features` features.\n            - If float, then draw `max(1, int(max_features * n_features_in_))` features.\n\n        Note: using a float number less than 1.0 or integer less than number of\n        features will enable feature subsampling and leads to a longer runtime.\n\n    bootstrap : bool, default=False\n        If True, individual trees are fit on random subsets of the training\n        data sampled with replacement. If False, sampling without replacement\n        is performed.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel for both :meth:`fit` and\n        :meth:`predict`. ``None`` means 1 unless in a\n        :obj:`joblib.parallel_backend` context. ``-1`` means using all\n        processors. See :term:`Glossary <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the pseudo-randomness of the selection of the feature\n        and split values for each branching step and each tree in the forest.\n\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    verbose : int, default=0\n        Controls the verbosity of the tree building process.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n        .. versionadded:: 0.21\n\n    Attributes\n    ----------\n    estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance\n        The child estimator template used to create the collection of\n        fitted sub-estimators.\n\n        .. versionadded:: 1.2\n           `base_estimator_` was renamed to `estimator_`.\n\n    base_estimator_ : ExtraTreeRegressor instance\n        The child estimator template used to create the collection of\n        fitted sub-estimators.\n\n        .. deprecated:: 1.2\n            `base_estimator_` is deprecated and will be removed in 1.4.\n            Use `estimator_` instead.\n\n    estimators_ : list of ExtraTreeRegressor instances\n        The collection of fitted sub-estimators.\n\n    estimators_features_ : list of ndarray\n        The subset of drawn features for each base estimator.\n\n    estimators_samples_ : list of ndarray\n        The subset of drawn samples (i.e., the in-bag samples) for each base\n        estimator.\n\n    max_samples_ : int\n        The actual number of samples.\n\n    offset_ : float\n        Offset used to define the decision function from the raw scores. We\n        have the relation: ``decision_function = score_samples - offset_``.\n        ``offset_`` is defined as follows. When the contamination parameter is\n        set to \"auto\", the offset is equal to -0.5 as the scores of inliers are\n        close to 0 and the scores of outliers are close to -1. When a\n        contamination parameter different than \"auto\" is provided, the offset\n        is defined in such a way we obtain the expected number of outliers\n        (samples with decision function < 0) in training.\n\n        .. versionadded:: 0.20\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    sklearn.covariance.EllipticEnvelope : An object for detecting outliers in a\n        Gaussian distributed dataset.\n    sklearn.svm.OneClassSVM : Unsupervised Outlier Detection.\n        Estimate the support of a high-dimensional distribution.\n        The implementation is based on libsvm.\n    sklearn.neighbors.LocalOutlierFactor : Unsupervised Outlier Detection\n        using Local Outlier Factor (LOF).\n\n    Notes\n    -----\n    The implementation is based on an ensemble of ExtraTreeRegressor. The\n    maximum depth of each tree is set to ``ceil(log_2(n))`` where\n    :math:`n` is the number of samples used to build the tree\n    (see (Liu et al., 2008) for more details).\n\n    References\n    ----------\n    .. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n           Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n    .. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n           anomaly detection.\" ACM Transactions on Knowledge Discovery from\n           Data (TKDD) 6.1 (2012): 3.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import IsolationForest\n    >>> X = [[-1.1], [0.3], [0.5], [100]]\n    >>> clf = IsolationForest(random_state=0).fit(X)\n    >>> clf.predict([[0.1], [0], [90]])\n    array([ 1,  1, -1])\n    \"\"\"",
                "filename": "sklearn/ensemble/_iforest.py",
                "start_index": 774,
                "end_index": 7799,
                "start_line": 29,
                "end_line": 545,
                "max_line": 564,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            }
        ],
        "classification": "feature request",
        "snippet_processor_task": {
            "sklearn/ensemble/_iforest.py": [
                {
                    "chunk": {
                        "code": "# Authors: Nicolas Goix <nicolas.goix@telecom-paristech.fr>\n#          Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n# License: BSD 3 clause\n\nimport numbers\nfrom numbers import Integral, Real\nfrom warnings import warn\n\nimport numpy as np\nfrom scipy.sparse import issparse\n\nfrom ..base import OutlierMixin, _fit_context\nfrom ..tree import ExtraTreeRegressor\nfrom ..tree._tree import DTYPE as tree_dtype\nfrom ..utils import (\n    check_array,\n    check_random_state,\n    gen_batches,\n    get_chunk_n_rows,\n)\nfrom ..utils._param_validation import Interval, RealNotInt, StrOptions\nfrom ..utils.validation import _num_samples, check_is_fitted\nfrom ._bagging import BaseBagging\n\n__all__ = [\"IsolationForest\"]",
                        "filename": "sklearn/ensemble/_iforest.py",
                        "start_index": 0,
                        "end_index": 717,
                        "start_line": 1,
                        "end_line": 25,
                        "max_line": 564,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the import of BaseBagging in the IsolationForest implementation file, indicating that IsolationForest inherits from BaseBagging, which is relevant to the issue."
                },
                {
                    "chunk": {
                        "code": "class IsolationForest(OutlierMixin, BaseBagging):",
                        "filename": "sklearn/ensemble/_iforest.py",
                        "start_index": 720,
                        "end_index": 769,
                        "start_line": 28,
                        "end_line": 28,
                        "max_line": 564,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet defines the IsolationForest class and shows that it inherits from BaseBagging, which is directly related to the issue of exposing the warm_start parameter."
                },
                {
                    "chunk": {
                        "code": "\"\"\"\n    Isolation Forest Algorithm.\n\n    Return the anomaly score of each sample using the IsolationForest algorithm\n\n    The IsolationForest 'isolates' observations by randomly selecting a feature\n    and then randomly selecting a split value between the maximum and minimum\n    values of the selected feature.\n\n    Since recursive partitioning can be represented by a tree structure, the\n    number of splittings required to isolate a sample is equivalent to the path\n    length from the root node to the terminating node.\n\n    This path length, averaged over a forest of such random trees, is a\n    measure of normality and our decision function.\n\n    Random partitioning produces noticeably shorter paths for anomalies.\n    Hence, when a forest of random trees collectively produce shorter path\n    lengths for particular samples, they are highly likely to be anomalies.\n\n    Read more in the :ref:`User Guide <isolation_forest>`.\n\n    .. versionadded:: 0.18\n\n    Parameters\n    ----------\n    n_estimators : int, default=100\n        The number of base estimators in the ensemble.\n\n    max_samples : \"auto\", int or float, default=\"auto\"\n        The number of samples to draw from X to train each base estimator.\n            - If int, then draw `max_samples` samples.\n            - If float, then draw `max_samples * X.shape[0]` samples.\n            - If \"auto\", then `max_samples=min(256, n_samples)`.\n\n        If max_samples is larger than the number of samples provided,\n        all samples will be used for all trees (no sampling).\n\n    contamination : 'auto' or float, default='auto'\n        The amount of contamination of the data set, i.e. the proportion\n        of outliers in the data set. Used when fitting to define the threshold\n        on the scores of the samples.\n\n            - If 'auto', the threshold is determined as in the\n              original paper.\n            - If float, the contamination should be in the range (0, 0.5].\n\n        .. versionchanged:: 0.22\n           The default value of ``contamination`` changed from 0.1\n           to ``'auto'``.\n\n    max_features : int or float, default=1.0\n        The number of features to draw from X to train each base estimator.\n\n            - If int, then draw `max_features` features.\n            - If float, then draw `max(1, int(max_features * n_features_in_))` features.\n\n        Note: using a float number less than 1.0 or integer less than number of\n        features will enable feature subsampling and leads to a longer runtime.\n\n    bootstrap : bool, default=False\n        If True, individual trees are fit on random subsets of the training\n        data sampled with replacement. If False, sampling without replacement\n        is performed.\n\n    n_jobs : int, default=None\n        The number of jobs to run in parallel for both :meth:`fit` and\n        :meth:`predict`. ``None`` means 1 unless in a\n        :obj:`joblib.parallel_backend` context. ``-1`` means using all\n        processors. See :term:`Glossary <n_jobs>` for more details.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the pseudo-randomness of the selection of the feature\n        and split values for each branching step and each tree in the forest.\n\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    verbose : int, default=0\n        Controls the verbosity of the tree building process.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n\n        .. versionadded:: 0.21\n\n    Attributes\n    ----------\n    estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance\n        The child estimator template used to create the collection of\n        fitted sub-estimators.\n\n        .. versionadded:: 1.2\n           `base_estimator_` was renamed to `estimator_`.\n\n    base_estimator_ : ExtraTreeRegressor instance\n        The child estimator template used to create the collection of\n        fitted sub-estimators.\n\n        .. deprecated:: 1.2\n            `base_estimator_` is deprecated and will be removed in 1.4.\n            Use `estimator_` instead.\n\n    estimators_ : list of ExtraTreeRegressor instances\n        The collection of fitted sub-estimators.\n\n    estimators_features_ : list of ndarray\n        The subset of drawn features for each base estimator.\n\n    estimators_samples_ : list of ndarray\n        The subset of drawn samples (i.e., the in-bag samples) for each base\n        estimator.\n\n    max_samples_ : int\n        The actual number of samples.\n\n    offset_ : float\n        Offset used to define the decision function from the raw scores. We\n        have the relation: ``decision_function = score_samples - offset_``.\n        ``offset_`` is defined as follows. When the contamination parameter is\n        set to \"auto\", the offset is equal to -0.5 as the scores of inliers are\n        close to 0 and the scores of outliers are close to -1. When a\n        contamination parameter different than \"auto\" is provided, the offset\n        is defined in such a way we obtain the expected number of outliers\n        (samples with decision function < 0) in training.\n\n        .. versionadded:: 0.20\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    sklearn.covariance.EllipticEnvelope : An object for detecting outliers in a\n        Gaussian distributed dataset.\n    sklearn.svm.OneClassSVM : Unsupervised Outlier Detection.\n        Estimate the support of a high-dimensional distribution.\n        The implementation is based on libsvm.\n    sklearn.neighbors.LocalOutlierFactor : Unsupervised Outlier Detection\n        using Local Outlier Factor (LOF).\n\n    Notes\n    -----\n    The implementation is based on an ensemble of ExtraTreeRegressor. The\n    maximum depth of each tree is set to ``ceil(log_2(n))`` where\n    :math:`n` is the number of samples used to build the tree\n    (see (Liu et al., 2008) for more details).\n\n    References\n    ----------\n    .. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n           Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n    .. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n           anomaly detection.\" ACM Transactions on Knowledge Discovery from\n           Data (TKDD) 6.1 (2012): 3.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import IsolationForest\n    >>> X = [[-1.1], [0.3], [0.5], [100]]\n    >>> clf = IsolationForest(random_state=0).fit(X)\n    >>> clf.predict([[0.1], [0], [90]])\n    array([ 1,  1, -1])\n    \"\"\"",
                        "filename": "sklearn/ensemble/_iforest.py",
                        "start_index": 774,
                        "end_index": 7799,
                        "start_line": 29,
                        "end_line": 545,
                        "max_line": 564,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the IsolationForest class definition with its parameters, which is where the warm_start parameter should be exposed as per the issue description."
                }
            ],
            "examples/ensemble/plot_isolation_forest.py": [
                {
                    "chunk": {
                        "code": "\"\"\"\n=======================\nIsolationForest example\n=======================\n\nAn example using :class:`~sklearn.ensemble.IsolationForest` for anomaly\ndetection.\n\nThe :ref:`isolation_forest` is an ensemble of \"Isolation Trees\" that \"isolate\"\nobservations by recursive random partitioning, which can be represented by a\ntree structure. The number of splittings required to isolate a sample is lower\nfor outliers and higher for inliers.\n\nIn the present example we demo two ways to visualize the decision boundary of an\nIsolation Forest trained on a toy dataset.\n\n\"\"\"\n\n# %%\n# Data generation\n# ---------------\n#\n# We generate two clusters (each one containing `n_samples`) by randomly\n# sampling the standard normal distribution as returned by\n# :func:`numpy.random.randn`. One of them is spherical and the other one is\n# slightly deformed.\n#\n# For consistency with the :class:`~sklearn.ensemble.IsolationForest` notation,\n# the inliers (i.e. the gaussian clusters) are assigned a ground truth label `1`\n# whereas the outliers (created with :func:`numpy.random.uniform`) are assigned\n# the label `-1`.\n\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nn_samples, n_outliers = 120, 40\nrng = np.random.RandomState(0)\ncovariance = np.array([[0.5, -0.1], [0.7, 0.4]])\ncluster_1 = 0.4 * rng.randn(n_samples, 2) @ covariance + np.array([2, 2])  # general\ncluster_2 = 0.3 * rng.randn(n_samples, 2) + np.array([-2, -2])  # spherical\noutliers = rng.uniform(low=-4, high=4, size=(n_outliers, 2))\n\nX = np.concatenate([cluster_1, cluster_2, outliers])\ny = np.concatenate(\n    [np.ones((2 * n_samples), dtype=int), -np.ones((n_outliers), dtype=int)]\n)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n# %%\n# We can visualize the resulting clusters:\n\nimport matplotlib.pyplot as plt\n\nscatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\nhandles, labels = scatter.legend_elements()\nplt.axis(\"square\")\nplt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\nplt.title(\"Gaussian inliers with \\nuniformly distributed outliers\")\nplt.show()\n\n# %%\n# Training of the model\n# ---------------------\n\nfrom sklearn.ensemble import IsolationForest\n\nclf = IsolationForest(max_samples=100, random_state=0)\nclf.fit(X_train)\n\n# %%\n# Plot discrete decision boundary\n# -------------------------------\n#\n# We use the class :class:`~sklearn.inspection.DecisionBoundaryDisplay` to\n# visualize a discrete decision boundary. The background color represents\n# whether a sample in that given area is predicted to be an outlier\n# or not. The scatter plot displays the true labels.\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\ndisp = DecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    response_method=\"predict\",\n    alpha=0.5,\n)\ndisp.ax_.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\ndisp.ax_.set_title(\"Binary decision boundary \\nof IsolationForest\")\nplt.axis(\"square\")",
                        "filename": "examples/ensemble/plot_isolation_forest.py",
                        "start_index": 0,
                        "end_index": 2985,
                        "start_line": 1,
                        "end_line": 120,
                        "max_line": 123,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet is from the IsolationForest example documentation, which should mention the warm_start parameter as suggested in the issue."
                }
            ]
        },
        "solution_task": "To implement the feature request described in the ticket, follow these steps:\n\n- Expose the `warm_start` parameter in the `IsolationForest` constructor.\n- Document the `warm_start` parameter in the class docstring.\n- Add a test to ensure that the `warm_start` functionality works as expected.\n- Update the \"IsolationForest example\" documentation to mention the `warm_start` parameter.\n\nHere is a high-level overview of the changes to be made in the codebase:\n\n1. Modify the `IsolationForest` class definition in `_iforest.py` to include the `warm_start` parameter:\n\n```python\nclass IsolationForest(OutlierMixin, BaseBagging):\n    def __init__(self, n_estimators=100, max_samples=\"auto\", contamination=\"auto\",\n                 max_features=1.0, bootstrap=False, n_jobs=None, random_state=None,\n                 verbose=0, warm_start=False):  # Add warm_start parameter here\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(max_features=1,\n                                              splitter='random',\n                                              random_state=random_state),\n            # ... other parameters remain unchanged ...\n            warm_start=warm_start,  # Pass warm_start to the superclass\n        )\n        # ... rest of the constructor remains unchanged ...\n```\n\n2. Update the class docstring to document the `warm_start` parameter:\n\n```python\n\"\"\"\n    Isolation Forest Algorithm.\n    ...\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n    ...\n\"\"\"\n```\n\n3. Add a test for the `warm_start` functionality in `sklearn/ensemble/tests/test_iforest.py`:\n\n```python\ndef test_warm_start():\n    rng = np.random.RandomState(42)\n    X = 0.3 * rng.randn(100, 2)\n    clf = IsolationForest(n_estimators=100, warm_start=True, random_state=rng)\n    clf.fit(X)\n    n_estimators_first_fit = len(clf.estimators_)\n    \n    clf.set_params(n_estimators=110)\n    clf.fit(X)\n    n_estimators_second_fit = len(clf.estimators_)\n    \n    assert n_estimators_second_fit == n_estimators_first_fit + 10\n```\n\n4. Update the \"IsolationForest example\" documentation in `examples/ensemble/plot_isolation_forest.py` to mention the `warm_start` parameter:\n\n```python\n# ...\nclf = IsolationForest(max_samples=100, random_state=0, warm_start=True)\n# ...\n```\n\nMake sure to run the test suite to ensure that the new test passes and that no other parts of the codebase are affected by this change. Additionally, ensure that the documentation builds correctly and that the `warm_start` parameter is properly documented.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant suggestions that can directly lead to the successful implementation or improvement of the requested feature.",
            "supporting_evidence": "The RESPONSE provides a detailed and actionable plan for implementing the feature request, including code modifications, documentation updates, and testing procedures. It addresses all aspects of the feature request: exposing the warm_start parameter, documenting it, adding a test, and updating the example documentation. The inclusion of code snippets and specific file paths adds significant value and relevance to the response. The only reason it does not score a 10 is that it does not explicitly mention the need to ensure backward compatibility, which is typically an important consideration when modifying the API of a library. However, this is a minor omission and does not detract significantly from the overall utility of the response.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711740643.712991,
        "result": "To implement the feature request described in the ticket, follow these steps:\n\n- Expose the `warm_start` parameter in the `IsolationForest` constructor.\n- Document the `warm_start` parameter in the class docstring.\n- Add a test to ensure that the `warm_start` functionality works as expected.\n- Update the \"IsolationForest example\" documentation to mention the `warm_start` parameter.\n\nHere is a high-level overview of the changes to be made in the codebase:\n\n1. Modify the `IsolationForest` class definition in `_iforest.py` to include the `warm_start` parameter:\n\n```python\nclass IsolationForest(OutlierMixin, BaseBagging):\n    def __init__(self, n_estimators=100, max_samples=\"auto\", contamination=\"auto\",\n                 max_features=1.0, bootstrap=False, n_jobs=None, random_state=None,\n                 verbose=0, warm_start=False):  # Add warm_start parameter here\n        super().__init__(\n            base_estimator=ExtraTreeRegressor(max_features=1,\n                                              splitter='random',\n                                              random_state=random_state),\n            # ... other parameters remain unchanged ...\n            warm_start=warm_start,  # Pass warm_start to the superclass\n        )\n        # ... rest of the constructor remains unchanged ...\n```\n\n2. Update the class docstring to document the `warm_start` parameter:\n\n```python\n\"\"\"\n    Isolation Forest Algorithm.\n    ...\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just fit a whole\n        new forest. See :term:`the Glossary <warm_start>`.\n    ...\n\"\"\"\n```\n\n3. Add a test for the `warm_start` functionality in `sklearn/ensemble/tests/test_iforest.py`:\n\n```python\ndef test_warm_start():\n    rng = np.random.RandomState(42)\n    X = 0.3 * rng.randn(100, 2)\n    clf = IsolationForest(n_estimators=100, warm_start=True, random_state=rng)\n    clf.fit(X)\n    n_estimators_first_fit = len(clf.estimators_)\n    \n    clf.set_params(n_estimators=110)\n    clf.fit(X)\n    n_estimators_second_fit = len(clf.estimators_)\n    \n    assert n_estimators_second_fit == n_estimators_first_fit + 10\n```\n\n4. Update the \"IsolationForest example\" documentation in `examples/ensemble/plot_isolation_forest.py` to mention the `warm_start` parameter:\n\n```python\n# ...\nclf = IsolationForest(max_samples=100, random_state=0, warm_start=True)\n# ...\n```\n\nMake sure to run the test suite to ensure that the new test passes and that no other parts of the codebase are affected by this change. Additionally, ensure that the documentation builds correctly and that the `warm_start` parameter is properly documented.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [ensemble](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/ensemble)- [Lines 1 - 25](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/ensemble/_iforest.py#L1-L25)\n\n This snippet shows the import of BaseBagging in the IsolationForest implementation file, indicating that IsolationForest inherits from BaseBagging, which is relevant to the issue. \n\n\n   - [Lines 28 - 28](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/ensemble/_iforest.py#L28-L28)\n\n This snippet defines the IsolationForest class and shows that it inherits from BaseBagging, which is directly related to the issue of exposing the warm_start parameter. \n\n\n   - [Lines 29 - 545](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/ensemble/_iforest.py#L29-L545)\n\n This snippet contains the IsolationForest class definition with its parameters, which is where the warm_start parameter should be exposed as per the issue description.\n-  \u2192 [examples](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//examples) \u2192 [ensemble](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//examples/ensemble)- [Lines 1 - 120](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//examples/ensemble/plot_isolation_forest.py#L1-L120)\n\n This snippet is from the IsolationForest example documentation, which should mention the warm_start parameter as suggested in the issue."
    }
}