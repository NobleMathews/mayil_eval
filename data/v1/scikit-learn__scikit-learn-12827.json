{
    "id": "scikit-learn__scikit-learn-12827",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
    "task_id": 7524298429732913465,
    "title": "DOC: add details to QuantileTransformer documentation",
    "body": "I think the documentation of `QuantileTransformer` should say how it is implemented. There is even a [stats.stackexchange question](https://stats.stackexchange.com/questions/325570/quantile-transformation-with-gaussian-distribution-sklearn-implementation/327102#327102) about it and we could take some elements of the answer.\r\n\r\nBesides I was thinking that to map to a uniform distribution, the implementation was just computing the empirical cdf of the columns but it does not seem to be the case.\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.21.git",
    "requested_commit": "",
    "mayil_response": "To implement the feature request to add details to the `QuantileTransformer` documentation, follow these steps:\n\n- Update the class docstring of `QuantileTransformer` in `_data.py` to include implementation details.\n- Mention the empirical cumulative distribution function (CDF) computation for the features.\n- Clarify that the transformation maps the computed empirical CDF to a uniform distribution, and then to the desired output distribution (uniform or normal).\n- Explain that the transformation is non-linear and may distort linear correlations between variables.\n- Include information about handling of values outside the fitted range (they are clipped to the bounds of the output distribution).\n- Add a reference to the Stack Exchange answer or summarize the key points from it as appropriate.\n- Ensure that the updated documentation explains any parameters that affect the implementation, such as `n_quantiles`, `output_distribution`, `subsample`, and `random_state`.\n- Include examples that demonstrate the effect of the transformation on synthetic data, similar to the existing examples but with more emphasis on the implementation details.\n\nHere is a pseudo-code snippet to illustrate how you might update the docstring:\n\n```python\nclass QuantileTransformer(OneToOneFeatureMixin, TransformerMixin, BaseEstimator):\n    \"\"\"\n    Transform features using quantiles information.\n\n    This method transforms the features to follow a uniform or a normal\n    distribution. For each feature, the transformation proceeds as follows:\n    1. Compute the empirical cumulative distribution function (CDF) of the feature.\n    2. Use the empirical CDF to map the original feature values to a uniform distribution.\n    3. Map the uniformly distributed features to the desired output distribution (uniform or normal)\n       using the corresponding quantile function.\n\n    The transformation is non-linear and may distort correlations between variables measured at the same scale.\n    However, it makes variables measured at different scales more directly comparable.\n\n    Out-of-bounds feature values (new data points that fall below or above the fitted range) are mapped to the\n    bounds of the output distribution. This ensures that the transform is robust to outliers during the fit.\n\n    Implementation details:\n    - The empirical CDF is estimated using the provided number of quantiles (`n_quantiles`).\n    - The transformation to a uniform distribution is a non-parametric operation based on the sorted feature values.\n    - When mapping to a normal distribution, the inverse of the normal CDF (the probit function) is used.\n\n    For more details on the implementation, see [Stack Exchange reference].\n\n    Parameters\n    ----------\n    n_quantiles : int, default=1000 or n_samples\n        Number of quantiles to be computed. It corresponds to the number\n        of landmarks used to discretize the cumulative distribution function.\n        If n_quantiles is larger than the number of samples, n_quantiles is set\n        to the number of samples as a larger number of quantiles does not give\n        a better approximation of the cumulative distribution function\n        estimator.\n    ...\n    \"\"\"\n    ...\n```\n\nRemember to include any relevant examples or visualizations that can help users understand the transformation process. You can modify existing examples or add new ones to illustrate the implementation details.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [preprocessing](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/preprocessing)- [Lines 2425 - 3517](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/preprocessing/_data.py#L2425-L3517)\n\n This snippet contains the class definition and documentation for QuantileTransformer, which is directly related to the issue of enhancing the documentation. \n\n\n   - [Lines 2687 - 2747](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/preprocessing/_data.py#L2687-L2747)\n\n This snippet includes the private function to transform a single feature using QuantileTransformer, which may contain implementation details that should be documented. \n\n\n   - [Lines 436 - 3145](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/preprocessing/_data.py#L436-L3145)\n\n This snippet includes the fit method of QuantileTransformer, which computes the quantiles used for transforming. It may contain details on the implementation that should be documented. \n\n\n   - [Lines 2983 - 3411](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/preprocessing/_data.py#L2983-L3411)\n\n This snippet shows the use of QuantileTransformer in a function, which might be relevant for understanding how it is implemented and used in practice. \n\n\n   - [Lines 2858 - 3421](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/preprocessing/_data.py#L2858-L3421)\n\n This snippet defines the quantile_transform function, which is related to QuantileTransformer and might contain implementation details that should be documented.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0004185,
        "snippet_processor": 0.08203,
        "issue_star_creation": 0.032729999999999995,
        "issue_star_solver": 0.07929000000000001,
        "bouncer": 0.030320000000000003
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711741138.4510028,
        "relevant_snippets": [
            {
                "code": "class QuantileTransformer(OneToOneFeatureMixin, TransformerMixin, BaseEstimator):",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 80446,
                "end_index": 80527,
                "start_line": 2424,
                "end_line": 2424,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n=================================\nMap data to a normal distribution\n=================================\n\n.. currentmodule:: sklearn.preprocessing\n\nThis example demonstrates the use of the Box-Cox and Yeo-Johnson transforms\nthrough :class:`~PowerTransformer` to map data from various\ndistributions to a normal distribution.\n\nThe power transform is useful as a transformation in modeling problems where\nhomoscedasticity and normality are desired. Below are examples of Box-Cox and\nYeo-Johnwon applied to six different probability distributions: Lognormal,\nChi-squared, Weibull, Gaussian, Uniform, and Bimodal.\n\nNote that the transformations successfully map the data to a normal\ndistribution when applied to certain datasets, but are ineffective with others.\nThis highlights the importance of visualizing the data before and after\ntransformation.\n\nAlso note that even though Box-Cox seems to perform better than Yeo-Johnson for\nlognormal and chi-squared distributions, keep in mind that Box-Cox does not\nsupport inputs with negative values.\n\nFor comparison, we also add the output from\n:class:`~QuantileTransformer`. It can force any arbitrary\ndistribution into a gaussian, provided that there are enough training samples\n(thousands). Because it is a non-parametric method, it is harder to interpret\nthan the parametric ones (Box-Cox and Yeo-Johnson).\n\nOn \"small\" datasets (less than a few hundred points), the quantile transformer\nis prone to overfitting. The use of the power transform is then recommended.\n\n\"\"\"\n\n# Author: Eric Chang <ericchang2017@u.northwestern.edu>\n#         Nicolas Hug <contact@nicolas-hug.com>\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PowerTransformer, QuantileTransformer\n\nN_SAMPLES = 1000\nFONT_SIZE = 6\nBINS = 30\n\n\nrng = np.random.RandomState(304)\nbc = PowerTransformer(method=\"box-cox\")\nyj = PowerTransformer(method=\"yeo-johnson\")\n# n_quantiles is set to the training set size rather than the default value\n# to avoid a warning being raised by this example\nqt = QuantileTransformer(\n    n_quantiles=500, output_distribution=\"normal\", random_state=rng\n)\nsize = (N_SAMPLES, 1)\n\n\n# lognormal distribution\nX_lognormal = rng.lognormal(size=size)\n\n# chi-squared distribution\ndf = 3\nX_chisq = rng.chisquare(df=df, size=size)\n\n# weibull distribution\na = 50\nX_weibull = rng.weibull(a=a, size=size)\n\n# gaussian distribution\nloc = 100\nX_gaussian = rng.normal(loc=loc, size=size)\n\n# uniform distribution\nX_uniform = rng.uniform(low=0, high=1, size=size)\n\n# bimodal distribution\nloc_a, loc_b = 100, 105\nX_a, X_b = rng.normal(loc=loc_a, size=size), rng.normal(loc=loc_b, size=size)\nX_bimodal = np.concatenate([X_a, X_b], axis=0)\n\n\n# create plots\ndistributions = [\n    (\"Lognormal\", X_lognormal),\n    (\"Chi-squared\", X_chisq),\n    (\"Weibull\", X_weibull),\n    (\"Gaussian\", X_gaussian),\n    (\"Uniform\", X_uniform),\n    (\"Bimodal\", X_bimodal),\n]",
                "filename": "examples/preprocessing/plot_map_data_to_normal.py",
                "start_index": 0,
                "end_index": 2978,
                "start_line": 1,
                "end_line": 141,
                "max_line": 145,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "# transformed values lie in a [-2, 3] range as seen in the zoomed-in figure.\n# Note that the outliers themselves are still present in the transformed data.\n# If a separate outlier clipping is desirable, a non-linear transformation is\n# required (see below).\n\nmake_plot(4)\n\n# %%\n# PowerTransformer\n# ----------------\n#\n# :class:`~sklearn.preprocessing.PowerTransformer` applies a power\n# transformation to each feature to make the data more Gaussian-like in order\n# to stabilize variance and minimize skewness. Currently the Yeo-Johnson\n# and Box-Cox transforms are supported and the optimal\n# scaling factor is determined via maximum likelihood estimation in both\n# methods. By default, :class:`~sklearn.preprocessing.PowerTransformer` applies\n# zero-mean, unit variance normalization. Note that\n# Box-Cox can only be applied to strictly positive data. Income and average\n# house occupancy happen to be strictly positive, but if negative values are\n# present the Yeo-Johnson transformed is preferred.\n\nmake_plot(5)\nmake_plot(6)\n\n# %%\n# QuantileTransformer (uniform output)\n# ------------------------------------\n#\n# :class:`~sklearn.preprocessing.QuantileTransformer` applies a non-linear\n# transformation such that the\n# probability density function of each feature will be mapped to a uniform\n# or Gaussian distribution. In this case, all the data, including outliers,\n# will be mapped to a uniform distribution with the range [0, 1], making\n# outliers indistinguishable from inliers.\n#\n# :class:`~sklearn.preprocessing.RobustScaler` and\n# :class:`~sklearn.preprocessing.QuantileTransformer` are robust to outliers in\n# the sense that adding or removing outliers in the training set will yield\n# approximately the same transformation. But contrary to\n# :class:`~sklearn.preprocessing.RobustScaler`,\n# :class:`~sklearn.preprocessing.QuantileTransformer` will also automatically\n# collapse any outlier by setting them to the a priori defined range boundaries\n# (0 and 1). This can result in saturation artifacts for extreme values.\n\nmake_plot(7)\n\n##############################################################################\n# QuantileTransformer (Gaussian output)\n# -------------------------------------\n#\n# To map to a Gaussian distribution, set the parameter\n# ``output_distribution='normal'``.\n\nmake_plot(8)\n\n# %%\n# Normalizer\n# ----------\n#\n# The :class:`~sklearn.preprocessing.Normalizer` rescales the vector for each\n# sample to have unit norm,\n# independently of the distribution of the samples. It can be seen on both\n# figures below where all samples are mapped onto the unit circle. In our\n# example the two selected features have only positive values; therefore the\n# transformed data only lie in the positive quadrant. This would not be the\n# case if some original features had a mix of positive and negative values.\n\nmake_plot(9)\n\nplt.show()",
                "filename": "examples/preprocessing/plot_all_scaling.py",
                "start_index": 11499,
                "end_index": 14355,
                "start_line": 330,
                "end_line": 400,
                "max_line": 400,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"Transform features using quantiles information.\n\n    This method transforms the features to follow a uniform or a normal\n    distribution. Therefore, for a given feature, this transformation tends\n    to spread out the most frequent values. It also reduces the impact of\n    (marginal) outliers: this is therefore a robust preprocessing scheme.\n\n    The transformation is applied on each feature independently. First an\n    estimate of the cumulative distribution function of a feature is\n    used to map the original values to a uniform distribution. The obtained\n    values are then mapped to the desired output distribution using the\n    associated quantile function. Features values of new/unseen data that fall\n    below or above the fitted range will be mapped to the bounds of the output\n    distribution. Note that this transform is non-linear. It may distort linear\n    correlations between variables measured at the same scale but renders\n    variables measured at different scales more directly comparable.\n\n    Read more in the :ref:`User Guide <preprocessing_transformer>`.\n\n    .. versionadded:: 0.19\n\n    Parameters\n    ----------\n    n_quantiles : int, default=1000 or n_samples\n        Number of quantiles to be computed. It corresponds to the number\n        of landmarks used to discretize the cumulative distribution function.\n        If n_quantiles is larger than the number of samples, n_quantiles is set\n        to the number of samples as a larger number of quantiles does not give\n        a better approximation of the cumulative distribution function\n        estimator.\n\n    output_distribution : {'uniform', 'normal'}, default='uniform'\n        Marginal distribution for the transformed data. The choices are\n        'uniform' (default) or 'normal'.\n\n    ignore_implicit_zeros : bool, default=False\n        Only applies to sparse matrices. If True, the sparse entries of the\n        matrix are discarded to compute the quantile statistics. If False,\n        these entries are treated as zeros.\n\n    subsample : int, default=10_000\n        Maximum number of samples used to estimate the quantiles for\n        computational efficiency. Note that the subsampling procedure may\n        differ for value-identical sparse and dense matrices.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for subsampling and smoothing\n        noise.\n        Please see ``subsample`` for more details.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    copy : bool, default=True\n        Set to False to perform inplace transformation and avoid a copy (if the\n        input is already a numpy array).\n\n    Attributes\n    ----------\n    n_quantiles_ : int\n        The actual number of quantiles used to discretize the cumulative\n        distribution function.\n\n    quantiles_ : ndarray of shape (n_quantiles, n_features)\n        The values corresponding the quantiles of reference.\n\n    references_ : ndarray of shape (n_quantiles, )\n        Quantiles of references.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    quantile_transform : Equivalent function without the estimator API.\n    PowerTransformer : Perform mapping to a normal distribution using a power\n        transform.\n    StandardScaler : Perform standardization that is faster, but less robust\n        to outliers.\n    RobustScaler : Perform robust standardization that removes the influence\n        of outliers but does not put outliers and inliers on the same scale.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in fit, and maintained in\n    transform.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.preprocessing import QuantileTransformer\n    >>> rng = np.random.RandomState(0)\n    >>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)\n    >>> qt = QuantileTransformer(n_quantiles=10, random_state=0)\n    >>> qt.fit_transform(X)\n    array([...])\n    \"\"\"",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 80532,
                "end_index": 85078,
                "start_line": 2425,
                "end_line": 3517,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"Private function to transform a single feature.\"\"\"\n\n        output_distribution = self.output_distribution\n\n        if not inverse:\n            lower_bound_x = quantiles[0]\n            upper_bound_x = quantiles[-1]\n            lower_bound_y = 0\n            upper_bound_y = 1\n        else:\n            lower_bound_x = 0\n            upper_bound_x = 1\n            lower_bound_y = quantiles[0]\n            upper_bound_y = quantiles[-1]\n            # for inverse transform, match a uniform distribution\n            with np.errstate(invalid=\"ignore\"):  # hide NaN comparison warnings\n                if output_distribution == \"normal\":\n                    X_col = stats.norm.cdf(X_col)\n                # else output distribution is already a uniform distribution\n\n        # find index for lower and higher bounds\n        with np.errstate(invalid=\"ignore\"):  # hide NaN comparison warnings\n            if output_distribution == \"normal\":\n                lower_bounds_idx = X_col - BOUNDS_THRESHOLD < lower_bound_x\n                upper_bounds_idx = X_col + BOUNDS_THRESHOLD > upper_bound_x\n            if output_distribution == \"uniform\":\n                lower_bounds_idx = X_col == lower_bound_x\n                upper_bounds_idx = X_col == upper_bound_x\n\n        isfinite_mask = ~np.isnan(X_col)\n        X_col_finite = X_col[isfinite_mask]\n        if not inverse:\n            # Interpolate in one direction and in the other and take the\n            # mean. This is in case of repeated values in the features\n            # and hence repeated quantiles\n            #\n            # If we don't do this, only one extreme of the duplicated is\n            # used (the upper when we do ascending, and the\n            # lower for descending). We take the mean of these two\n            X_col[isfinite_mask] = 0.5 * (\n                np.interp(X_col_finite, quantiles, self.references_)\n                - np.interp(-X_col_finite, -quantiles[::-1], -self.references_[::-1])\n            )\n        else:\n            X_col[isfinite_mask] = np.interp(X_col_finite, self.references_, quantiles)\n\n        X_col[upper_bounds_idx] = upper_bound_y\n        X_col[lower_bounds_idx] = lower_bound_y\n        # for forward transform, match the output distribution\n        if not inverse:\n            with np.errstate(invalid=\"ignore\"):  # hide NaN comparison warnings\n                if output_distribution == \"normal\":\n                    X_col = stats.norm.ppf(X_col)\n                    # find the value to clip the data to avoid mapping to\n                    # infinity. Clip such that the inverse transform will be\n                    # consistent\n                    clip_min = stats.norm.ppf(BOUNDS_THRESHOLD - np.spacing(1))\n                    clip_max = stats.norm.ppf(1 - (BOUNDS_THRESHOLD - np.spacing(1)))\n                    X_col = np.clip(X_col, clip_min, clip_max)\n                # else output distribution is uniform and the ppf is the\n                # identity function so we let X_col unchanged",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 91148,
                "end_index": 94137,
                "start_line": 2687,
                "end_line": 2747,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n===================\nQuantile regression\n===================\n\nThis example illustrates how quantile regression can predict non-trivial\nconditional quantiles.\n\nThe left figure shows the case when the error distribution is normal,\nbut has non-constant variance, i.e. with heteroscedasticity.\n\nThe right figure shows an example of an asymmetric error distribution,\nnamely the Pareto distribution.\n\n\"\"\"\n\n# Authors: David Dale <dale.david@mail.ru>\n#          Christian Lorentzen <lorentzen.ch@gmail.com>\n#          Guillaume Lemaitre <glemaitre58@gmail.com>\n# License: BSD 3 clause\n\n# %%\n# Dataset generation\n# ------------------\n#\n# To illustrate the behaviour of quantile regression, we will generate two\n# synthetic datasets. The true generative random processes for both datasets\n# will be composed by the same expected value with a linear relationship with a\n# single feature `x`.\nimport numpy as np\n\nrng = np.random.RandomState(42)\nx = np.linspace(start=0, stop=10, num=100)\nX = x[:, np.newaxis]\ny_true_mean = 10 + 0.5 * x\n\n# %%\n# We will create two subsequent problems by changing the distribution of the\n# target `y` while keeping the same expected value:\n#\n# - in the first case, a heteroscedastic Normal noise is added;\n# - in the second case, an asymmetric Pareto noise is added.\ny_normal = y_true_mean + rng.normal(loc=0, scale=0.5 + 0.5 * x, size=x.shape[0])\na = 5\ny_pareto = y_true_mean + 10 * (rng.pareto(a, size=x.shape[0]) - 1 / (a - 1))\n\n# %%\n# Let's first visualize the datasets as well as the distribution of the\n# residuals `y - mean(y)`.\nimport matplotlib.pyplot as plt\n\n_, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 11), sharex=\"row\", sharey=\"row\")\n\naxs[0, 0].plot(x, y_true_mean, label=\"True mean\")\naxs[0, 0].scatter(x, y_normal, color=\"black\", alpha=0.5, label=\"Observations\")\naxs[1, 0].hist(y_true_mean - y_normal, edgecolor=\"black\")\n\n\naxs[0, 1].plot(x, y_true_mean, label=\"True mean\")\naxs[0, 1].scatter(x, y_pareto, color=\"black\", alpha=0.5, label=\"Observations\")\naxs[1, 1].hist(y_true_mean - y_pareto, edgecolor=\"black\")\n\naxs[0, 0].set_title(\"Dataset with heteroscedastic Normal distributed targets\")\naxs[0, 1].set_title(\"Dataset with asymmetric Pareto distributed target\")\naxs[1, 0].set_title(\n    \"Residuals distribution for heteroscedastic Normal distributed targets\"\n)\naxs[1, 1].set_title(\"Residuals distribution for asymmetric Pareto distributed target\")\naxs[0, 0].legend()\naxs[0, 1].legend()\naxs[0, 0].set_ylabel(\"y\")\naxs[1, 0].set_ylabel(\"Counts\")\naxs[0, 1].set_xlabel(\"x\")\naxs[0, 0].set_xlabel(\"x\")\naxs[1, 0].set_xlabel(\"Residuals\")\n_ = axs[1, 1].set_xlabel(\"Residuals\")\n\n# %%\n# With the heteroscedastic Normal distributed target, we observe that the\n# variance of the noise is increasing when the value of the feature `x` is\n# increasing.\n#\n# With the asymmetric Pareto distributed target, we observe that the positive\n# residuals are bounded.\n#\n# These types of noisy targets make the estimation via",
                "filename": "examples/linear_model/plot_quantile_regression.py",
                "start_index": 0,
                "end_index": 2942,
                "start_line": 1,
                "end_line": 86,
                "max_line": 311,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "n = QuantileTransformer(\n        n_quantiles=n_quantiles,\n        output_distribution=output_distribution,\n        subsample=subsample,\n        ignore_implicit_zeros=ignore_implicit_zeros,\n        random_state=random_state,\n        copy=copy,\n    )\n    if axis == 0:\n        X = n.fit_transform(X)\n    else:  # axis == 1\n        X = n.fit_transform(X.T).T\n    return X",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 103262,
                "end_index": 103630,
                "start_line": 2983,
                "end_line": 3411,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "# :class:`~sklearn.linear_model.LinearRegression` less efficient, i.e. we need\n# more data to get stable results and, in addition, large outliers can have a\n# huge impact on the fitted coefficients. (Stated otherwise: in a setting with\n# constant variance, ordinary least squares estimators converge much faster to\n# the *true* coefficients with increasing sample size.)\n#\n# In this asymmetric setting, the median or different quantiles give additional\n# insights. On top of that, median estimation is much more robust to outliers\n# and heavy tailed distributions. But note that extreme quantiles are estimated\n# by very few data points. 95% quantile are more or less estimated by the 5%\n# largest values and thus also a bit sensitive outliers.\n#\n# In the remainder of this tutorial, we will show how\n# :class:`~sklearn.linear_model.QuantileRegressor` can be used in practice and\n# give the intuition into the properties of the fitted models. Finally,\n# we will compare the both :class:`~sklearn.linear_model.QuantileRegressor`\n# and :class:`~sklearn.linear_model.LinearRegression`.\n#\n# Fitting a `QuantileRegressor`\n# -----------------------------\n#\n# In this section, we want to estimate the conditional median as well as\n# a low and high quantile fixed at 5% and 95%, respectively. Thus, we will get\n# three linear models, one for each quantile.\n#\n# We will use the quantiles at 5% and 95% to find the outliers in the training\n# sample beyond the central 90% interval.\nfrom sklearn.utils.fixes import parse_version, sp_version\n\n# This is line is to avoid incompatibility if older SciPy version.\n# You should use `solver=\"highs\"` with recent version of SciPy.\nsolver = \"highs\" if sp_version >= parse_version(\"1.6.0\") else \"interior-point\"\n\n# %%\nfrom sklearn.linear_model import QuantileRegressor\n\nquantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = np.zeros_like(y_true_mean, dtype=np.bool_)\nfor quantile in quantiles:\n    qr = QuantileRegressor(quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_normal).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = np.logical_or(\n            out_bounds_predictions, y_pred >= y_normal\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = np.logical_or(\n            out_bounds_predictions, y_pred <= y_normal\n        )\n\n# %%\n# Now, we can plot the three linear models and the distinguished samples that\n# are within the central 90% interval from samples that are outside this\n# interval.\nplt.plot(X, y_true_mean, color=\"black\", linestyle=\"dashed\", label=\"True mean\")\n\nfor quantile, y_pred in predictions.items():\n    plt.plot(X, y_pred, label=f\"Quantile: {quantile}\")\n\nplt.scatter(\n    x[out_bounds_predictions],\n    y_normal[out_bounds_predictions],\n    color=\"black\",\n    marker=\"+\",\n    alpha=0.5,\n    label=\"Outside interval\",\n)",
                "filename": "examples/linear_model/plot_quantile_regression.py",
                "start_index": 2943,
                "end_index": 5845,
                "start_line": 87,
                "end_line": 308,
                "max_line": 311,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "@_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Compute the quantiles used for transforming.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data used to scale along the features axis. If a sparse\n            matrix is provided, it will be converted into a sparse\n            ``csc_matrix``. Additionally, the sparse matrix needs to be\n            nonnegative if `ignore_implicit_zeros` is False.\n\n        y : None\n            Ignored.\n\n        Returns\n        -------\n        self : object\n           Fitted transformer.\n        \"\"\"\n        if self.n_quantiles > self.subsample:\n            raise ValueError(\n                \"The number of quantiles cannot be greater than\"\n                \" the number of samples used. Got {} quantiles\"\n                \" and {} samples.\".format(self.n_quantiles, self.subsample)\n            )\n\n        X = self._check_inputs(X, in_fit=True, copy=False)\n        n_samples = X.shape[0]\n\n        if self.n_quantiles > n_samples:\n            warnings.warn(\n                \"n_quantiles (%s) is greater than the total number \"\n                \"of samples (%s). n_quantiles is set to \"\n                \"n_samples.\" % (self.n_quantiles, n_samples)\n            )\n        self.n_quantiles_ = max(1, min(self.n_quantiles, n_samples))\n\n        rng = check_random_state(self.random_state)\n\n        # Create the quantiles of reference\n        self.references_ = np.linspace(0, 1, self.n_quantiles_, endpoint=True)\n        if sparse.issparse(X):\n            self._sparse_fit(X, rng)\n        else:\n            self._dense_fit(X, rng)\n\n        return self",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 89383,
                "end_index": 91081,
                "start_line": 436,
                "end_line": 3145,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "ef quantile_transform(\n    X,\n    *,\n    axis=0,\n    n_quantiles=1000,\n    output_distribution=\"uniform\",\n    ignore_implicit_zeros=False,\n    subsample=int(1e5),\n    random_state=None,\n    copy=True,\n):",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 97894,
                "end_index": 98097,
                "start_line": 2858,
                "end_line": 3421,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            }
        ],
        "classification": "feature request",
        "snippet_processor_task": {
            "sklearn/preprocessing/_data.py": [
                {
                    "chunk": {
                        "code": "\"\"\"Transform features using quantiles information.\n\n    This method transforms the features to follow a uniform or a normal\n    distribution. Therefore, for a given feature, this transformation tends\n    to spread out the most frequent values. It also reduces the impact of\n    (marginal) outliers: this is therefore a robust preprocessing scheme.\n\n    The transformation is applied on each feature independently. First an\n    estimate of the cumulative distribution function of a feature is\n    used to map the original values to a uniform distribution. The obtained\n    values are then mapped to the desired output distribution using the\n    associated quantile function. Features values of new/unseen data that fall\n    below or above the fitted range will be mapped to the bounds of the output\n    distribution. Note that this transform is non-linear. It may distort linear\n    correlations between variables measured at the same scale but renders\n    variables measured at different scales more directly comparable.\n\n    Read more in the :ref:`User Guide <preprocessing_transformer>`.\n\n    .. versionadded:: 0.19\n\n    Parameters\n    ----------\n    n_quantiles : int, default=1000 or n_samples\n        Number of quantiles to be computed. It corresponds to the number\n        of landmarks used to discretize the cumulative distribution function.\n        If n_quantiles is larger than the number of samples, n_quantiles is set\n        to the number of samples as a larger number of quantiles does not give\n        a better approximation of the cumulative distribution function\n        estimator.\n\n    output_distribution : {'uniform', 'normal'}, default='uniform'\n        Marginal distribution for the transformed data. The choices are\n        'uniform' (default) or 'normal'.\n\n    ignore_implicit_zeros : bool, default=False\n        Only applies to sparse matrices. If True, the sparse entries of the\n        matrix are discarded to compute the quantile statistics. If False,\n        these entries are treated as zeros.\n\n    subsample : int, default=10_000\n        Maximum number of samples used to estimate the quantiles for\n        computational efficiency. Note that the subsampling procedure may\n        differ for value-identical sparse and dense matrices.\n\n    random_state : int, RandomState instance or None, default=None\n        Determines random number generation for subsampling and smoothing\n        noise.\n        Please see ``subsample`` for more details.\n        Pass an int for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    copy : bool, default=True\n        Set to False to perform inplace transformation and avoid a copy (if the\n        input is already a numpy array).\n\n    Attributes\n    ----------\n    n_quantiles_ : int\n        The actual number of quantiles used to discretize the cumulative\n        distribution function.\n\n    quantiles_ : ndarray of shape (n_quantiles, n_features)\n        The values corresponding the quantiles of reference.\n\n    references_ : ndarray of shape (n_quantiles, )\n        Quantiles of references.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    quantile_transform : Equivalent function without the estimator API.\n    PowerTransformer : Perform mapping to a normal distribution using a power\n        transform.\n    StandardScaler : Perform standardization that is faster, but less robust\n        to outliers.\n    RobustScaler : Perform robust standardization that removes the influence\n        of outliers but does not put outliers and inliers on the same scale.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in fit, and maintained in\n    transform.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.preprocessing import QuantileTransformer\n    >>> rng = np.random.RandomState(0)\n    >>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)\n    >>> qt = QuantileTransformer(n_quantiles=10, random_state=0)\n    >>> qt.fit_transform(X)\n    array([...])\n    \"\"\"",
                        "filename": "sklearn/preprocessing/_data.py",
                        "start_index": 80532,
                        "end_index": 85078,
                        "start_line": 2425,
                        "end_line": 3517,
                        "max_line": 3519,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the class definition and documentation for QuantileTransformer, which is directly related to the issue of enhancing the documentation."
                },
                {
                    "chunk": {
                        "code": "\"\"\"Private function to transform a single feature.\"\"\"\n\n        output_distribution = self.output_distribution\n\n        if not inverse:\n            lower_bound_x = quantiles[0]\n            upper_bound_x = quantiles[-1]\n            lower_bound_y = 0\n            upper_bound_y = 1\n        else:\n            lower_bound_x = 0\n            upper_bound_x = 1\n            lower_bound_y = quantiles[0]\n            upper_bound_y = quantiles[-1]\n            # for inverse transform, match a uniform distribution\n            with np.errstate(invalid=\"ignore\"):  # hide NaN comparison warnings\n                if output_distribution == \"normal\":\n                    X_col = stats.norm.cdf(X_col)\n                # else output distribution is already a uniform distribution\n\n        # find index for lower and higher bounds\n        with np.errstate(invalid=\"ignore\"):  # hide NaN comparison warnings\n            if output_distribution == \"normal\":\n                lower_bounds_idx = X_col - BOUNDS_THRESHOLD < lower_bound_x\n                upper_bounds_idx = X_col + BOUNDS_THRESHOLD > upper_bound_x\n            if output_distribution == \"uniform\":\n                lower_bounds_idx = X_col == lower_bound_x\n                upper_bounds_idx = X_col == upper_bound_x\n\n        isfinite_mask = ~np.isnan(X_col)\n        X_col_finite = X_col[isfinite_mask]\n        if not inverse:\n            # Interpolate in one direction and in the other and take the\n            # mean. This is in case of repeated values in the features\n            # and hence repeated quantiles\n            #\n            # If we don't do this, only one extreme of the duplicated is\n            # used (the upper when we do ascending, and the\n            # lower for descending). We take the mean of these two\n            X_col[isfinite_mask] = 0.5 * (\n                np.interp(X_col_finite, quantiles, self.references_)\n                - np.interp(-X_col_finite, -quantiles[::-1], -self.references_[::-1])\n            )\n        else:\n            X_col[isfinite_mask] = np.interp(X_col_finite, self.references_, quantiles)\n\n        X_col[upper_bounds_idx] = upper_bound_y\n        X_col[lower_bounds_idx] = lower_bound_y\n        # for forward transform, match the output distribution\n        if not inverse:\n            with np.errstate(invalid=\"ignore\"):  # hide NaN comparison warnings\n                if output_distribution == \"normal\":\n                    X_col = stats.norm.ppf(X_col)\n                    # find the value to clip the data to avoid mapping to\n                    # infinity. Clip such that the inverse transform will be\n                    # consistent\n                    clip_min = stats.norm.ppf(BOUNDS_THRESHOLD - np.spacing(1))\n                    clip_max = stats.norm.ppf(1 - (BOUNDS_THRESHOLD - np.spacing(1)))\n                    X_col = np.clip(X_col, clip_min, clip_max)\n                # else output distribution is uniform and the ppf is the\n                # identity function so we let X_col unchanged",
                        "filename": "sklearn/preprocessing/_data.py",
                        "start_index": 91148,
                        "end_index": 94137,
                        "start_line": 2687,
                        "end_line": 2747,
                        "max_line": 3519,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes the private function to transform a single feature using QuantileTransformer, which may contain implementation details that should be documented."
                },
                {
                    "chunk": {
                        "code": "@_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y=None):\n        \"\"\"Compute the quantiles used for transforming.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data used to scale along the features axis. If a sparse\n            matrix is provided, it will be converted into a sparse\n            ``csc_matrix``. Additionally, the sparse matrix needs to be\n            nonnegative if `ignore_implicit_zeros` is False.\n\n        y : None\n            Ignored.\n\n        Returns\n        -------\n        self : object\n           Fitted transformer.\n        \"\"\"\n        if self.n_quantiles > self.subsample:\n            raise ValueError(\n                \"The number of quantiles cannot be greater than\"\n                \" the number of samples used. Got {} quantiles\"\n                \" and {} samples.\".format(self.n_quantiles, self.subsample)\n            )\n\n        X = self._check_inputs(X, in_fit=True, copy=False)\n        n_samples = X.shape[0]\n\n        if self.n_quantiles > n_samples:\n            warnings.warn(\n                \"n_quantiles (%s) is greater than the total number \"\n                \"of samples (%s). n_quantiles is set to \"\n                \"n_samples.\" % (self.n_quantiles, n_samples)\n            )\n        self.n_quantiles_ = max(1, min(self.n_quantiles, n_samples))\n\n        rng = check_random_state(self.random_state)\n\n        # Create the quantiles of reference\n        self.references_ = np.linspace(0, 1, self.n_quantiles_, endpoint=True)\n        if sparse.issparse(X):\n            self._sparse_fit(X, rng)\n        else:\n            self._dense_fit(X, rng)\n\n        return self",
                        "filename": "sklearn/preprocessing/_data.py",
                        "start_index": 89383,
                        "end_index": 91081,
                        "start_line": 436,
                        "end_line": 3145,
                        "max_line": 3519,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes the fit method of QuantileTransformer, which computes the quantiles used for transforming. It may contain details on the implementation that should be documented."
                },
                {
                    "chunk": {
                        "code": "n = QuantileTransformer(\n        n_quantiles=n_quantiles,\n        output_distribution=output_distribution,\n        subsample=subsample,\n        ignore_implicit_zeros=ignore_implicit_zeros,\n        random_state=random_state,\n        copy=copy,\n    )\n    if axis == 0:\n        X = n.fit_transform(X)\n    else:  # axis == 1\n        X = n.fit_transform(X.T).T\n    return X",
                        "filename": "sklearn/preprocessing/_data.py",
                        "start_index": 103262,
                        "end_index": 103630,
                        "start_line": 2983,
                        "end_line": 3411,
                        "max_line": 3519,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the use of QuantileTransformer in a function, which might be relevant for understanding how it is implemented and used in practice."
                },
                {
                    "chunk": {
                        "code": "ef quantile_transform(\n    X,\n    *,\n    axis=0,\n    n_quantiles=1000,\n    output_distribution=\"uniform\",\n    ignore_implicit_zeros=False,\n    subsample=int(1e5),\n    random_state=None,\n    copy=True,\n):",
                        "filename": "sklearn/preprocessing/_data.py",
                        "start_index": 97894,
                        "end_index": 98097,
                        "start_line": 2858,
                        "end_line": 3421,
                        "max_line": 3519,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet defines the quantile_transform function, which is related to QuantileTransformer and might contain implementation details that should be documented."
                }
            ]
        },
        "solution_task": "To implement the feature request to add details to the `QuantileTransformer` documentation, follow these steps:\n\n- Update the class docstring of `QuantileTransformer` in `_data.py` to include implementation details.\n- Mention the empirical cumulative distribution function (CDF) computation for the features.\n- Clarify that the transformation maps the computed empirical CDF to a uniform distribution, and then to the desired output distribution (uniform or normal).\n- Explain that the transformation is non-linear and may distort linear correlations between variables.\n- Include information about handling of values outside the fitted range (they are clipped to the bounds of the output distribution).\n- Add a reference to the Stack Exchange answer or summarize the key points from it as appropriate.\n- Ensure that the updated documentation explains any parameters that affect the implementation, such as `n_quantiles`, `output_distribution`, `subsample`, and `random_state`.\n- Include examples that demonstrate the effect of the transformation on synthetic data, similar to the existing examples but with more emphasis on the implementation details.\n\nHere is a pseudo-code snippet to illustrate how you might update the docstring:\n\n```python\nclass QuantileTransformer(OneToOneFeatureMixin, TransformerMixin, BaseEstimator):\n    \"\"\"\n    Transform features using quantiles information.\n\n    This method transforms the features to follow a uniform or a normal\n    distribution. For each feature, the transformation proceeds as follows:\n    1. Compute the empirical cumulative distribution function (CDF) of the feature.\n    2. Use the empirical CDF to map the original feature values to a uniform distribution.\n    3. Map the uniformly distributed features to the desired output distribution (uniform or normal)\n       using the corresponding quantile function.\n\n    The transformation is non-linear and may distort correlations between variables measured at the same scale.\n    However, it makes variables measured at different scales more directly comparable.\n\n    Out-of-bounds feature values (new data points that fall below or above the fitted range) are mapped to the\n    bounds of the output distribution. This ensures that the transform is robust to outliers during the fit.\n\n    Implementation details:\n    - The empirical CDF is estimated using the provided number of quantiles (`n_quantiles`).\n    - The transformation to a uniform distribution is a non-parametric operation based on the sorted feature values.\n    - When mapping to a normal distribution, the inverse of the normal CDF (the probit function) is used.\n\n    For more details on the implementation, see [Stack Exchange reference].\n\n    Parameters\n    ----------\n    n_quantiles : int, default=1000 or n_samples\n        Number of quantiles to be computed. It corresponds to the number\n        of landmarks used to discretize the cumulative distribution function.\n        If n_quantiles is larger than the number of samples, n_quantiles is set\n        to the number of samples as a larger number of quantiles does not give\n        a better approximation of the cumulative distribution function\n        estimator.\n    ...\n    \"\"\"\n    ...\n```\n\nRemember to include any relevant examples or visualizations that can help users understand the transformation process. You can modify existing examples or add new ones to illustrate the implementation details.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant suggestions that can directly lead to the successful implementation or improvement of the requested feature.",
            "supporting_evidence": "The RESPONSE provides a comprehensive list of steps to enhance the `QuantileTransformer` documentation, including updating the class docstring, explaining the empirical CDF computation, and detailing the transformation process. It also suggests including information about handling values outside the fitted range and referencing the Stack Exchange answer. The inclusion of a pseudo-code snippet gives a clear example of how to implement the changes, and the links to specific code snippets in the sklearn repository are highly relevant for locating where the documentation should be updated. The RESPONSE addresses nearly all aspects of the feature request, providing actionable guidance for improving the documentation.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711741335.70404,
        "result": "To implement the feature request to add details to the `QuantileTransformer` documentation, follow these steps:\n\n- Update the class docstring of `QuantileTransformer` in `_data.py` to include implementation details.\n- Mention the empirical cumulative distribution function (CDF) computation for the features.\n- Clarify that the transformation maps the computed empirical CDF to a uniform distribution, and then to the desired output distribution (uniform or normal).\n- Explain that the transformation is non-linear and may distort linear correlations between variables.\n- Include information about handling of values outside the fitted range (they are clipped to the bounds of the output distribution).\n- Add a reference to the Stack Exchange answer or summarize the key points from it as appropriate.\n- Ensure that the updated documentation explains any parameters that affect the implementation, such as `n_quantiles`, `output_distribution`, `subsample`, and `random_state`.\n- Include examples that demonstrate the effect of the transformation on synthetic data, similar to the existing examples but with more emphasis on the implementation details.\n\nHere is a pseudo-code snippet to illustrate how you might update the docstring:\n\n```python\nclass QuantileTransformer(OneToOneFeatureMixin, TransformerMixin, BaseEstimator):\n    \"\"\"\n    Transform features using quantiles information.\n\n    This method transforms the features to follow a uniform or a normal\n    distribution. For each feature, the transformation proceeds as follows:\n    1. Compute the empirical cumulative distribution function (CDF) of the feature.\n    2. Use the empirical CDF to map the original feature values to a uniform distribution.\n    3. Map the uniformly distributed features to the desired output distribution (uniform or normal)\n       using the corresponding quantile function.\n\n    The transformation is non-linear and may distort correlations between variables measured at the same scale.\n    However, it makes variables measured at different scales more directly comparable.\n\n    Out-of-bounds feature values (new data points that fall below or above the fitted range) are mapped to the\n    bounds of the output distribution. This ensures that the transform is robust to outliers during the fit.\n\n    Implementation details:\n    - The empirical CDF is estimated using the provided number of quantiles (`n_quantiles`).\n    - The transformation to a uniform distribution is a non-parametric operation based on the sorted feature values.\n    - When mapping to a normal distribution, the inverse of the normal CDF (the probit function) is used.\n\n    For more details on the implementation, see [Stack Exchange reference].\n\n    Parameters\n    ----------\n    n_quantiles : int, default=1000 or n_samples\n        Number of quantiles to be computed. It corresponds to the number\n        of landmarks used to discretize the cumulative distribution function.\n        If n_quantiles is larger than the number of samples, n_quantiles is set\n        to the number of samples as a larger number of quantiles does not give\n        a better approximation of the cumulative distribution function\n        estimator.\n    ...\n    \"\"\"\n    ...\n```\n\nRemember to include any relevant examples or visualizations that can help users understand the transformation process. You can modify existing examples or add new ones to illustrate the implementation details.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [preprocessing](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/preprocessing)- [Lines 2425 - 3517](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/preprocessing/_data.py#L2425-L3517)\n\n This snippet contains the class definition and documentation for QuantileTransformer, which is directly related to the issue of enhancing the documentation. \n\n\n   - [Lines 2687 - 2747](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/preprocessing/_data.py#L2687-L2747)\n\n This snippet includes the private function to transform a single feature using QuantileTransformer, which may contain implementation details that should be documented. \n\n\n   - [Lines 436 - 3145](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/preprocessing/_data.py#L436-L3145)\n\n This snippet includes the fit method of QuantileTransformer, which computes the quantiles used for transforming. It may contain details on the implementation that should be documented. \n\n\n   - [Lines 2983 - 3411](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/preprocessing/_data.py#L2983-L3411)\n\n This snippet shows the use of QuantileTransformer in a function, which might be relevant for understanding how it is implemented and used in practice. \n\n\n   - [Lines 2858 - 3421](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/preprocessing/_data.py#L2858-L3421)\n\n This snippet defines the quantile_transform function, which is related to QuantileTransformer and might contain implementation details that should be documented."
    }
}