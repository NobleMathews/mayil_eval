{
    "id": "scikit-learn__scikit-learn-12758",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
    "task_id": 5553326482366638288,
    "title": "Showing micro-average in classification report is confusing",
    "body": "This is a follow up on #11679.\r\nI don't think it makes sense to include the micro-average for multi-class classification. The three columns will always show the same value, all of which being the same as accuracy. I find that confusing. If you want to show this (I don't see why you'd want to show the same number three times), I would at least make it clear in the report that it's accuracy.\nIncrementalPCA fails if data size % batch size < n_components\n#### Description\r\n\r\n`IncrementalPCA` throws`n_components=%r must be less or equal to the batch number of samples %d`\r\n\r\nThe error occurs because the last batch generated by `utils.gen_batch` may be smaller than `batch_size`.\r\n\r\n#### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.decomposition import PCA, IncrementalPCA\r\n   \r\niris = load_iris()\r\nX = iris.data[:101]\r\nipca = IncrementalPCA(n_components=2, batch_size=10)\r\nX_ipca = ipca.fit_transform(X)\r\n```\r\n\r\nI reduced the iris data to 101 instances, so the last batch has only a single data instance, which is less than the number of components.\r\n\r\nAs far as I see, none of the current unit tests run into this. (`test_incremental_pca_batch_signs` could, if the code that raises the exception would compare `self.n_components_` with `n_samples` - which it should, but doesn't).\r\n\r\nSkipping the last batch if it is to small, that is, changing\r\n\r\n```\r\n        for batch in gen_batches(n_samples, self.batch_size_):\r\n                self.partial_fit(X[batch], check_input=False)\r\n```\r\n\r\nto\r\n\r\n```\r\n        for batch in gen_batches(n_samples, self.batch_size_):\r\n            if self.n_components is None \\\r\n                    or X[batch].shape[0] >= self.n_components:\r\n                self.partial_fit(X[batch], check_input=False)\r\n```\r\n\r\nfixes the problem. @kastnerkyle, please confirm that this solution seems OK before I go preparing the PR and tests.\r\n\r\n#### Expected Results\r\n\r\nNo error is thrown.\r\n\r\n#### Actual Results\r\n\r\n`ValueError: n_components=2 must be less or equal to the batch number of samples 1.`\r\n\r\n#### Versions\r\n\r\n```\r\nDarwin-18.0.0-x86_64-i386-64bit\r\nPython 3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 13:14:59)\r\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]\r\nNumPy 1.15.2\r\nSciPy 1.1.0\r\nScikit-Learn 0.20.0\r\n```\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.21.git",
    "requested_commit": "",
    "mayil_response": "- Ensure `n_components` is less than or equal to the number of samples in each batch before calling `partial_fit` on `IncrementalPCA`.\n- Modify the batch processing logic to handle the case when the last batch is smaller than `n_components`.\n- If the last batch is smaller than `n_components`, either skip `partial_fit` for this batch or accumulate more samples until `n_components` is satisfied.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [decomposition](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/decomposition)- [Lines 1 - 16](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/decomposition/_incremental_pca.py#L1-L16)\n\n This snippet contains the file header for '_incremental_pca.py' which is the file likely needing changes to address the IncrementalPCA issue. \n\n\n   - [Lines 19 - 19](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/decomposition/_incremental_pca.py#L19-L19)\n\n This snippet shows the class definition for IncrementalPCA, where the proposed fix to the issue would be implemented.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0004185,
        "snippet_processor": 0.08203,
        "issue_star_creation": 0.032729999999999995,
        "issue_star_solver": 0.07929000000000001,
        "bouncer": 0.030320000000000003
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711741138.473161,
        "relevant_snippets": [
            {
                "code": "\"\"\"\n\n===============\nIncremental PCA\n===============\n\nIncremental principal component analysis (IPCA) is typically used as a\nreplacement for principal component analysis (PCA) when the dataset to be\ndecomposed is too large to fit in memory. IPCA builds a low-rank approximation\nfor the input data using an amount of memory which is independent of the\nnumber of input data samples. It is still dependent on the input data features,\nbut changing the batch size allows for control of memory usage.\n\nThis example serves as a visual check that IPCA is able to find a similar\nprojection of the data to PCA (to a sign flip), while only processing a\nfew samples at a time. This can be considered a \"toy example\", as IPCA is\nintended for large datasets which do not fit in main memory, requiring\nincremental approaches.\n\n\"\"\"\n\n# Authors: Kyle Kastner\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA, IncrementalPCA\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nn_components = 2\nipca = IncrementalPCA(n_components=n_components, batch_size=10)\nX_ipca = ipca.fit_transform(X)\n\npca = PCA(n_components=n_components)\nX_pca = pca.fit_transform(X)\n\ncolors = [\"navy\", \"turquoise\", \"darkorange\"]\n\nfor X_transformed, title in [(X_ipca, \"Incremental PCA\"), (X_pca, \"PCA\")]:\n    plt.figure(figsize=(8, 8))\n    for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n        plt.scatter(\n            X_transformed[y == i, 0],\n            X_transformed[y == i, 1],\n            color=color,\n            lw=2,\n            label=target_name,\n        )\n\n    if \"Incremental\" in title:\n        err = np.abs(np.abs(X_pca) - np.abs(X_ipca)).mean()\n        plt.title(title + \" of iris dataset\\nMean absolute unsigned error %.6f\" % err)\n    else:\n        plt.title(title + \" of iris dataset\")\n    plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n    plt.axis([-4, 4, -1.5, 1.5])\n\nplt.show()",
                "filename": "examples/decomposition/plot_incremental_pca.py",
                "start_index": 0,
                "end_index": 1988,
                "start_line": 1,
                "end_line": 63,
                "max_line": 63,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "def fixed_batch_size_comparison(data):\n    all_features = [\n        i.astype(int) for i in np.linspace(data.shape[1] // 10, data.shape[1], num=5)\n    ]\n    batch_size = 1000\n    # Compare runtimes and error for fixed batch size\n    all_times = defaultdict(list)\n    all_errors = defaultdict(list)\n    for n_components in all_features:\n        pca = PCA(n_components=n_components)\n        ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n        results_dict = {\n            k: benchmark(est, data) for k, est in [(\"pca\", pca), (\"ipca\", ipca)]\n        }\n\n        for k in sorted(results_dict.keys()):\n            all_times[k].append(results_dict[k][\"time\"])\n            all_errors[k].append(results_dict[k][\"error\"])\n\n    plot_feature_times(all_times, batch_size, all_features, data)\n    plot_feature_errors(all_errors, batch_size, all_features, data)\n\n\ndef variable_batch_size_comparison(data):\n    batch_sizes = [\n        i.astype(int) for i in np.linspace(data.shape[0] // 10, data.shape[0], num=10)\n    ]\n\n    for n_components in [\n        i.astype(int) for i in np.linspace(data.shape[1] // 10, data.shape[1], num=4)\n    ]:\n        all_times = defaultdict(list)\n        all_errors = defaultdict(list)\n        pca = PCA(n_components=n_components)\n        rpca = PCA(\n            n_components=n_components, svd_solver=\"randomized\", random_state=1999\n        )\n        results_dict = {\n            k: benchmark(est, data) for k, est in [(\"pca\", pca), (\"rpca\", rpca)]\n        }\n\n        # Create flat baselines to compare the variation over batch size\n        all_times[\"pca\"].extend([results_dict[\"pca\"][\"time\"]] * len(batch_sizes))\n        all_errors[\"pca\"].extend([results_dict[\"pca\"][\"error\"]] * len(batch_sizes))\n        all_times[\"rpca\"].extend([results_dict[\"rpca\"][\"time\"]] * len(batch_sizes))\n        all_errors[\"rpca\"].extend([results_dict[\"rpca\"][\"error\"]] * len(batch_sizes))\n        for batch_size in batch_sizes:\n            ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n            results_dict = {k: benchmark(est, data) for k, est in [(\"ipca\", ipca)]}\n            all_times[\"ipca\"].append(results_dict[\"ipca\"][\"time\"])\n            all_errors[\"ipca\"].append(results_dict[\"ipca\"][\"error\"])\n\n        plot_batch_times(all_times, n_components, batch_sizes, data)\n        plot_batch_errors(all_errors, n_components, batch_sizes, data)\n\n\nfaces = fetch_lfw_people(resize=0.2, min_faces_per_person=5)\n# limit dataset to 5000 people (don't care who they are!)\nX = faces.data[:5000]\nn_samples, h, w = faces.images.shape\nn_features = X.shape[1]\n\nX -= X.mean(axis=0)\nX /= X.std(axis=0)\n\nfixed_batch_size_comparison(X)\nvariable_batch_size_comparison(X)\nplt.show()",
                "filename": "benchmarks/bench_plot_incremental_pca.py",
                "start_index": 2845,
                "end_index": 5560,
                "start_line": 92,
                "end_line": 159,
                "max_line": 159,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n\ntarget_classes = range(0, 3)\ncolors = (\"blue\", \"red\", \"green\")\nmarkers = (\"^\", \"s\", \"o\")\n\nfor target_class, color, marker in zip(target_classes, colors, markers):\n    ax1.scatter(\n        x=X_train_transformed[y_train == target_class, 0],\n        y=X_train_transformed[y_train == target_class, 1],\n        color=color,\n        label=f\"class {target_class}\",\n        alpha=0.5,\n        marker=marker,\n    )\n\n    ax2.scatter(\n        x=X_train_std_transformed[y_train == target_class, 0],\n        y=X_train_std_transformed[y_train == target_class, 1],\n        color=color,\n        label=f\"class {target_class}\",\n        alpha=0.5,\n        marker=marker,\n    )\n\nax1.set_title(\"Unscaled training dataset after PCA\")\nax2.set_title(\"Standardized training dataset after PCA\")\n\nfor ax in (ax1, ax2):\n    ax.set_xlabel(\"1st principal component\")\n    ax.set_ylabel(\"2nd principal component\")\n    ax.legend(loc=\"upper right\")\n    ax.grid()\n\n_ = plt.tight_layout()\n\n# %%\n# From the plot above we observe that scaling the features before reducing the\n# dimensionality results in components with the same order of magnitude. In this\n# case it also improves the separability of the classes. Indeed, in the next\n# section we confirm that a better separability has a good repercussion on the\n# overall model's performance.\n#\n# Effect of rescaling on model's performance\n# ==========================================\n#\n# First we show how the optimal regularization of a\n# :class:`~sklearn.linear_model.LogisticRegressionCV` depends on the scaling or\n# non-scaling of the data:\n\nimport numpy as np\n\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.pipeline import make_pipeline\n\nCs = np.logspace(-5, 5, 20)\n\nunscaled_clf = make_pipeline(pca, LogisticRegressionCV(Cs=Cs))\nunscaled_clf.fit(X_train, y_train)\n\nscaled_clf = make_pipeline(scaler, pca, LogisticRegressionCV(Cs=Cs))\nscaled_clf.fit(X_train, y_train)\n\nprint(f\"Optimal C for the unscaled PCA: {unscaled_clf[-1].C_[0]:.4f}\\n\")\nprint(f\"Optimal C for the standardized data with PCA: {scaled_clf[-1].C_[0]:.2f}\")\n\n# %%\n# The need for regularization is higher (lower values of `C`) for the data that\n# was not scaled before applying PCA. We now evaluate the effect of scaling on\n# the accuracy and the mean log-loss of the optimal models:\n\nfrom sklearn.metrics import accuracy_score, log_loss\n\ny_pred = unscaled_clf.predict(X_test)\ny_pred_scaled = scaled_clf.predict(X_test)\ny_proba = unscaled_clf.predict_proba(X_test)\ny_proba_scaled = scaled_clf.predict_proba(X_test)\n\nprint(\"Test accuracy for the unscaled PCA\")\nprint(f\"{accuracy_score(y_test, y_pred):.2%}\\n\")\nprint(\"Test accuracy for the standardized data with PCA\")\nprint(f\"{accuracy_score(y_test, y_pred_scaled):.2%}\\n\")\nprint(\"Log-loss for the unscaled PCA\")\nprint(f\"{log_loss(y_test, y_proba):.3}\\n\")\nprint(\"Log-loss for the standardized data with PCA\")\nprint(f\"{log_loss(y_test, y_proba_scaled):.3}\")\n\n# %%",
                "filename": "examples/preprocessing/plot_scaling_importance.py",
                "start_index": 5853,
                "end_index": 8837,
                "start_line": 152,
                "end_line": 239,
                "max_line": 256,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n========================\nIncrementalPCA benchmark\n========================\n\nBenchmarks for IncrementalPCA\n\n\"\"\"\n\nimport gc\nfrom collections import defaultdict\nfrom time import time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.decomposition import PCA, IncrementalPCA\n\n\ndef plot_results(X, y, label):\n    plt.plot(X, y, label=label, marker=\"o\")\n\n\ndef benchmark(estimator, data):\n    gc.collect()\n    print(\"Benching %s\" % estimator)\n    t0 = time()\n    estimator.fit(data)\n    training_time = time() - t0\n    data_t = estimator.transform(data)\n    data_r = estimator.inverse_transform(data_t)\n    reconstruction_error = np.mean(np.abs(data - data_r))\n    return {\"time\": training_time, \"error\": reconstruction_error}\n\n\ndef plot_feature_times(all_times, batch_size, all_components, data):\n    plt.figure()\n    plot_results(all_components, all_times[\"pca\"], label=\"PCA\")\n    plot_results(\n        all_components, all_times[\"ipca\"], label=\"IncrementalPCA, bsize=%i\" % batch_size\n    )\n    plt.legend(loc=\"upper left\")\n    plt.suptitle(\n        \"Algorithm runtime vs. n_components\\n                  LFW, size %i x %i\"\n        % data.shape\n    )\n    plt.xlabel(\"Number of components (out of max %i)\" % data.shape[1])\n    plt.ylabel(\"Time (seconds)\")\n\n\ndef plot_feature_errors(all_errors, batch_size, all_components, data):\n    plt.figure()\n    plot_results(all_components, all_errors[\"pca\"], label=\"PCA\")\n    plot_results(\n        all_components,\n        all_errors[\"ipca\"],\n        label=\"IncrementalPCA, bsize=%i\" % batch_size,\n    )\n    plt.legend(loc=\"lower left\")\n    plt.suptitle(\"Algorithm error vs. n_components\\nLFW, size %i x %i\" % data.shape)\n    plt.xlabel(\"Number of components (out of max %i)\" % data.shape[1])\n    plt.ylabel(\"Mean absolute error\")\n\n\ndef plot_batch_times(all_times, n_features, all_batch_sizes, data):\n    plt.figure()\n    plot_results(all_batch_sizes, all_times[\"pca\"], label=\"PCA\")\n    plot_results(all_batch_sizes, all_times[\"ipca\"], label=\"IncrementalPCA\")\n    plt.legend(loc=\"lower left\")\n    plt.suptitle(\n        \"Algorithm runtime vs. batch_size for n_components %i\\n                  LFW,\"\n        \" size %i x %i\" % (n_features, data.shape[0], data.shape[1])\n    )\n    plt.xlabel(\"Batch size\")\n    plt.ylabel(\"Time (seconds)\")\n\n\ndef plot_batch_errors(all_errors, n_features, all_batch_sizes, data):\n    plt.figure()\n    plot_results(all_batch_sizes, all_errors[\"pca\"], label=\"PCA\")\n    plot_results(all_batch_sizes, all_errors[\"ipca\"], label=\"IncrementalPCA\")\n    plt.legend(loc=\"lower left\")\n    plt.suptitle(\n        \"Algorithm error vs. batch_size for n_components %i\\n                  LFW,\"\n        \" size %i x %i\" % (n_features, data.shape[0], data.shape[1])\n    )\n    plt.xlabel(\"Batch size\")\n    plt.ylabel(\"Mean absolute error\")",
                "filename": "benchmarks/bench_plot_incremental_pca.py",
                "start_index": 0,
                "end_index": 2842,
                "start_line": 1,
                "end_line": 89,
                "max_line": 159,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"Incremental Principal Components Analysis.\"\"\"\n\n# Author: Kyle Kastner <kastnerkyle@gmail.com>\n#         Giorgio Patrini\n# License: BSD 3 clause\n\nfrom numbers import Integral\n\nimport numpy as np\nfrom scipy import linalg, sparse\n\nfrom ..base import _fit_context\nfrom ..utils import gen_batches\nfrom ..utils._param_validation import Interval\nfrom ..utils.extmath import _incremental_mean_and_var, svd_flip\nfrom ._base import _BasePCA",
                "filename": "sklearn/decomposition/_incremental_pca.py",
                "start_index": 0,
                "end_index": 433,
                "start_line": 1,
                "end_line": 16,
                "max_line": 406,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"Principal component analysis (PCA).\n\n    Linear dimensionality reduction using Singular Value Decomposition of the\n    data to project it to a lower dimensional space. The input data is centered\n    but not scaled for each feature before applying the SVD.\n\n    It uses the LAPACK implementation of the full SVD or a randomized truncated\n    SVD by the method of Halko et al. 2009, depending on the shape of the input\n    data and the number of components to extract.\n\n    It can also use the scipy.sparse.linalg ARPACK implementation of the\n    truncated SVD.\n\n    Notice that this class does not support sparse input. See\n    :class:`TruncatedSVD` for an alternative with sparse data.\n\n    Read more in the :ref:`User Guide <PCA>`.\n\n    Parameters\n    ----------\n    n_components : int, float or 'mle', default=None\n        Number of components to keep.\n        if n_components is not set all components are kept::\n\n            n_components == min(n_samples, n_features)\n\n        If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n        MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n        will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n\n        If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n        number of components such that the amount of variance that needs to be\n        explained is greater than the percentage specified by n_components.\n\n        If ``svd_solver == 'arpack'``, the number of components must be\n        strictly less than the minimum of n_features and n_samples.\n\n        Hence, the None case results in::\n\n            n_components == min(n_samples, n_features) - 1\n\n    copy : bool, default=True\n        If False, data passed to fit are overwritten and running\n        fit(X).transform(X) will not yield the expected results,\n        use fit_transform(X) instead.\n\n    whiten : bool, default=False\n        When True (False by default) the `components_` vectors are multiplied\n        by the square root of n_samples and then divided by the singular values\n        to ensure uncorrelated outputs with unit component-wise variances.\n\n        Whitening will remove some information from the transformed signal\n        (the relative variance scales of the components) but can sometime\n        improve the predictive accuracy of the downstream estimators by\n        making their data respect some hard-wired assumptions.\n\n    svd_solver : {'auto', 'full', 'arpack', 'randomized'}, default='auto'\n        If auto :\n            The solver is selected by a default policy based on `X.shape` and\n            `n_components`: if the input data is larger than 500x500 and the\n            number of components to extract is lower than 80% of the smallest\n            dimension of the data, then the more efficient 'randomized'\n            method is enabled. Otherwise the exact full SVD is computed and\n            optionally truncated afterwards.\n        If full :\n            run exact full SVD calling the standard LAPACK solver via\n            `scipy.linalg.svd` and select the components by postprocessing\n        If arpack :\n            run SVD truncated to n_components calling ARPACK solver via\n            `scipy.sparse.linalg.svds`. It requires strictly\n            0 < n_components < min(X.shape)\n        If randomized :\n            run randomized SVD by the method of Halko et al.\n\n        .. versionadded:: 0.18.0\n\n    tol : float, default=0.0\n        Tolerance for singular values computed by svd_solver == 'arpack'.\n        Must be of range [0.0, infinity).\n\n        .. versionadded:: 0.18.0\n\n    iterated_power : int or 'auto', default='auto'\n        Number of iterations for the power method computed by\n        svd_solver == 'randomized'.\n        Must be of range [0, infinity).\n\n        .. versionadded:: 0.18.0\n\n    n_oversamples : int, default=10\n        This parameter is only relevant when `svd_solver=\"randomized\"`.\n        It corresponds to the additional number of random vectors to sample the\n        range of `X` so as to ensure proper conditioning. See\n        :func:`~sklearn.utils.extmath.randomized_svd` for more details.\n\n        .. versionadded:: 1.1\n\n    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'\n        Power iteration normalizer for randomized SVD solver.\n        Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd`\n        for more details.\n\n        .. versionadded:: 1.1\n\n    random_state : int, RandomState instance or None, default=None\n        Used when the 'arpack' or 'randomized' solvers are used. Pass an int\n        for reproducible results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n        .. versionadded:: 0.18.0\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Principal axes in feature space, representing the directions of\n        maximum variance in the data. Equivalently, the right singular\n        vectors of the centered input data, parallel to its eigenvectors.\n        The components are sorted by decreasing ``explained_variance_``.\n\n    explained_variance_ : ndarray of shape (n_components,)\n        The amount of variance explained by each of the selected components.\n        The variance estimation uses `n_samples - 1` degrees of freedom.\n\n        Equal to n_components largest eigenvalues\n        of the covariance matrix of X.\n\n        .. versionadded:: 0.18\n\n    explained_variance_ratio_ : ndarray of shape (n_components,)\n        Percentage of variance explained by each of the selected components.\n\n        If ``n_components`` is not set then all components are stored and the\n        sum of the ratios is equal to 1.0.\n\n    singular_values_ : ndarray of shape (n_components,)\n        The singular values corresponding to each of the selected components.\n        The singular values are equal to the 2-norms of the ``n_components``\n        variables in the lower-dimensional space.\n\n        .. versionadded:: 0.19\n\n    mean_ : ndarray of shape (n_features,)\n        Per-feature empirical mean, estimated from the training set.\n\n        Equal to `X.mean(axis=0)`.\n\n    n_components_ : int\n        The estimated number of components. When n_components is set\n        to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n        number is estimated from input data. Otherwise it equals the parameter\n        n_components, or the lesser value of n_features and n_samples\n        if n_components is None.\n\n    n_features_ : int\n        Number of features in the training data.\n\n    n_samples_ : int\n        Number of samples in the training data.\n\n    noise_variance_ : float\n        The estimated noise covariance following the Probabilistic PCA model\n        from Tipping and Bishop 1999. See \"Pattern Recognition and\n        Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n        http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n        compute the estimated data covariance and score samples.\n\n        Equal to the average of (min(n_features, n_samples) - n_components)\n        smallest eigenvalues of the covariance matrix of X.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    KernelPCA : Kernel Principal Component Analysis.\n    SparsePCA : Sparse Principal Component Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n    IncrementalPCA : Incremental Principal Component Analysis.\n\n    References\n    ----------\n    For n_components == 'mle', this class uses the method from:\n    `Minka, T. P.. \"Automatic choice of dimensionality for PCA\".\n    In NIPS, pp. 598-604 <https://tminka.github.io/papers/pca/minka-pca.pdf>`_\n\n    Implements the probabilistic PCA model from:\n    `Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\n    component analysis\". Journal of the Royal Statistical Society:\n    Series B (Statistical Methodology), 61(3), 611-622.\n    <http://www.miketipping.com/papers/met-mppca.pdf>`_\n    via the score and score_samples methods.\n\n    For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n\n    For svd_solver == 'randomized', see:\n    :doi:`Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n    \"Finding structure with randomness: Probabilistic algorithms for\n    constructing approximate matrix decompositions\".\n    SIAM review, 53(2), 217-288.\n    <10.1137/090771806>`\n    and also\n    :doi:`Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n    \"A randomized algorithm for the decomposition of matrices\".\n    Applied and Computational Harmonic Analysis, 30(1), 47-68.\n    <10.1016/j.acha.2010.02.003>`\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.decomposition import PCA\n    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    >>> pca = PCA(n_components=2)\n    >>> pca.fit(X)\n    PCA(n_components=2)\n    >>> print(pca.explained_variance_ratio_)\n    [0.9924... 0.0075...]\n    >>> print(pca.singular_values_)\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=2, svd_solver='full')\n    >>> pca.fit(X)\n    PCA(n_components=2, svd_solver='full')\n    >>> print(pca.explained_variance_ratio_)\n    [0.9924... 0.00755...]\n    >>> print(pca.singular_values_)\n    [6.30061... 0.54980...]\n\n    >>> pca = PCA(n_components=1, svd_solver='arpack')\n    >>> pca.fit(X)\n    PCA(n_components=1, svd_solver='arpack')\n    >>> print(pca.explained_variance_ratio_)\n    [0.99244...]\n    >>> print(pca.singular_values_)\n    [6.30061...]\n    \"\"\"",
                "filename": "sklearn/decomposition/_pca.py",
                "start_index": 3922,
                "end_index": 13751,
                "start_line": 122,
                "end_line": 719,
                "max_line": 724,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "class IncrementalPCA(_BasePCA):",
                "filename": "sklearn/decomposition/_incremental_pca.py",
                "start_index": 436,
                "end_index": 467,
                "start_line": 19,
                "end_line": 19,
                "max_line": 406,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "- :class:`linear_model.LogisticRegression`\n# - :class:`linear_model.GammaRegressor`\n# - :class:`linear_model.PoissonRegressor`\n# - :class:`linear_model.TweedieRegressor`\n\n# %%\n# :class:`~decomposition.MiniBatchNMF`: an online version of NMF\n# --------------------------------------------------------------\n# The new class :class:`~decomposition.MiniBatchNMF` implements a faster but\n# less accurate version of non-negative matrix factorization\n# (:class:`~decomposition.NMF`). :class:`~decomposition.MiniBatchNMF` divides the\n# data into mini-batches and optimizes the NMF model in an online manner by\n# cycling over the mini-batches, making it better suited for large datasets. In\n# particular, it implements `partial_fit`, which can be used for online\n# learning when the data is not readily available from the start, or when the\n# data does not fit into memory.\nimport numpy as np\nfrom sklearn.decomposition import MiniBatchNMF\n\nrng = np.random.RandomState(0)\nn_samples, n_features, n_components = 10, 10, 5\ntrue_W = rng.uniform(size=(n_samples, n_components))\ntrue_H = rng.uniform(size=(n_components, n_features))\nX = true_W @ true_H\n\nnmf = MiniBatchNMF(n_components=n_components, random_state=0)\n\nfor _ in range(10):\n    nmf.partial_fit(X)\n\nW = nmf.transform(X)\nH = nmf.components_\nX_reconstructed = W @ H\n\nprint(\n    f\"relative reconstruction error: \",\n    f\"{np.sum((X - X_reconstructed) ** 2) / np.sum(X**2):.5f}\",\n)\n\n# %%\n# :class:`~cluster.BisectingKMeans`: divide and cluster\n# -----------------------------------------------------\n# The new class :class:`~cluster.BisectingKMeans` is a variant of\n# :class:`~cluster.KMeans`, using divisive hierarchical clustering. Instead of\n# creating all centroids at once, centroids are picked progressively based on a\n# previous clustering: a cluster is split into two new clusters repeatedly\n# until the target number of clusters is reached, giving a hierarchical\n# structure to the clustering.\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans, BisectingKMeans\nimport matplotlib.pyplot as plt\n\nX, _ = make_blobs(n_samples=1000, centers=2, random_state=0)\n\nkm = KMeans(n_clusters=5, random_state=0, n_init=\"auto\").fit(X)\nbisect_km = BisectingKMeans(n_clusters=5, random_state=0).fit(X)\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].scatter(X[:, 0], X[:, 1], s=10, c=km.labels_)\nax[0].scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=20, c=\"r\")\nax[0].set_title(\"KMeans\")\n\nax[1].scatter(X[:, 0], X[:, 1], s=10, c=bisect_km.labels_)\nax[1].scatter(\n    bisect_km.cluster_centers_[:, 0], bisect_km.cluster_centers_[:, 1], s=20, c=\"r\"\n)\n_ = ax[1].set_title(\"BisectingKMeans\")",
                "filename": "examples/release_highlights/plot_release_highlights_1_1_0.py",
                "start_index": 5956,
                "end_index": 8623,
                "start_line": 163,
                "end_line": 229,
                "max_line": 229,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "# A clear difference in prediction accuracies is observed when the data is\n# scaled before :class:`~sklearn.decomposition.PCA`, as it vastly outperforms\n# the unscaled version. This corresponds to the intuition obtained from the plot\n# in the previous section, where the components become linearly separable when\n# scaling before using :class:`~sklearn.decomposition.PCA`.\n#\n# Notice that in this case the models with scaled features perform better than\n# the models with non-scaled features because all the variables are expected to\n# be predictive and we rather avoid some of them being comparatively ignored.\n#\n# If the variables in lower scales were not predictive, one may experience a\n# decrease of the performance after scaling the features: noisy features would\n# contribute more to the prediction after scaling and therefore scaling would\n# increase overfitting.\n#\n# Last but not least, we observe that one achieves a lower log-loss by means of\n# the scaling step.",
                "filename": "examples/preprocessing/plot_scaling_importance.py",
                "start_index": 8838,
                "end_index": 9811,
                "start_line": 240,
                "end_line": 256,
                "max_line": 256,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_classifiers_one_label(name, classifier_orig):\n    error_string_fit = \"Classifier can't train when only one class is present.\"\n    error_string_predict = \"Classifier can't predict when only one class is present.\"\n    rnd = np.random.RandomState(0)\n    X_train = rnd.uniform(size=(10, 3))\n    X_test = rnd.uniform(size=(10, 3))\n    y = np.ones(10)\n    # catch deprecation warnings\n    with ignore_warnings(category=FutureWarning):\n        classifier = clone(classifier_orig)\n        with raises(\n            ValueError, match=\"class\", may_pass=True, err_msg=error_string_fit\n        ) as cm:\n            classifier.fit(X_train, y)\n\n        if cm.raised_and_matched:\n            # ValueError was raised with proper error message\n            return\n\n        assert_array_equal(classifier.predict(X_test), y, err_msg=error_string_predict)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_classifiers_one_label_sample_weights(name, classifier_orig):\n    \"\"\"Check that classifiers accepting sample_weight fit or throws a ValueError with\n    an explicit message if the problem is reduced to one class.\n    \"\"\"\n    error_fit = (\n        f\"{name} failed when fitted on one label after sample_weight trimming. Error \"\n        \"message is not explicit, it should have 'class'.\"\n    )\n    error_predict = f\"{name} prediction results should only output the remaining class.\"\n    rnd = np.random.RandomState(0)\n    # X should be square for test on SVC with precomputed kernel\n    X_train = rnd.uniform(size=(10, 10))\n    X_test = rnd.uniform(size=(10, 10))\n    y = np.arange(10) % 2\n    sample_weight = y.copy()  # select a single class\n    classifier = clone(classifier_orig)\n\n    if has_fit_parameter(classifier, \"sample_weight\"):\n        match = [r\"\\bclass(es)?\\b\", error_predict]\n        err_type, err_msg = (AssertionError, ValueError), error_fit\n    else:\n        match = r\"\\bsample_weight\\b\"\n        err_type, err_msg = (TypeError, ValueError), None\n\n    with raises(err_type, match=match, may_pass=True, err_msg=err_msg) as cm:\n        classifier.fit(X_train, y, sample_weight=sample_weight)\n        if cm.raised_and_matched:\n            # raise the proper error type with the proper error message\n            return\n        # for estimators that do not fail, they should be able to predict the only\n        # class remaining during fit\n        assert_array_equal(\n            classifier.predict(X_test), np.ones(10), err_msg=error_predict\n        )",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 79371,
                "end_index": 81868,
                "start_line": 181,
                "end_line": 4639,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            }
        ],
        "classification": "other",
        "snippet_processor_task": {
            "sklearn/decomposition/_incremental_pca.py": [
                {
                    "chunk": {
                        "code": "\"\"\"Incremental Principal Components Analysis.\"\"\"\n\n# Author: Kyle Kastner <kastnerkyle@gmail.com>\n#         Giorgio Patrini\n# License: BSD 3 clause\n\nfrom numbers import Integral\n\nimport numpy as np\nfrom scipy import linalg, sparse\n\nfrom ..base import _fit_context\nfrom ..utils import gen_batches\nfrom ..utils._param_validation import Interval\nfrom ..utils.extmath import _incremental_mean_and_var, svd_flip\nfrom ._base import _BasePCA",
                        "filename": "sklearn/decomposition/_incremental_pca.py",
                        "start_index": 0,
                        "end_index": 433,
                        "start_line": 1,
                        "end_line": 16,
                        "max_line": 406,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the file header for '_incremental_pca.py' which is the file likely needing changes to address the IncrementalPCA issue."
                },
                {
                    "chunk": {
                        "code": "class IncrementalPCA(_BasePCA):",
                        "filename": "sklearn/decomposition/_incremental_pca.py",
                        "start_index": 436,
                        "end_index": 467,
                        "start_line": 19,
                        "end_line": 19,
                        "max_line": 406,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the class definition for IncrementalPCA, where the proposed fix to the issue would be implemented."
                }
            ]
        },
        "solution_task": "- Ensure `n_components` is less than or equal to the number of samples in each batch before calling `partial_fit` on `IncrementalPCA`.\n- Modify the batch processing logic to handle the case when the last batch is smaller than `n_components`.\n- If the last batch is smaller than `n_components`, either skip `partial_fit` for this batch or accumulate more samples until `n_components` is satisfied.",
        "bouncer_response": {
            "score": 7,
            "criteria": "The RESPONSE must provide actionable, relevant guidance to more parts of the PROMPT's described issue to score higher.",
            "supporting_evidence": "The RESPONSE provides actionable advice that is relevant to the described bug in the PROMPT. It suggests ensuring that `n_components` is less than or equal to the number of samples in each batch before calling `partial_fit`, which is a direct response to the error message provided in the PROMPT. Additionally, it recommends modifying the batch processing logic to handle cases when the last batch is smaller than `n_components`, which is a practical solution to the problem described. The RESPONSE also offers an alternative solution to skip `partial_fit` for the last batch if it's too small or to accumulate more samples until `n_components` is satisfied, which are both reasonable approaches to the issue. The inclusion of code snippets to check is helpful, as it directs the reader to the specific parts of the codebase that are likely to require changes. However, the RESPONSE does not address the first part of the PROMPT regarding the confusion about micro-average in classification reports, which is why it does not score higher.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711741352.4821608,
        "result": "- Ensure `n_components` is less than or equal to the number of samples in each batch before calling `partial_fit` on `IncrementalPCA`.\n- Modify the batch processing logic to handle the case when the last batch is smaller than `n_components`.\n- If the last batch is smaller than `n_components`, either skip `partial_fit` for this batch or accumulate more samples until `n_components` is satisfied.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [decomposition](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/decomposition)- [Lines 1 - 16](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/decomposition/_incremental_pca.py#L1-L16)\n\n This snippet contains the file header for '_incremental_pca.py' which is the file likely needing changes to address the IncrementalPCA issue. \n\n\n   - [Lines 19 - 19](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/decomposition/_incremental_pca.py#L19-L19)\n\n This snippet shows the class definition for IncrementalPCA, where the proposed fix to the issue would be implemented."
    }
}