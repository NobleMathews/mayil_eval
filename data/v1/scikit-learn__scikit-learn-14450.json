{
    "id": "scikit-learn__scikit-learn-14450",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
    "task_id": 7866407408915062635,
    "title": "PLS reports \"array must not contain nan\" if a feature is constant",
    "body": "Originally reported at https://github.com/scikit-learn/scikit-learn/issues/2089#issuecomment-152753095 by @Franck-Dernoncourt. Reproduce with:\r\n```py\r\nimport numpy as np\r\nimport sklearn.cross_decomposition\r\n\r\npls2 = sklearn.cross_decomposition.PLSRegression()\r\nxx = np.random.random((5,5))\r\nyy = np.zeros((5,5) ) \r\n\r\nyy[0,:] = [0,1,0,0,0]\r\nyy[1,:] = [0,0,0,1,0]\r\nyy[2,:] = [0,0,0,0,1]\r\n#yy[3,:] = [1,0,0,0,0] # Uncommenting this line solves the issue\r\n\r\npls2.fit(xx, yy)\r\n```\r\n\r\nThe obscure error message is due to the presence of a column containing only 0.\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.22.git",
    "requested_commit": "",
    "mayil_response": "Doesn't seem like we have fully grasped all the specifics of this ticket \ud83d\ude41.                   But we've outlined some ideas that might help you get started:\n\n- Check if the input arrays `xx` and `yy` to `PLSRegression.fit()` method contain any NaN values before calling `fit()`. If NaNs are present, handle them appropriately (e.g., imputation or removal).\n- If the issue is due to a constant feature in `xx` or `yy`, you can add a small amount of noise to the constant feature to make it non-constant. For example:\n  ```python\n  # Add a small amount of noise to a constant feature column 'i' in 'xx'\n  xx[:, i] += np.random.normal(0, 1e-4, size=xx.shape[0])\n  ```\n- Alternatively, remove the constant feature from the dataset before fitting the model.\n- Ensure that the workaround does not introduce any side effects that could affect the model's performance.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [preprocessing](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/preprocessing)- [Lines 1 - 84](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/preprocessing/_data.py#L1-L84)\n\n This snippet contains the function '_is_constant_feature' which is used to detect constant features that could be causing the issue with PLSRegression when a feature is constant.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [cross_decomposition](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/cross_decomposition)- [Lines 499 - 499](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/cross_decomposition/_pls.py#L499-L499)\n\n This snippet points to the PLSRegression class which is directly related to the issue as the error occurs when using the PLSRegression.fit method. \n\n\n   - [Lines 1 - 56](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/cross_decomposition/_pls.py#L1-L56)\n\n This snippet provides context on the PLS module and may contain relevant information for understanding the implementation details of PLSRegression.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0009725,
        "snippet_processor": 0.08512000000000002,
        "issue_star_creation": 0.049199999999999994,
        "issue_star_solver": 0.054560000000000004,
        "bouncer": 0.02766
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711739001.067692,
        "relevant_snippets": [
            {
                "code": "plt.title(\n    \"X comp. 1 vs X comp. 2 (test corr = %.2f)\"\n    % np.corrcoef(X_test_r[:, 0], X_test_r[:, 1])[0, 1]\n)\nplt.legend(loc=\"best\")\nplt.xticks(())\nplt.yticks(())\n\nplt.subplot(223)\nplt.scatter(Y_train_r[:, 0], Y_train_r[:, 1], label=\"train\", marker=\"*\", s=50)\nplt.scatter(Y_test_r[:, 0], Y_test_r[:, 1], label=\"test\", marker=\"*\", s=50)\nplt.xlabel(\"Y comp. 1\")\nplt.ylabel(\"Y comp. 2\")\nplt.title(\n    \"Y comp. 1 vs Y comp. 2 , (test corr = %.2f)\"\n    % np.corrcoef(Y_test_r[:, 0], Y_test_r[:, 1])[0, 1]\n)\nplt.legend(loc=\"best\")\nplt.xticks(())\nplt.yticks(())\nplt.show()\n\n# %%\n# PLS regression, with multivariate response, a.k.a. PLS2\n# -------------------------------------------------------\n\nfrom sklearn.cross_decomposition import PLSRegression\n\nn = 1000\nq = 3\np = 10\nX = np.random.normal(size=n * p).reshape((n, p))\nB = np.array([[1, 2] + [0] * (p - 2)] * q).T\n# each Yj = 1*X1 + 2*X2 + noize\nY = np.dot(X, B) + np.random.normal(size=n * q).reshape((n, q)) + 5\n\npls2 = PLSRegression(n_components=3)\npls2.fit(X, Y)\nprint(\"True B (such that: Y = XB + Err)\")\nprint(B)\n# compare pls2.coef_ with B\nprint(\"Estimated B\")\nprint(np.round(pls2.coef_, 1))\npls2.predict(X)\n\n# %%\n# PLS regression, with univariate response, a.k.a. PLS1\n# -----------------------------------------------------\n\nn = 1000\np = 10\nX = np.random.normal(size=n * p).reshape((n, p))\ny = X[:, 0] + 2 * X[:, 1] + np.random.normal(size=n * 1) + 5\npls1 = PLSRegression(n_components=3)\npls1.fit(X, y)\n# note that the number of components exceeds 1 (the dimension of y)\nprint(\"Estimated betas\")\nprint(np.round(pls1.coef_, 1))\n\n# %%\n# CCA (PLS mode B with symmetric deflation)\n# -----------------------------------------\n\nfrom sklearn.cross_decomposition import CCA\n\ncca = CCA(n_components=2)\ncca.fit(X_train, Y_train)\nX_train_r, Y_train_r = cca.transform(X_train, Y_train)\nX_test_r, Y_test_r = cca.transform(X_test, Y_test)",
                "filename": "examples/cross_decomposition/plot_compare_cross_decomposition.py",
                "start_index": 2919,
                "end_index": 4805,
                "start_line": 76,
                "end_line": 171,
                "max_line": 171,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"\n=========================================================\nSparsity Example: Fitting only features 1  and 2\n=========================================================\n\nFeatures 1 and 2 of the diabetes-dataset are fitted and\nplotted below. It illustrates that although feature 2\nhas a strong coefficient on the full model, it does not\ngive us much regarding `y` when compared to just feature 1.\n\"\"\"\n\n# Code source: Ga\u00ebl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\n# %%\n# First we load the diabetes dataset.\n\nimport numpy as np\n\nfrom sklearn import datasets\n\nX, y = datasets.load_diabetes(return_X_y=True)\nindices = (0, 1)\n\nX_train = X[:-20, indices]\nX_test = X[-20:, indices]\ny_train = y[:-20]\ny_test = y[-20:]\n\n# %%\n# Next we fit a linear regression model.\n\nfrom sklearn import linear_model\n\nols = linear_model.LinearRegression()\n_ = ols.fit(X_train, y_train)\n\n\n# %%\n# Finally we plot the figure from three different views.\n\nimport matplotlib.pyplot as plt\n\n# unused but required import for doing 3d projections with matplotlib < 3.2\nimport mpl_toolkits.mplot3d  # noqa: F401\n\n\ndef plot_figs(fig_num, elev, azim, X_train, clf):\n    fig = plt.figure(fig_num, figsize=(4, 3))\n    plt.clf()\n    ax = fig.add_subplot(111, projection=\"3d\", elev=elev, azim=azim)\n\n    ax.scatter(X_train[:, 0], X_train[:, 1], y_train, c=\"k\", marker=\"+\")\n    ax.plot_surface(\n        np.array([[-0.1, -0.1], [0.15, 0.15]]),\n        np.array([[-0.1, 0.15], [-0.1, 0.15]]),\n        clf.predict(\n            np.array([[-0.1, -0.1, 0.15, 0.15], [-0.1, 0.15, -0.1, 0.15]]).T\n        ).reshape((2, 2)),\n        alpha=0.5,\n    )\n    ax.set_xlabel(\"X_1\")\n    ax.set_ylabel(\"X_2\")\n    ax.set_zlabel(\"Y\")\n    ax.xaxis.set_ticklabels([])\n    ax.yaxis.set_ticklabels([])\n    ax.zaxis.set_ticklabels([])\n\n\n# Generate the three different figures from different views\nelev = 43.5\nazim = -110\nplot_figs(1, elev, azim, X_train, ols)\n\nelev = -0.5\nazim = 0\nplot_figs(2, elev, azim, X_train, ols)\n\nelev = -0.5\nazim = 90\nplot_figs(3, elev, azim, X_train, ols)\n\nplt.show()",
                "filename": "examples/linear_model/plot_ols_3d.py",
                "start_index": 0,
                "end_index": 2066,
                "start_line": 1,
                "end_line": 84,
                "max_line": 84,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"\n===================================\nCompare cross decomposition methods\n===================================\n\nSimple usage of various cross decomposition algorithms:\n\n- PLSCanonical\n- PLSRegression, with multivariate response, a.k.a. PLS2\n- PLSRegression, with univariate response, a.k.a. PLS1\n- CCA\n\nGiven 2 multivariate covarying two-dimensional datasets, X, and Y,\nPLS extracts the 'directions of covariance', i.e. the components of each\ndatasets that explain the most shared variance between both datasets.\nThis is apparent on the **scatterplot matrix** display: components 1 in\ndataset X and dataset Y are maximally correlated (points lie around the\nfirst diagonal). This is also true for components 2 in both dataset,\nhowever, the correlation across datasets for different components is\nweak: the point cloud is very spherical.\n\n\"\"\"\n\n# %%\n# Dataset based latent variables model\n# ------------------------------------\n\nimport numpy as np\n\nn = 500\n# 2 latents vars:\nl1 = np.random.normal(size=n)\nl2 = np.random.normal(size=n)\n\nlatents = np.array([l1, l1, l2, l2]).T\nX = latents + np.random.normal(size=4 * n).reshape((n, 4))\nY = latents + np.random.normal(size=4 * n).reshape((n, 4))\n\nX_train = X[: n // 2]\nY_train = Y[: n // 2]\nX_test = X[n // 2 :]\nY_test = Y[n // 2 :]\n\nprint(\"Corr(X)\")\nprint(np.round(np.corrcoef(X.T), 2))\nprint(\"Corr(Y)\")\nprint(np.round(np.corrcoef(Y.T), 2))\n\n# %%\n# Canonical (symmetric) PLS\n# -------------------------\n#\n# Transform data\n# ~~~~~~~~~~~~~~\n\nfrom sklearn.cross_decomposition import PLSCanonical\n\nplsca = PLSCanonical(n_components=2)\nplsca.fit(X_train, Y_train)\nX_train_r, Y_train_r = plsca.transform(X_train, Y_train)\nX_test_r, Y_test_r = plsca.transform(X_test, Y_test)\n\n# %%\n# Scatter plot of scores\n# ~~~~~~~~~~~~~~~~~~~~~~\n\nimport matplotlib.pyplot as plt\n\n# On diagonal plot X vs Y scores on each components\nplt.figure(figsize=(12, 8))\nplt.subplot(221)\nplt.scatter(X_train_r[:, 0], Y_train_r[:, 0], label=\"train\", marker=\"o\", s=25)\nplt.scatter(X_test_r[:, 0], Y_test_r[:, 0], label=\"test\", marker=\"o\", s=25)\nplt.xlabel(\"x scores\")\nplt.ylabel(\"y scores\")\nplt.title(\n    \"Comp. 1: X vs Y (test corr = %.2f)\"\n    % np.corrcoef(X_test_r[:, 0], Y_test_r[:, 0])[0, 1]\n)\nplt.xticks(())\nplt.yticks(())\nplt.legend(loc=\"best\")\n\nplt.subplot(224)\nplt.scatter(X_train_r[:, 1], Y_train_r[:, 1], label=\"train\", marker=\"o\", s=25)\nplt.scatter(X_test_r[:, 1], Y_test_r[:, 1], label=\"test\", marker=\"o\", s=25)\nplt.xlabel(\"x scores\")\nplt.ylabel(\"y scores\")\nplt.title(\n    \"Comp. 2: X vs Y (test corr = %.2f)\"\n    % np.corrcoef(X_test_r[:, 1], Y_test_r[:, 1])[0, 1]\n)\nplt.xticks(())\nplt.yticks(())\nplt.legend(loc=\"best\")\n\n# Off diagonal plot components 1 vs 2 for X and Y\nplt.subplot(222)\nplt.scatter(X_train_r[:, 0], X_train_r[:, 1], label=\"train\", marker=\"*\", s=50)\nplt.scatter(X_test_r[:, 0], X_test_r[:, 1], label=\"test\", marker=\"*\", s=50)\nplt.xlabel(\"X comp. 1\")\nplt.ylabel(\"X comp. 2\")",
                "filename": "examples/cross_decomposition/plot_compare_cross_decomposition.py",
                "start_index": 0,
                "end_index": 2918,
                "start_line": 1,
                "end_line": 102,
                "max_line": 171,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"\n==========================\nNon-negative least squares\n==========================\n\nIn this example, we fit a linear model with positive constraints on the\nregression coefficients and compare the estimated coefficients to a classic\nlinear regression.\n\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.metrics import r2_score\n\n# %%\n# Generate some random data\nnp.random.seed(42)\n\nn_samples, n_features = 200, 50\nX = np.random.randn(n_samples, n_features)\ntrue_coef = 3 * np.random.randn(n_features)\n# Threshold coefficients to render them non-negative\ntrue_coef[true_coef < 0] = 0\ny = np.dot(X, true_coef)\n\n# Add some noise\ny += 5 * np.random.normal(size=(n_samples,))\n\n# %%\n# Split the data in train set and test set\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n\n# %%\n# Fit the Non-Negative least squares.\nfrom sklearn.linear_model import LinearRegression\n\nreg_nnls = LinearRegression(positive=True)\ny_pred_nnls = reg_nnls.fit(X_train, y_train).predict(X_test)\nr2_score_nnls = r2_score(y_test, y_pred_nnls)\nprint(\"NNLS R2 score\", r2_score_nnls)\n\n# %%\n# Fit an OLS.\nreg_ols = LinearRegression()\ny_pred_ols = reg_ols.fit(X_train, y_train).predict(X_test)\nr2_score_ols = r2_score(y_test, y_pred_ols)\nprint(\"OLS R2 score\", r2_score_ols)\n\n\n# %%\n# Comparing the regression coefficients between OLS and NNLS, we can observe\n# they are highly correlated (the dashed line is the identity relation),\n# but the non-negative constraint shrinks some to 0.\n# The Non-Negative Least squares inherently yield sparse results.\n\nfig, ax = plt.subplots()\nax.plot(reg_ols.coef_, reg_nnls.coef_, linewidth=0, marker=\".\")\n\nlow_x, high_x = ax.get_xlim()\nlow_y, high_y = ax.get_ylim()\nlow = max(low_x, low_y)\nhigh = min(high_x, high_y)\nax.plot([low, high], [low, high], ls=\"--\", c=\".3\", alpha=0.5)\nax.set_xlabel(\"OLS regression coefficients\", fontweight=\"bold\")\nax.set_ylabel(\"NNLS regression coefficients\", fontweight=\"bold\")",
                "filename": "examples/linear_model/plot_nnls.py",
                "start_index": 0,
                "end_index": 2006,
                "start_line": 1,
                "end_line": 69,
                "max_line": 69,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Andreas Mueller <amueller@ais.uni-bonn.de>\n#          Eric Martin <eric@ericmart.in>\n#          Giorgio Patrini <giorgio.patrini@anu.edu.au>\n#          Eric Chang <ericchang2017@u.northwestern.edu>\n# License: BSD 3 clause\n\n\nimport warnings\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy import optimize, sparse, stats\nfrom scipy.special import boxcox\n\nfrom ..base import (\n    BaseEstimator,\n    ClassNamePrefixFeaturesOutMixin,\n    OneToOneFeatureMixin,\n    TransformerMixin,\n    _fit_context,\n)\nfrom ..utils import check_array\nfrom ..utils._param_validation import Interval, Options, StrOptions, validate_params\nfrom ..utils.extmath import _incremental_mean_and_var, row_norms\nfrom ..utils.sparsefuncs import (\n    incr_mean_variance_axis,\n    inplace_column_scale,\n    mean_variance_axis,\n    min_max_axis,\n)\nfrom ..utils.sparsefuncs_fast import (\n    inplace_csr_row_normalize_l1,\n    inplace_csr_row_normalize_l2,\n)\nfrom ..utils.validation import (\n    FLOAT_DTYPES,\n    _check_sample_weight,\n    check_is_fitted,\n    check_random_state,\n)\nfrom ._encoders import OneHotEncoder\n\nBOUNDS_THRESHOLD = 1e-7\n\n__all__ = [\n    \"Binarizer\",\n    \"KernelCenterer\",\n    \"MinMaxScaler\",\n    \"MaxAbsScaler\",\n    \"Normalizer\",\n    \"OneHotEncoder\",\n    \"RobustScaler\",\n    \"StandardScaler\",\n    \"QuantileTransformer\",\n    \"PowerTransformer\",\n    \"add_dummy_feature\",\n    \"binarize\",\n    \"normalize\",\n    \"scale\",\n    \"robust_scale\",\n    \"maxabs_scale\",\n    \"minmax_scale\",\n    \"quantile_transform\",\n    \"power_transform\",\n]\n\n\ndef _is_constant_feature(var, mean, n_samples):\n    \"\"\"Detect if a feature is indistinguishable from a constant feature.\n\n    The detection is based on its computed variance and on the theoretical\n    error bounds of the '2 pass algorithm' for variance computation.\n\n    See \"Algorithms for computing the sample variance: analysis and\n    recommendations\", by Chan, Golub, and LeVeque.\n    \"\"\"\n    # In scikit-learn, variance is always computed using float64 accumulators.\n    eps = np.finfo(np.float64).eps\n\n    upper_bound = n_samples * eps * var + (n_samples * mean * eps) ** 2\n    return var <= upper_bound",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 0,
                "end_index": 2318,
                "start_line": 1,
                "end_line": 84,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings\ndef check_fit2d_1feature(name, estimator_orig):\n    # check fitting a 2d array with only 1 feature either works or returns\n    # informative message\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(10, 1))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    y = X[:, 0].astype(int)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n    # ensure two labels in subsample for RandomizedLogisticRegression\n    if name == \"RandomizedLogisticRegression\":\n        estimator.sample_fraction = 1\n    # ensure non skipped trials for RANSACRegressor\n    if name == \"RANSACRegressor\":\n        estimator.residual_threshold = 0.5\n\n    y = _enforce_estimator_tags_y(estimator, y)\n    set_random_state(estimator, 1)\n\n    msgs = [r\"1 feature\\(s\\)\", \"n_features = 1\", \"n_features=1\"]\n\n    with raises(ValueError, match=msgs, may_pass=True):\n        estimator.fit(X, y)\n\n\n@ignore_warnings\ndef check_fit1d(name, estimator_orig):\n    # check fitting 1d X array raises a ValueError\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20))\n    y = X.astype(int)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n    with raises(ValueError):\n        estimator.fit(X, y)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_transformer_general(name, transformer, readonly_memmap=False):\n    X, y = make_blobs(\n        n_samples=30,\n        centers=[[0, 0, 0], [1, 1, 1]],\n        random_state=0,\n        n_features=2,\n        cluster_std=0.1,\n    )\n    X = StandardScaler().fit_transform(X)\n    X = _enforce_estimator_tags_X(transformer, X)\n\n    if readonly_memmap:\n        X, y = create_memmap_backed_data([X, y])\n\n    _check_transformer(name, transformer, X, y)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_transformer_data_not_an_array(name, transformer):\n    X, y = make_blobs(\n        n_samples=30,\n        centers=[[0, 0, 0], [1, 1, 1]],\n        random_state=0,\n        n_features=2,\n        cluster_std=0.1,\n    )\n    X = StandardScaler().fit_transform(X)\n    X = _enforce_estimator_tags_X(transformer, X)\n    this_X = _NotAnArray(X)\n    this_y = _NotAnArray(np.asarray(y))\n    _check_transformer(name, transformer, this_X, this_y)\n    # try the same with some list\n    _check_transformer(name, transformer, X.tolist(), y.tolist())",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 55999,
                "end_index": 58685,
                "start_line": 181,
                "end_line": 1729,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "class PLSRegression(_PLS):",
                "filename": "sklearn/cross_decomposition/_pls.py",
                "start_index": 17085,
                "end_index": 17111,
                "start_line": 499,
                "end_line": 499,
                "max_line": 1068,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"\n==================================================================\nPrincipal Component Regression vs Partial Least Squares Regression\n==================================================================\n\nThis example compares `Principal Component Regression\n<https://en.wikipedia.org/wiki/Principal_component_regression>`_ (PCR) and\n`Partial Least Squares Regression\n<https://en.wikipedia.org/wiki/Partial_least_squares_regression>`_ (PLS) on a\ntoy dataset. Our goal is to illustrate how PLS can outperform PCR when the\ntarget is strongly correlated with some directions in the data that have a\nlow variance.\n\nPCR is a regressor composed of two steps: first,\n:class:`~sklearn.decomposition.PCA` is applied to the training data, possibly\nperforming dimensionality reduction; then, a regressor (e.g. a linear\nregressor) is trained on the transformed samples. In\n:class:`~sklearn.decomposition.PCA`, the transformation is purely\nunsupervised, meaning that no information about the targets is used. As a\nresult, PCR may perform poorly in some datasets where the target is strongly\ncorrelated with *directions* that have low variance. Indeed, the\ndimensionality reduction of PCA projects the data into a lower dimensional\nspace where the variance of the projected data is greedily maximized along\neach axis. Despite them having the most predictive power on the target, the\ndirections with a lower variance will be dropped, and the final regressor\nwill not be able to leverage them.\n\nPLS is both a transformer and a regressor, and it is quite similar to PCR: it\nalso applies a dimensionality reduction to the samples before applying a\nlinear regressor to the transformed data. The main difference with PCR is\nthat the PLS transformation is supervised. Therefore, as we will see in this\nexample, it does not suffer from the issue we just mentioned.\n\n\"\"\"\n\n# %%\n# The data\n# --------\n#\n# We start by creating a simple dataset with two features. Before we even dive\n# into PCR and PLS, we fit a PCA estimator to display the two principal\n# components of this dataset, i.e. the two directions that explain the most\n# variance in the data.\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.decomposition import PCA\n\nrng = np.random.RandomState(0)\nn_samples = 500\ncov = [[3, 3], [3, 4]]\nX = rng.multivariate_normal(mean=[0, 0], cov=cov, size=n_samples)\npca = PCA(n_components=2).fit(X)\n\n\nplt.scatter(X[:, 0], X[:, 1], alpha=0.3, label=\"samples\")\nfor i, (comp, var) in enumerate(zip(pca.components_, pca.explained_variance_)):\n    comp = comp * var  # scale component by its variance explanation power\n    plt.plot(\n        [0, comp[0]],\n        [0, comp[1]],\n        label=f\"Component {i}\",\n        linewidth=5,\n        color=f\"C{i + 2}\",\n    )\nplt.gca().set(\n    aspect=\"equal\",\n    title=\"2-dimensional dataset with principal components\",\n    xlabel=\"first feature\",\n    ylabel=\"second feature\",\n)\nplt.legend()\nplt.show()\n\n# %%",
                "filename": "examples/cross_decomposition/plot_pcr_vs_pls.py",
                "start_index": 0,
                "end_index": 2933,
                "start_line": 1,
                "end_line": 159,
                "max_line": 166,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"\nThe :mod:`sklearn.pls` module implements Partial Least Squares (PLS).\n\"\"\"\n\n# Author: Edouard Duchesnay <edouard.duchesnay@cea.fr>\n# License: BSD 3 clause\n\nimport warnings\nfrom abc import ABCMeta, abstractmethod\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy.linalg import svd\n\nfrom ..base import (\n    BaseEstimator,\n    ClassNamePrefixFeaturesOutMixin,\n    MultiOutputMixin,\n    RegressorMixin,\n    TransformerMixin,\n    _fit_context,\n)\nfrom ..exceptions import ConvergenceWarning\nfrom ..utils import check_array, check_consistent_length\nfrom ..utils._param_validation import Interval, StrOptions\nfrom ..utils.extmath import svd_flip\nfrom ..utils.fixes import parse_version, sp_version\nfrom ..utils.validation import FLOAT_DTYPES, check_is_fitted\n\n__all__ = [\"PLSCanonical\", \"PLSRegression\", \"PLSSVD\"]\n\n\nif sp_version >= parse_version(\"1.7\"):\n    # Starting in scipy 1.7 pinv2 was deprecated in favor of pinv.\n    # pinv now uses the svd to compute the pseudo-inverse.\n    from scipy.linalg import pinv as pinv2\nelse:\n    from scipy.linalg import pinv2\n\n\ndef _pinv2_old(a):\n    # Used previous scipy pinv2 that was updated in:\n    # https://github.com/scipy/scipy/pull/10067\n    # We can not set `cond` or `rcond` for pinv2 in scipy >= 1.3 to keep the\n    # same behavior of pinv2 for scipy < 1.3, because the condition used to\n    # determine the rank is dependent on the output of svd.\n    u, s, vh = svd(a, full_matrices=False, check_finite=False)\n\n    t = u.dtype.char.lower()\n    factor = {\"f\": 1e3, \"d\": 1e6}\n    cond = np.max(s) * factor[t] * np.finfo(t).eps\n    rank = np.sum(s > cond)\n\n    u = u[:, :rank]\n    u /= s[:rank]\n    return np.transpose(np.conjugate(np.dot(u, vh[:rank])))",
                "filename": "sklearn/cross_decomposition/_pls.py",
                "start_index": 0,
                "end_index": 1716,
                "start_line": 1,
                "end_line": 56,
                "max_line": 1068,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_estimators_empty_data_messages(name, estimator_orig):\n    e = clone(estimator_orig)\n    set_random_state(e, 1)\n\n    X_zero_samples = np.empty(0).reshape(0, 3)\n    # The precise message can change depending on whether X or y is\n    # validated first. Let us test the type of exception only:\n    err_msg = (\n        f\"The estimator {name} does not raise a ValueError when an \"\n        \"empty data is used to train. Perhaps use check_array in train.\"\n    )\n    with raises(ValueError, err_msg=err_msg):\n        e.fit(X_zero_samples, [])\n\n    X_zero_features = np.empty(0).reshape(12, 0)\n    # the following y should be accepted by both classifiers and regressors\n    # and ignored by unsupervised models\n    y = _enforce_estimator_tags_y(e, np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]))\n    msg = r\"0 feature\\(s\\) \\(shape=\\(\\d*, 0\\)\\) while a minimum of \\d* \" \"is required.\"\n    with raises(ValueError, match=msg):\n        e.fit(X_zero_features, y)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_estimators_nan_inf(name, estimator_orig):\n    # Checks that Estimator X's do not contain NaN or inf.\n    rnd = np.random.RandomState(0)\n    X_train_finite = _enforce_estimator_tags_X(\n        estimator_orig, rnd.uniform(size=(10, 3))\n    )\n    X_train_nan = rnd.uniform(size=(10, 3))\n    X_train_nan[0, 0] = np.nan\n    X_train_inf = rnd.uniform(size=(10, 3))\n    X_train_inf[0, 0] = np.inf\n    y = np.ones(10)\n    y[:5] = 0\n    y = _enforce_estimator_tags_y(estimator_orig, y)\n    error_string_fit = f\"Estimator {name} doesn't check for NaN and inf in fit.\"\n    error_string_predict = f\"Estimator {name} doesn't check for NaN and inf in predict.\"\n    error_string_transform = (\n        f\"Estimator {name} doesn't check for NaN and inf in transform.\"\n    )\n    for X_train in [X_train_nan, X_train_inf]:\n        # catch deprecation warnings\n        with ignore_warnings(category=FutureWarning):\n            estimator = clone(estimator_orig)\n            set_random_state(estimator, 1)\n            # try to fit\n            with raises(ValueError, match=[\"inf\", \"NaN\"], err_msg=error_string_fit):\n                estimator.fit(X_train, y)\n            # actually fit\n            estimator.fit(X_train_finite, y)\n\n            # predict\n            if hasattr(estimator, \"predict\"):\n                with raises(\n                    ValueError,\n                    match=[\"inf\", \"NaN\"],\n                    err_msg=error_string_predict,\n                ):\n                    estimator.predict(X_train)\n\n            # transform\n            if hasattr(estimator, \"transform\"):\n                with raises(\n                    ValueError,\n                    match=[\"inf\", \"NaN\"],\n                    err_msg=error_string_transform,\n                ):\n                    estimator.transform(X_train)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 67270,
                "end_index": 70112,
                "start_line": 181,
                "end_line": 2043,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/preprocessing/_data.py": [
                {
                    "chunk": {
                        "code": "# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Andreas Mueller <amueller@ais.uni-bonn.de>\n#          Eric Martin <eric@ericmart.in>\n#          Giorgio Patrini <giorgio.patrini@anu.edu.au>\n#          Eric Chang <ericchang2017@u.northwestern.edu>\n# License: BSD 3 clause\n\n\nimport warnings\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy import optimize, sparse, stats\nfrom scipy.special import boxcox\n\nfrom ..base import (\n    BaseEstimator,\n    ClassNamePrefixFeaturesOutMixin,\n    OneToOneFeatureMixin,\n    TransformerMixin,\n    _fit_context,\n)\nfrom ..utils import check_array\nfrom ..utils._param_validation import Interval, Options, StrOptions, validate_params\nfrom ..utils.extmath import _incremental_mean_and_var, row_norms\nfrom ..utils.sparsefuncs import (\n    incr_mean_variance_axis,\n    inplace_column_scale,\n    mean_variance_axis,\n    min_max_axis,\n)\nfrom ..utils.sparsefuncs_fast import (\n    inplace_csr_row_normalize_l1,\n    inplace_csr_row_normalize_l2,\n)\nfrom ..utils.validation import (\n    FLOAT_DTYPES,\n    _check_sample_weight,\n    check_is_fitted,\n    check_random_state,\n)\nfrom ._encoders import OneHotEncoder\n\nBOUNDS_THRESHOLD = 1e-7\n\n__all__ = [\n    \"Binarizer\",\n    \"KernelCenterer\",\n    \"MinMaxScaler\",\n    \"MaxAbsScaler\",\n    \"Normalizer\",\n    \"OneHotEncoder\",\n    \"RobustScaler\",\n    \"StandardScaler\",\n    \"QuantileTransformer\",\n    \"PowerTransformer\",\n    \"add_dummy_feature\",\n    \"binarize\",\n    \"normalize\",\n    \"scale\",\n    \"robust_scale\",\n    \"maxabs_scale\",\n    \"minmax_scale\",\n    \"quantile_transform\",\n    \"power_transform\",\n]\n\n\ndef _is_constant_feature(var, mean, n_samples):\n    \"\"\"Detect if a feature is indistinguishable from a constant feature.\n\n    The detection is based on its computed variance and on the theoretical\n    error bounds of the '2 pass algorithm' for variance computation.\n\n    See \"Algorithms for computing the sample variance: analysis and\n    recommendations\", by Chan, Golub, and LeVeque.\n    \"\"\"\n    # In scikit-learn, variance is always computed using float64 accumulators.\n    eps = np.finfo(np.float64).eps\n\n    upper_bound = n_samples * eps * var + (n_samples * mean * eps) ** 2\n    return var <= upper_bound",
                        "filename": "sklearn/preprocessing/_data.py",
                        "start_index": 0,
                        "end_index": 2318,
                        "start_line": 1,
                        "end_line": 84,
                        "max_line": 3519,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the function '_is_constant_feature' which is used to detect constant features that could be causing the issue with PLSRegression when a feature is constant."
                }
            ],
            "sklearn/cross_decomposition/_pls.py": [
                {
                    "chunk": {
                        "code": "class PLSRegression(_PLS):",
                        "filename": "sklearn/cross_decomposition/_pls.py",
                        "start_index": 17085,
                        "end_index": 17111,
                        "start_line": 499,
                        "end_line": 499,
                        "max_line": 1068,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet points to the PLSRegression class which is directly related to the issue as the error occurs when using the PLSRegression.fit method."
                },
                {
                    "chunk": {
                        "code": "\"\"\"\nThe :mod:`sklearn.pls` module implements Partial Least Squares (PLS).\n\"\"\"\n\n# Author: Edouard Duchesnay <edouard.duchesnay@cea.fr>\n# License: BSD 3 clause\n\nimport warnings\nfrom abc import ABCMeta, abstractmethod\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy.linalg import svd\n\nfrom ..base import (\n    BaseEstimator,\n    ClassNamePrefixFeaturesOutMixin,\n    MultiOutputMixin,\n    RegressorMixin,\n    TransformerMixin,\n    _fit_context,\n)\nfrom ..exceptions import ConvergenceWarning\nfrom ..utils import check_array, check_consistent_length\nfrom ..utils._param_validation import Interval, StrOptions\nfrom ..utils.extmath import svd_flip\nfrom ..utils.fixes import parse_version, sp_version\nfrom ..utils.validation import FLOAT_DTYPES, check_is_fitted\n\n__all__ = [\"PLSCanonical\", \"PLSRegression\", \"PLSSVD\"]\n\n\nif sp_version >= parse_version(\"1.7\"):\n    # Starting in scipy 1.7 pinv2 was deprecated in favor of pinv.\n    # pinv now uses the svd to compute the pseudo-inverse.\n    from scipy.linalg import pinv as pinv2\nelse:\n    from scipy.linalg import pinv2\n\n\ndef _pinv2_old(a):\n    # Used previous scipy pinv2 that was updated in:\n    # https://github.com/scipy/scipy/pull/10067\n    # We can not set `cond` or `rcond` for pinv2 in scipy >= 1.3 to keep the\n    # same behavior of pinv2 for scipy < 1.3, because the condition used to\n    # determine the rank is dependent on the output of svd.\n    u, s, vh = svd(a, full_matrices=False, check_finite=False)\n\n    t = u.dtype.char.lower()\n    factor = {\"f\": 1e3, \"d\": 1e6}\n    cond = np.max(s) * factor[t] * np.finfo(t).eps\n    rank = np.sum(s > cond)\n\n    u = u[:, :rank]\n    u /= s[:rank]\n    return np.transpose(np.conjugate(np.dot(u, vh[:rank])))",
                        "filename": "sklearn/cross_decomposition/_pls.py",
                        "start_index": 0,
                        "end_index": 1716,
                        "start_line": 1,
                        "end_line": 56,
                        "max_line": 1068,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet provides context on the PLS module and may contain relevant information for understanding the implementation details of PLSRegression."
                }
            ]
        },
        "solution_task": "- Check if the input arrays `xx` and `yy` to `PLSRegression.fit()` method contain any NaN values before calling `fit()`. If NaNs are present, handle them appropriately (e.g., imputation or removal).\n- If the issue is due to a constant feature in `xx` or `yy`, you can add a small amount of noise to the constant feature to make it non-constant. For example:\n  ```python\n  # Add a small amount of noise to a constant feature column 'i' in 'xx'\n  xx[:, i] += np.random.normal(0, 1e-4, size=xx.shape[0])\n  ```\n- Alternatively, remove the constant feature from the dataset before fitting the model.\n- Ensure that the workaround does not introduce any side effects that could affect the model's performance.",
        "bouncer_response": {
            "score": 3,
            "criteria": "The RESPONSE should provide actionable, relevant guidance to more parts of the PROMPT's described issue to score higher. RESPONSES that are tangentially relevant but lack actionable content should receive scores between 1 and 3.",
            "supporting_evidence": "The RESPONSE suggests checking for NaN values and handling them, which is not relevant to the issue described in the PROMPT, as the error is specifically due to a constant feature, not NaN values. The suggestion to add noise to a constant feature or remove it is somewhat actionable but does not directly address the error message or the specific context of the PLSRegression model. The links to the code snippets are potentially useful for someone looking to understand the underlying implementation, but they do not provide a direct solution to the problem. The RESPONSE fails to address the specific bug described in the PROMPT, which is the presence of a column containing only zeros, not NaNs, and how this leads to an error in the PLSRegression model.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711739212.873313,
        "result": "Doesn't seem like we have fully grasped all the specifics of this ticket \ud83d\ude41.                   But we've outlined some ideas that might help you get started:\n\n- Check if the input arrays `xx` and `yy` to `PLSRegression.fit()` method contain any NaN values before calling `fit()`. If NaNs are present, handle them appropriately (e.g., imputation or removal).\n- If the issue is due to a constant feature in `xx` or `yy`, you can add a small amount of noise to the constant feature to make it non-constant. For example:\n  ```python\n  # Add a small amount of noise to a constant feature column 'i' in 'xx'\n  xx[:, i] += np.random.normal(0, 1e-4, size=xx.shape[0])\n  ```\n- Alternatively, remove the constant feature from the dataset before fitting the model.\n- Ensure that the workaround does not introduce any side effects that could affect the model's performance.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [preprocessing](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/preprocessing)- [Lines 1 - 84](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/preprocessing/_data.py#L1-L84)\n\n This snippet contains the function '_is_constant_feature' which is used to detect constant features that could be causing the issue with PLSRegression when a feature is constant.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [cross_decomposition](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/cross_decomposition)- [Lines 499 - 499](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/cross_decomposition/_pls.py#L499-L499)\n\n This snippet points to the PLSRegression class which is directly related to the issue as the error occurs when using the PLSRegression.fit method. \n\n\n   - [Lines 1 - 56](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/cross_decomposition/_pls.py#L1-L56)\n\n This snippet provides context on the PLS module and may contain relevant information for understanding the implementation details of PLSRegression."
    }
}