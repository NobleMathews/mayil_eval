{
    "id": "scikit-learn__scikit-learn-12760",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
    "task_id": 485038766325940391,
    "title": "Davies Bouldin measure: division by zero",
    "body": "I'm facing a problem with the davies bouldin measure.\r\n\r\nThis is the warning that I get:\r\n\r\n    .local/lib/python3.7/site-packages/sklearn/metrics/cluster/unsupervised.py:342: RuntimeWarning: divide by zero encountered in true_divide \r\n        score = (intra_dists[:, None] + intra_dists) / centroid_distances\r\n\r\nThis is the implementation in sklearn:\r\n```python\r\ndef davies_bouldin_score(X, labels):\r\n    X, labels = check_X_y(X, labels)\r\n    le = LabelEncoder()\r\n    labels = le.fit_transform(labels)\r\n    n_samples, _ = X.shape\r\n    n_labels = len(le.classes_)\r\n    check_number_of_labels(n_labels, n_samples)\r\n\r\n    intra_dists = np.zeros(n_labels)\r\n    centroids = np.zeros((n_labels, len(X[0])), dtype=np.float)\r\n    for k in range(n_labels):\r\n        cluster_k = safe_indexing(X, labels == k)\r\n        centroid = cluster_k.mean(axis=0)\r\n        centroids[k] = centroid\r\n        intra_dists[k] = np.average(pairwise_distances(\r\n            cluster_k, [centroid]))\r\n\r\n    centroid_distances = pairwise_distances(centroids)\r\n\r\n    if np.allclose(intra_dists, 0) or np.allclose(centroid_distances, 0):\r\n        return 0.0\r\n\r\n    score = (intra_dists[:, None] + intra_dists) / centroid_distances\r\n    score[score == np.inf] = np.nan\r\n    return np.mean(np.nanmax(score, axis=1))\r\n```\r\nI found [another implementation](https://stackoverflow.com/a/48189218) on stack overflow:\r\n\r\n```python\r\nfrom scipy.spatial.distance import pdist, euclidean\r\n\r\ndef DaviesBouldin(X, labels):\r\n    n_cluster = len(np.bincount(labels))\r\n    cluster_k = [X[labels == k] for k in range(n_cluster)]\r\n    centroids = [np.mean(k, axis = 0) for k in cluster_k]\r\n    variances = [np.mean([euclidean(p, centroids[i]) for p in k]) for i, k in enumerate(cluster_k)]\r\n    db = []\r\n\r\n    for i in range(n_cluster):\r\n        for j in range(n_cluster):\r\n            if j != i:\r\n                db.append((variances[i] + variances[j]) / euclidean(centroids[i], centroids[j]))\r\n\r\n    return(np.max(db) / n_cluster)\r\n```\r\nWith this implementation I don't get any warnings, but the results differ:\r\n```\r\nStack overflow implementation: 0.012955275662036738\r\n/home/luca/.local/lib/python3.7/site-packages/sklearn/metrics/cluster/unsupervised.py:342: RuntimeWarning: divide by zero encountered in true_divide\r\n  score = (intra_dists[:, None] + intra_dists) / centroid_distances\r\nSklearn implementation: 2.1936185396772485\r\n\r\n```\nDavies Bouldin measure: division by zero\nI'm facing a problem with the davies bouldin measure.\r\n\r\nThis is the warning that I get:\r\n\r\n    .local/lib/python3.7/site-packages/sklearn/metrics/cluster/unsupervised.py:342: RuntimeWarning: divide by zero encountered in true_divide \r\n        score = (intra_dists[:, None] + intra_dists) / centroid_distances\r\n\r\nThis is the implementation in sklearn:\r\n```python\r\ndef davies_bouldin_score(X, labels):\r\n    X, labels = check_X_y(X, labels)\r\n    le = LabelEncoder()\r\n    labels = le.fit_transform(labels)\r\n    n_samples, _ = X.shape\r\n    n_labels = len(le.classes_)\r\n    check_number_of_labels(n_labels, n_samples)\r\n\r\n    intra_dists = np.zeros(n_labels)\r\n    centroids = np.zeros((n_labels, len(X[0])), dtype=np.float)\r\n    for k in range(n_labels):\r\n        cluster_k = safe_indexing(X, labels == k)\r\n        centroid = cluster_k.mean(axis=0)\r\n        centroids[k] = centroid\r\n        intra_dists[k] = np.average(pairwise_distances(\r\n            cluster_k, [centroid]))\r\n\r\n    centroid_distances = pairwise_distances(centroids)\r\n\r\n    if np.allclose(intra_dists, 0) or np.allclose(centroid_distances, 0):\r\n        return 0.0\r\n\r\n    score = (intra_dists[:, None] + intra_dists) / centroid_distances\r\n    score[score == np.inf] = np.nan\r\n    return np.mean(np.nanmax(score, axis=1))\r\n```\r\nI found [another implementation](https://stackoverflow.com/a/48189218) on stack overflow:\r\n\r\n```python\r\nfrom scipy.spatial.distance import pdist, euclidean\r\n\r\ndef DaviesBouldin(X, labels):\r\n    n_cluster = len(np.bincount(labels))\r\n    cluster_k = [X[labels == k] for k in range(n_cluster)]\r\n    centroids = [np.mean(k, axis = 0) for k in cluster_k]\r\n    variances = [np.mean([euclidean(p, centroids[i]) for p in k]) for i, k in enumerate(cluster_k)]\r\n    db = []\r\n\r\n    for i in range(n_cluster):\r\n        for j in range(n_cluster):\r\n            if j != i:\r\n                db.append((variances[i] + variances[j]) / euclidean(centroids[i], centroids[j]))\r\n\r\n    return(np.max(db) / n_cluster)\r\n```\r\nWith this implementation I don't get any warnings, but the results differ:\r\n```\r\nStack overflow implementation: 0.012955275662036738\r\n/home/luca/.local/lib/python3.7/site-packages/sklearn/metrics/cluster/unsupervised.py:342: RuntimeWarning: divide by zero encountered in true_divide\r\n  score = (intra_dists[:, None] + intra_dists) / centroid_distances\r\nSklearn implementation: 2.1936185396772485\r\n\r\n```\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.21.git",
    "requested_commit": "",
    "mayil_response": "- Check if `centroid_distances` contains zeros before division and handle it appropriately to avoid division by zero.\n- Modify the `davies_bouldin_score` function in the `_unsupervised.py` file to include a check for zeros in `centroid_distances` before the division operation.\n- If `centroid_distances` contains zeros, replace them with a small positive value (e.g., `np.finfo(float).eps`) or handle them in a way that avoids division by zero.\n\n```python\n# In the davies_bouldin_score function, before the division operation:\ncentroid_distances[centroid_distances == 0] = np.finfo(float).eps\n# Then proceed with the division\nscores = np.max(combined_intra_dists / centroid_distances, axis=1)\n```\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [metrics](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/metrics) \u2192 [cluster](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/metrics/cluster)- [Lines 43 - 425](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/metrics/cluster/_unsupervised.py#L43-L425)\n\n This snippet contains the implementation of the davies_bouldin_score function in sklearn, which is where the division by zero issue occurs. It is directly relevant to the issue and needs to be examined for potential fixes.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0004185,
        "snippet_processor": 0.08203,
        "issue_star_creation": 0.032729999999999995,
        "issue_star_solver": 0.07929000000000001,
        "bouncer": 0.030320000000000003
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711741138.456527,
        "relevant_snippets": [
            {
                "code": "@validate_params(\n    {\n        \"X\": [\"array-like\"],\n        \"labels\": [\"array-like\"],\n    },\n    prefer_skip_nested_validation=True,\n)\ndef davies_bouldin_score(X, labels):\n    \"\"\"Compute the Davies-Bouldin score.\n\n    The score is defined as the average similarity measure of each cluster with\n    its most similar cluster, where similarity is the ratio of within-cluster\n    distances to between-cluster distances. Thus, clusters which are farther\n    apart and less dispersed will result in a better score.\n\n    The minimum score is zero, with lower values indicating better clustering.\n\n    Read more in the :ref:`User Guide <davies-bouldin_index>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        A list of ``n_features``-dimensional data points. Each row corresponds\n        to a single data point.\n\n    labels : array-like of shape (n_samples,)\n        Predicted labels for each sample.\n\n    Returns\n    -------\n    score: float\n        The resulting Davies-Bouldin score.\n\n    References\n    ----------\n    .. [1] Davies, David L.; Bouldin, Donald W. (1979).\n       `\"A Cluster Separation Measure\"\n       <https://ieeexplore.ieee.org/document/4766909>`__.\n       IEEE Transactions on Pattern Analysis and Machine Intelligence.\n       PAMI-1 (2): 224-227\n    \"\"\"\n    X, labels = check_X_y(X, labels)\n    le = LabelEncoder()\n    labels = le.fit_transform(labels)\n    n_samples, _ = X.shape\n    n_labels = len(le.classes_)\n    check_number_of_labels(n_labels, n_samples)\n\n    intra_dists = np.zeros(n_labels)\n    centroids = np.zeros((n_labels, len(X[0])), dtype=float)\n    for k in range(n_labels):\n        cluster_k = _safe_indexing(X, labels == k)\n        centroid = cluster_k.mean(axis=0)\n        centroids[k] = centroid\n        intra_dists[k] = np.average(pairwise_distances(cluster_k, [centroid]))\n\n    centroid_distances = pairwise_distances(centroids)\n\n    if np.allclose(intra_dists, 0) or np.allclose(centroid_distances, 0):\n        return 0.0\n\n    centroid_distances[centroid_distances == 0] = np.inf\n    combined_intra_dists = intra_dists[:, None] + intra_dists\n    scores = np.max(combined_intra_dists / centroid_distances, axis=1)\n    return np.mean(scores)",
                "filename": "sklearn/metrics/cluster/_unsupervised.py",
                "start_index": 13558,
                "end_index": 15805,
                "start_line": 43,
                "end_line": 425,
                "max_line": 425,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\nBenchmark scikit-learn's Ward implement compared to SciPy's\n\"\"\"\n\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.cluster import hierarchy\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nward = AgglomerativeClustering(n_clusters=3, linkage=\"ward\")\n\nn_samples = np.logspace(0.5, 3, 9)\nn_features = np.logspace(1, 3.5, 7)\nN_samples, N_features = np.meshgrid(n_samples, n_features)\nscikits_time = np.zeros(N_samples.shape)\nscipy_time = np.zeros(N_samples.shape)\n\nfor i, n in enumerate(n_samples):\n    for j, p in enumerate(n_features):\n        X = np.random.normal(size=(n, p))\n        t0 = time.time()\n        ward.fit(X)\n        scikits_time[j, i] = time.time() - t0\n        t0 = time.time()\n        hierarchy.ward(X)\n        scipy_time[j, i] = time.time() - t0\n\nratio = scikits_time / scipy_time\n\nplt.figure(\"scikit-learn Ward's method benchmark results\")\nplt.imshow(np.log(ratio), aspect=\"auto\", origin=\"lower\")\nplt.colorbar()\nplt.contour(\n    ratio,\n    levels=[\n        1,\n    ],\n    colors=\"k\",\n)\nplt.yticks(range(len(n_features)), n_features.astype(int))\nplt.ylabel(\"N features\")\nplt.xticks(range(len(n_samples)), n_samples.astype(int))\nplt.xlabel(\"N samples\")\nplt.title(\"Scikit's time, in units of scipy time (log)\")\nplt.show()",
                "filename": "benchmarks/bench_plot_ward.py",
                "start_index": 0,
                "end_index": 1269,
                "start_line": 1,
                "end_line": 48,
                "max_line": 48,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\nDBSCAN: Density-Based Spatial Clustering of Applications with Noise\n\"\"\"\n\n# Author: Robert Layton <robertlayton@gmail.com>\n#         Joel Nothman <joel.nothman@gmail.com>\n#         Lars Buitinck\n#\n# License: BSD 3 clause\n\nimport warnings\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy import sparse\n\nfrom ..base import BaseEstimator, ClusterMixin, _fit_context\nfrom ..metrics.pairwise import _VALID_METRICS\nfrom ..neighbors import NearestNeighbors\nfrom ..utils._param_validation import Interval, StrOptions\nfrom ..utils.validation import _check_sample_weight\nfrom ._dbscan_inner import dbscan_inner",
                "filename": "sklearn/cluster/_dbscan.py",
                "start_index": 0,
                "end_index": 620,
                "start_line": 1,
                "end_line": 22,
                "max_line": 452,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"Utilities to evaluate the clustering performance of models.\n\nFunctions named as *_score return a scalar value to maximize: the higher the\nbetter.\n\"\"\"\n\n# Authors: Olivier Grisel <olivier.grisel@ensta.org>\n#          Wei LI <kuantkid@gmail.com>\n#          Diego Molla <dmolla-aliod@gmail.com>\n#          Arnaud Fouchet <foucheta@gmail.com>\n#          Thierry Guillemot <thierry.guillemot.work@gmail.com>\n#          Gregory Stupp <stuppie@gmail.com>\n#          Joel Nothman <joel.nothman@gmail.com>\n#          Arya McCarthy <arya@jhu.edu>\n#          Uwe F Mayer <uwe_f_mayer@yahoo.com>\n# License: BSD 3 clause\n\n\nimport warnings\nfrom math import log\nfrom numbers import Real\n\nimport numpy as np\nfrom scipy import sparse as sp\n\nfrom ...utils._param_validation import Interval, StrOptions, validate_params\nfrom ...utils.multiclass import type_of_target\nfrom ...utils.validation import check_array, check_consistent_length\nfrom ._expected_mutual_info_fast import expected_mutual_information\n\n\ndef check_clusterings(labels_true, labels_pred):\n    \"\"\"Check that the labels arrays are 1D and of same dimension.\n\n    Parameters\n    ----------\n    labels_true : array-like of shape (n_samples,)\n        The true labels.\n\n    labels_pred : array-like of shape (n_samples,)\n        The predicted labels.\n    \"\"\"\n    labels_true = check_array(\n        labels_true,\n        ensure_2d=False,\n        ensure_min_samples=0,\n        dtype=None,\n    )\n\n    labels_pred = check_array(\n        labels_pred,\n        ensure_2d=False,\n        ensure_min_samples=0,\n        dtype=None,\n    )\n\n    type_label = type_of_target(labels_true)\n    type_pred = type_of_target(labels_pred)\n\n    if \"continuous\" in (type_pred, type_label):\n        msg = (\n            \"Clustering metrics expects discrete values but received\"\n            f\" {type_label} values for label, and {type_pred} values \"\n            \"for target\"\n        )\n        warnings.warn(msg, UserWarning)\n\n    # input checks\n    if labels_true.ndim != 1:\n        raise ValueError(\"labels_true must be 1D: shape is %r\" % (labels_true.shape,))\n    if labels_pred.ndim != 1:\n        raise ValueError(\"labels_pred must be 1D: shape is %r\" % (labels_pred.shape,))\n    check_consistent_length(labels_true, labels_pred)\n\n    return labels_true, labels_pred\n\n\ndef _generalized_average(U, V, average_method):\n    \"\"\"Return a particular mean of two numbers.\"\"\"\n    if average_method == \"min\":\n        return min(U, V)\n    elif average_method == \"geometric\":\n        return np.sqrt(U * V)\n    elif average_method == \"arithmetic\":\n        return np.mean([U, V])\n    elif average_method == \"max\":\n        return max(U, V)\n    else:\n        raise ValueError(\n            \"'average_method' must be 'min', 'geometric', 'arithmetic', or 'max'\"\n        )",
                "filename": "sklearn/metrics/cluster/_supervised.py",
                "start_index": 0,
                "end_index": 2770,
                "start_line": 1,
                "end_line": 869,
                "max_line": 1273,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "if metric == \"precomputed\":\n        error_msg = ValueError(\n            \"The precomputed distance matrix contains non-zero \"\n            \"elements on the diagonal. Use np.fill_diagonal(X, 0).\"\n        )\n        if X.dtype.kind == \"f\":\n            atol = np.finfo(X.dtype).eps * 100\n            if np.any(np.abs(X.diagonal()) > atol):\n                raise error_msg\n        elif np.any(X.diagonal() != 0):  # integral dtype\n            raise error_msg\n\n    le = LabelEncoder()\n    labels = le.fit_transform(labels)\n    n_samples = len(labels)\n    label_freqs = np.bincount(labels)\n    check_number_of_labels(len(le.classes_), n_samples)\n\n    kwds[\"metric\"] = metric\n    reduce_func = functools.partial(\n        _silhouette_reduce, labels=labels, label_freqs=label_freqs\n    )\n    results = zip(*pairwise_distances_chunked(X, reduce_func=reduce_func, **kwds))\n    intra_clust_dists, inter_clust_dists = results\n    intra_clust_dists = np.concatenate(intra_clust_dists)\n    inter_clust_dists = np.concatenate(inter_clust_dists)\n\n    denom = (label_freqs - 1).take(labels, mode=\"clip\")\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        intra_clust_dists /= denom\n\n    sil_samples = inter_clust_dists - intra_clust_dists\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        sil_samples /= np.maximum(intra_clust_dists, inter_clust_dists)\n    # nan values are for clusters of size 1, and should be 0\n    return np.nan_to_num(sil_samples)",
                "filename": "sklearn/metrics/cluster/_unsupervised.py",
                "start_index": 10370,
                "end_index": 11829,
                "start_line": 126,
                "end_line": 295,
                "max_line": 425,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_clustering(name, clusterer_orig, readonly_memmap=False):\n    clusterer = clone(clusterer_orig)\n    X, y = make_blobs(n_samples=50, random_state=1)\n    X, y = shuffle(X, y, random_state=7)\n    X = StandardScaler().fit_transform(X)\n    rng = np.random.RandomState(7)\n    X_noise = np.concatenate([X, rng.uniform(low=-3, high=3, size=(5, 2))])\n\n    if readonly_memmap:\n        X, y, X_noise = create_memmap_backed_data([X, y, X_noise])\n\n    n_samples, n_features = X.shape\n    # catch deprecation and neighbors warnings\n    if hasattr(clusterer, \"n_clusters\"):\n        clusterer.set_params(n_clusters=3)\n    set_random_state(clusterer)\n    if name == \"AffinityPropagation\":\n        clusterer.set_params(preference=-100)\n        clusterer.set_params(max_iter=100)\n\n    # fit\n    clusterer.fit(X)\n    # with lists\n    clusterer.fit(X.tolist())\n\n    pred = clusterer.labels_\n    assert pred.shape == (n_samples,)\n    assert adjusted_rand_score(pred, y) > 0.4\n    if _safe_tags(clusterer, key=\"non_deterministic\"):\n        return\n    set_random_state(clusterer)\n    with warnings.catch_warnings(record=True):\n        pred2 = clusterer.fit_predict(X)\n    assert_array_equal(pred, pred2)\n\n    # fit_predict(X) and labels_ should be of type int\n    assert pred.dtype in [np.dtype(\"int32\"), np.dtype(\"int64\")]\n    assert pred2.dtype in [np.dtype(\"int32\"), np.dtype(\"int64\")]\n\n    # Add noise to X to test the possible values of the labels\n    labels = clusterer.fit_predict(X_noise)\n\n    # There should be at least one sample in every cluster. Equivalently\n    # labels_ should contain all the consecutive values between its\n    # min and its max.\n    labels_sorted = np.unique(labels)\n    assert_array_equal(\n        labels_sorted, np.arange(labels_sorted[0], labels_sorted[-1] + 1)\n    )\n\n    # Labels are expected to start at 0 (no noise) or -1 (if noise)\n    assert labels_sorted[0] in [0, -1]\n    # Labels should be less than n_clusters - 1\n    if hasattr(clusterer, \"n_clusters\"):\n        n_clusters = getattr(clusterer, \"n_clusters\")\n        assert n_clusters - 1 >= labels_sorted[-1]\n    # else labels should be less than max(labels_) which is necessarily true\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_clusterer_compute_labels_predict(name, clusterer_orig):\n    \"\"\"Check that predict is invariant of compute_labels.\"\"\"\n    X, y = make_blobs(n_samples=20, random_state=0)\n    clusterer = clone(clusterer_orig)\n    set_random_state(clusterer)\n\n    if hasattr(clusterer, \"compute_labels\"):\n        # MiniBatchKMeans\n        X_pred1 = clusterer.fit(X).predict(X)\n        clusterer.set_params(compute_labels=False)\n        X_pred2 = clusterer.fit(X).predict(X)\n        assert_array_equal(X_pred1, X_pred2)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 76608,
                "end_index": 79368,
                "start_line": 181,
                "end_line": 2304,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n================================================================\nComparing different hierarchical linkage methods on toy datasets\n================================================================\n\nThis example shows characteristics of different linkage\nmethods for hierarchical clustering on datasets that are\n\"interesting\" but still in 2D.\n\nThe main observations to make are:\n\n- single linkage is fast, and can perform well on\n  non-globular data, but it performs poorly in the\n  presence of noise.\n- average and complete linkage perform well on\n  cleanly separated globular clusters, but have mixed\n  results otherwise.\n- Ward is the most effective method for noisy data.\n\nWhile these examples give some intuition about the\nalgorithms, this intuition might not apply to very high\ndimensional data.\n\n\"\"\"\n\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets\nfrom sklearn.preprocessing import StandardScaler\n\nnp.random.seed(0)\n\n# %%\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n\nn_samples = 1500\nnoisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\nno_structure = np.random.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n# %%\n# Run the clustering and plot\n\n# Set up cluster parameters\nplt.figure(figsize=(9 * 1.3 + 2, 14.5))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\n\ndatasets = [\n    (noisy_circles, {\"n_clusters\": 2}),\n    (noisy_moons, {\"n_clusters\": 2}),\n    (varied, {\"n_neighbors\": 2}),\n    (aniso, {\"n_neighbors\": 2}),\n    (blobs, {}),\n    (no_structure, {}),\n]",
                "filename": "examples/cluster/plot_linkage_comparison.py",
                "start_index": 0,
                "end_index": 2308,
                "start_line": 1,
                "end_line": 159,
                "max_line": 175,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n===================================\nDemo of DBSCAN clustering algorithm\n===================================\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) finds core\nsamples in regions of high density and expands clusters from them. This\nalgorithm is good for data which contains clusters of similar density.\n\nSee the :ref:`sphx_glr_auto_examples_cluster_plot_cluster_comparison.py` example\nfor a demo of different clustering algorithms on 2D datasets.\n\n\"\"\"\n\n# %%\n# Data generation\n# ---------------\n#\n# We use :class:`~sklearn.datasets.make_blobs` to create 3 synthetic clusters.\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\n\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = make_blobs(\n    n_samples=750, centers=centers, cluster_std=0.4, random_state=0\n)\n\nX = StandardScaler().fit_transform(X)\n\n# %%\n# We can visualize the resulting data:\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(X[:, 0], X[:, 1])\nplt.show()\n\n# %%\n# Compute DBSCAN\n# --------------\n#\n# One can access the labels assigned by :class:`~sklearn.cluster.DBSCAN` using\n# the `labels_` attribute. Noisy samples are given the label math:`-1`.\n\nimport numpy as np\n\nfrom sklearn import metrics\nfrom sklearn.cluster import DBSCAN\n\ndb = DBSCAN(eps=0.3, min_samples=10).fit(X)\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters_)\nprint(\"Estimated number of noise points: %d\" % n_noise_)\n\n# %%\n# Clustering algorithms are fundamentally unsupervised learning methods.\n# However, since :class:`~sklearn.datasets.make_blobs` gives access to the true\n# labels of the synthetic clusters, it is possible to use evaluation metrics\n# that leverage this \"supervised\" ground truth information to quantify the\n# quality of the resulting clusters. Examples of such metrics are the\n# homogeneity, completeness, V-measure, Rand-Index, Adjusted Rand-Index and\n# Adjusted Mutual Information (AMI).\n#\n# If the ground truth labels are not known, evaluation can only be performed\n# using the model results itself. In that case, the Silhouette Coefficient comes\n# in handy.\n#\n# For more information, see the\n# :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`\n# example or the :ref:`clustering_evaluation` module.\n\nprint(f\"Homogeneity: {metrics.homogeneity_score(labels_true, labels):.3f}\")\nprint(f\"Completeness: {metrics.completeness_score(labels_true, labels):.3f}\")\nprint(f\"V-measure: {metrics.v_measure_score(labels_true, labels):.3f}\")\nprint(f\"Adjusted Rand Index: {metrics.adjusted_rand_score(labels_true, labels):.3f}\")\nprint(\n    \"Adjusted Mutual Information:\"\n    f\" {metrics.adjusted_mutual_info_score(labels_true, labels):.3f}\"\n)\nprint(f\"Silhouette Coefficient: {metrics.silhouette_score(X, labels):.3f}\")\n\n# %%\n# Plot results\n# ------------\n#",
                "filename": "examples/cluster/plot_dbscan.py",
                "start_index": 0,
                "end_index": 2976,
                "start_line": 1,
                "end_line": 103,
                "max_line": 129,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n=========================================================\nComparing different clustering algorithms on toy datasets\n=========================================================\n\nThis example shows characteristics of different\nclustering algorithms on datasets that are \"interesting\"\nbut still in 2D. With the exception of the last dataset,\nthe parameters of each of these dataset-algorithm pairs\nhas been tuned to produce good clustering results. Some\nalgorithms are more sensitive to parameter values than\nothers.\n\nThe last dataset is an example of a 'null' situation for\nclustering: the data is homogeneous, and there is no good\nclustering. For this example, the null dataset uses the\nsame parameters as the dataset in the row above it, which\nrepresents a mismatch in the parameter values and the\ndata structure.\n\nWhile these examples give some intuition about the\nalgorithms, this intuition might not apply to very high\ndimensional data.\n\n\"\"\"\n\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\nnp.random.seed(0)\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 500\nnoisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\nno_structure = np.random.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(9 * 2 + 3, 13))\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n}",
                "filename": "examples/cluster/plot_cluster_comparison.py",
                "start_index": 0,
                "end_index": 2572,
                "start_line": 1,
                "end_line": 213,
                "max_line": 273,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "score_funcs = [\n    (\"V-measure\", metrics.v_measure_score),\n    (\"Rand index\", metrics.rand_score),\n    (\"ARI\", metrics.adjusted_rand_score),\n    (\"MI\", metrics.mutual_info_score),\n    (\"NMI\", metrics.normalized_mutual_info_score),\n    (\"AMI\", metrics.adjusted_mutual_info_score),\n]\n\n# %%\n# First experiment: fixed ground truth labels and growing number of clusters\n# --------------------------------------------------------------------------\n#\n# We first define a function that creates uniformly-distributed random labeling.\n\nimport numpy as np\n\nrng = np.random.RandomState(0)\n\n\ndef random_labels(n_samples, n_classes):\n    return rng.randint(low=0, high=n_classes, size=n_samples)\n\n\n# %%\n# Another function will use the `random_labels` function to create a fixed set\n# of ground truth labels (`labels_a`) distributed in `n_classes` and then score\n# several sets of randomly \"predicted\" labels (`labels_b`) to assess the\n# variability of a given metric at a given `n_clusters`.\n\n\ndef fixed_classes_uniform_labelings_scores(\n    score_func, n_samples, n_clusters_range, n_classes, n_runs=5\n):\n    scores = np.zeros((len(n_clusters_range), n_runs))\n    labels_a = random_labels(n_samples=n_samples, n_classes=n_classes)\n\n    for i, n_clusters in enumerate(n_clusters_range):\n        for j in range(n_runs):\n            labels_b = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            scores[i, j] = score_func(labels_a, labels_b)\n    return scores\n\n\n# %%\n# In this first example we set the number of classes (true number of clusters) to\n# `n_classes=10`. The number of clusters varies over the values provided by\n# `n_clusters_range`.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nn_samples = 1000\nn_classes = 10\nn_clusters_range = np.linspace(2, 100, 10).astype(int)\nplots = []\nnames = []\n\nsns.color_palette(\"colorblind\")\nplt.figure(1)\n\nfor marker, (score_name, score_func) in zip(\"d^vx.,\", score_funcs):\n    scores = fixed_classes_uniform_labelings_scores(\n        score_func, n_samples, n_clusters_range, n_classes=n_classes\n    )\n    plots.append(\n        plt.errorbar(\n            n_clusters_range,\n            scores.mean(axis=1),\n            scores.std(axis=1),\n            alpha=0.8,\n            linewidth=1,\n            marker=marker,\n        )[0]\n    )\n    names.append(score_name)\n\nplt.title(\n    \"Clustering measures for random uniform labeling\\n\"\n    f\"against reference assignment with {n_classes} classes\"\n)\nplt.xlabel(f\"Number of clusters (Number of samples is fixed to {n_samples})\")\nplt.ylabel(\"Score value\")\nplt.ylim(bottom=-0.05, top=1.05)\nplt.legend(plots, names, bbox_to_anchor=(0.5, 0.5))\nplt.show()\n\n# %%\n# The Rand index saturates for `n_clusters` > `n_classes`. Other non-adjusted\n# measures such as the V-Measure show a linear dependency between the number of\n# clusters and the number of samples.\n#\n# Adjusted for chance measure, such as ARI and AMI, display some random",
                "filename": "examples/cluster/plot_adjusted_for_chance_measures.py",
                "start_index": 2733,
                "end_index": 5655,
                "start_line": 60,
                "end_line": 152,
                "max_line": 230,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/metrics/cluster/_unsupervised.py": [
                {
                    "chunk": {
                        "code": "@validate_params(\n    {\n        \"X\": [\"array-like\"],\n        \"labels\": [\"array-like\"],\n    },\n    prefer_skip_nested_validation=True,\n)\ndef davies_bouldin_score(X, labels):\n    \"\"\"Compute the Davies-Bouldin score.\n\n    The score is defined as the average similarity measure of each cluster with\n    its most similar cluster, where similarity is the ratio of within-cluster\n    distances to between-cluster distances. Thus, clusters which are farther\n    apart and less dispersed will result in a better score.\n\n    The minimum score is zero, with lower values indicating better clustering.\n\n    Read more in the :ref:`User Guide <davies-bouldin_index>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        A list of ``n_features``-dimensional data points. Each row corresponds\n        to a single data point.\n\n    labels : array-like of shape (n_samples,)\n        Predicted labels for each sample.\n\n    Returns\n    -------\n    score: float\n        The resulting Davies-Bouldin score.\n\n    References\n    ----------\n    .. [1] Davies, David L.; Bouldin, Donald W. (1979).\n       `\"A Cluster Separation Measure\"\n       <https://ieeexplore.ieee.org/document/4766909>`__.\n       IEEE Transactions on Pattern Analysis and Machine Intelligence.\n       PAMI-1 (2): 224-227\n    \"\"\"\n    X, labels = check_X_y(X, labels)\n    le = LabelEncoder()\n    labels = le.fit_transform(labels)\n    n_samples, _ = X.shape\n    n_labels = len(le.classes_)\n    check_number_of_labels(n_labels, n_samples)\n\n    intra_dists = np.zeros(n_labels)\n    centroids = np.zeros((n_labels, len(X[0])), dtype=float)\n    for k in range(n_labels):\n        cluster_k = _safe_indexing(X, labels == k)\n        centroid = cluster_k.mean(axis=0)\n        centroids[k] = centroid\n        intra_dists[k] = np.average(pairwise_distances(cluster_k, [centroid]))\n\n    centroid_distances = pairwise_distances(centroids)\n\n    if np.allclose(intra_dists, 0) or np.allclose(centroid_distances, 0):\n        return 0.0\n\n    centroid_distances[centroid_distances == 0] = np.inf\n    combined_intra_dists = intra_dists[:, None] + intra_dists\n    scores = np.max(combined_intra_dists / centroid_distances, axis=1)\n    return np.mean(scores)",
                        "filename": "sklearn/metrics/cluster/_unsupervised.py",
                        "start_index": 13558,
                        "end_index": 15805,
                        "start_line": 43,
                        "end_line": 425,
                        "max_line": 425,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the implementation of the davies_bouldin_score function in sklearn, which is where the division by zero issue occurs. It is directly relevant to the issue and needs to be examined for potential fixes."
                }
            ]
        },
        "solution_task": "- Check if `centroid_distances` contains zeros before division and handle it appropriately to avoid division by zero.\n- Modify the `davies_bouldin_score` function in the `_unsupervised.py` file to include a check for zeros in `centroid_distances` before the division operation.\n- If `centroid_distances` contains zeros, replace them with a small positive value (e.g., `np.finfo(float).eps`) or handle them in a way that avoids division by zero.\n\n```python\n# In the davies_bouldin_score function, before the division operation:\ncentroid_distances[centroid_distances == 0] = np.finfo(float).eps\n# Then proceed with the division\nscores = np.max(combined_intra_dists / centroid_distances, axis=1)\n```",
        "bouncer_response": {
            "score": 7,
            "criteria": "The RESPONSE offers clear and comprehensive guidance or solutions for the described bug.",
            "supporting_evidence": "The RESPONSE provides a direct solution to the division by zero problem by suggesting a check for zeros in the `centroid_distances` array before performing the division. It also offers a practical solution by replacing zeros with a small positive value to avoid the division by zero error. The code snippet provided is actionable and relevant to the issue described in the PROMPT. However, the RESPONSE does not address the discrepancy in the results between the two implementations, which is also part of the PROMPT's concern. Therefore, while the RESPONSE is highly relevant and offers clear guidance for resolving the division by zero error, it is not complete in addressing all aspects of the described bug.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711741330.114655,
        "result": "- Check if `centroid_distances` contains zeros before division and handle it appropriately to avoid division by zero.\n- Modify the `davies_bouldin_score` function in the `_unsupervised.py` file to include a check for zeros in `centroid_distances` before the division operation.\n- If `centroid_distances` contains zeros, replace them with a small positive value (e.g., `np.finfo(float).eps`) or handle them in a way that avoids division by zero.\n\n```python\n# In the davies_bouldin_score function, before the division operation:\ncentroid_distances[centroid_distances == 0] = np.finfo(float).eps\n# Then proceed with the division\nscores = np.max(combined_intra_dists / centroid_distances, axis=1)\n```\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [metrics](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/metrics) \u2192 [cluster](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/metrics/cluster)- [Lines 43 - 425](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/metrics/cluster/_unsupervised.py#L43-L425)\n\n This snippet contains the implementation of the davies_bouldin_score function in sklearn, which is where the division by zero issue occurs. It is directly relevant to the issue and needs to be examined for potential fixes."
    }
}