{
    "id": "pytest-dev__pytest-9133",
    "repo_name": "testbed/pytest-dev__pytest__7.0",
    "task_id": 3572705325446021016,
    "title": "Add a `deselected` parameter to `assert_outcomes()`",
    "body": "<!--\r\nThanks for suggesting a feature!\r\n\r\nQuick check-list while suggesting features:\r\n-->\r\n\r\n#### What's the problem this feature will solve?\r\n<!-- What are you trying to do, that you are unable to achieve with pytest as it currently stands? -->\r\nI'd like to be able to use `pytester.RunResult.assert_outcomes()` to check deselected count.\r\n\r\n#### Describe the solution you'd like\r\n<!-- A clear and concise description of what you want to happen. -->\r\nAdd a `deselected` parameter to `pytester.RunResult.assert_outcomes()`\r\n\r\n<!-- Provide examples of real-world use cases that this would enable and how it solves the problem described above. -->\r\nPlugins that use `pytest_collection_modifyitems` to change the `items` and add change the deselected items need to be tested. Using `assert_outcomes()` to check the deselected count would be helpful.\r\n\r\n#### Alternative Solutions\r\n<!-- Have you tried to workaround the problem using a pytest plugin or other tools? Or a different approach to solving this issue? Please elaborate here. -->\r\nUse `parseoutcomes()` instead of `assert_outcomes()`. `parseoutcomes()` returns a dictionary that includes `deselected`, if there are any.\r\nHowever, if we have a series of tests, some that care about deselected, and some that don't, then we may have some tests using `assert_outcomes()` and some using `parseoutcomes()`, which is slightly annoying.\r\n\r\n#### Additional context\r\n<!-- Add any other context, links, etc. about the feature here. -->\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/pytest-dev__pytest__7.0.git",
    "requested_commit": "",
    "mayil_response": "To implement the feature request of adding a `deselected` parameter to `pytester.RunResult.assert_outcomes()`, follow these steps:\n\n- Locate the `pytester` plugin/module in the pytest codebase.\n- Find the `RunResult` class within the `pytester` module.\n- Add a new parameter `deselected=None` to the signature of the `assert_outcomes()` method.\n- Within the `assert_outcomes()` method, add logic to check the `deselected` count if the `deselected` parameter is not `None`.\n- Use the existing `parseoutcomes()` method to get the current outcomes, which should include the `deselected` count.\n- Compare the `deselected` count from `parseoutcomes()` with the provided `deselected` parameter.\n- Raise an assertion error if the counts do not match.\n- Add or update unit tests for `assert_outcomes()` to cover the new `deselected` parameter.\n\nHere is a pseudo-code example of how the updated `assert_outcomes()` method might look:\n\n```python\nclass RunResult:\n    # ... existing code ...\n\n    def assert_outcomes(self, passed=None, skipped=None, failed=None, deselected=None):\n        outcomes = self.parseoutcomes()\n        if passed is not None and outcomes.get('passed', 0) != passed:\n            raise AssertionError(f\"Expected {passed} passed tests, got {outcomes.get('passed', 0)}\")\n        # ... similar checks for skipped and failed ...\n        if deselected is not None and outcomes.get('deselected', 0) != deselected:\n            raise AssertionError(f\"Expected {deselected} deselected tests, got {outcomes.get('deselected', 0)}\")\n        # ... rest of the method ...\n```\n\nAnd an example unit test:\n\n```python\ndef test_assert_outcomes_with_deselected(pytester):\n    # Setup pytester to run a test that will be deselected\n    # ...\n\n    result = pytester.runpytest()\n    # Assume 1 test is deselected\n    result.assert_outcomes(deselected=1)\n```\n\nRemember to document the new parameter in the method's docstring and update any relevant user documentation.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0002455,
        "snippet_processor": 0.04286,
        "issue_star_creation": 0.0429,
        "issue_star_solver": 0.04152,
        "bouncer": 0.03196
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711734459.089857,
        "relevant_snippets": [
            {
                "code": "blank_issues_enabled: false\ncontact_links:\n  - name: \u2753 Support Question\n    url: https://github.com/pytest-dev/pytest/discussions\n    about: Use GitHub's new Discussions feature for questions",
                "filename": ".github/ISSUE_TEMPLATE/config.yml",
                "start_index": 0,
                "end_index": 191,
                "start_line": 1,
                "end_line": 5,
                "max_line": 5,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.0",
                "sha": ""
            },
            {
                "code": "rtd:\n  project: pytest",
                "filename": ".github/config.yml",
                "start_index": 0,
                "end_index": 22,
                "start_line": 1,
                "end_line": 2,
                "max_line": 2,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.0",
                "sha": ""
            },
            {
                "code": "import pytest\n\n\n@pytest.fixture(scope=\"module\", params=range(966))\ndef foo(request):\n    return request.param\n\n\ndef test_it(foo):\n    pass\n\n\ndef test_it2(foo):\n    pass",
                "filename": "bench/manyparam.py",
                "start_index": 0,
                "end_index": 168,
                "start_line": 1,
                "end_line": 14,
                "max_line": 14,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.0",
                "sha": ""
            },
            {
                "code": "# info:\n# * https://help.github.com/en/articles/displaying-a-sponsor-button-in-your-repository\n# * https://tidelift.com/subscription/how-to-connect-tidelift-with-github\ntidelift: pypi/pytest\nopen_collective: pytest",
                "filename": ".github/FUNDING.yml",
                "start_index": 0,
                "end_index": 214,
                "start_line": 1,
                "end_line": 5,
                "max_line": 5,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.0",
                "sha": ""
            },
            {
                "code": "\"\"\"\nThis script is part of the pytest release process which is triggered manually in the Actions\ntab of the repository.\n\nThe user will need to enter the base branch to start the release from (for example\n``6.1.x`` or ``main``) and if it should be a major release.\n\nThe appropriate version will be obtained based on the given branch automatically.\n\nAfter that, it will create a release using the `release` tox environment, and push a new PR.\n\n**Token**: currently the token from the GitHub Actions is used, pushed with\n`pytest bot <pytestbot@gmail.com>` commit author.\n\"\"\"\nimport argparse\nimport re\nfrom pathlib import Path\nfrom subprocess import check_call\nfrom subprocess import check_output\nfrom subprocess import run\n\nfrom colorama import Fore\nfrom colorama import init\nfrom github3.repos import Repository\n\n\nclass InvalidFeatureRelease(Exception):\n    pass\n\n\nSLUG = \"pytest-dev/pytest\"\n\nPR_BODY = \"\"\"\\\nCreated automatically from manual trigger.\n\nOnce all builds pass and it has been **approved** by one or more maintainers, the build\ncan be released by pushing a tag `{version}` to this repository.\n\"\"\"\n\n\ndef login(token: str) -> Repository:\n    import github3\n\n    github = github3.login(token=token)\n    owner, repo = SLUG.split(\"/\")\n    return github.repository(owner, repo)",
                "filename": "scripts/prepare-release-pr.py",
                "start_index": 0,
                "end_index": 1281,
                "start_line": 1,
                "end_line": 46,
                "max_line": 174,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.0",
                "sha": ""
            },
            {
                "code": "import pytest\n\nSKIP = True\n\n\n@pytest.mark.parametrize(\"x\", range(5000))\ndef test_foo(x):\n    if SKIP:\n        pytest.skip(\"heh\")",
                "filename": "bench/skip.py",
                "start_index": 0,
                "end_index": 128,
                "start_line": 1,
                "end_line": 9,
                "max_line": 9,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.0",
                "sha": ""
            },
            {
                "code": "import datetime\nimport pathlib\nimport re\nfrom textwrap import dedent\nfrom textwrap import indent\n\nimport packaging.version\nimport platformdirs\nimport tabulate\nimport wcwidth\nfrom requests_cache import CachedResponse\nfrom requests_cache import CachedSession\nfrom requests_cache import OriginalResponse\nfrom requests_cache import SQLiteCache\nfrom tqdm import tqdm\n\n\nFILE_HEAD = r\"\"\"\n.. Note this file is autogenerated by scripts/update-plugin-list.py - usually weekly via github action\n\n.. _plugin-list:\n\nPlugin List\n===========\n\nPyPI projects that match \"pytest-\\*\" are considered plugins and are listed\nautomatically together with a manually-maintained list in `the source\ncode <https://github.com/pytest-dev/pytest/blob/main/scripts/update-plugin-list.py>`_.\nPackages classified as inactive are excluded.\n\n.. The following conditional uses a different format for this list when\n   creating a PDF, because otherwise the table gets far too wide for the\n   page.\n\n\"\"\"\nDEVELOPMENT_STATUS_CLASSIFIERS = (\n    \"Development Status :: 1 - Planning\",\n    \"Development Status :: 2 - Pre-Alpha\",\n    \"Development Status :: 3 - Alpha\",\n    \"Development Status :: 4 - Beta\",\n    \"Development Status :: 5 - Production/Stable\",\n    \"Development Status :: 6 - Mature\",\n    \"Development Status :: 7 - Inactive\",\n)\nADDITIONAL_PROJECTS = {  # set of additional projects to consider as plugins\n    \"logassert\",\n    \"nuts\",\n}\n\n\ndef escape_rst(text: str) -> str:\n    \"\"\"Rudimentary attempt to escape special RST characters to appear as\n    plain text.\"\"\"\n    text = (\n        text.replace(\"*\", \"\\\\*\")\n        .replace(\"<\", \"\\\\<\")\n        .replace(\">\", \"\\\\>\")\n        .replace(\"`\", \"\\\\`\")\n    )\n    text = re.sub(r\"_\\b\", \"\", text)\n    return text\n\n\ndef project_response_with_refresh(\n    session: CachedSession, name: str, last_serial: int\n) -> OriginalResponse | CachedResponse:\n    \"\"\"Get a http cached pypi project\n\n    force refresh in case of last serial mismatch\n    \"\"\"\n\n    response = session.get(f\"https://pypi.org/pypi/{name}/json\")\n    if int(response.headers.get(\"X-PyPI-Last-Serial\", -1)) != last_serial:\n        response = session.get(f\"https://pypi.org/pypi/{name}/json\", refresh=True)\n    return response\n\n\ndef get_session() -> CachedSession:\n    \"\"\"Configures the requests-cache session\"\"\"\n    cache_path = platformdirs.user_cache_path(\"pytest-plugin-list\")\n    cache_path.mkdir(exist_ok=True, parents=True)\n    cache_file = cache_path.joinpath(\"http_cache.sqlite3\")\n    return CachedSession(backend=SQLiteCache(cache_file))\n\n\ndef pytest_plugin_projects_from_pypi(session: CachedSession) -> dict[str, int]:\n    response = session.get(\n        \"https://pypi.org/simple\",\n        headers={\"Accept\": \"application/vnd.pypi.simple.v1+json\"},\n        refresh=True,\n    )\n    return {\n        name: p[\"_last-serial\"]\n        for p in response.json()[\"projects\"]\n        if (name := p[\"name\"]).startswith(\"pytest-\") or name in ADDITIONAL_PROJECTS\n    }",
                "filename": "scripts/update-plugin-list.py",
                "start_index": 0,
                "end_index": 2939,
                "start_line": 1,
                "end_line": 155,
                "max_line": 195,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.0",
                "sha": ""
            },
            {
                "code": "def iter_plugins():\n    session = get_session()\n    name_2_serial = pytest_plugin_projects_from_pypi(session)\n\n    for name, last_serial in tqdm(name_2_serial.items(), smoothing=0):\n        response = project_response_with_refresh(session, name, last_serial)\n        if response.status_code == 404:\n            # Some packages, like pytest-azurepipelines42, are included in https://pypi.org/simple\n            # but return 404 on the JSON API. Skip.\n            continue\n        response.raise_for_status()\n        info = response.json()[\"info\"]\n        if \"Development Status :: 7 - Inactive\" in info[\"classifiers\"]:\n            continue\n        for classifier in DEVELOPMENT_STATUS_CLASSIFIERS:\n            if classifier in info[\"classifiers\"]:\n                status = classifier[22:]\n                break\n        else:\n            status = \"N/A\"\n        requires = \"N/A\"\n        if info[\"requires_dist\"]:\n            for requirement in info[\"requires_dist\"]:\n                if re.match(r\"pytest(?![-.\\w])\", requirement):\n                    requires = requirement\n                    break\n\n        def version_sort_key(version_string):\n            \"\"\"\n            Return the sort key for the given version string\n            returned by the API.\n            \"\"\"\n            try:\n                return packaging.version.parse(version_string)\n            except packaging.version.InvalidVersion:\n                # Use a hard-coded pre-release version.\n                return packaging.version.Version(\"0.0.0alpha\")\n\n        releases = response.json()[\"releases\"]\n        for release in sorted(releases, key=version_sort_key, reverse=True):\n            if releases[release]:\n                release_date = datetime.date.fromisoformat(\n                    releases[release][-1][\"upload_time_iso_8601\"].split(\"T\")[0]\n                )\n                last_release = release_date.strftime(\"%b %d, %Y\")\n                break\n        name = f':pypi:`{info[\"name\"]}`'\n        summary = \"\"\n        if info[\"summary\"]:\n            summary = escape_rst(info[\"summary\"].replace(\"\\n\", \"\"))\n        yield {\n            \"name\": name,\n            \"summary\": summary.strip(),\n            \"last release\": last_release,\n            \"status\": status,\n            \"requires\": requires,\n        }\n\n\ndef plugin_definitions(plugins):\n    \"\"\"Return RST for the plugin list that fits better on a vertical page.\"\"\"\n\n    for plugin in plugins:\n        yield dedent(\n            f\"\"\"\n            {plugin['name']}\n               *last release*: {plugin[\"last release\"]},\n               *status*: {plugin[\"status\"]},\n               *requires*: {plugin[\"requires\"]}\n\n               {plugin[\"summary\"]}\n            \"\"\"\n        )",
                "filename": "scripts/update-plugin-list.py",
                "start_index": 2942,
                "end_index": 5644,
                "start_line": 99,
                "end_line": 171,
                "max_line": 195,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.0",
                "sha": ""
            },
            {
                "code": "# reference: https://docs.codecov.io/docs/codecovyml-reference\ncoverage:\n  status:\n    patch: true\n    project: false\ncomment: false",
                "filename": "codecov.yml",
                "start_index": 0,
                "end_index": 132,
                "start_line": 1,
                "end_line": 6,
                "max_line": 6,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.0",
                "sha": ""
            },
            {
                "code": "import json\nfrom pathlib import Path\n\nimport requests\n\nissues_url = \"https://api.github.com/repos/pytest-dev/pytest/issues\"\n\n\ndef get_issues():\n    issues = []\n    url = issues_url\n    while 1:\n        get_data = {\"state\": \"all\"}\n        r = requests.get(url, params=get_data)\n        data = r.json()\n        if r.status_code == 403:\n            # API request limit exceeded\n            print(data[\"message\"])\n            exit(1)\n        issues.extend(data)\n\n        # Look for next page\n        links = requests.utils.parse_header_links(r.headers[\"Link\"])\n        another_page = False\n        for link in links:\n            if link[\"rel\"] == \"next\":\n                url = link[\"url\"]\n                another_page = True\n        if not another_page:\n            return issues\n\n\ndef main(args):\n    cachefile = Path(args.cache)\n    if not cachefile.exists() or args.refresh:\n        issues = get_issues()\n        cachefile.write_text(json.dumps(issues), \"utf-8\")\n    else:\n        issues = json.loads(cachefile.read_text(\"utf-8\"))\n\n    open_issues = [x for x in issues if x[\"state\"] == \"open\"]\n\n    open_issues.sort(key=lambda x: x[\"number\"])\n    report(open_issues)\n\n\ndef _get_kind(issue):\n    labels = [label[\"name\"] for label in issue[\"labels\"]]\n    for key in (\"bug\", \"enhancement\", \"proposal\"):\n        if key in labels:\n            return key\n    return \"issue\"\n\n\ndef report(issues):\n    for issue in issues:\n        title = issue[\"title\"]\n        # body = issue[\"body\"]\n        kind = _get_kind(issue)\n        status = issue[\"state\"]\n        number = issue[\"number\"]\n        link = \"https://github.com/pytest-dev/pytest/issues/%s/\" % number\n        print(\"----\")\n        print(status, kind, link)\n        print(title)\n        # print()\n        # lines = body.split(\"\\n\")\n        # print(\"\\n\".join(lines[:3]))\n        # if len(lines) > 3 or len(body) > 240:\n        #    print(\"...\")\n    print(\"\\n\\nFound %s open issues\" % len(issues))\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser(\"process bitbucket issues\")\n    parser.add_argument(\n        \"--refresh\", action=\"store_true\", help=\"invalidate cache, refresh issues\"\n    )\n    parser.add_argument(\n        \"--cache\", action=\"store\", default=\"issues.json\", help=\"cache file\"\n    )\n    args = parser.parse_args()\n    main(args)",
                "filename": "extra/get_issues.py",
                "start_index": 0,
                "end_index": 2321,
                "start_line": 1,
                "end_line": 85,
                "max_line": 85,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.0",
                "sha": ""
            }
        ],
        "classification": "feature request",
        "snippet_processor_task": {},
        "solution_task": "To implement the feature request of adding a `deselected` parameter to `pytester.RunResult.assert_outcomes()`, follow these steps:\n\n- Locate the `pytester` plugin/module in the pytest codebase.\n- Find the `RunResult` class within the `pytester` module.\n- Add a new parameter `deselected=None` to the signature of the `assert_outcomes()` method.\n- Within the `assert_outcomes()` method, add logic to check the `deselected` count if the `deselected` parameter is not `None`.\n- Use the existing `parseoutcomes()` method to get the current outcomes, which should include the `deselected` count.\n- Compare the `deselected` count from `parseoutcomes()` with the provided `deselected` parameter.\n- Raise an assertion error if the counts do not match.\n- Add or update unit tests for `assert_outcomes()` to cover the new `deselected` parameter.\n\nHere is a pseudo-code example of how the updated `assert_outcomes()` method might look:\n\n```python\nclass RunResult:\n    # ... existing code ...\n\n    def assert_outcomes(self, passed=None, skipped=None, failed=None, deselected=None):\n        outcomes = self.parseoutcomes()\n        if passed is not None and outcomes.get('passed', 0) != passed:\n            raise AssertionError(f\"Expected {passed} passed tests, got {outcomes.get('passed', 0)}\")\n        # ... similar checks for skipped and failed ...\n        if deselected is not None and outcomes.get('deselected', 0) != deselected:\n            raise AssertionError(f\"Expected {deselected} deselected tests, got {outcomes.get('deselected', 0)}\")\n        # ... rest of the method ...\n```\n\nAnd an example unit test:\n\n```python\ndef test_assert_outcomes_with_deselected(pytester):\n    # Setup pytester to run a test that will be deselected\n    # ...\n\n    result = pytester.runpytest()\n    # Assume 1 test is deselected\n    result.assert_outcomes(deselected=1)\n```\n\nRemember to document the new parameter in the method's docstring and update any relevant user documentation.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant suggestions that can directly lead to the successful implementation or improvement of the requested feature.",
            "supporting_evidence": "The RESPONSE provides a clear and detailed step-by-step guide on how to implement the requested feature, including where to locate the necessary code, how to modify the method signature, and the logic to add for the new functionality. It also includes pseudo-code to illustrate the changes and an example of how to write a unit test for the new parameter. The RESPONSE addresses the need for documentation updates, ensuring that the feature is not only implemented but also properly communicated to users. The only reason it does not score a 10 is that it does not explicitly mention updating the changelog or versioning, which are often important steps in adding new features to a codebase.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711734521.93959,
        "result": "To implement the feature request of adding a `deselected` parameter to `pytester.RunResult.assert_outcomes()`, follow these steps:\n\n- Locate the `pytester` plugin/module in the pytest codebase.\n- Find the `RunResult` class within the `pytester` module.\n- Add a new parameter `deselected=None` to the signature of the `assert_outcomes()` method.\n- Within the `assert_outcomes()` method, add logic to check the `deselected` count if the `deselected` parameter is not `None`.\n- Use the existing `parseoutcomes()` method to get the current outcomes, which should include the `deselected` count.\n- Compare the `deselected` count from `parseoutcomes()` with the provided `deselected` parameter.\n- Raise an assertion error if the counts do not match.\n- Add or update unit tests for `assert_outcomes()` to cover the new `deselected` parameter.\n\nHere is a pseudo-code example of how the updated `assert_outcomes()` method might look:\n\n```python\nclass RunResult:\n    # ... existing code ...\n\n    def assert_outcomes(self, passed=None, skipped=None, failed=None, deselected=None):\n        outcomes = self.parseoutcomes()\n        if passed is not None and outcomes.get('passed', 0) != passed:\n            raise AssertionError(f\"Expected {passed} passed tests, got {outcomes.get('passed', 0)}\")\n        # ... similar checks for skipped and failed ...\n        if deselected is not None and outcomes.get('deselected', 0) != deselected:\n            raise AssertionError(f\"Expected {deselected} deselected tests, got {outcomes.get('deselected', 0)}\")\n        # ... rest of the method ...\n```\n\nAnd an example unit test:\n\n```python\ndef test_assert_outcomes_with_deselected(pytester):\n    # Setup pytester to run a test that will be deselected\n    # ...\n\n    result = pytester.runpytest()\n    # Assume 1 test is deselected\n    result.assert_outcomes(deselected=1)\n```\n\nRemember to document the new parameter in the method's docstring and update any relevant user documentation."
    }
}