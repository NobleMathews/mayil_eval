{
    "id": "scikit-learn__scikit-learn-14114",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
    "task_id": 6904853689987940452,
    "title": "AdaBoost's \"SAMME\" algorithm uses 'predict' while fitting and 'predict_proba' while predicting probas",
    "body": "Subj. This seems to me to be a wrong approach, moreover this drives to such mistakes:\n\n<pre>\nAdaBoostClassifier(algorithm=\"SAMME\", base_estimator=SVC()).fit(trainX, trainY).predict_proba(testX)\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\n<ipython-input-108-1d666912dada> in <module>()\n----> 1 AdaBoostClassifier(algorithm=\"SAMME\", base_estimator=SVC()).fit(trainX, trainY).predict_proba(testX)\n\n/Library/Python/2.7/site-packages/sklearn/ensemble/weight_boosting.pyc in predict_proba(self, X)\n    716             proba = sum(estimator.predict_proba(X) * w\n    717                         for estimator, w in zip(self.estimators_,\n--> 718                                                 self.estimator_weights_))\n    719 \n    720         proba /= self.estimator_weights_.sum()\n\n/Library/Python/2.7/site-packages/sklearn/ensemble/weight_boosting.pyc in <genexpr>((estimator, w))\n    715         else:   # self.algorithm == \"SAMME\"\n    716             proba = sum(estimator.predict_proba(X) * w\n--> 717                         for estimator, w in zip(self.estimators_,\n    718                                                 self.estimator_weights_))\n    719 \n\n/Library/Python/2.7/site-packages/sklearn/svm/base.pyc in predict_proba(self, X)\n    493         if not self.probability:\n    494             raise NotImplementedError(\n--> 495                 \"probability estimates must be enabled to use this method\")\n    496 \n    497         if self._impl not in ('c_svc', 'nu_svc'):\n\nNotImplementedError: probability estimates must be enabled to use this method\n</pre>\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.22.git",
    "requested_commit": "",
    "mayil_response": "To resolve the `NotImplementedError` when calling `.predict_proba(testX)` with `AdaBoostClassifier` using `SVC` as the `base_estimator` and `algorithm=\"SAMME\"`, you need to enable probability estimates in the `SVC` instance. Modify the `SVC` instantiation to include the `probability=True` parameter.\n\nHere is the solution:\n\n- Locate the instantiation of the `SVC` class used as the `base_estimator` in `AdaBoostClassifier`.\n- Add the `probability=True` parameter to the `SVC` constructor to enable probability estimates.\n\nExample:\n```python\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Enable probability estimates in SVC\nsvc = SVC(probability=True)\n\n# Use the modified SVC as the base estimator for AdaBoostClassifier\nada_boost_clf = AdaBoostClassifier(base_estimator=svc, algorithm=\"SAMME\")\nada_boost_clf.fit(trainX, trainY)\nada_boost_clf.predict_proba(testX)\n```\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [ensemble](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/ensemble)- [Lines 643 - 715](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/ensemble/_weight_boosting.py#L643-L715)\n\n This snippet contains the implementation of the SAMME algorithm for AdaBoost, which is relevant to the issue as it involves the fitting process that uses 'predict'. \n\n\n   - [Lines 584 - 641](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/ensemble/_weight_boosting.py#L584-L641)\n\n This snippet contains the implementation of the SAMME.R algorithm for AdaBoost, which is relevant to the issue as it involves the fitting process that uses 'predict_proba'.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [svm](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/svm)- [Lines 901 - 1006](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/svm/_base.py#L901-L1006)\n\n This snippet is from the SVM base file and includes the 'predict_proba' method, which is relevant because the issue occurs when 'predict_proba' is called on an SVM base estimator that does not support probability estimates.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0009725,
        "snippet_processor": 0.08512000000000002,
        "issue_star_creation": 0.049199999999999994,
        "issue_star_solver": 0.054560000000000004,
        "bouncer": 0.02766
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711739001.1351411,
        "relevant_snippets": [
            {
                "code": "def _boost_discrete(self, iboost, X, y, sample_weight, random_state):\n        \"\"\"Implement a single boost using the SAMME discrete algorithm.\"\"\"\n        estimator = self._make_estimator(random_state=random_state)\n\n        estimator.fit(X, y, sample_weight=sample_weight)\n\n        y_predict = estimator.predict(X)\n\n        if iboost == 0:\n            self.classes_ = getattr(estimator, \"classes_\", None)\n            self.n_classes_ = len(self.classes_)\n\n        # Instances incorrectly classified\n        incorrect = y_predict != y\n\n        # Error fraction\n        estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n\n        # Stop if classification is perfect\n        if estimator_error <= 0:\n            return sample_weight, 1.0, 0.0\n\n        n_classes = self.n_classes_\n\n        # Stop if the error is at least as bad as random guessing\n        if estimator_error >= 1.0 - (1.0 / n_classes):\n            self.estimators_.pop(-1)\n            if len(self.estimators_) == 0:\n                raise ValueError(\n                    \"BaseClassifier in AdaBoostClassifier \"\n                    \"ensemble is worse than random, ensemble \"\n                    \"can not be fit.\"\n                )\n            return None, None, None\n\n        # Boost weight using multi-class AdaBoost SAMME alg\n        estimator_weight = self.learning_rate * (\n            np.log((1.0 - estimator_error) / estimator_error) + np.log(n_classes - 1.0)\n        )\n\n        # Only boost the weights if it will fit again\n        if not iboost == self.n_estimators - 1:\n            # Only boost positive weights\n            sample_weight = np.exp(\n                np.log(sample_weight)\n                + estimator_weight * incorrect * (sample_weight > 0)\n            )\n\n        return sample_weight, estimator_weight, estimator_error\n\n    def predict(self, X):\n        \"\"\"Predict classes for X.\n\n        The predicted class of an input sample is computed as the weighted mean\n        prediction of the classifiers in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        pred = self.decision_function(X)\n\n        if self.n_classes_ == 2:\n            return self.classes_.take(pred > 0, axis=0)\n\n        return self.classes_.take(np.argmax(pred, axis=1), axis=0)",
                "filename": "sklearn/ensemble/_weight_boosting.py",
                "start_index": 22509,
                "end_index": 25109,
                "start_line": 643,
                "end_line": 715,
                "max_line": 1266,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"\n=============================\nDiscrete versus Real AdaBoost\n=============================\n\nThis notebook is based on Figure 10.2 from Hastie et al 2009 [1]_ and\nillustrates the difference in performance between the discrete SAMME [2]_\nboosting algorithm and real SAMME.R boosting algorithm. Both algorithms are\nevaluated on a binary classification task where the target Y is a non-linear\nfunction of 10 input features.\n\nDiscrete SAMME AdaBoost adapts based on errors in predicted class labels\nwhereas real SAMME.R uses the predicted class probabilities.\n\n.. [1] T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical\n    Learning Ed. 2\", Springer, 2009.\n\n.. [2] J Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\",\n    Statistics and Its Interface, 2009.\n\n\"\"\"\n\n# %%\n# Preparing the data and baseline models\n# --------------------------------------\n# We start by generating the binary classification dataset\n# used in Hastie et al. 2009, Example 10.2.\n\n# Authors: Peter Prettenhofer <peter.prettenhofer@gmail.com>,\n#          Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\nfrom sklearn import datasets\n\nX, y = datasets.make_hastie_10_2(n_samples=12_000, random_state=1)\n\n# %%\n# Now, we set the hyperparameters for our AdaBoost classifiers.\n# Be aware, a learning rate of 1.0 may not be optimal for both SAMME and SAMME.R\n\nn_estimators = 400\nlearning_rate = 1.0\n\n# %%\n# We split the data into a training and a test set.\n# Then, we train our baseline classifiers, a `DecisionTreeClassifier` with `depth=9`\n# and a \"stump\" `DecisionTreeClassifier` with `depth=1` and compute the test error.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=2_000, shuffle=False\n)\n\ndt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)\ndt_stump.fit(X_train, y_train)\ndt_stump_err = 1.0 - dt_stump.score(X_test, y_test)\n\ndt = DecisionTreeClassifier(max_depth=9, min_samples_leaf=1)\ndt.fit(X_train, y_train)\ndt_err = 1.0 - dt.score(X_test, y_test)\n\n# %%\n# Adaboost with discrete SAMME and real SAMME.R\n# ---------------------------------------------\n# We now define the discrete and real AdaBoost classifiers\n# and fit them to the training set.\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada_discrete = AdaBoostClassifier(\n    estimator=dt_stump,\n    learning_rate=learning_rate,\n    n_estimators=n_estimators,\n    algorithm=\"SAMME\",\n)\nada_discrete.fit(X_train, y_train)\n\n# %%\n\nada_real = AdaBoostClassifier(\n    estimator=dt_stump,\n    learning_rate=learning_rate,\n    n_estimators=n_estimators,\n    algorithm=\"SAMME.R\",\n)\nada_real.fit(X_train, y_train)\n\n# %%\n# Now, let's compute the test error of the discrete and\n# real AdaBoost classifiers for each new stump in `n_estimators`\n# added to the ensemble.\n\nimport numpy as np\n\nfrom sklearn.metrics import zero_one_loss\n\nada_discrete_err = np.zeros((n_estimators,))",
                "filename": "examples/ensemble/plot_adaboost_hastie_10_2.py",
                "start_index": 0,
                "end_index": 2974,
                "start_line": 1,
                "end_line": 100,
                "max_line": 172,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "def _boost_real(self, iboost, X, y, sample_weight, random_state):\n        \"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\n        estimator = self._make_estimator(random_state=random_state)\n\n        estimator.fit(X, y, sample_weight=sample_weight)\n\n        y_predict_proba = estimator.predict_proba(X)\n\n        if iboost == 0:\n            self.classes_ = getattr(estimator, \"classes_\", None)\n            self.n_classes_ = len(self.classes_)\n\n        y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1), axis=0)\n\n        # Instances incorrectly classified\n        incorrect = y_predict != y\n\n        # Error fraction\n        estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n\n        # Stop if classification is perfect\n        if estimator_error <= 0:\n            return sample_weight, 1.0, 0.0\n\n        # Construct y coding as described in Zhu et al [2]:\n        #\n        #    y_k = 1 if c == k else -1 / (K - 1)\n        #\n        # where K == n_classes_ and c, k in [0, K) are indices along the second\n        # axis of the y coding with c being the index corresponding to the true\n        # class label.\n        n_classes = self.n_classes_\n        classes = self.classes_\n        y_codes = np.array([-1.0 / (n_classes - 1), 1.0])\n        y_coding = y_codes.take(classes == y[:, np.newaxis])\n\n        # Displace zero probabilities so the log is defined.\n        # Also fix negative elements which may occur with\n        # negative sample weights.\n        proba = y_predict_proba  # alias for readability\n        np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\n\n        # Boost weight using multi-class AdaBoost SAMME.R alg\n        estimator_weight = (\n            -1.0\n            * self.learning_rate\n            * ((n_classes - 1.0) / n_classes)\n            * xlogy(y_coding, y_predict_proba).sum(axis=1)\n        )\n\n        # Only boost the weights if it will fit again\n        if not iboost == self.n_estimators - 1:\n            # Only boost positive weights\n            sample_weight *= np.exp(\n                estimator_weight * ((sample_weight > 0) | (estimator_weight < 0))\n            )\n\n        return sample_weight, 1.0, estimator_error",
                "filename": "sklearn/ensemble/_weight_boosting.py",
                "start_index": 20282,
                "end_index": 22503,
                "start_line": 584,
                "end_line": 641,
                "max_line": 1266,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "def _dense_predict_proba(self, X):\n        X = self._compute_kernel(X)\n\n        kernel = self.kernel\n        if callable(kernel):\n            kernel = \"precomputed\"\n\n        svm_type = LIBSVM_IMPL.index(self._impl)\n        pprob = libsvm.predict_proba(\n            X,\n            self.support_,\n            self.support_vectors_,\n            self._n_support,\n            self._dual_coef_,\n            self._intercept_,\n            self._probA,\n            self._probB,\n            svm_type=svm_type,\n            kernel=kernel,\n            degree=self.degree,\n            cache_size=self.cache_size,\n            coef0=self.coef0,\n            gamma=self._gamma,\n        )\n\n        return pprob\n\n    def _sparse_predict_proba(self, X):\n        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")\n\n        kernel = self.kernel\n        if callable(kernel):\n            kernel = \"precomputed\"\n\n        kernel_type = self._sparse_kernels.index(kernel)\n\n        return libsvm_sparse.libsvm_sparse_predict_proba(\n            X.data,\n            X.indices,\n            X.indptr,\n            self.support_vectors_.data,\n            self.support_vectors_.indices,\n            self.support_vectors_.indptr,\n            self._dual_coef_.data,\n            self._intercept_,\n            LIBSVM_IMPL.index(self._impl),\n            kernel_type,\n            self.degree,\n            self._gamma,\n            self.coef0,\n            self.tol,\n            self.C,\n            # TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\n            getattr(self, \"_class_weight\", np.empty(0)),\n            self.nu,\n            self.epsilon,\n            self.shrinking,\n            self.probability,\n            self._n_support,\n            self._probA,\n            self._probB,\n        )\n\n    def _get_coef(self):\n        if self.dual_coef_.shape[0] == 1:\n            # binary classifier\n            coef = safe_sparse_dot(self.dual_coef_, self.support_vectors_)\n        else:\n            # 1vs1 classifier\n            coef = _one_vs_one_coef(\n                self.dual_coef_, self._n_support, self.support_vectors_\n            )\n            if sp.issparse(coef[0]):\n                coef = sp.vstack(coef).tocsr()\n            else:\n                coef = np.vstack(coef)\n\n        return coef\n\n    @property\n    def probA_(self):\n        \"\"\"Parameter learned in Platt scaling when `probability=True`.\n\n        Returns\n        -------\n        ndarray of shape  (n_classes * (n_classes - 1) / 2)\n        \"\"\"\n        return self._probA\n\n    @property\n    def probB_(self):\n        \"\"\"Parameter learned in Platt scaling when `probability=True`.\n\n        Returns\n        -------\n        ndarray of shape  (n_classes * (n_classes - 1) / 2)\n        \"\"\"\n        return self._probB\n\n    # TODO(1.4): Remove\n    @property\n    def _class_weight(self):\n        \"\"\"Weights per class\"\"\"\n        # Class weights are defined for classifiers during\n        # fit.\n        return self.class_weight_",
                "filename": "sklearn/svm/_base.py",
                "start_index": 30893,
                "end_index": 33853,
                "start_line": 901,
                "end_line": 1006,
                "max_line": 1254,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@available_if(_estimator_has(\"predict_proba\"))\n    def predict_proba(self, X):\n        \"\"\"Call predict_proba on the estimator with the best found parameters.\n\n        Only available if ``refit=True`` and the underlying estimator supports\n        ``predict_proba``.\n\n        Parameters\n        ----------\n        X : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Predicted class probabilities for `X` based on the estimator with\n            the best found parameters. The order of the classes corresponds\n            to that in the fitted attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        return self.best_estimator_.predict_proba(X)\n\n    @available_if(_estimator_has(\"predict_log_proba\"))\n    def predict_log_proba(self, X):\n        \"\"\"Call predict_log_proba on the estimator with the best found parameters.\n\n        Only available if ``refit=True`` and the underlying estimator supports\n        ``predict_log_proba``.\n\n        Parameters\n        ----------\n        X : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)\n            Predicted class log-probabilities for `X` based on the estimator\n            with the best found parameters. The order of the classes\n            corresponds to that in the fitted attribute :term:`classes_`.\n        \"\"\"\n        check_is_fitted(self)\n        return self.best_estimator_.predict_log_proba(X)\n\n    @available_if(_estimator_has(\"decision_function\"))\n    def decision_function(self, X):\n        \"\"\"Call decision_function on the estimator with the best found parameters.\n\n        Only available if ``refit=True`` and the underlying estimator supports\n        ``decision_function``.\n\n        Parameters\n        ----------\n        X : indexable, length n_samples\n            Must fulfill the input assumptions of the\n            underlying estimator.\n\n        Returns\n        -------\n        y_score : ndarray of shape (n_samples,) or (n_samples, n_classes) \\\n                or (n_samples, n_classes * (n_classes-1) / 2)\n            Result of the decision function for `X` based on the estimator with\n            the best found parameters.\n        \"\"\"\n        check_is_fitted(self)\n        return self.best_estimator_.decision_function(X)",
                "filename": "sklearn/model_selection/_search.py",
                "start_index": 18575,
                "end_index": 21143,
                "start_line": 521,
                "end_line": 588,
                "max_line": 1813,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "boosting_errors = pd.DataFrame(\n    {\n        \"Number of trees\": range(1, n_estimators + 1),\n        \"AdaBoost\": [\n            misclassification_error(y_test, y_pred)\n            for y_pred in adaboost_clf.staged_predict(X_test)\n        ],\n    }\n).set_index(\"Number of trees\")\nax = boosting_errors.plot()\nax.set_ylabel(\"Misclassification error on test set\")\nax.set_title(\"Convergence of AdaBoost algorithm\")\n\nplt.plot(\n    [boosting_errors.index.min(), boosting_errors.index.max()],\n    [weak_learners_misclassification_error, weak_learners_misclassification_error],\n    color=\"tab:orange\",\n    linestyle=\"dashed\",\n)\nplt.plot(\n    [boosting_errors.index.min(), boosting_errors.index.max()],\n    [\n        dummy_classifiers_misclassification_error,\n        dummy_classifiers_misclassification_error,\n    ],\n    color=\"c\",\n    linestyle=\"dotted\",\n)\nplt.legend([\"AdaBoost\", \"DecisionTreeClassifier\", \"DummyClassifier\"], loc=1)\nplt.show()\n\n# %%\n# The plot shows the missclassification error on the test set after each\n# boosting iteration. We see that the error of the boosted trees converges to an\n# error of around 0.3 after 50 iterations, indicating a significantly higher\n# accuracy compared to a single tree, as illustrated by the dashed line in the\n# plot.\n#\n# The misclassification error jitters because the `SAMME` algorithm uses the\n# discrete outputs of the weak learners to train the boosted model.\n#\n# The convergence of :class:`~sklearn.ensemble.AdaBoostClassifier` is mainly\n# influenced by the learning rate (i.e `learning_rate`), the number of weak\n# learners used (`n_estimators`), and the expressivity of the weak learners\n# (e.g. `max_leaf_nodes`).\n\n# %%\n# Errors and weights of the Weak Learners\n# ***************************************\n# As previously mentioned, AdaBoost is a forward stagewise additive model. We\n# now focus on understanding the relationship between the attributed weights of\n# the weak learners and their statistical performance.\n#\n# We use the fitted :class:`~sklearn.ensemble.AdaBoostClassifier`'s attributes\n# `estimator_errors_` and `estimator_weights_` to investigate this link.\nweak_learners_info = pd.DataFrame(\n    {\n        \"Number of trees\": range(1, n_estimators + 1),\n        \"Errors\": adaboost_clf.estimator_errors_,\n        \"Weights\": adaboost_clf.estimator_weights_,\n    }\n).set_index(\"Number of trees\")\n\naxs = weak_learners_info.plot(\n    subplots=True, layout=(1, 2), figsize=(10, 4), legend=False, color=\"tab:blue\"\n)\naxs[0, 0].set_ylabel(\"Train error\")\naxs[0, 0].set_title(\"Weak learner's training error\")\naxs[0, 1].set_ylabel(\"Weight\")\naxs[0, 1].set_title(\"Weak learner's weight\")\nfig = axs[0, 0].get_figure()\nfig.suptitle(\"Weak learner's errors and weights for the AdaBoostClassifier\")\nfig.tight_layout()\n\n# %%\n# On the left plot, we show the weighted error of each weak learner on the\n# reweighted training set at each boosting iteration. On the right plot, we show\n# the weights associated with each weak learner later used to make the",
                "filename": "examples/ensemble/plot_adaboost_multiclass.py",
                "start_index": 5807,
                "end_index": 8801,
                "start_line": 148,
                "end_line": 225,
                "max_line": 253,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "#   one away from these values. Because predictions are restricted to the\n#   interval [0,1], errors caused by variance tend to be one- sided near zero\n#   and one. For example, if a model should predict p = 0 for a case, the only\n#   way bagging can achieve this is if all bagged trees predict zero. If we add\n#   noise to the trees that bagging is averaging over, this noise will cause\n#   some trees to predict values larger than 0 for this case, thus moving the\n#   average prediction of the bagged ensemble away from 0. We observe this\n#   effect most strongly with random forests because the base-level trees\n#   trained with random forests have relatively high variance due to feature\n#   subsetting.\" As a result, the calibration curve shows a characteristic\n#   sigmoid shape, indicating that the classifier is under-confident\n#   and could return probabilities closer to 0 or 1.\n#\n# * To show the performance of :class:`~sklearn.svm.LinearSVC`, we naively\n#   scale the output of the :term:`decision_function` into [0, 1] by applying\n#   min-max scaling, since SVC does not output probabilities by default.\n#   :class:`~sklearn.svm.LinearSVC` shows an\n#   even more sigmoid curve than the\n#   :class:`~sklearn.ensemble.RandomForestClassifier`, which is typical for\n#   maximum-margin methods [1]_ as they focus on difficult to classify samples\n#   that are close to the decision boundary (the support vectors).\n#\n# References\n# ----------\n#\n# .. [1] `Predicting Good Probabilities with Supervised Learning\n#        <https://dl.acm.org/doi/pdf/10.1145/1102351.1102430>`_,\n#        A. Niculescu-Mizil & R. Caruana, ICML 2005\n# .. [2] `Beyond independence: Conditions for the optimality of the simple\n#        bayesian classifier\n#        <https://www.ics.uci.edu/~pazzani/Publications/mlc96-pedro.pdf>`_\n#        Domingos, P., & Pazzani, M., Proc. 13th Intl. Conf. Machine Learning.\n#        1996.\n# .. [3] `Obtaining calibrated probability estimates from decision trees and\n#        naive Bayesian classifiers\n#        <https://citeseerx.ist.psu.edu/doc_view/pid/4f67a122ec3723f08ad5cbefecad119b432b3304>`_\n#        Zadrozny, Bianca, and Charles Elkan. Icml. Vol. 1. 2001.",
                "filename": "examples/calibration/plot_compare_calibration.py",
                "start_index": 5970,
                "end_index": 8151,
                "start_line": 174,
                "end_line": 210,
                "max_line": 210,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "#if 0\nvoid cross_validation(const problem *prob, const parameter *param, int nr_fold, double *target)\n{\n\tint i;\n\tint *fold_start;\n\tint l = prob->l;\n\tint *perm = Malloc(int,l);\n\tif (nr_fold > l)\n\t{\n\t\tnr_fold = l;\n\t\tfprintf(stderr,\"WARNING: # folds > # data. Will use # folds = # data instead (i.e., leave-one-out cross validation)\\n\");\n\t}\n\tfold_start = Malloc(int,nr_fold+1);\n\tfor(i=0;i<l;i++) perm[i]=i;\n\tfor(i=0;i<l;i++)\n\t{\n\t\tint j = i+bounded_rand_int(l-i);\n\t\tswap(perm[i],perm[j]);\n\t}\n\tfor(i=0;i<=nr_fold;i++)\n\t\tfold_start[i]=i*l/nr_fold;\n\n\tfor(i=0;i<nr_fold;i++)\n\t{\n\t\tint begin = fold_start[i];\n\t\tint end = fold_start[i+1];\n\t\tint j,k;\n\t\tstruct problem subprob;\n\n\t\tsubprob.bias = prob->bias;\n\t\tsubprob.n = prob->n;\n\t\tsubprob.l = l-(end-begin);\n\t\tsubprob.x = Malloc(struct feature_node*,subprob.l);\n\t\tsubprob.y = Malloc(double,subprob.l);\n\n\t\tk=0;\n\t\tfor(j=0;j<begin;j++)\n\t\t{\n\t\t\tsubprob.x[k] = prob->x[perm[j]];\n\t\t\tsubprob.y[k] = prob->y[perm[j]];\n\t\t\t++k;\n\t\t}\n\t\tfor(j=end;j<l;j++)\n\t\t{\n\t\t\tsubprob.x[k] = prob->x[perm[j]];\n\t\t\tsubprob.y[k] = prob->y[perm[j]];\n\t\t\t++k;\n\t\t}\n\t\tstruct model *submodel = train(&subprob,param);\n\t\tfor(j=begin;j<end;j++)\n\t\t\ttarget[perm[j]] = predict(submodel,prob->x[perm[j]]);\n\t\tfree_and_destroy_model(&submodel);\n\t\tfree(subprob.x);\n\t\tfree(subprob.y);\n\t}\n\tfree(fold_start);\n\tfree(perm);\n}\n\ndouble predict_values(const struct model *model_, const struct feature_node *x, double *dec_values)\n{\n\tint idx;\n\tint n;\n\tif(model_->bias>=0)\n\t\tn=model_->nr_feature+1;\n\telse\n\t\tn=model_->nr_feature;\n\tdouble *w=model_->w;\n\tint nr_class=model_->nr_class;\n\tint i;\n\tint nr_w;\n\tif(nr_class==2 && model_->param.solver_type != MCSVM_CS)\n\t\tnr_w = 1;\n\telse\n\t\tnr_w = nr_class;\n\n\tconst feature_node *lx=x;\n\tfor(i=0;i<nr_w;i++)\n\t\tdec_values[i] = 0;\n\tfor(; (idx=lx->index)!=-1; lx++)\n\t{\n\t\t// the dimension of testing data may exceed that of training\n\t\tif(idx<=n)\n\t\t\tfor(i=0;i<nr_w;i++)\n\t\t\t\tdec_values[i] += w[(idx-1)*nr_w+i]*lx->value;\n\t}\n\n\tif(nr_class==2)\n\t{\n\t\tif(check_regression_model(model_))\n\t\t\treturn dec_values[0];\n\t\telse\n\t\t\treturn (dec_values[0]>0)?model_->label[0]:model_->label[1];\n\t}\n\telse\n\t{\n\t\tint dec_max_idx = 0;\n\t\tfor(i=1;i<nr_class;i++)\n\t\t{\n\t\t\tif(dec_values[i] > dec_values[dec_max_idx])\n\t\t\t\tdec_max_idx = i;\n\t\t}\n\t\treturn model_->label[dec_max_idx];\n\t}\n}\n\ndouble predict(const model *model_, const feature_node *x)\n{\n\tdouble *dec_values = Malloc(double, model_->nr_class);\n\tdouble label=predict_values(model_, x, dec_values);\n\tfree(dec_values);\n\treturn label;\n}",
                "filename": "sklearn/svm/src/liblinear/linear.cpp",
                "start_index": 52321,
                "end_index": 54797,
                "start_line": 2269,
                "end_line": 3075,
                "max_line": 3075,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"\n=====================================\nMulti-class AdaBoosted Decision Trees\n=====================================\n\nThis example shows how boosting can improve the prediction accuracy on a\nmulti-label classification problem. It reproduces a similar experiment as\ndepicted by Figure 1 in Zhu et al [1]_.\n\nThe core principle of AdaBoost (Adaptive Boosting) is to fit a sequence of weak\nlearners (e.g. Decision Trees) on repeatedly re-sampled versions of the data.\nEach sample carries a weight that is adjusted after each training step, such\nthat misclassified samples will be assigned higher weights. The re-sampling\nprocess with replacement takes into account the weights assigned to each sample.\nSamples with higher weights have a greater chance of being selected multiple\ntimes in the new data set, while samples with lower weights are less likely to\nbe selected. This ensures that subsequent iterations of the algorithm focus on\nthe difficult-to-classify samples.\n\n.. topic:: References:\n\n    .. [1] :doi:`J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class adaboost.\"\n           Statistics and its Interface 2.3 (2009): 349-360.\n           <10.4310/SII.2009.v2.n3.a8>`\n\n\"\"\"\n\n# Noel Dawe <noel.dawe@gmail.com>\n# License: BSD 3 clause\n\n# %%\n# Creating the dataset\n# --------------------\n# The classification dataset is constructed by taking a ten-dimensional standard\n# normal distribution (:math:`x` in :math:`R^{10}`) and defining three classes\n# separated by nested concentric ten-dimensional spheres such that roughly equal\n# numbers of samples are in each class (quantiles of the :math:`\\chi^2`\n# distribution).\nfrom sklearn.datasets import make_gaussian_quantiles\n\nX, y = make_gaussian_quantiles(\n    n_samples=2_000, n_features=10, n_classes=3, random_state=1\n)\n\n# %%\n# We split the dataset into 2 sets: 70 percent of the samples are used for\n# training and the remaining 30 percent for testing.\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=0.7, random_state=42\n)\n\n# %%\n# Training the `AdaBoostClassifier`\n# ---------------------------------\n# We train the :class:`~sklearn.ensemble.AdaBoostClassifier`. The estimator\n# utilizes boosting to improve the classification accuracy. Boosting is a method\n# designed to train weak learners (i.e. `base_estimator`) that learn from their\n# predecessor's mistakes.\n#\n# Here, we define the weak learner as a\n# :class:`~sklearn.tree.DecisionTreeClassifier` and set the maximum number of\n# leaves to 8. In a real setting, this parameter should be tuned. We set it to a\n# rather low value to limit the runtime of the example.\n#\n# The `SAMME` algorithm build into the\n# :class:`~sklearn.ensemble.AdaBoostClassifier` then uses the correct or\n# incorrect predictions made be the current weak learner to update the sample\n# weights used for training the consecutive weak learners. Also, the weight of",
                "filename": "examples/ensemble/plot_adaboost_multiclass.py",
                "start_index": 0,
                "end_index": 2925,
                "start_line": 1,
                "end_line": 70,
                "max_line": 253,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"Calibration of predicted probabilities.\"\"\"\n\n# Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n#         Balazs Kegl <balazs.kegl@gmail.com>\n#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n#         Mathieu Blondel <mathieu@mblondel.org>\n#\n# License: BSD 3 clause\n\nimport warnings\nfrom functools import partial\nfrom inspect import signature\nfrom math import log\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy.optimize import fmin_bfgs\nfrom scipy.special import expit, xlogy\n\nfrom sklearn.utils import Bunch\n\nfrom .base import (\n    BaseEstimator,\n    ClassifierMixin,\n    MetaEstimatorMixin,\n    RegressorMixin,\n    _fit_context,\n    clone,\n)\nfrom .isotonic import IsotonicRegression\nfrom .model_selection import check_cv, cross_val_predict\nfrom .preprocessing import LabelEncoder, label_binarize\nfrom .svm import LinearSVC\nfrom .utils import (\n    _safe_indexing,\n    column_or_1d,\n    indexable,\n)\nfrom .utils._param_validation import (\n    HasMethods,\n    Hidden,\n    Interval,\n    StrOptions,\n    validate_params,\n)\nfrom .utils._plotting import _BinaryClassifierCurveDisplayMixin\nfrom .utils.metadata_routing import (\n    MetadataRouter,\n    MethodMapping,\n    _routing_enabled,\n    process_routing,\n)\nfrom .utils.multiclass import check_classification_targets\nfrom .utils.parallel import Parallel, delayed\nfrom .utils.validation import (\n    _check_method_params,\n    _check_pos_label_consistency,\n    _check_sample_weight,\n    _num_samples,\n    check_consistent_length,\n    check_is_fitted,\n)",
                "filename": "sklearn/calibration.py",
                "start_index": 0,
                "end_index": 1552,
                "start_line": 1,
                "end_line": 1447,
                "max_line": 1447,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/ensemble/_weight_boosting.py": [
                {
                    "chunk": {
                        "code": "def _boost_discrete(self, iboost, X, y, sample_weight, random_state):\n        \"\"\"Implement a single boost using the SAMME discrete algorithm.\"\"\"\n        estimator = self._make_estimator(random_state=random_state)\n\n        estimator.fit(X, y, sample_weight=sample_weight)\n\n        y_predict = estimator.predict(X)\n\n        if iboost == 0:\n            self.classes_ = getattr(estimator, \"classes_\", None)\n            self.n_classes_ = len(self.classes_)\n\n        # Instances incorrectly classified\n        incorrect = y_predict != y\n\n        # Error fraction\n        estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n\n        # Stop if classification is perfect\n        if estimator_error <= 0:\n            return sample_weight, 1.0, 0.0\n\n        n_classes = self.n_classes_\n\n        # Stop if the error is at least as bad as random guessing\n        if estimator_error >= 1.0 - (1.0 / n_classes):\n            self.estimators_.pop(-1)\n            if len(self.estimators_) == 0:\n                raise ValueError(\n                    \"BaseClassifier in AdaBoostClassifier \"\n                    \"ensemble is worse than random, ensemble \"\n                    \"can not be fit.\"\n                )\n            return None, None, None\n\n        # Boost weight using multi-class AdaBoost SAMME alg\n        estimator_weight = self.learning_rate * (\n            np.log((1.0 - estimator_error) / estimator_error) + np.log(n_classes - 1.0)\n        )\n\n        # Only boost the weights if it will fit again\n        if not iboost == self.n_estimators - 1:\n            # Only boost positive weights\n            sample_weight = np.exp(\n                np.log(sample_weight)\n                + estimator_weight * incorrect * (sample_weight > 0)\n            )\n\n        return sample_weight, estimator_weight, estimator_error\n\n    def predict(self, X):\n        \"\"\"Predict classes for X.\n\n        The predicted class of an input sample is computed as the weighted mean\n        prediction of the classifiers in the ensemble.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The training input samples. Sparse matrix can be CSC, CSR, COO,\n            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            The predicted classes.\n        \"\"\"\n        pred = self.decision_function(X)\n\n        if self.n_classes_ == 2:\n            return self.classes_.take(pred > 0, axis=0)\n\n        return self.classes_.take(np.argmax(pred, axis=1), axis=0)",
                        "filename": "sklearn/ensemble/_weight_boosting.py",
                        "start_index": 22509,
                        "end_index": 25109,
                        "start_line": 643,
                        "end_line": 715,
                        "max_line": 1266,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the implementation of the SAMME algorithm for AdaBoost, which is relevant to the issue as it involves the fitting process that uses 'predict'."
                },
                {
                    "chunk": {
                        "code": "def _boost_real(self, iboost, X, y, sample_weight, random_state):\n        \"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\n        estimator = self._make_estimator(random_state=random_state)\n\n        estimator.fit(X, y, sample_weight=sample_weight)\n\n        y_predict_proba = estimator.predict_proba(X)\n\n        if iboost == 0:\n            self.classes_ = getattr(estimator, \"classes_\", None)\n            self.n_classes_ = len(self.classes_)\n\n        y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1), axis=0)\n\n        # Instances incorrectly classified\n        incorrect = y_predict != y\n\n        # Error fraction\n        estimator_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n\n        # Stop if classification is perfect\n        if estimator_error <= 0:\n            return sample_weight, 1.0, 0.0\n\n        # Construct y coding as described in Zhu et al [2]:\n        #\n        #    y_k = 1 if c == k else -1 / (K - 1)\n        #\n        # where K == n_classes_ and c, k in [0, K) are indices along the second\n        # axis of the y coding with c being the index corresponding to the true\n        # class label.\n        n_classes = self.n_classes_\n        classes = self.classes_\n        y_codes = np.array([-1.0 / (n_classes - 1), 1.0])\n        y_coding = y_codes.take(classes == y[:, np.newaxis])\n\n        # Displace zero probabilities so the log is defined.\n        # Also fix negative elements which may occur with\n        # negative sample weights.\n        proba = y_predict_proba  # alias for readability\n        np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\n\n        # Boost weight using multi-class AdaBoost SAMME.R alg\n        estimator_weight = (\n            -1.0\n            * self.learning_rate\n            * ((n_classes - 1.0) / n_classes)\n            * xlogy(y_coding, y_predict_proba).sum(axis=1)\n        )\n\n        # Only boost the weights if it will fit again\n        if not iboost == self.n_estimators - 1:\n            # Only boost positive weights\n            sample_weight *= np.exp(\n                estimator_weight * ((sample_weight > 0) | (estimator_weight < 0))\n            )\n\n        return sample_weight, 1.0, estimator_error",
                        "filename": "sklearn/ensemble/_weight_boosting.py",
                        "start_index": 20282,
                        "end_index": 22503,
                        "start_line": 584,
                        "end_line": 641,
                        "max_line": 1266,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the implementation of the SAMME.R algorithm for AdaBoost, which is relevant to the issue as it involves the fitting process that uses 'predict_proba'."
                }
            ],
            "sklearn/svm/_base.py": [
                {
                    "chunk": {
                        "code": "def _dense_predict_proba(self, X):\n        X = self._compute_kernel(X)\n\n        kernel = self.kernel\n        if callable(kernel):\n            kernel = \"precomputed\"\n\n        svm_type = LIBSVM_IMPL.index(self._impl)\n        pprob = libsvm.predict_proba(\n            X,\n            self.support_,\n            self.support_vectors_,\n            self._n_support,\n            self._dual_coef_,\n            self._intercept_,\n            self._probA,\n            self._probB,\n            svm_type=svm_type,\n            kernel=kernel,\n            degree=self.degree,\n            cache_size=self.cache_size,\n            coef0=self.coef0,\n            gamma=self._gamma,\n        )\n\n        return pprob\n\n    def _sparse_predict_proba(self, X):\n        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")\n\n        kernel = self.kernel\n        if callable(kernel):\n            kernel = \"precomputed\"\n\n        kernel_type = self._sparse_kernels.index(kernel)\n\n        return libsvm_sparse.libsvm_sparse_predict_proba(\n            X.data,\n            X.indices,\n            X.indptr,\n            self.support_vectors_.data,\n            self.support_vectors_.indices,\n            self.support_vectors_.indptr,\n            self._dual_coef_.data,\n            self._intercept_,\n            LIBSVM_IMPL.index(self._impl),\n            kernel_type,\n            self.degree,\n            self._gamma,\n            self.coef0,\n            self.tol,\n            self.C,\n            # TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\n            getattr(self, \"_class_weight\", np.empty(0)),\n            self.nu,\n            self.epsilon,\n            self.shrinking,\n            self.probability,\n            self._n_support,\n            self._probA,\n            self._probB,\n        )\n\n    def _get_coef(self):\n        if self.dual_coef_.shape[0] == 1:\n            # binary classifier\n            coef = safe_sparse_dot(self.dual_coef_, self.support_vectors_)\n        else:\n            # 1vs1 classifier\n            coef = _one_vs_one_coef(\n                self.dual_coef_, self._n_support, self.support_vectors_\n            )\n            if sp.issparse(coef[0]):\n                coef = sp.vstack(coef).tocsr()\n            else:\n                coef = np.vstack(coef)\n\n        return coef\n\n    @property\n    def probA_(self):\n        \"\"\"Parameter learned in Platt scaling when `probability=True`.\n\n        Returns\n        -------\n        ndarray of shape  (n_classes * (n_classes - 1) / 2)\n        \"\"\"\n        return self._probA\n\n    @property\n    def probB_(self):\n        \"\"\"Parameter learned in Platt scaling when `probability=True`.\n\n        Returns\n        -------\n        ndarray of shape  (n_classes * (n_classes - 1) / 2)\n        \"\"\"\n        return self._probB\n\n    # TODO(1.4): Remove\n    @property\n    def _class_weight(self):\n        \"\"\"Weights per class\"\"\"\n        # Class weights are defined for classifiers during\n        # fit.\n        return self.class_weight_",
                        "filename": "sklearn/svm/_base.py",
                        "start_index": 30893,
                        "end_index": 33853,
                        "start_line": 901,
                        "end_line": 1006,
                        "max_line": 1254,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet is from the SVM base file and includes the 'predict_proba' method, which is relevant because the issue occurs when 'predict_proba' is called on an SVM base estimator that does not support probability estimates."
                }
            ]
        },
        "solution_task": "To resolve the `NotImplementedError` when calling `.predict_proba(testX)` with `AdaBoostClassifier` using `SVC` as the `base_estimator` and `algorithm=\"SAMME\"`, you need to enable probability estimates in the `SVC` instance. Modify the `SVC` instantiation to include the `probability=True` parameter.\n\nHere is the solution:\n\n- Locate the instantiation of the `SVC` class used as the `base_estimator` in `AdaBoostClassifier`.\n- Add the `probability=True` parameter to the `SVC` constructor to enable probability estimates.\n\nExample:\n```python\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Enable probability estimates in SVC\nsvc = SVC(probability=True)\n\n# Use the modified SVC as the base estimator for AdaBoostClassifier\nada_boost_clf = AdaBoostClassifier(base_estimator=svc, algorithm=\"SAMME\")\nada_boost_clf.fit(trainX, trainY)\nada_boost_clf.predict_proba(testX)\n```",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE accurately identifies the cause of the NotImplementedError, which is the use of SVC without enabling probability estimates. It provides a clear and actionable solution by instructing to set the 'probability' parameter to 'True' when instantiating the SVC class. This directly addresses the issue of the AdaBoostClassifier's 'predict_proba' method failing due to the base estimator not supporting probability estimates. The RESPONSE also includes an example code snippet demonstrating how to correctly instantiate the SVC with probability estimates enabled and how to use it with AdaBoostClassifier. Additionally, the RESPONSE provides relevant code snippets from the scikit-learn repository that pertain to the implementation of the SAMME algorithm and the 'predict_proba' method in SVM, which could be useful for further understanding and debugging.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711739198.2698739,
        "result": "To resolve the `NotImplementedError` when calling `.predict_proba(testX)` with `AdaBoostClassifier` using `SVC` as the `base_estimator` and `algorithm=\"SAMME\"`, you need to enable probability estimates in the `SVC` instance. Modify the `SVC` instantiation to include the `probability=True` parameter.\n\nHere is the solution:\n\n- Locate the instantiation of the `SVC` class used as the `base_estimator` in `AdaBoostClassifier`.\n- Add the `probability=True` parameter to the `SVC` constructor to enable probability estimates.\n\nExample:\n```python\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Enable probability estimates in SVC\nsvc = SVC(probability=True)\n\n# Use the modified SVC as the base estimator for AdaBoostClassifier\nada_boost_clf = AdaBoostClassifier(base_estimator=svc, algorithm=\"SAMME\")\nada_boost_clf.fit(trainX, trainY)\nada_boost_clf.predict_proba(testX)\n```\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [ensemble](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/ensemble)- [Lines 643 - 715](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/ensemble/_weight_boosting.py#L643-L715)\n\n This snippet contains the implementation of the SAMME algorithm for AdaBoost, which is relevant to the issue as it involves the fitting process that uses 'predict'. \n\n\n   - [Lines 584 - 641](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/ensemble/_weight_boosting.py#L584-L641)\n\n This snippet contains the implementation of the SAMME.R algorithm for AdaBoost, which is relevant to the issue as it involves the fitting process that uses 'predict_proba'.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [svm](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/svm)- [Lines 901 - 1006](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/svm/_base.py#L901-L1006)\n\n This snippet is from the SVM base file and includes the 'predict_proba' method, which is relevant because the issue occurs when 'predict_proba' is called on an SVM base estimator that does not support probability estimates."
    }
}