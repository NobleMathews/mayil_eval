{
    "id": "pydata__xarray-7019",
    "repo_name": "testbed/pydata__xarray__2022.06",
    "task_id": 4202929177821088124,
    "title": "Alternative parallel execution frameworks in xarray",
    "body": "### Is your feature request related to a problem?\n\nSince early on the project xarray has supported wrapping `dask.array` objects in a first-class manner. However recent work on flexible array wrapping has made it possible to wrap all sorts of array types (and with #6804 we should support wrapping any array that conforms to the [array API standard](https://data-apis.org/array-api/latest/index.html)).\r\n\r\nCurrently though the only way to parallelize array operations with xarray \"automatically\" is to use dask. (You could use [xarray-beam](https://github.com/google/xarray-beam) or other options too but they don't \"automatically\" generate the computation for you like dask does.)\r\n\r\nWhen dask is the only type of parallel framework exposing an array-like API then there is no need for flexibility, but now we have nascent projects like [cubed](https://github.com/tomwhite/cubed) to consider too. @tomwhite \n\n### Describe the solution you'd like\n\nRefactor the internals so that dask is one option among many, and that any newer options can plug in in an extensible way.\r\n\r\nIn particular cubed deliberately uses the same API as `dask.array`, exposing:\r\n1) the methods needed to conform to the array API standard\r\n2) a `.chunk` and `.compute` method, which we could dispatch to\r\n3) dask-like functions to create computation graphs including [`blockwise`](https://github.com/tomwhite/cubed/blob/400dc9adcf21c8b468fce9f24e8d4b8cb9ef2f11/cubed/core/ops.py#L43), [`map_blocks`](https://github.com/tomwhite/cubed/blob/400dc9adcf21c8b468fce9f24e8d4b8cb9ef2f11/cubed/core/ops.py#L221), and [`rechunk`](https://github.com/tomwhite/cubed/blob/main/cubed/primitive/rechunk.py)\r\n\r\nI would like to see xarray able to wrap any array-like object which offers this set of methods / functions, and call the corresponding version of that method for the correct library (i.e. dask vs cubed) automatically.\r\n\r\nThat way users could try different parallel execution frameworks simply via a switch like \r\n```python\r\nds.chunk(**chunk_pattern, manager=\"dask\")\r\n```\r\nand see which one works best for their particular problem.\n\n### Describe alternatives you've considered\n\nIf we leave it the way it is now then xarray will not be truly flexible in this respect.\r\n\r\nAny library can wrap (or subclass if they are really brave) xarray objects to provide parallelism but that's not the same level of flexibility.\n\n### Additional context\n\n[cubed repo](https://github.com/tomwhite/cubed)\r\n\r\n[PR](https://github.com/pydata/xarray/pull/6804) about making xarray able to wrap objects conforming to the new [array API standard](https://data-apis.org/array-api/latest/index.html)\r\n\r\ncc @shoyer @rabernat @dcherian @keewis \n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/pydata__xarray__2022.06.git",
    "requested_commit": "",
    "mayil_response": "To implement the feature request for alternative parallel execution frameworks in xarray, follow these steps:\n\n- Refactor the `ChunkManagerEntrypoint` class in `xarray/core/parallelcompat.py` to support a generic interface for chunked array operations such as `.chunk`, `.compute`, and `.rechunk`.\n- Implement subclasses of `ChunkManagerEntrypoint` for each new parallel execution framework (e.g., `CubedManagerEntrypoint` for the `cubed` library).\n- Ensure that each subclass implements the required abstract methods, such as `chunks`, `rechunk`, and `compute`.\n- Modify the `list_chunkmanagers` function to discover and register the new chunk managers via entry points.\n- Update the `xarray.core.computation` module to use the generic chunk manager interface when performing operations that can be parallelized.\n- Add a new parameter to the `Dataset.chunk` method (and other relevant methods) to allow users to specify the chunk manager (e.g., `manager=\"dask\"` or `manager=\"cubed\"`).\n- Ensure that the `Dataset.chunk` method dispatches to the correct chunk manager based on the `manager` parameter.\n- Add unit tests to verify that the new feature works correctly with different parallel execution frameworks.\n- Update the documentation to include examples and guidance on how to use the new feature.\n\nHere is a high-level overview of the changes in pseudo-code:\n\n```python\n# In xarray/core/parallelcompat.py\n\nclass ChunkManagerEntrypoint(ABC):\n    # ... existing code ...\n\n    @abstractmethod\n    def rechunk(self, data, chunks, **kwargs):\n        # ... existing code ...\n\n    @abstractmethod\n    def compute(self, *data, **kwargs):\n        # ... existing code ...\n\n# New subclass for the cubed library\nclass CubedManagerEntrypoint(ChunkManagerEntrypoint):\n    array_cls = CubedArray  # Assuming CubedArray is the array class provided by cubed\n\n    def __init__(self):\n        # ... implementation specific to cubed ...\n\n    def chunks(self, data):\n        # ... implementation specific to cubed ...\n\n    def rechunk(self, data, chunks, **kwargs):\n        # ... implementation specific to cubed ...\n\n    def compute(self, *data, **kwargs):\n        # ... implementation specific to cubed ...\n\n# In xarray/core/dataset.py\n\nclass Dataset:\n    # ... existing code ...\n\n    def chunk(self, chunks=None, name_prefix='xarray-', token=None, lock=False, manager=None):\n        # ... existing code ...\n        if manager is not None:\n            chunk_manager = get_chunk_manager(manager)\n            # Use chunk_manager to perform chunking operations\n        # ... existing code ...\n\n# In xarray/core/computation.py\n\ndef apply_ufunc(..., dask='forbidden', manager=None, ...):\n    # ... existing code ...\n    if dask == 'parallelized' and manager is not None:\n        chunk_manager = get_chunk_manager(manager)\n        # Use chunk_manager to perform parallelized operations\n    # ... existing code ...\n```\n\nRemember to handle edge cases and ensure that the existing functionality for dask arrays remains unchanged. This implementation should be extensible, allowing for the addition of other parallel execution frameworks in the future.\n\n### Code snippets to check\n\n-  \u2192 [xarray](https://github.com/testbed/pydata__xarray__2022.06/tree//xarray) \u2192 [core](https://github.com/testbed/pydata__xarray__2022.06/tree//xarray/core)- [Lines 1 - 68](https://github.com/testbed/pydata__xarray__2022.06/blob//xarray/core/parallelcompat.py#L1-L68)\n\n This snippet defines the mechanism for listing and loading chunk managers, which is directly related to the issue of supporting alternative parallel execution frameworks. \n\n\n   - [Lines 1 - 645](https://github.com/testbed/pydata__xarray__2022.06/blob//xarray/core/parallelcompat.py#L1-L645)\n\n This snippet defines the abstract base class for chunk managers, which is essential for integrating new parallel computing frameworks like cubed. \n\n\n   - [Lines 284 - 645](https://github.com/testbed/pydata__xarray__2022.06/blob//xarray/core/parallelcompat.py#L284-L645)\n\n This snippet shows the implementation of the rechunk method, which is one of the methods that should be supported by alternative parallel execution frameworks.\n-  \u2192 [xarray](https://github.com/testbed/pydata__xarray__2022.06/tree//xarray) \u2192 [core](https://github.com/testbed/pydata__xarray__2022.06/tree//xarray/core)- [Lines 688 - 830](https://github.com/testbed/pydata__xarray__2022.06/blob//xarray/core/computation.py#L688-L830)\n\n This snippet is part of the computation module and shows how dask is currently integrated. Refactoring this to support alternative frameworks like cubed is relevant to the issue.\n-  \u2192 [xarray](https://github.com/testbed/pydata__xarray__2022.06/tree//xarray) \u2192 [core](https://github.com/testbed/pydata__xarray__2022.06/tree//xarray/core)- [Lines 1 - 95](https://github.com/testbed/pydata__xarray__2022.06/blob//xarray/core/pycompat.py#L1-L95)\n\n This snippet includes the DuckArrayModule class, which is used for internal checks and could be relevant when adding support for new array types like cubed.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.000583,
        "snippet_processor": 0.07980000000000001,
        "issue_star_creation": 0.02964,
        "issue_star_solver": 0.06906000000000001,
        "bouncer": 0.02747
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711751755.9017432,
        "relevant_snippets": [
            {
                "code": "name: \ud83d\udca1 Feature Request\ndescription: Suggest an idea for xarray\nlabels: [enhancement]\nbody:\n  - type: textarea\n    id: description\n    attributes:\n      label: Is your feature request related to a problem?\n      description: |\n        Please do a quick search of existing issues to make sure that this has not been asked before.\n        Please provide a clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n    validations:\n      required: true\n  - type: textarea\n    id: solution\n    attributes:\n      label: Describe the solution you'd like\n      description: |\n        A clear and concise description of what you want to happen.\n  - type: textarea\n    id: alternatives\n    attributes:\n      label: Describe alternatives you've considered\n      description: |\n        A clear and concise description of any alternative solutions or features you've considered.\n    validations:\n      required: false\n  - type: textarea\n    id: additional-context\n    attributes:\n      label: Additional context\n      description: |\n        Add any other context about the feature request here.\n    validations:\n      required: false",
                "filename": ".github/ISSUE_TEMPLATE/newfeature.yml",
                "start_index": 0,
                "end_index": 1154,
                "start_line": 1,
                "end_line": 35,
                "max_line": 35,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.06",
                "sha": ""
            },
            {
                "code": "\"\"\"\nThe code in this module is an experiment in going from N=1 to N=2 parallel computing frameworks in xarray.\nIt could later be used as the basis for a public interface allowing any N frameworks to interoperate with xarray,\nbut for now it is just a private experiment.\n\"\"\"\nfrom __future__ import annotations\n\nimport functools\nimport sys\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Iterable, Sequence\nfrom importlib.metadata import EntryPoint, entry_points\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Generic,\n    TypeVar,\n)\n\nimport numpy as np\n\nfrom xarray.core.pycompat import is_chunked_array\n\nT_ChunkedArray = TypeVar(\"T_ChunkedArray\")\n\nif TYPE_CHECKING:\n    from xarray.core.types import T_Chunks, T_NormalizedChunks\n\n\n@functools.lru_cache(maxsize=1)\ndef list_chunkmanagers() -> dict[str, ChunkManagerEntrypoint]:\n    \"\"\"\n    Return a dictionary of available chunk managers and their ChunkManagerEntrypoint subclass objects.\n\n    Returns\n    -------\n    chunkmanagers : dict\n        Dictionary whose values are registered ChunkManagerEntrypoint subclass instances, and whose values\n        are the strings under which they are registered.\n\n    Notes\n    -----\n    # New selection mechanism introduced with Python 3.10. See GH6514.\n    \"\"\"\n    if sys.version_info >= (3, 10):\n        entrypoints = entry_points(group=\"xarray.chunkmanagers\")\n    else:\n        entrypoints = entry_points().get(\"xarray.chunkmanagers\", ())\n\n    return load_chunkmanagers(entrypoints)\n\n\ndef load_chunkmanagers(\n    entrypoints: Sequence[EntryPoint],\n) -> dict[str, ChunkManagerEntrypoint]:\n    \"\"\"Load entrypoints and instantiate chunkmanagers only once.\"\"\"\n\n    loaded_entrypoints = {\n        entrypoint.name: entrypoint.load() for entrypoint in entrypoints\n    }\n\n    available_chunkmanagers = {\n        name: chunkmanager()\n        for name, chunkmanager in loaded_entrypoints.items()\n        if chunkmanager.available\n    }\n    return available_chunkmanagers",
                "filename": "xarray/core/parallelcompat.py",
                "start_index": 0,
                "end_index": 1988,
                "start_line": 1,
                "end_line": 68,
                "max_line": 645,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.06",
                "sha": ""
            },
            {
                "code": "from __future__ import annotations\n\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING, Any, Callable\n\nimport numpy as np\nfrom packaging.version import Version\n\nfrom xarray.core.duck_array_ops import dask_available\nfrom xarray.core.indexing import ImplicitToExplicitIndexingAdapter\nfrom xarray.core.parallelcompat import ChunkManagerEntrypoint, T_ChunkedArray\nfrom xarray.core.pycompat import is_duck_dask_array\n\nif TYPE_CHECKING:\n    from xarray.core.types import DaskArray, T_Chunks, T_NormalizedChunks",
                "filename": "xarray/core/daskmanager.py",
                "start_index": 0,
                "end_index": 533,
                "start_line": 1,
                "end_line": 15,
                "max_line": 215,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.06",
                "sha": ""
            },
            {
                "code": "\"\"\"\n    Interface between a particular parallel computing framework and xarray.\n\n    This abstract base class must be subclassed by libraries implementing chunked array types, and\n    registered via the ``chunkmanagers`` entrypoint.\n\n    Abstract methods on this class must be implemented, whereas non-abstract methods are only required in order to\n    enable a subset of xarray functionality, and by default will raise a ``NotImplementedError`` if called.\n\n    Attributes\n    ----------\n    array_cls\n        Type of the array class this parallel computing framework provides.\n\n        Parallel frameworks need to provide an array class that supports the array API standard.\n        This attribute is used for array instance type checking at runtime.\n    \"\"\"\n\n    array_cls: type[T_ChunkedArray]\n    available: bool = True\n\n    @abstractmethod\n    def __init__(self) -> None:\n        \"\"\"Used to set the array_cls attribute at import time.\"\"\"\n        raise NotImplementedError()\n\n    def is_chunked_array(self, data: Any) -> bool:\n        \"\"\"\n        Check if the given object is an instance of this type of chunked array.\n\n        Compares against the type stored in the array_cls attribute by default.\n\n        Parameters\n        ----------\n        data : Any\n\n        Returns\n        -------\n        is_chunked : bool\n\n        See Also\n        --------\n        dask.is_dask_collection\n        \"\"\"\n        return isinstance(data, self.array_cls)\n\n    @abstractmethod\n    def chunks(self, data: T_ChunkedArray) -> T_NormalizedChunks:\n        \"\"\"\n        Return the current chunks of the given array.\n\n        Returns chunks explicitly as a tuple of tuple of ints.\n\n        Used internally by xarray objects' .chunks and .chunksizes properties.\n\n        Parameters\n        ----------\n        data : chunked array\n\n        Returns\n        -------\n        chunks : tuple[tuple[int, ...], ...]\n\n        See Also\n        --------\n        dask.array.Array.chunks\n        cubed.Array.chunks\n        \"\"\"\n        raise NotImplementedError()",
                "filename": "xarray/core/parallelcompat.py",
                "start_index": 4812,
                "end_index": 6844,
                "start_line": 1,
                "end_line": 645,
                "max_line": 645,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.06",
                "sha": ""
            },
            {
                "code": "def rechunk(\n        self,\n        data: T_ChunkedArray,\n        chunks: T_NormalizedChunks | tuple[int, ...] | T_Chunks,\n        **kwargs,\n    ) -> T_ChunkedArray:\n        \"\"\"\n        Changes the chunking pattern of the given array.\n\n        Called when the .chunk method is called on an xarray object that is already chunked.\n\n        Parameters\n        ----------\n        data : dask array\n            Array to be rechunked.\n        chunks :  int, tuple, dict or str, optional\n            The new block dimensions to create. -1 indicates the full size of the\n            corresponding dimension. Default is \"auto\" which automatically\n            determines chunk sizes.\n\n        Returns\n        -------\n        chunked array\n\n        See Also\n        --------\n        dask.array.Array.rechunk\n        cubed.Array.rechunk\n        \"\"\"\n        return data.rechunk(chunks, **kwargs)  # type: ignore[attr-defined]\n\n    @abstractmethod\n    def compute(self, *data: T_ChunkedArray | Any, **kwargs) -> tuple[np.ndarray, ...]:\n        \"\"\"\n        Computes one or more chunked arrays, returning them as eager numpy arrays.\n\n        Called anytime something needs to computed, including multiple arrays at once.\n        Used by `.compute`, `.persist`, `.values`.\n\n        Parameters\n        ----------\n        *data : object\n            Any number of objects. If an object is an instance of the chunked array type, it is computed\n            and the in-memory result returned as a numpy array. All other types should be passed through unchanged.\n\n        Returns\n        -------\n        objs\n            The input, but with all chunked arrays now computed.\n\n        See Also\n        --------\n        dask.compute\n        cubed.compute\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    def array_api(self) -> Any:\n        \"\"\"\n        Return the array_api namespace following the python array API standard.\n\n        See https://data-apis.org/array-api/latest/ . Currently used to access the array API function\n        ``full_like``, which is called within the xarray constructors ``xarray.full_like``, ``xarray.ones_like``,\n        ``xarray.zeros_like``, etc.\n\n        See Also\n        --------\n        dask.array\n        cubed.array_api\n        \"\"\"\n        raise NotImplementedError()",
                "filename": "xarray/core/parallelcompat.py",
                "start_index": 8979,
                "end_index": 11271,
                "start_line": 284,
                "end_line": 645,
                "max_line": 645,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.06",
                "sha": ""
            },
            {
                "code": "\"\"\"Compatibility module defining operations on duck numpy-arrays.\n\nCurrently, this means Dask or NumPy arrays. None of these functions should\naccept or return xarray objects.\n\"\"\"\nfrom __future__ import annotations\n\nimport contextlib\nimport datetime\nimport inspect\nimport warnings\nfrom functools import partial\nfrom importlib import import_module\n\nimport numpy as np\nimport pandas as pd\nfrom numpy import all as array_all  # noqa\nfrom numpy import any as array_any  # noqa\nfrom numpy import (  # noqa\n    around,  # noqa\n    einsum,\n    gradient,\n    isclose,\n    isin,\n    isnat,\n    take,\n    tensordot,\n    transpose,\n    unravel_index,\n    zeros_like,  # noqa\n)\nfrom numpy import concatenate as _concatenate\nfrom numpy.core.multiarray import normalize_axis_index  # type: ignore[attr-defined]\nfrom numpy.lib.stride_tricks import sliding_window_view  # noqa\n\nfrom xarray.core import dask_array_ops, dtypes, nputils\nfrom xarray.core.parallelcompat import get_chunked_array_type, is_chunked_array\nfrom xarray.core.pycompat import array_type, is_duck_dask_array\nfrom xarray.core.utils import is_duck_array, module_available\n\ndask_available = module_available(\"dask\")\n\n\ndef get_array_namespace(x):\n    if hasattr(x, \"__array_namespace__\"):\n        return x.__array_namespace__()\n    else:\n        return np\n\n\ndef _dask_or_eager_func(\n    name,\n    eager_module=np,\n    dask_module=\"dask.array\",\n):\n    \"\"\"Create a function that dispatches to dask for dask array inputs.\"\"\"\n\n    def f(*args, **kwargs):\n        if any(is_duck_dask_array(a) for a in args):\n            mod = (\n                import_module(dask_module)\n                if isinstance(dask_module, str)\n                else dask_module\n            )\n            wrapped = getattr(mod, name)\n        else:\n            wrapped = getattr(eager_module, name)\n        return wrapped(*args, **kwargs)\n\n    return f\n\n\ndef fail_on_dask_array_input(values, msg=None, func_name=None):\n    if is_duck_dask_array(values):\n        if msg is None:\n            msg = \"%r is not yet a valid method on dask arrays\"\n        if func_name is None:\n            func_name = inspect.stack()[1][3]\n        raise NotImplementedError(msg % func_name)\n\n\n# Requires special-casing because pandas won't automatically dispatch to dask.isnull via NEP-18\npandas_isnull = _dask_or_eager_func(\"isnull\", eager_module=pd, dask_module=\"dask.array\")\n\n# np.around has failing doctests, overwrite it so they pass:\n# https://github.com/numpy/numpy/issues/19759\naround.__doc__ = str.replace(\n    around.__doc__ or \"\",\n    \"array([0.,  2.])\",\n    \"array([0., 2.])\",\n)\naround.__doc__ = str.replace(\n    around.__doc__ or \"\",\n    \"array([0.,  2.])\",\n    \"array([0., 2.])\",\n)\naround.__doc__ = str.replace(\n    around.__doc__ or \"\",\n    \"array([0.4,  1.6])\",\n    \"array([0.4, 1.6])\",\n)\naround.__doc__ = str.replace(\n    around.__doc__ or \"\",\n    \"array([0.,  2.,  2.,  4.,  4.])\",\n    \"array([0., 2., 2., 4., 4.])\",\n)",
                "filename": "xarray/core/duck_array_ops.py",
                "start_index": 0,
                "end_index": 2931,
                "start_line": 1,
                "end_line": 709,
                "max_line": 709,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.06",
                "sha": ""
            },
            {
                "code": "elif dask == \"parallelized\":\n            chunkmanager = get_chunked_array_type(*input_data)\n\n            numpy_func = func\n\n            if dask_gufunc_kwargs is None:\n                dask_gufunc_kwargs = {}\n            else:\n                dask_gufunc_kwargs = dask_gufunc_kwargs.copy()\n\n            allow_rechunk = dask_gufunc_kwargs.get(\"allow_rechunk\", None)\n            if allow_rechunk is None:\n                for n, (data, core_dims) in enumerate(\n                    zip(input_data, signature.input_core_dims)\n                ):\n                    if is_chunked_array(data):\n                        # core dimensions cannot span multiple chunks\n                        for axis, dim in enumerate(core_dims, start=-len(core_dims)):\n                            if len(data.chunks[axis]) != 1:\n                                raise ValueError(\n                                    f\"dimension {dim} on {n}th function argument to \"\n                                    \"apply_ufunc with dask='parallelized' consists of \"\n                                    \"multiple chunks, but is also a core dimension. To \"\n                                    \"fix, either rechunk into a single array chunk along \"\n                                    f\"this dimension, i.e., ``.chunk(dict({dim}=-1))``, or \"\n                                    \"pass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` \"\n                                    \"but beware that this may significantly increase memory usage.\"\n                                )\n                dask_gufunc_kwargs[\"allow_rechunk\"] = True\n\n            output_sizes = dask_gufunc_kwargs.pop(\"output_sizes\", {})\n            if output_sizes:\n                output_sizes_renamed = {}\n                for key, value in output_sizes.items():\n                    if key not in signature.all_output_core_dims:\n                        raise ValueError(\n                            f\"dimension '{key}' in 'output_sizes' must correspond to output_core_dims\"\n                        )\n                    output_sizes_renamed[signature.dims_map[key]] = value\n                dask_gufunc_kwargs[\"output_sizes\"] = output_sizes_renamed\n\n            for key in signature.all_output_core_dims:\n                if (\n                    key not in signature.all_input_core_dims or key in exclude_dims\n                ) and key not in output_sizes:\n                    raise ValueError(\n                        f\"dimension '{key}' in 'output_core_dims' needs corresponding (dim, size) in 'output_sizes'\"\n                    )\n\n            def func(*arrays):\n                res = chunkmanager.apply_gufunc(\n                    numpy_func,\n                    signature.to_gufunc_string(exclude_dims),\n                    *arrays,\n                    vectorize=vectorize,\n                    output_dtypes=output_dtypes,\n                    **dask_gufunc_kwargs,\n                )\n\n                return res\n\n        elif dask == \"allowed\":\n            pass",
                "filename": "xarray/core/computation.py",
                "start_index": 22684,
                "end_index": 25667,
                "start_line": 688,
                "end_line": 830,
                "max_line": 2165,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.06",
                "sha": ""
            },
            {
                "code": "from __future__ import annotations\n\nfrom importlib import import_module\nfrom types import ModuleType\nfrom typing import TYPE_CHECKING, Any, Literal\n\nimport numpy as np\nfrom packaging.version import Version\n\nfrom xarray.core.utils import is_duck_array, is_scalar, module_available\n\ninteger_types = (int, np.integer)\n\nif TYPE_CHECKING:\n    ModType = Literal[\"dask\", \"pint\", \"cupy\", \"sparse\", \"cubed\"]\n    DuckArrayTypes = tuple[type[Any], ...]  # TODO: improve this? maybe Generic\n\n\nclass DuckArrayModule:\n    \"\"\"\n    Solely for internal isinstance and version checks.\n\n    Motivated by having to only import pint when required (as pint currently imports xarray)\n    https://github.com/pydata/xarray/pull/5561#discussion_r664815718\n    \"\"\"\n\n    module: ModuleType | None\n    version: Version\n    type: DuckArrayTypes\n    available: bool\n\n    def __init__(self, mod: ModType) -> None:\n        duck_array_module: ModuleType | None\n        duck_array_version: Version\n        duck_array_type: DuckArrayTypes\n        try:\n            duck_array_module = import_module(mod)\n            duck_array_version = Version(duck_array_module.__version__)\n\n            if mod == \"dask\":\n                duck_array_type = (import_module(\"dask.array\").Array,)\n            elif mod == \"pint\":\n                duck_array_type = (duck_array_module.Quantity,)\n            elif mod == \"cupy\":\n                duck_array_type = (duck_array_module.ndarray,)\n            elif mod == \"sparse\":\n                duck_array_type = (duck_array_module.SparseArray,)\n            elif mod == \"cubed\":\n                duck_array_type = (duck_array_module.Array,)\n            else:\n                raise NotImplementedError\n\n        except (ImportError, AttributeError):  # pragma: no cover\n            duck_array_module = None\n            duck_array_version = Version(\"0.0.0\")\n            duck_array_type = ()\n\n        self.module = duck_array_module\n        self.version = duck_array_version\n        self.type = duck_array_type\n        self.available = duck_array_module is not None\n\n\n_cached_duck_array_modules: dict[ModType, DuckArrayModule] = {}\n\n\ndef _get_cached_duck_array_module(mod: ModType) -> DuckArrayModule:\n    if mod not in _cached_duck_array_modules:\n        duckmod = DuckArrayModule(mod)\n        _cached_duck_array_modules[mod] = duckmod\n        return duckmod\n    else:\n        return _cached_duck_array_modules[mod]\n\n\ndef array_type(mod: ModType) -> DuckArrayTypes:\n    \"\"\"Quick wrapper to get the array class of the module.\"\"\"\n    return _get_cached_duck_array_module(mod).type\n\n\ndef mod_version(mod: ModType) -> Version:\n    \"\"\"Quick wrapper to get the version of the module.\"\"\"\n    return _get_cached_duck_array_module(mod).version\n\n\ndef is_dask_collection(x):\n    if module_available(\"dask\"):\n        from dask.base import is_dask_collection\n\n        return is_dask_collection(x)\n    return False\n\n\ndef is_duck_dask_array(x):\n    return is_duck_array(x) and is_dask_collection(x)",
                "filename": "xarray/core/pycompat.py",
                "start_index": 0,
                "end_index": 2966,
                "start_line": 1,
                "end_line": 95,
                "max_line": 103,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.06",
                "sha": ""
            },
            {
                "code": "from __future__ import annotations\n\nimport datetime\nimport sys\nfrom collections.abc import Hashable, Iterable, Iterator, Mapping, Sequence\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Literal,\n    Protocol,\n    SupportsIndex,\n    TypeVar,\n    Union,\n)\n\nimport numpy as np\nimport pandas as pd\n\ntry:\n    if sys.version_info >= (3, 11):\n        from typing import Self\n    else:\n        from typing_extensions import Self\nexcept ImportError:\n    if TYPE_CHECKING:\n        raise\n    else:\n        Self: Any = None\n\nif TYPE_CHECKING:\n    from numpy._typing import _SupportsDType\n    from numpy.typing import ArrayLike\n\n    from xarray.backends.common import BackendEntrypoint\n    from xarray.core.alignment import Aligner\n    from xarray.core.common import AbstractArray, DataWithCoords\n    from xarray.core.coordinates import Coordinates\n    from xarray.core.dataarray import DataArray\n    from xarray.core.dataset import Dataset\n    from xarray.core.groupby import DataArrayGroupBy, GroupBy\n    from xarray.core.indexes import Index, Indexes\n    from xarray.core.utils import Frozen\n    from xarray.core.variable import Variable\n\n    try:\n        from dask.array import Array as DaskArray\n    except ImportError:\n        DaskArray = np.ndarray  # type: ignore\n\n    try:\n        from cubed import Array as CubedArray\n    except ImportError:\n        CubedArray = np.ndarray\n\n    try:\n        from zarr.core import Array as ZarrArray\n    except ImportError:\n        ZarrArray = np.ndarray\n\n    # Anything that can be coerced to a shape tuple\n    _ShapeLike = Union[SupportsIndex, Sequence[SupportsIndex]]\n    _DTypeLikeNested = Any  # TODO: wait for support for recursive types\n\n    # Xarray requires a Mapping[Hashable, dtype] in many places which\n    # conflics with numpys own DTypeLike (with dtypes for fields).\n    # https://numpy.org/devdocs/reference/typing.html#numpy.typing.DTypeLike\n    # This is a copy of this DTypeLike that allows only non-Mapping dtypes.\n    DTypeLikeSave = Union[\n        np.dtype[Any],\n        # default data type (float64)\n        None,\n        # array-scalar types and generic types\n        type[Any],\n        # character codes, type strings or comma-separated fields, e.g., 'float64'\n        str,\n        # (flexible_dtype, itemsize)\n        tuple[_DTypeLikeNested, int],\n        # (fixed_dtype, shape)\n        tuple[_DTypeLikeNested, _ShapeLike],\n        # (base_dtype, new_dtype)\n        tuple[_DTypeLikeNested, _DTypeLikeNested],\n        # because numpy does the same?\n        list[Any],\n        # anything with a dtype attribute\n        _SupportsDType[np.dtype[Any]],\n    ]\n    try:\n        from cftime import datetime as CFTimeDatetime\n    except ImportError:\n        CFTimeDatetime = Any\n    DatetimeLike = Union[pd.Timestamp, datetime.datetime, np.datetime64, CFTimeDatetime]\nelse:\n    DTypeLikeSave: Any = None",
                "filename": "xarray/core/types.py",
                "start_index": 0,
                "end_index": 2870,
                "start_line": 1,
                "end_line": 94,
                "max_line": 264,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.06",
                "sha": ""
            },
            {
                "code": "def astype(data, dtype, **kwargs):\n    if hasattr(data, \"__array_namespace__\"):\n        xp = get_array_namespace(data)\n        return xp.astype(data, dtype, **kwargs)\n    return data.astype(dtype, **kwargs)\n\n\ndef asarray(data, xp=np):\n    return data if is_duck_array(data) else xp.asarray(data)\n\n\ndef as_shared_dtype(scalars_or_arrays, xp=np):\n    \"\"\"Cast a arrays to a shared dtype using xarray's type promotion rules.\"\"\"\n    array_type_cupy = array_type(\"cupy\")\n    if array_type_cupy and any(\n        isinstance(x, array_type_cupy) for x in scalars_or_arrays\n    ):\n        import cupy as cp\n\n        arrays = [asarray(x, xp=cp) for x in scalars_or_arrays]\n    else:\n        arrays = [asarray(x, xp=xp) for x in scalars_or_arrays]\n    # Pass arrays directly instead of dtypes to result_type so scalars\n    # get handled properly.\n    # Note that result_type() safely gets the dtype from dask arrays without\n    # evaluating them.\n    out_type = dtypes.result_type(*arrays)\n    return [astype(x, out_type, copy=False) for x in arrays]\n\n\ndef broadcast_to(array, shape):\n    xp = get_array_namespace(array)\n    return xp.broadcast_to(array, shape)\n\n\ndef lazy_array_equiv(arr1, arr2):\n    \"\"\"Like array_equal, but doesn't actually compare values.\n    Returns True when arr1, arr2 identical or their dask tokens are equal.\n    Returns False when shapes are not equal.\n    Returns None when equality cannot determined: one or both of arr1, arr2 are numpy arrays;\n    or their dask tokens are not equal\n    \"\"\"\n    if arr1 is arr2:\n        return True\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    if arr1.shape != arr2.shape:\n        return False\n    if dask_available and is_duck_dask_array(arr1) and is_duck_dask_array(arr2):\n        from dask.base import tokenize\n\n        # GH3068, GH4221\n        if tokenize(arr1) == tokenize(arr2):\n            return True\n        else:\n            return None\n    return None\n\n\ndef allclose_or_equiv(arr1, arr2, rtol=1e-5, atol=1e-8):\n    \"\"\"Like np.allclose, but also allows values to be NaN in both arrays\"\"\"\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n\n    lazy_equiv = lazy_array_equiv(arr1, arr2)\n    if lazy_equiv is None:\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", r\"All-NaN (slice|axis) encountered\")\n            return bool(isclose(arr1, arr2, rtol=rtol, atol=atol, equal_nan=True).all())\n    else:\n        return lazy_equiv\n\n\ndef array_equiv(arr1, arr2):\n    \"\"\"Like np.array_equal, but also allows values to be NaN in both arrays\"\"\"\n    arr1 = asarray(arr1)\n    arr2 = asarray(arr2)\n    lazy_equiv = lazy_array_equiv(arr1, arr2)\n    if lazy_equiv is None:\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", \"In the future, 'NAT == x'\")\n            flag_array = (arr1 == arr2) | (isnull(arr1) & isnull(arr2))\n            return bool(flag_array.all())\n    else:\n        return lazy_equiv",
                "filename": "xarray/core/duck_array_ops.py",
                "start_index": 5731,
                "end_index": 8657,
                "start_line": 184,
                "end_line": 284,
                "max_line": 709,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__2022.06",
                "sha": ""
            }
        ],
        "classification": "feature request",
        "snippet_processor_task": {
            "xarray/core/parallelcompat.py": [
                {
                    "chunk": {
                        "code": "\"\"\"\nThe code in this module is an experiment in going from N=1 to N=2 parallel computing frameworks in xarray.\nIt could later be used as the basis for a public interface allowing any N frameworks to interoperate with xarray,\nbut for now it is just a private experiment.\n\"\"\"\nfrom __future__ import annotations\n\nimport functools\nimport sys\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Iterable, Sequence\nfrom importlib.metadata import EntryPoint, entry_points\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Generic,\n    TypeVar,\n)\n\nimport numpy as np\n\nfrom xarray.core.pycompat import is_chunked_array\n\nT_ChunkedArray = TypeVar(\"T_ChunkedArray\")\n\nif TYPE_CHECKING:\n    from xarray.core.types import T_Chunks, T_NormalizedChunks\n\n\n@functools.lru_cache(maxsize=1)\ndef list_chunkmanagers() -> dict[str, ChunkManagerEntrypoint]:\n    \"\"\"\n    Return a dictionary of available chunk managers and their ChunkManagerEntrypoint subclass objects.\n\n    Returns\n    -------\n    chunkmanagers : dict\n        Dictionary whose values are registered ChunkManagerEntrypoint subclass instances, and whose values\n        are the strings under which they are registered.\n\n    Notes\n    -----\n    # New selection mechanism introduced with Python 3.10. See GH6514.\n    \"\"\"\n    if sys.version_info >= (3, 10):\n        entrypoints = entry_points(group=\"xarray.chunkmanagers\")\n    else:\n        entrypoints = entry_points().get(\"xarray.chunkmanagers\", ())\n\n    return load_chunkmanagers(entrypoints)\n\n\ndef load_chunkmanagers(\n    entrypoints: Sequence[EntryPoint],\n) -> dict[str, ChunkManagerEntrypoint]:\n    \"\"\"Load entrypoints and instantiate chunkmanagers only once.\"\"\"\n\n    loaded_entrypoints = {\n        entrypoint.name: entrypoint.load() for entrypoint in entrypoints\n    }\n\n    available_chunkmanagers = {\n        name: chunkmanager()\n        for name, chunkmanager in loaded_entrypoints.items()\n        if chunkmanager.available\n    }\n    return available_chunkmanagers",
                        "filename": "xarray/core/parallelcompat.py",
                        "start_index": 0,
                        "end_index": 1988,
                        "start_line": 1,
                        "end_line": 68,
                        "max_line": 645,
                        "git_instance": "github",
                        "repo_name": "testbed/pydata__xarray__2022.06",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet defines the mechanism for listing and loading chunk managers, which is directly related to the issue of supporting alternative parallel execution frameworks."
                },
                {
                    "chunk": {
                        "code": "\"\"\"\n    Interface between a particular parallel computing framework and xarray.\n\n    This abstract base class must be subclassed by libraries implementing chunked array types, and\n    registered via the ``chunkmanagers`` entrypoint.\n\n    Abstract methods on this class must be implemented, whereas non-abstract methods are only required in order to\n    enable a subset of xarray functionality, and by default will raise a ``NotImplementedError`` if called.\n\n    Attributes\n    ----------\n    array_cls\n        Type of the array class this parallel computing framework provides.\n\n        Parallel frameworks need to provide an array class that supports the array API standard.\n        This attribute is used for array instance type checking at runtime.\n    \"\"\"\n\n    array_cls: type[T_ChunkedArray]\n    available: bool = True\n\n    @abstractmethod\n    def __init__(self) -> None:\n        \"\"\"Used to set the array_cls attribute at import time.\"\"\"\n        raise NotImplementedError()\n\n    def is_chunked_array(self, data: Any) -> bool:\n        \"\"\"\n        Check if the given object is an instance of this type of chunked array.\n\n        Compares against the type stored in the array_cls attribute by default.\n\n        Parameters\n        ----------\n        data : Any\n\n        Returns\n        -------\n        is_chunked : bool\n\n        See Also\n        --------\n        dask.is_dask_collection\n        \"\"\"\n        return isinstance(data, self.array_cls)\n\n    @abstractmethod\n    def chunks(self, data: T_ChunkedArray) -> T_NormalizedChunks:\n        \"\"\"\n        Return the current chunks of the given array.\n\n        Returns chunks explicitly as a tuple of tuple of ints.\n\n        Used internally by xarray objects' .chunks and .chunksizes properties.\n\n        Parameters\n        ----------\n        data : chunked array\n\n        Returns\n        -------\n        chunks : tuple[tuple[int, ...], ...]\n\n        See Also\n        --------\n        dask.array.Array.chunks\n        cubed.Array.chunks\n        \"\"\"\n        raise NotImplementedError()",
                        "filename": "xarray/core/parallelcompat.py",
                        "start_index": 4812,
                        "end_index": 6844,
                        "start_line": 1,
                        "end_line": 645,
                        "max_line": 645,
                        "git_instance": "github",
                        "repo_name": "testbed/pydata__xarray__2022.06",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet defines the abstract base class for chunk managers, which is essential for integrating new parallel computing frameworks like cubed."
                },
                {
                    "chunk": {
                        "code": "def rechunk(\n        self,\n        data: T_ChunkedArray,\n        chunks: T_NormalizedChunks | tuple[int, ...] | T_Chunks,\n        **kwargs,\n    ) -> T_ChunkedArray:\n        \"\"\"\n        Changes the chunking pattern of the given array.\n\n        Called when the .chunk method is called on an xarray object that is already chunked.\n\n        Parameters\n        ----------\n        data : dask array\n            Array to be rechunked.\n        chunks :  int, tuple, dict or str, optional\n            The new block dimensions to create. -1 indicates the full size of the\n            corresponding dimension. Default is \"auto\" which automatically\n            determines chunk sizes.\n\n        Returns\n        -------\n        chunked array\n\n        See Also\n        --------\n        dask.array.Array.rechunk\n        cubed.Array.rechunk\n        \"\"\"\n        return data.rechunk(chunks, **kwargs)  # type: ignore[attr-defined]\n\n    @abstractmethod\n    def compute(self, *data: T_ChunkedArray | Any, **kwargs) -> tuple[np.ndarray, ...]:\n        \"\"\"\n        Computes one or more chunked arrays, returning them as eager numpy arrays.\n\n        Called anytime something needs to computed, including multiple arrays at once.\n        Used by `.compute`, `.persist`, `.values`.\n\n        Parameters\n        ----------\n        *data : object\n            Any number of objects. If an object is an instance of the chunked array type, it is computed\n            and the in-memory result returned as a numpy array. All other types should be passed through unchanged.\n\n        Returns\n        -------\n        objs\n            The input, but with all chunked arrays now computed.\n\n        See Also\n        --------\n        dask.compute\n        cubed.compute\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    def array_api(self) -> Any:\n        \"\"\"\n        Return the array_api namespace following the python array API standard.\n\n        See https://data-apis.org/array-api/latest/ . Currently used to access the array API function\n        ``full_like``, which is called within the xarray constructors ``xarray.full_like``, ``xarray.ones_like``,\n        ``xarray.zeros_like``, etc.\n\n        See Also\n        --------\n        dask.array\n        cubed.array_api\n        \"\"\"\n        raise NotImplementedError()",
                        "filename": "xarray/core/parallelcompat.py",
                        "start_index": 8979,
                        "end_index": 11271,
                        "start_line": 284,
                        "end_line": 645,
                        "max_line": 645,
                        "git_instance": "github",
                        "repo_name": "testbed/pydata__xarray__2022.06",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the implementation of the rechunk method, which is one of the methods that should be supported by alternative parallel execution frameworks."
                }
            ],
            "xarray/core/computation.py": [
                {
                    "chunk": {
                        "code": "elif dask == \"parallelized\":\n            chunkmanager = get_chunked_array_type(*input_data)\n\n            numpy_func = func\n\n            if dask_gufunc_kwargs is None:\n                dask_gufunc_kwargs = {}\n            else:\n                dask_gufunc_kwargs = dask_gufunc_kwargs.copy()\n\n            allow_rechunk = dask_gufunc_kwargs.get(\"allow_rechunk\", None)\n            if allow_rechunk is None:\n                for n, (data, core_dims) in enumerate(\n                    zip(input_data, signature.input_core_dims)\n                ):\n                    if is_chunked_array(data):\n                        # core dimensions cannot span multiple chunks\n                        for axis, dim in enumerate(core_dims, start=-len(core_dims)):\n                            if len(data.chunks[axis]) != 1:\n                                raise ValueError(\n                                    f\"dimension {dim} on {n}th function argument to \"\n                                    \"apply_ufunc with dask='parallelized' consists of \"\n                                    \"multiple chunks, but is also a core dimension. To \"\n                                    \"fix, either rechunk into a single array chunk along \"\n                                    f\"this dimension, i.e., ``.chunk(dict({dim}=-1))``, or \"\n                                    \"pass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` \"\n                                    \"but beware that this may significantly increase memory usage.\"\n                                )\n                dask_gufunc_kwargs[\"allow_rechunk\"] = True\n\n            output_sizes = dask_gufunc_kwargs.pop(\"output_sizes\", {})\n            if output_sizes:\n                output_sizes_renamed = {}\n                for key, value in output_sizes.items():\n                    if key not in signature.all_output_core_dims:\n                        raise ValueError(\n                            f\"dimension '{key}' in 'output_sizes' must correspond to output_core_dims\"\n                        )\n                    output_sizes_renamed[signature.dims_map[key]] = value\n                dask_gufunc_kwargs[\"output_sizes\"] = output_sizes_renamed\n\n            for key in signature.all_output_core_dims:\n                if (\n                    key not in signature.all_input_core_dims or key in exclude_dims\n                ) and key not in output_sizes:\n                    raise ValueError(\n                        f\"dimension '{key}' in 'output_core_dims' needs corresponding (dim, size) in 'output_sizes'\"\n                    )\n\n            def func(*arrays):\n                res = chunkmanager.apply_gufunc(\n                    numpy_func,\n                    signature.to_gufunc_string(exclude_dims),\n                    *arrays,\n                    vectorize=vectorize,\n                    output_dtypes=output_dtypes,\n                    **dask_gufunc_kwargs,\n                )\n\n                return res\n\n        elif dask == \"allowed\":\n            pass",
                        "filename": "xarray/core/computation.py",
                        "start_index": 22684,
                        "end_index": 25667,
                        "start_line": 688,
                        "end_line": 830,
                        "max_line": 2165,
                        "git_instance": "github",
                        "repo_name": "testbed/pydata__xarray__2022.06",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet is part of the computation module and shows how dask is currently integrated. Refactoring this to support alternative frameworks like cubed is relevant to the issue."
                }
            ],
            "xarray/core/pycompat.py": [
                {
                    "chunk": {
                        "code": "from __future__ import annotations\n\nfrom importlib import import_module\nfrom types import ModuleType\nfrom typing import TYPE_CHECKING, Any, Literal\n\nimport numpy as np\nfrom packaging.version import Version\n\nfrom xarray.core.utils import is_duck_array, is_scalar, module_available\n\ninteger_types = (int, np.integer)\n\nif TYPE_CHECKING:\n    ModType = Literal[\"dask\", \"pint\", \"cupy\", \"sparse\", \"cubed\"]\n    DuckArrayTypes = tuple[type[Any], ...]  # TODO: improve this? maybe Generic\n\n\nclass DuckArrayModule:\n    \"\"\"\n    Solely for internal isinstance and version checks.\n\n    Motivated by having to only import pint when required (as pint currently imports xarray)\n    https://github.com/pydata/xarray/pull/5561#discussion_r664815718\n    \"\"\"\n\n    module: ModuleType | None\n    version: Version\n    type: DuckArrayTypes\n    available: bool\n\n    def __init__(self, mod: ModType) -> None:\n        duck_array_module: ModuleType | None\n        duck_array_version: Version\n        duck_array_type: DuckArrayTypes\n        try:\n            duck_array_module = import_module(mod)\n            duck_array_version = Version(duck_array_module.__version__)\n\n            if mod == \"dask\":\n                duck_array_type = (import_module(\"dask.array\").Array,)\n            elif mod == \"pint\":\n                duck_array_type = (duck_array_module.Quantity,)\n            elif mod == \"cupy\":\n                duck_array_type = (duck_array_module.ndarray,)\n            elif mod == \"sparse\":\n                duck_array_type = (duck_array_module.SparseArray,)\n            elif mod == \"cubed\":\n                duck_array_type = (duck_array_module.Array,)\n            else:\n                raise NotImplementedError\n\n        except (ImportError, AttributeError):  # pragma: no cover\n            duck_array_module = None\n            duck_array_version = Version(\"0.0.0\")\n            duck_array_type = ()\n\n        self.module = duck_array_module\n        self.version = duck_array_version\n        self.type = duck_array_type\n        self.available = duck_array_module is not None\n\n\n_cached_duck_array_modules: dict[ModType, DuckArrayModule] = {}\n\n\ndef _get_cached_duck_array_module(mod: ModType) -> DuckArrayModule:\n    if mod not in _cached_duck_array_modules:\n        duckmod = DuckArrayModule(mod)\n        _cached_duck_array_modules[mod] = duckmod\n        return duckmod\n    else:\n        return _cached_duck_array_modules[mod]\n\n\ndef array_type(mod: ModType) -> DuckArrayTypes:\n    \"\"\"Quick wrapper to get the array class of the module.\"\"\"\n    return _get_cached_duck_array_module(mod).type\n\n\ndef mod_version(mod: ModType) -> Version:\n    \"\"\"Quick wrapper to get the version of the module.\"\"\"\n    return _get_cached_duck_array_module(mod).version\n\n\ndef is_dask_collection(x):\n    if module_available(\"dask\"):\n        from dask.base import is_dask_collection\n\n        return is_dask_collection(x)\n    return False\n\n\ndef is_duck_dask_array(x):\n    return is_duck_array(x) and is_dask_collection(x)",
                        "filename": "xarray/core/pycompat.py",
                        "start_index": 0,
                        "end_index": 2966,
                        "start_line": 1,
                        "end_line": 95,
                        "max_line": 103,
                        "git_instance": "github",
                        "repo_name": "testbed/pydata__xarray__2022.06",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes the DuckArrayModule class, which is used for internal checks and could be relevant when adding support for new array types like cubed."
                }
            ]
        },
        "solution_task": "To implement the feature request for alternative parallel execution frameworks in xarray, follow these steps:\n\n- Refactor the `ChunkManagerEntrypoint` class in `xarray/core/parallelcompat.py` to support a generic interface for chunked array operations such as `.chunk`, `.compute`, and `.rechunk`.\n- Implement subclasses of `ChunkManagerEntrypoint` for each new parallel execution framework (e.g., `CubedManagerEntrypoint` for the `cubed` library).\n- Ensure that each subclass implements the required abstract methods, such as `chunks`, `rechunk`, and `compute`.\n- Modify the `list_chunkmanagers` function to discover and register the new chunk managers via entry points.\n- Update the `xarray.core.computation` module to use the generic chunk manager interface when performing operations that can be parallelized.\n- Add a new parameter to the `Dataset.chunk` method (and other relevant methods) to allow users to specify the chunk manager (e.g., `manager=\"dask\"` or `manager=\"cubed\"`).\n- Ensure that the `Dataset.chunk` method dispatches to the correct chunk manager based on the `manager` parameter.\n- Add unit tests to verify that the new feature works correctly with different parallel execution frameworks.\n- Update the documentation to include examples and guidance on how to use the new feature.\n\nHere is a high-level overview of the changes in pseudo-code:\n\n```python\n# In xarray/core/parallelcompat.py\n\nclass ChunkManagerEntrypoint(ABC):\n    # ... existing code ...\n\n    @abstractmethod\n    def rechunk(self, data, chunks, **kwargs):\n        # ... existing code ...\n\n    @abstractmethod\n    def compute(self, *data, **kwargs):\n        # ... existing code ...\n\n# New subclass for the cubed library\nclass CubedManagerEntrypoint(ChunkManagerEntrypoint):\n    array_cls = CubedArray  # Assuming CubedArray is the array class provided by cubed\n\n    def __init__(self):\n        # ... implementation specific to cubed ...\n\n    def chunks(self, data):\n        # ... implementation specific to cubed ...\n\n    def rechunk(self, data, chunks, **kwargs):\n        # ... implementation specific to cubed ...\n\n    def compute(self, *data, **kwargs):\n        # ... implementation specific to cubed ...\n\n# In xarray/core/dataset.py\n\nclass Dataset:\n    # ... existing code ...\n\n    def chunk(self, chunks=None, name_prefix='xarray-', token=None, lock=False, manager=None):\n        # ... existing code ...\n        if manager is not None:\n            chunk_manager = get_chunk_manager(manager)\n            # Use chunk_manager to perform chunking operations\n        # ... existing code ...\n\n# In xarray/core/computation.py\n\ndef apply_ufunc(..., dask='forbidden', manager=None, ...):\n    # ... existing code ...\n    if dask == 'parallelized' and manager is not None:\n        chunk_manager = get_chunk_manager(manager)\n        # Use chunk_manager to perform parallelized operations\n    # ... existing code ...\n```\n\nRemember to handle edge cases and ensure that the existing functionality for dask arrays remains unchanged. This implementation should be extensible, allowing for the addition of other parallel execution frameworks in the future.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE must provide precise, actionable guidance and directly address all aspects of the described feature request to facilitate its successful implementation or improvement.",
            "supporting_evidence": "The RESPONSE provides a comprehensive and detailed plan for implementing the feature request for alternative parallel execution frameworks in xarray. It outlines specific steps to refactor the existing codebase, create new subclasses for each parallel execution framework, and update the `Dataset.chunk` method to allow users to specify the chunk manager. The RESPONSE also includes pseudo-code to illustrate the proposed changes, which adds clarity and actionability to the suggestions. Additionally, the RESPONSE addresses the need for extensibility, ensuring that the implementation can accommodate future parallel execution frameworks. The inclusion of unit tests and documentation updates further demonstrates a thorough understanding of the feature development process. The only reason it does not score a 10 is that it does not explicitly mention ensuring compatibility with the array API standard, which is a key aspect of the feature request. However, the RESPONSE implicitly suggests this by mentioning the need to conform to the required abstract methods and by providing a generic interface for chunked array operations.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711751832.8881652,
        "result": "To implement the feature request for alternative parallel execution frameworks in xarray, follow these steps:\n\n- Refactor the `ChunkManagerEntrypoint` class in `xarray/core/parallelcompat.py` to support a generic interface for chunked array operations such as `.chunk`, `.compute`, and `.rechunk`.\n- Implement subclasses of `ChunkManagerEntrypoint` for each new parallel execution framework (e.g., `CubedManagerEntrypoint` for the `cubed` library).\n- Ensure that each subclass implements the required abstract methods, such as `chunks`, `rechunk`, and `compute`.\n- Modify the `list_chunkmanagers` function to discover and register the new chunk managers via entry points.\n- Update the `xarray.core.computation` module to use the generic chunk manager interface when performing operations that can be parallelized.\n- Add a new parameter to the `Dataset.chunk` method (and other relevant methods) to allow users to specify the chunk manager (e.g., `manager=\"dask\"` or `manager=\"cubed\"`).\n- Ensure that the `Dataset.chunk` method dispatches to the correct chunk manager based on the `manager` parameter.\n- Add unit tests to verify that the new feature works correctly with different parallel execution frameworks.\n- Update the documentation to include examples and guidance on how to use the new feature.\n\nHere is a high-level overview of the changes in pseudo-code:\n\n```python\n# In xarray/core/parallelcompat.py\n\nclass ChunkManagerEntrypoint(ABC):\n    # ... existing code ...\n\n    @abstractmethod\n    def rechunk(self, data, chunks, **kwargs):\n        # ... existing code ...\n\n    @abstractmethod\n    def compute(self, *data, **kwargs):\n        # ... existing code ...\n\n# New subclass for the cubed library\nclass CubedManagerEntrypoint(ChunkManagerEntrypoint):\n    array_cls = CubedArray  # Assuming CubedArray is the array class provided by cubed\n\n    def __init__(self):\n        # ... implementation specific to cubed ...\n\n    def chunks(self, data):\n        # ... implementation specific to cubed ...\n\n    def rechunk(self, data, chunks, **kwargs):\n        # ... implementation specific to cubed ...\n\n    def compute(self, *data, **kwargs):\n        # ... implementation specific to cubed ...\n\n# In xarray/core/dataset.py\n\nclass Dataset:\n    # ... existing code ...\n\n    def chunk(self, chunks=None, name_prefix='xarray-', token=None, lock=False, manager=None):\n        # ... existing code ...\n        if manager is not None:\n            chunk_manager = get_chunk_manager(manager)\n            # Use chunk_manager to perform chunking operations\n        # ... existing code ...\n\n# In xarray/core/computation.py\n\ndef apply_ufunc(..., dask='forbidden', manager=None, ...):\n    # ... existing code ...\n    if dask == 'parallelized' and manager is not None:\n        chunk_manager = get_chunk_manager(manager)\n        # Use chunk_manager to perform parallelized operations\n    # ... existing code ...\n```\n\nRemember to handle edge cases and ensure that the existing functionality for dask arrays remains unchanged. This implementation should be extensible, allowing for the addition of other parallel execution frameworks in the future.\n\n### Code snippets to check\n\n-  \u2192 [xarray](https://github.com/testbed/pydata__xarray__2022.06/tree//xarray) \u2192 [core](https://github.com/testbed/pydata__xarray__2022.06/tree//xarray/core)- [Lines 1 - 68](https://github.com/testbed/pydata__xarray__2022.06/blob//xarray/core/parallelcompat.py#L1-L68)\n\n This snippet defines the mechanism for listing and loading chunk managers, which is directly related to the issue of supporting alternative parallel execution frameworks. \n\n\n   - [Lines 1 - 645](https://github.com/testbed/pydata__xarray__2022.06/blob//xarray/core/parallelcompat.py#L1-L645)\n\n This snippet defines the abstract base class for chunk managers, which is essential for integrating new parallel computing frameworks like cubed. \n\n\n   - [Lines 284 - 645](https://github.com/testbed/pydata__xarray__2022.06/blob//xarray/core/parallelcompat.py#L284-L645)\n\n This snippet shows the implementation of the rechunk method, which is one of the methods that should be supported by alternative parallel execution frameworks.\n-  \u2192 [xarray](https://github.com/testbed/pydata__xarray__2022.06/tree//xarray) \u2192 [core](https://github.com/testbed/pydata__xarray__2022.06/tree//xarray/core)- [Lines 688 - 830](https://github.com/testbed/pydata__xarray__2022.06/blob//xarray/core/computation.py#L688-L830)\n\n This snippet is part of the computation module and shows how dask is currently integrated. Refactoring this to support alternative frameworks like cubed is relevant to the issue.\n-  \u2192 [xarray](https://github.com/testbed/pydata__xarray__2022.06/tree//xarray) \u2192 [core](https://github.com/testbed/pydata__xarray__2022.06/tree//xarray/core)- [Lines 1 - 95](https://github.com/testbed/pydata__xarray__2022.06/blob//xarray/core/pycompat.py#L1-L95)\n\n This snippet includes the DuckArrayModule class, which is used for internal checks and could be relevant when adding support for new array types like cubed."
    }
}