{
    "id": "scikit-learn__scikit-learn-14464",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
    "task_id": 8486673518855324317,
    "title": "Cloning custom transform replaces values in __init__ dictionary",
    "body": "<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn\r\n- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn\r\nFor more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->\r\nLet us say we have a custom transform `A` that has some arguments. When the `A` is instantiated, these arguments are set in the init. \r\n\r\nWhen we clone `A` (as happens in `cross_val_score`, for example), the arguments get copied successfully. \r\n\r\nHowever, if the arguments are sent to a structure such as a dictionary, the clone replaces them with None.  \r\n\r\nIn cases where None does not cause errors, this creates a silent error, as the cloned version of `A` will run, producing different results from its original version (which is how I ran into this problem in the first place). \r\n\r\nFully replicable example follows. \r\n\r\n#### Steps/Code to Reproduce\r\n```\r\nfrom sklearn.base import BaseEstimator, TransformerMixin\r\nfrom sklearn.base import clone\r\n\r\n\r\nclass MyTransformA(BaseEstimator, TransformerMixin):\r\n    \r\n    def __init__(self, n_cols_to_keep):\r\n        self.cols_to_keep_dict = {'n_cols': n_cols_to_keep}  \r\n    \r\n    def fit(self, X, *_):\r\n        return self \r\n        \r\n    def transform(self, X, *_):\r\n        return X\r\n    \r\n    \r\nclass MyTransformB(BaseEstimator, TransformerMixin):\r\n\r\n    def __init__(self, n_cols_to_keep):\r\n        self.n_cols_to_keep = n_cols_to_keep  # <--- this time we save the input immediately \r\n        self.cols_to_keep_dict = {'n_cols': self.n_cols_to_keep}  \r\n    \r\n    def fit(self, X, *_):\r\n        return self \r\n        \r\n    def transform(self, X, *_):\r\n        return X\r\n\r\nmy_transform_a = MyTransformA(n_cols_to_keep=5)\r\nmy_transform_a_clone = clone(my_transform_a)\r\n\r\nmy_transform_b = MyTransformB(n_cols_to_keep=5)\r\nmy_transform_b_clone = clone(my_transform_b)\r\n\r\nprint('Using MyTransformA:')\r\nprint('  my_transform_a.cols_to_keep_dict:        %s' % str(my_transform_a.cols_to_keep_dict))\r\nprint('  my_transform_a_clone.cols_to_keep_dict:  %s  <------ ?' % str(my_transform_a_clone.cols_to_keep_dict))\r\n\r\nprint('\\nUsing MyTransformB:')\r\nprint('  my_transform_b.cols_to_keep_dict:        %s' % str(my_transform_b.cols_to_keep_dict))\r\nprint('  my_transform_b_clone.cols_to_keep_dict): %s' % str(my_transform_b_clone.cols_to_keep_dict))\r\n```\r\n#### Expected Results\r\n```\r\nUsing MyTransformA:\r\n  my_transform_a.cols_to_keep_dict:        ('n_cols', 5)\r\n  my_transform_a_clone.cols_to_keep_dict:  ('n_cols', 5)  <------ Does not happen\r\n\r\nUsing MyTransformB:\r\n  my_transform_b.cols_to_keep_dict:        {'n_cols': 5}\r\n  my_transform_b_clone.cols_to_keep_dict): {'n_cols': 5}\r\n```\r\n#### Actual Results\r\n```\r\nUsing MyTransformA:\r\n  my_transform_a.cols_to_keep_dict:        ('n_cols', 5)\r\n  my_transform_a_clone.cols_to_keep_dict:  ('n_cols', None)  <------ ?\r\n\r\nUsing MyTransformB:\r\n  my_transform_b.cols_to_keep_dict:        {'n_cols': 5}\r\n  my_transform_b_clone.cols_to_keep_dict): {'n_cols': 5}\r\n```\r\n\r\n#### Versions\r\n```\r\nSystem:\r\n    python: 3.7.3 (default, Mar 27 2019, 16:54:48)  [Clang 4.0.1 (tags/RELEASE_401/final)]\r\nexecutable: /anaconda3/bin/python\r\n   machine: Darwin-18.6.0-x86_64-i386-64bit\r\n\r\nBLAS:\r\n    macros: SCIPY_MKL_H=None, HAVE_CBLAS=None\r\n  lib_dirs: /anaconda3/lib\r\ncblas_libs: mkl_rt, pthread\r\n\r\nPython deps:\r\n       pip: 19.0.3\r\nsetuptools: 40.8.0\r\n   sklearn: 0.20.3\r\n     numpy: 1.16.2\r\n     scipy: 1.2.1\r\n    Cython: 0.29.6\r\n    pandas: 0.24.2\r\nPython 3.7.3 (default, Mar 27 2019, 16:54:48) \r\n[Clang 4.0.1 (tags/RELEASE_401/final)]\r\nNumPy 1.16.2\r\nSciPy 1.2.1\r\nScikit-Learn 0.20.3\r\n```\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.22.git",
    "requested_commit": "",
    "mayil_response": "Based on the provided information, the issue seems to be related to the cloning process where dictionary values are replaced with `None`. The relevant code snippet is from `sklearn/base.py` where the `_clone_parametrized` function is defined. This function is responsible for cloning estimators, and the issue likely arises when cloning an estimator with a dictionary attribute.\n\nTo address the issue, ensure that dictionary attributes are properly cloned. Here's a solution:\n\n- Modify the `_clone_parametrized` function in `sklearn/base.py` to handle dictionary attributes correctly during the cloning process.\n- When cloning the parameters of the estimator, if the parameter is a dictionary, perform a deep copy of the dictionary to ensure that the cloned estimator has its own separate copy of the dictionary.\n\nHere is the modified part of the `_clone_parametrized` function:\n\n```python\nfrom copy import deepcopy\n\ndef _clone_parametrized(estimator, *, safe=True):\n    # ... [existing code before the loop] ...\n\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        if isinstance(param, dict):\n            new_object_params[name] = deepcopy(param)  # Deep copy the dictionary\n        else:\n            new_object_params[name] = clone(param, safe=False)\n\n    # ... [rest of the existing code] ...\n```\n\nThis change ensures that when an estimator with a dictionary attribute is cloned, the dictionary is deeply copied, preventing the issue where dictionary values are replaced with `None`.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn)- [Lines 1 - 76](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/base.py#L1-L76)\n\n This snippet contains the definition of the clone function, which is central to the issue as the problem occurs when cloning a custom transformer. \n\n\n   - [Lines 79 - 134](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/base.py#L79-L134)\n\n This snippet provides the implementation details of the _clone_parametrized function called by clone, which is likely where the issue needs to be addressed.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0009725,
        "snippet_processor": 0.08512000000000002,
        "issue_star_creation": 0.049199999999999994,
        "issue_star_solver": 0.054560000000000004,
        "bouncer": 0.02766
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711739001.044922,
        "relevant_snippets": [
            {
                "code": "def check_set_output_transform(name, transformer_orig):\n    # Check transformer.set_output with the default configuration does not\n    # change the transform output.\n    tags = transformer_orig._get_tags()\n    if \"2darray\" not in tags[\"X_types\"] or tags[\"no_validation\"]:\n        return\n\n    rng = np.random.RandomState(0)\n    transformer = clone(transformer_orig)\n\n    X = rng.uniform(size=(20, 5))\n    X = _enforce_estimator_tags_X(transformer_orig, X)\n    y = rng.randint(0, 2, size=20)\n    y = _enforce_estimator_tags_y(transformer_orig, y)\n    set_random_state(transformer)\n\n    def fit_then_transform(est):\n        if name in CROSS_DECOMPOSITION:\n            return est.fit(X, y).transform(X, y)\n        return est.fit(X, y).transform(X)\n\n    def fit_transform(est):\n        return est.fit_transform(X, y)\n\n    transform_methods = {\n        \"transform\": fit_then_transform,\n        \"fit_transform\": fit_transform,\n    }\n    for name, transform_method in transform_methods.items():\n        transformer = clone(transformer)\n        if not hasattr(transformer, name):\n            continue\n        X_trans_no_setting = transform_method(transformer)\n\n        # Auto wrapping only wraps the first array\n        if name in CROSS_DECOMPOSITION:\n            X_trans_no_setting = X_trans_no_setting[0]\n\n        transformer.set_output(transform=\"default\")\n        X_trans_default = transform_method(transformer)\n\n        if name in CROSS_DECOMPOSITION:\n            X_trans_default = X_trans_default[0]\n\n        # Default and no setting -> returns the same transformation\n        assert_allclose_dense_sparse(X_trans_no_setting, X_trans_default)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 157079,
                "end_index": 158718,
                "start_line": 4434,
                "end_line": 4479,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"Base classes for all estimators.\"\"\"\n\n# Author: Gael Varoquaux <gael.varoquaux@normalesup.org>\n# License: BSD 3 clause\n\nimport copy\nimport functools\nimport inspect\nimport platform\nimport re\nimport warnings\nfrom collections import defaultdict\n\nimport numpy as np\n\nfrom . import __version__\nfrom ._config import config_context, get_config\nfrom .exceptions import InconsistentVersionWarning\nfrom .utils import _IS_32BIT\nfrom .utils._estimator_html_repr import estimator_html_repr\nfrom .utils._metadata_requests import _MetadataRequester, _routing_enabled\nfrom .utils._param_validation import validate_parameter_constraints\nfrom .utils._set_output import _SetOutputMixin\nfrom .utils._tags import (\n    _DEFAULT_TAGS,\n)\nfrom .utils.validation import (\n    _check_feature_names_in,\n    _check_y,\n    _generate_get_feature_names_out,\n    _get_feature_names,\n    _is_fitted,\n    _num_features,\n    check_array,\n    check_is_fitted,\n    check_X_y,\n)\n\n\ndef clone(estimator, *, safe=True):\n    \"\"\"Construct a new unfitted estimator with the same parameters.\n\n    Clone does a deep copy of the model in an estimator\n    without actually copying attached data. It returns a new estimator\n    with the same parameters that has not been fitted on any data.\n\n    .. versionchanged:: 1.3\n        Delegates to `estimator.__sklearn_clone__` if the method exists.\n\n    Parameters\n    ----------\n    estimator : {list, tuple, set} of estimator instance or a single \\\n            estimator instance\n        The estimator or group of estimators to be cloned.\n    safe : bool, default=True\n        If safe is False, clone will fall back to a deep copy on objects\n        that are not estimators. Ignored if `estimator.__sklearn_clone__`\n        exists.\n\n    Returns\n    -------\n    estimator : object\n        The deep copy of the input, an estimator if input is an estimator.\n\n    Notes\n    -----\n    If the estimator's `random_state` parameter is an integer (or if the\n    estimator doesn't have a `random_state` parameter), an *exact clone* is\n    returned: the clone and the original estimator will give the exact same\n    results. Otherwise, *statistical clone* is returned: the clone might\n    return different results from the original estimator. More details can be\n    found in :ref:`randomness`.\n    \"\"\"\n    if hasattr(estimator, \"__sklearn_clone__\") and not inspect.isclass(estimator):\n        return estimator.__sklearn_clone__()\n    return _clone_parametrized(estimator, safe=safe)",
                "filename": "sklearn/base.py",
                "start_index": 0,
                "end_index": 2471,
                "start_line": 1,
                "end_line": 76,
                "max_line": 1219,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "def check_transformer_get_feature_names_out(name, transformer_orig):\n    tags = transformer_orig._get_tags()\n    if \"2darray\" not in tags[\"X_types\"] or tags[\"no_validation\"]:\n        return\n\n    X, y = make_blobs(\n        n_samples=30,\n        centers=[[0, 0, 0], [1, 1, 1]],\n        random_state=0,\n        n_features=2,\n        cluster_std=0.1,\n    )\n    X = StandardScaler().fit_transform(X)\n\n    transformer = clone(transformer_orig)\n    X = _enforce_estimator_tags_X(transformer, X)\n\n    n_features = X.shape[1]\n    set_random_state(transformer)\n\n    y_ = y\n    if name in CROSS_DECOMPOSITION:\n        y_ = np.c_[np.asarray(y), np.asarray(y)]\n        y_[::2, 1] *= 2\n\n    X_transform = transformer.fit_transform(X, y=y_)\n    input_features = [f\"feature{i}\" for i in range(n_features)]\n\n    # input_features names is not the same length as n_features_in_\n    with raises(ValueError, match=\"input_features should have length equal\"):\n        transformer.get_feature_names_out(input_features[::2])\n\n    feature_names_out = transformer.get_feature_names_out(input_features)\n    assert feature_names_out is not None\n    assert isinstance(feature_names_out, np.ndarray)\n    assert feature_names_out.dtype == object\n    assert all(isinstance(name, str) for name in feature_names_out)\n\n    if isinstance(X_transform, tuple):\n        n_features_out = X_transform[0].shape[1]\n    else:\n        n_features_out = X_transform.shape[1]\n\n    assert (\n        len(feature_names_out) == n_features_out\n    ), f\"Expected {n_features_out} feature names, got {len(feature_names_out)}\"",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 148349,
                "end_index": 149918,
                "start_line": 4210,
                "end_line": 4255,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "def check_fit_check_is_fitted(name, estimator_orig):\n    # Make sure that estimator doesn't pass check_is_fitted before calling fit\n    # and that passes check_is_fitted once it's fit.\n\n    rng = np.random.RandomState(42)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    if \"warm_start\" in estimator.get_params():\n        estimator.set_params(warm_start=False)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n    if is_regressor(estimator_orig):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(low=0, high=2, size=n_samples)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if not _safe_tags(estimator).get(\"stateless\", False):\n        # stateless estimators (such as FunctionTransformer) are always \"fit\"!\n        try:\n            check_is_fitted(estimator)\n            raise AssertionError(\n                f\"{estimator.__class__.__name__} passes check_is_fitted before being\"\n                \" fit!\"\n            )\n        except NotFittedError:\n            pass\n    estimator.fit(X, y)\n    try:\n        check_is_fitted(estimator)\n    except NotFittedError as e:\n        raise NotFittedError(\n            \"Estimator fails to pass `check_is_fitted` even though it has been fit.\"\n        ) from e\n\n\ndef check_n_features_in(name, estimator_orig):\n    # Make sure that n_features_in_ attribute doesn't exist until fit is\n    # called, and that its value is correct.\n\n    rng = np.random.RandomState(0)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    if \"warm_start\" in estimator.get_params():\n        estimator.set_params(warm_start=False)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n    if is_regressor(estimator_orig):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(low=0, high=2, size=n_samples)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    assert not hasattr(estimator, \"n_features_in_\")\n    estimator.fit(X, y)\n    assert hasattr(estimator, \"n_features_in_\")\n    assert estimator.n_features_in_ == X.shape[1]\n\n\ndef check_requires_y_none(name, estimator_orig):\n    # Make sure that an estimator with requires_y=True fails gracefully when\n    # given y=None\n\n    rng = np.random.RandomState(0)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n\n    expected_err_msgs = (\n        \"requires y to be passed, but the target y is None\",\n        \"Expected array-like (array or non-string sequence), got None\",\n        \"y should be a 1d array\",\n    )\n\n    try:\n        estimator.fit(X, None)\n    except ValueError as ve:\n        if not any(msg in str(ve) for msg in expected_err_msgs):\n            raise ve",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 138164,
                "end_index": 141063,
                "start_line": 3892,
                "end_line": 3980,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "n_samples, n_features = np.asarray(X).shape\n    transformer = clone(transformer_orig)\n    set_random_state(transformer)\n\n    # fit\n\n    if name in CROSS_DECOMPOSITION:\n        y_ = np.c_[np.asarray(y), np.asarray(y)]\n        y_[::2, 1] *= 2\n        if isinstance(X, _NotAnArray):\n            y_ = _NotAnArray(y_)\n    else:\n        y_ = y\n\n    transformer.fit(X, y_)\n    # fit_transform method should work on non fitted estimator\n    transformer_clone = clone(transformer)\n    X_pred = transformer_clone.fit_transform(X, y=y_)\n\n    if isinstance(X_pred, tuple):\n        for x_pred in X_pred:\n            assert x_pred.shape[0] == n_samples\n    else:\n        # check for consistent n_samples\n        assert X_pred.shape[0] == n_samples",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 59691,
                "end_index": 60424,
                "start_line": 1765,
                "end_line": 1789,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "def _clone_parametrized(estimator, *, safe=True):\n    \"\"\"Default implementation of clone. See :func:`sklearn.base.clone` for details.\"\"\"\n\n    estimator_type = type(estimator)\n    if estimator_type is dict:\n        return {k: clone(v, safe=safe) for k, v in estimator.items()}\n    elif estimator_type in (list, tuple, set, frozenset):\n        return estimator_type([clone(e, safe=safe) for e in estimator])\n    elif not hasattr(estimator, \"get_params\") or isinstance(estimator, type):\n        if not safe:\n            return copy.deepcopy(estimator)\n        else:\n            if isinstance(estimator, type):\n                raise TypeError(\n                    \"Cannot clone object. \"\n                    + \"You should provide an instance of \"\n                    + \"scikit-learn estimator instead of a class.\"\n                )\n            else:\n                raise TypeError(\n                    \"Cannot clone object '%s' (type %s): \"\n                    \"it does not seem to be a scikit-learn \"\n                    \"estimator as it does not implement a \"\n                    \"'get_params' method.\" % (repr(estimator), type(estimator))\n                )\n\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        new_object_params[name] = clone(param, safe=False)\n\n    new_object = klass(**new_object_params)\n    try:\n        new_object._metadata_request = copy.deepcopy(estimator._metadata_request)\n    except AttributeError:\n        pass\n\n    params_set = new_object.get_params(deep=False)\n\n    # quick sanity check of the parameters of the clone\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is not param2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n\n    # _sklearn_output_config is used by `set_output` to configure the output\n    # container of an estimator.\n    if hasattr(estimator, \"_sklearn_output_config\"):\n        new_object._sklearn_output_config = copy.deepcopy(\n            estimator._sklearn_output_config\n        )\n    return new_object",
                "filename": "sklearn/base.py",
                "start_index": 2474,
                "end_index": 4748,
                "start_line": 79,
                "end_line": 134,
                "max_line": 1219,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_non_transformer_estimators_n_iter(name, estimator_orig):\n    # Test that estimators that are not transformers with a parameter\n    # max_iter, return the attribute of n_iter_ at least 1.\n\n    # These models are dependent on external solvers like\n    # libsvm and accessing the iter parameter is non-trivial.\n    # SelfTrainingClassifier does not perform an iteration if all samples are\n    # labeled, hence n_iter_ = 0 is valid.\n    not_run_check_n_iter = [\n        \"Ridge\",\n        \"RidgeClassifier\",\n        \"RandomizedLasso\",\n        \"LogisticRegressionCV\",\n        \"LinearSVC\",\n        \"LogisticRegression\",\n        \"SelfTrainingClassifier\",\n    ]\n\n    # Tested in test_transformer_n_iter\n    not_run_check_n_iter += CROSS_DECOMPOSITION\n    if name in not_run_check_n_iter:\n        return\n\n    # LassoLars stops early for the default alpha=1.0 the iris dataset.\n    if name == \"LassoLars\":\n        estimator = clone(estimator_orig).set_params(alpha=0.0)\n    else:\n        estimator = clone(estimator_orig)\n    if hasattr(estimator, \"max_iter\"):\n        iris = load_iris()\n        X, y_ = iris.data, iris.target\n        y_ = _enforce_estimator_tags_y(estimator, y_)\n\n        set_random_state(estimator, 0)\n\n        X = _enforce_estimator_tags_X(estimator_orig, X)\n\n        estimator.fit(X, y_)\n\n        assert np.all(estimator.n_iter_ >= 1)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_transformer_n_iter(name, estimator_orig):\n    # Test that transformers with a parameter max_iter, return the\n    # attribute of n_iter_ at least 1.\n    estimator = clone(estimator_orig)\n    if hasattr(estimator, \"max_iter\"):\n        if name in CROSS_DECOMPOSITION:\n            # Check using default data\n            X = [[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [2.0, 2.0, 2.0], [2.0, 5.0, 4.0]]\n            y_ = [[0.1, -0.2], [0.9, 1.1], [0.1, -0.5], [0.3, -0.2]]\n\n        else:\n            X, y_ = make_blobs(\n                n_samples=30,\n                centers=[[0, 0, 0], [1, 1, 1]],\n                random_state=0,\n                n_features=2,\n                cluster_std=0.1,\n            )\n            X = _enforce_estimator_tags_X(estimator_orig, X)\n        set_random_state(estimator, 0)\n        estimator.fit(X, y_)\n\n        # These return a n_iter per component.\n        if name in CROSS_DECOMPOSITION:\n            for iter_ in estimator.n_iter_:\n                assert iter_ >= 1\n        else:\n            assert estimator.n_iter_ >= 1\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_get_params_invariance(name, estimator_orig):\n    # Checks if get_params(deep=False) is a subset of get_params(deep=True)\n    e = clone(estimator_orig)\n\n    shallow_params = e.get_params(deep=False)\n    deep_params = e.get_params(deep=True)\n\n    assert all(item in deep_params.items() for item in shallow_params.items())",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 126361,
                "end_index": 129224,
                "start_line": 181,
                "end_line": 3662,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_dont_overwrite_parameters(name, estimator_orig):\n    # check that fit method only changes or sets private attributes\n    if hasattr(estimator_orig.__init__, \"deprecated_original\"):\n        # to not check deprecated classes\n        return\n    estimator = clone(estimator_orig)\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20, 3))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    y = X[:, 0].astype(int)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n    dict_before_fit = estimator.__dict__.copy()\n    estimator.fit(X, y)\n\n    dict_after_fit = estimator.__dict__\n\n    public_keys_after_fit = [\n        key for key in dict_after_fit.keys() if _is_public_parameter(key)\n    ]\n\n    attrs_added_by_fit = [\n        key for key in public_keys_after_fit if key not in dict_before_fit.keys()\n    ]\n\n    # check that fit doesn't add any public attribute\n    assert not attrs_added_by_fit, (\n        \"Estimator adds public attribute(s) during\"\n        \" the fit method.\"\n        \" Estimators are only allowed to add private attributes\"\n        \" either started with _ or ended\"\n        \" with _ but %s added\"\n        % \", \".join(attrs_added_by_fit)\n    )\n\n    # check that fit doesn't change any public attribute\n    attrs_changed_by_fit = [\n        key\n        for key in public_keys_after_fit\n        if (dict_before_fit[key] is not dict_after_fit[key])\n    ]\n\n    assert not attrs_changed_by_fit, (\n        \"Estimator changes public attribute(s) during\"\n        \" the fit method. Estimators are only allowed\"\n        \" to change attributes started\"\n        \" or ended with _, but\"\n        \" %s changed\"\n        % \", \".join(attrs_changed_by_fit)\n    )\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_fit2d_predict1d(name, estimator_orig):\n    # check by fitting a 2d array and predicting with a 1d array\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20, 3))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    y = X[:, 0].astype(int)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n    estimator.fit(X, y)\n\n    for method in [\"predict\", \"transform\", \"decision_function\", \"predict_proba\"]:\n        if hasattr(estimator, method):\n            assert_raise_message(\n                ValueError, \"Reshape your data\", getattr(estimator, method), X[0]\n            )",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 48853,
                "end_index": 51626,
                "start_line": 181,
                "end_line": 4630,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings\ndef check_dict_unchanged(name, estimator_orig):\n    # this estimator raises\n    # ValueError: Found array with 0 feature(s) (shape=(23, 0))\n    # while a minimum of 1 is required.\n    # error\n    if name in [\"SpectralCoclustering\"]:\n        return\n    rnd = np.random.RandomState(0)\n    if name in [\"RANSACRegressor\"]:\n        X = 3 * rnd.uniform(size=(20, 3))\n    else:\n        X = 2 * rnd.uniform(size=(20, 3))\n\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n\n    y = X[:, 0].astype(int)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    if hasattr(estimator, \"n_best\"):\n        estimator.n_best = 1\n\n    set_random_state(estimator, 1)\n\n    estimator.fit(X, y)\n    for method in [\"predict\", \"transform\", \"decision_function\", \"predict_proba\"]:\n        if hasattr(estimator, method):\n            dict_before = estimator.__dict__.copy()\n            getattr(estimator, method)(X)\n            assert estimator.__dict__ == dict_before, (\n                \"Estimator changes __dict__ during %s\" % method\n            )\n\n\ndef _is_public_parameter(attr):\n    return not (attr.startswith(\"_\") or attr.endswith(\"_\"))",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 47528,
                "end_index": 48850,
                "start_line": 181,
                "end_line": 1419,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "if hasattr(transformer, \"transform\"):\n        if name in CROSS_DECOMPOSITION:\n            X_pred2 = transformer.transform(X, y_)\n            X_pred3 = transformer.fit_transform(X, y=y_)\n        else:\n            X_pred2 = transformer.transform(X)\n            X_pred3 = transformer.fit_transform(X, y=y_)\n\n        if _safe_tags(transformer_orig, key=\"non_deterministic\"):\n            msg = name + \" is non deterministic\"\n            raise SkipTest(msg)\n        if isinstance(X_pred, tuple) and isinstance(X_pred2, tuple):\n            for x_pred, x_pred2, x_pred3 in zip(X_pred, X_pred2, X_pred3):\n                assert_allclose_dense_sparse(\n                    x_pred,\n                    x_pred2,\n                    atol=1e-2,\n                    err_msg=\"fit_transform and transform outcomes not consistent in %s\"\n                    % transformer,\n                )\n                assert_allclose_dense_sparse(\n                    x_pred,\n                    x_pred3,\n                    atol=1e-2,\n                    err_msg=\"consecutive fit_transform outcomes not consistent in %s\"\n                    % transformer,\n                )\n        else:\n            assert_allclose_dense_sparse(\n                X_pred,\n                X_pred2,\n                err_msg=\"fit_transform and transform outcomes not consistent in %s\"\n                % transformer,\n                atol=1e-2,\n            )\n            assert_allclose_dense_sparse(\n                X_pred,\n                X_pred3,\n                atol=1e-2,\n                err_msg=\"consecutive fit_transform outcomes not consistent in %s\"\n                % transformer,\n            )\n            assert _num_samples(X_pred2) == n_samples\n            assert _num_samples(X_pred3) == n_samples\n\n        # raises error on malformed input for transform\n        if (\n            hasattr(X, \"shape\")\n            and not _safe_tags(transformer, key=\"stateless\")\n            and X.ndim == 2\n            and X.shape[1] > 1\n        ):\n            # If it's not an array, it does not have a 'T' property\n            with raises(\n                ValueError,\n                err_msg=(\n                    f\"The transformer {name} does not raise an error \"\n                    \"when the number of features in transform is different from \"\n                    \"the number of features in fit.\"\n                ),\n            ):\n                transformer.transform(X[:, :-1])",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 60430,
                "end_index": 62856,
                "start_line": 1791,
                "end_line": 1852,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/base.py": [
                {
                    "chunk": {
                        "code": "\"\"\"Base classes for all estimators.\"\"\"\n\n# Author: Gael Varoquaux <gael.varoquaux@normalesup.org>\n# License: BSD 3 clause\n\nimport copy\nimport functools\nimport inspect\nimport platform\nimport re\nimport warnings\nfrom collections import defaultdict\n\nimport numpy as np\n\nfrom . import __version__\nfrom ._config import config_context, get_config\nfrom .exceptions import InconsistentVersionWarning\nfrom .utils import _IS_32BIT\nfrom .utils._estimator_html_repr import estimator_html_repr\nfrom .utils._metadata_requests import _MetadataRequester, _routing_enabled\nfrom .utils._param_validation import validate_parameter_constraints\nfrom .utils._set_output import _SetOutputMixin\nfrom .utils._tags import (\n    _DEFAULT_TAGS,\n)\nfrom .utils.validation import (\n    _check_feature_names_in,\n    _check_y,\n    _generate_get_feature_names_out,\n    _get_feature_names,\n    _is_fitted,\n    _num_features,\n    check_array,\n    check_is_fitted,\n    check_X_y,\n)\n\n\ndef clone(estimator, *, safe=True):\n    \"\"\"Construct a new unfitted estimator with the same parameters.\n\n    Clone does a deep copy of the model in an estimator\n    without actually copying attached data. It returns a new estimator\n    with the same parameters that has not been fitted on any data.\n\n    .. versionchanged:: 1.3\n        Delegates to `estimator.__sklearn_clone__` if the method exists.\n\n    Parameters\n    ----------\n    estimator : {list, tuple, set} of estimator instance or a single \\\n            estimator instance\n        The estimator or group of estimators to be cloned.\n    safe : bool, default=True\n        If safe is False, clone will fall back to a deep copy on objects\n        that are not estimators. Ignored if `estimator.__sklearn_clone__`\n        exists.\n\n    Returns\n    -------\n    estimator : object\n        The deep copy of the input, an estimator if input is an estimator.\n\n    Notes\n    -----\n    If the estimator's `random_state` parameter is an integer (or if the\n    estimator doesn't have a `random_state` parameter), an *exact clone* is\n    returned: the clone and the original estimator will give the exact same\n    results. Otherwise, *statistical clone* is returned: the clone might\n    return different results from the original estimator. More details can be\n    found in :ref:`randomness`.\n    \"\"\"\n    if hasattr(estimator, \"__sklearn_clone__\") and not inspect.isclass(estimator):\n        return estimator.__sklearn_clone__()\n    return _clone_parametrized(estimator, safe=safe)",
                        "filename": "sklearn/base.py",
                        "start_index": 0,
                        "end_index": 2471,
                        "start_line": 1,
                        "end_line": 76,
                        "max_line": 1219,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the definition of the clone function, which is central to the issue as the problem occurs when cloning a custom transformer."
                },
                {
                    "chunk": {
                        "code": "def _clone_parametrized(estimator, *, safe=True):\n    \"\"\"Default implementation of clone. See :func:`sklearn.base.clone` for details.\"\"\"\n\n    estimator_type = type(estimator)\n    if estimator_type is dict:\n        return {k: clone(v, safe=safe) for k, v in estimator.items()}\n    elif estimator_type in (list, tuple, set, frozenset):\n        return estimator_type([clone(e, safe=safe) for e in estimator])\n    elif not hasattr(estimator, \"get_params\") or isinstance(estimator, type):\n        if not safe:\n            return copy.deepcopy(estimator)\n        else:\n            if isinstance(estimator, type):\n                raise TypeError(\n                    \"Cannot clone object. \"\n                    + \"You should provide an instance of \"\n                    + \"scikit-learn estimator instead of a class.\"\n                )\n            else:\n                raise TypeError(\n                    \"Cannot clone object '%s' (type %s): \"\n                    \"it does not seem to be a scikit-learn \"\n                    \"estimator as it does not implement a \"\n                    \"'get_params' method.\" % (repr(estimator), type(estimator))\n                )\n\n    klass = estimator.__class__\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        new_object_params[name] = clone(param, safe=False)\n\n    new_object = klass(**new_object_params)\n    try:\n        new_object._metadata_request = copy.deepcopy(estimator._metadata_request)\n    except AttributeError:\n        pass\n\n    params_set = new_object.get_params(deep=False)\n\n    # quick sanity check of the parameters of the clone\n    for name in new_object_params:\n        param1 = new_object_params[name]\n        param2 = params_set[name]\n        if param1 is not param2:\n            raise RuntimeError(\n                \"Cannot clone object %s, as the constructor \"\n                \"either does not set or modifies parameter %s\" % (estimator, name)\n            )\n\n    # _sklearn_output_config is used by `set_output` to configure the output\n    # container of an estimator.\n    if hasattr(estimator, \"_sklearn_output_config\"):\n        new_object._sklearn_output_config = copy.deepcopy(\n            estimator._sklearn_output_config\n        )\n    return new_object",
                        "filename": "sklearn/base.py",
                        "start_index": 2474,
                        "end_index": 4748,
                        "start_line": 79,
                        "end_line": 134,
                        "max_line": 1219,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet provides the implementation details of the _clone_parametrized function called by clone, which is likely where the issue needs to be addressed."
                }
            ]
        },
        "solution_task": "Based on the provided information, the issue seems to be related to the cloning process where dictionary values are replaced with `None`. The relevant code snippet is from `sklearn/base.py` where the `_clone_parametrized` function is defined. This function is responsible for cloning estimators, and the issue likely arises when cloning an estimator with a dictionary attribute.\n\nTo address the issue, ensure that dictionary attributes are properly cloned. Here's a solution:\n\n- Modify the `_clone_parametrized` function in `sklearn/base.py` to handle dictionary attributes correctly during the cloning process.\n- When cloning the parameters of the estimator, if the parameter is a dictionary, perform a deep copy of the dictionary to ensure that the cloned estimator has its own separate copy of the dictionary.\n\nHere is the modified part of the `_clone_parametrized` function:\n\n```python\nfrom copy import deepcopy\n\ndef _clone_parametrized(estimator, *, safe=True):\n    # ... [existing code before the loop] ...\n\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        if isinstance(param, dict):\n            new_object_params[name] = deepcopy(param)  # Deep copy the dictionary\n        else:\n            new_object_params[name] = clone(param, safe=False)\n\n    # ... [rest of the existing code] ...\n```\n\nThis change ensures that when an estimator with a dictionary attribute is cloned, the dictionary is deeply copied, preventing the issue where dictionary values are replaced with `None`.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE accurately identifies the issue with the cloning process in scikit-learn, where dictionary values are being replaced with `None`. It provides a specific and actionable solution by suggesting a modification to the `_clone_parametrized` function in `sklearn/base.py`. The RESPONSE includes a code snippet that demonstrates how to implement a deep copy of dictionary attributes during the cloning process, which is directly relevant to the bug described in the PROMPT. The RESPONSE also provides links to the relevant code sections in the scikit-learn repository, which could be useful for further investigation and verification of the solution. The RESPONSE does not address all aspects of the described bug, such as testing the solution or considering potential side effects, which is why it does not receive a perfect score.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711739227.654739,
        "result": "Based on the provided information, the issue seems to be related to the cloning process where dictionary values are replaced with `None`. The relevant code snippet is from `sklearn/base.py` where the `_clone_parametrized` function is defined. This function is responsible for cloning estimators, and the issue likely arises when cloning an estimator with a dictionary attribute.\n\nTo address the issue, ensure that dictionary attributes are properly cloned. Here's a solution:\n\n- Modify the `_clone_parametrized` function in `sklearn/base.py` to handle dictionary attributes correctly during the cloning process.\n- When cloning the parameters of the estimator, if the parameter is a dictionary, perform a deep copy of the dictionary to ensure that the cloned estimator has its own separate copy of the dictionary.\n\nHere is the modified part of the `_clone_parametrized` function:\n\n```python\nfrom copy import deepcopy\n\ndef _clone_parametrized(estimator, *, safe=True):\n    # ... [existing code before the loop] ...\n\n    new_object_params = estimator.get_params(deep=False)\n    for name, param in new_object_params.items():\n        if isinstance(param, dict):\n            new_object_params[name] = deepcopy(param)  # Deep copy the dictionary\n        else:\n            new_object_params[name] = clone(param, safe=False)\n\n    # ... [rest of the existing code] ...\n```\n\nThis change ensures that when an estimator with a dictionary attribute is cloned, the dictionary is deeply copied, preventing the issue where dictionary values are replaced with `None`.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn)- [Lines 1 - 76](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/base.py#L1-L76)\n\n This snippet contains the definition of the clone function, which is central to the issue as the problem occurs when cloning a custom transformer. \n\n\n   - [Lines 79 - 134](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/base.py#L79-L134)\n\n This snippet provides the implementation details of the _clone_parametrized function called by clone, which is likely where the issue needs to be addressed."
    }
}