{
    "id": "scikit-learn__scikit-learn-14894",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
    "task_id": 9179888290898310883,
    "title": "ZeroDivisionError in _sparse_fit for SVM with empty support_vectors_",
    "body": "#### Description\r\nWhen using sparse data, in the case where the support_vectors_ attribute is be empty, _fit_sparse gives a ZeroDivisionError\r\n\r\n#### Steps/Code to Reproduce\r\n```\r\nimport numpy as np\r\nimport scipy\r\nimport sklearn\r\nfrom sklearn.svm import SVR\r\nx_train = np.array([[0, 1, 0, 0],\r\n[0, 0, 0, 1],\r\n[0, 0, 1, 0],\r\n[0, 0, 0, 1]])\r\ny_train = np.array([0.04, 0.04, 0.10, 0.16])\r\nmodel = SVR(C=316.227766017, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\r\n  \t    gamma=1.0, kernel='linear', max_iter=15000,\r\n  \t    shrinking=True, tol=0.001, verbose=False)\r\n# dense x_train has no error\r\nmodel.fit(x_train, y_train)\r\n\r\n# convert to sparse\r\nxtrain= scipy.sparse.csr_matrix(x_train)\r\nmodel.fit(xtrain, y_train)\r\n\r\n```\r\n#### Expected Results\r\nNo error is thrown and  `self.dual_coef_ = sp.csr_matrix([])`\r\n\r\n#### Actual Results\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py\", line 209, in fit\r\n    fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\r\n  File \"/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py\", line 302, in _sparse_fit\r\n    dual_coef_indices.size / n_class)\r\nZeroDivisionError: float division by zero\r\n```\r\n\r\n#### Versions\r\n```\r\n>>> sklearn.show_versions() \r\n\r\nSystem:\r\nexecutable: /usr/bin/python3\r\n    python: 3.5.2 (default, Nov 12 2018, 13:43:14)  [GCC 5.4.0 20160609]\r\n   machine: Linux-4.15.0-58-generic-x86_64-with-Ubuntu-16.04-xenial\r\n\r\nPython deps:\r\n     numpy: 1.17.0\r\n    Cython: None\r\n       pip: 19.2.1\r\n    pandas: 0.22.0\r\n   sklearn: 0.21.3\r\n     scipy: 1.3.0\r\nsetuptools: 40.4.3\r\n```\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.22.git",
    "requested_commit": "",
    "mayil_response": "To address the `ZeroDivisionError: float division by zero` issue in the `_sparse_fit` method of the `SVR` class, modify the code as follows:\n\n- In the `sklearn/svm/_base.py` file, locate the `_sparse_fit` method within the `SVR` class or a related class.\n- Update the section where `dual_coef_` is being constructed from `dual_coef_data` to handle the case where `n_class` or `dual_coef_indices.size` is zero to avoid division by zero.\n\nHere is the updated code snippet:\n\n```python\n# sklearn/svm/_base.py\n\n# ... [existing code above] ...\n\nif hasattr(self, \"classes_\"):\n    n_class = len(self.classes_) - 1\nelse:  # regression\n    n_class = 1\nn_SV = self.support_vectors_.shape[0]\n\ndual_coef_indices = np.tile(np.arange(n_SV), n_class)\nif not n_SV or n_class == 0:  # Check if n_SV or n_class is zero to avoid division by zero\n    self.dual_coef_ = sp.csr_matrix((0, 0))  # Create an empty csr_matrix with appropriate shape\nelse:\n    dual_coef_indptr = np.arange(\n        0, dual_coef_indices.size + 1, dual_coef_indices.size // n_class\n    )\n    self.dual_coef_ = sp.csr_matrix(\n        (dual_coef_data, dual_coef_indices, dual_coef_indptr), (n_class, n_SV)\n    )\n\n# ... [existing code below] ...\n```\n\nMake sure to replace the division operator `/` with the floor division operator `//` to ensure an integer result, which is required for the indices of a sparse matrix. Additionally, handle the case where `n_SV` or `n_class` is zero by creating an empty `csr_matrix` with the appropriate shape.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [svm](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/svm)- [Lines 353 - 433](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/svm/_base.py#L353-L433)\n\n This snippet contains the '_sparse_fit' method where the ZeroDivisionError occurs due to division by 'n_class' when 'n_SV' (number of support vectors) is zero. \n\n\n   - [Lines 229 - 1006](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/svm/_base.py#L229-L1006)\n\n This snippet from '_base.py' includes the call to 'fit' method which leads to the '_sparse_fit' method where the error is raised.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0003105,
        "snippet_processor": 0.11024,
        "issue_star_creation": 0.03695,
        "issue_star_solver": 0.1084,
        "bouncer": 0.03046
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711738738.7507389,
        "relevant_snippets": [
            {
                "code": "def check_array_api_input_and_values(\n    name,\n    estimator_orig,\n    array_namespace,\n    device=None,\n    dtype=\"float64\",\n):\n    return check_array_api_input(\n        name,\n        estimator_orig,\n        array_namespace=array_namespace,\n        device=device,\n        dtype=dtype,\n        check_values=True,\n    )\n\n\ndef check_estimator_sparse_data(name, estimator_orig):\n    rng = np.random.RandomState(0)\n    X = rng.uniform(size=(40, 3))\n    X[X < 0.8] = 0\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    X_csr = sparse.csr_matrix(X)\n    y = (4 * rng.uniform(size=40)).astype(int)\n    # catch deprecation warnings\n    with ignore_warnings(category=FutureWarning):\n        estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n    tags = _safe_tags(estimator_orig)\n    for matrix_format, X in _generate_sparse_matrix(X_csr):\n        # catch deprecation warnings\n        with ignore_warnings(category=FutureWarning):\n            estimator = clone(estimator_orig)\n            if name in [\"Scaler\", \"StandardScaler\"]:\n                estimator.set_params(with_mean=False)\n        # fit and predict\n        if \"64\" in matrix_format:\n            err_msg = (\n                f\"Estimator {name} doesn't seem to support {matrix_format} \"\n                \"matrix, and is not failing gracefully, e.g. by using \"\n                \"check_array(X, accept_large_sparse=False)\"\n            )\n        else:\n            err_msg = (\n                f\"Estimator {name} doesn't seem to fail gracefully on sparse \"\n                \"data: error message should state explicitly that sparse \"\n                \"input is not supported if this is not the case.\"\n            )\n        with raises(\n            (TypeError, ValueError),\n            match=[\"sparse\", \"Sparse\"],\n            may_pass=True,\n            err_msg=err_msg,\n        ):\n            with ignore_warnings(category=FutureWarning):\n                estimator.fit(X, y)\n            if hasattr(estimator, \"predict\"):\n                pred = estimator.predict(X)\n                if tags[\"multioutput_only\"]:\n                    assert pred.shape == (X.shape[0], 1)\n                else:\n                    assert pred.shape == (X.shape[0],)\n            if hasattr(estimator, \"predict_proba\"):\n                probs = estimator.predict_proba(X)\n                if tags[\"binary_only\"]:\n                    expected_probs_shape = (X.shape[0], 2)\n                else:\n                    expected_probs_shape = (X.shape[0], 4)\n                assert probs.shape == expected_probs_shape",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 36016,
                "end_index": 38577,
                "start_line": 1025,
                "end_line": 1093,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "/*\n * Copy support vectors into a scipy.sparse.csr matrix\n */\nint csr_copy_SV (char *data, npy_intp *n_indices,\n\t\tchar *indices, npy_intp *n_indptr, char *indptr,\n\t\tstruct svm_csr_model *model, int n_features)\n{\n\tint i, j, k=0, index;\n\tdouble *dvalues = (double *) data;\n\tint *iindices = (int *) indices;\n\tint *iindptr  = (int *) indptr;\n\tiindptr[0] = 0;\n\tfor (i=0; i<model->l; ++i) { /* iterate over support vectors */\n\t\tindex = model->SV[i][0].index;\n        for(j=0; index >=0 ; ++j) {\n        \tiindices[k] = index - 1;\n            dvalues[k] = model->SV[i][j].value;\n            index = model->SV[i][j+1].index;\n            ++k;\n        }\n        iindptr[i+1] = k;\n\t}\n\n\treturn 0;\n}\n\n/* get number of nonzero coefficients in support vectors */\nnpy_intp get_nonzero_SV (struct svm_csr_model *model) {\n\tint i, j;\n\tnpy_intp count=0;\n\tfor (i=0; i<model->l; ++i) {\n\t\tj = 0;\n\t\twhile (model->SV[i][j].index != -1) {\n\t\t\t++j;\n\t\t\t++count;\n\t\t}\n\t}\n\treturn count;\n}\n\n\n/*\n * Predict using a model, where data is expected to be encoded into a csr matrix.\n */\nint csr_copy_predict (npy_intp *data_size, char *data, npy_intp *index_size,\n\t\tchar *index, npy_intp *intptr_size, char *intptr, struct svm_csr_model *model,\n\t\tchar *dec_values, BlasFunctions *blas_functions) {\n    double *t = (double *) dec_values;\n    struct svm_csr_node **predict_nodes;\n    npy_intp i;\n\n    predict_nodes = csr_to_libsvm((double *) data, (int *) index,\n                                  (int *) intptr, intptr_size[0]-1);\n\n    if (predict_nodes == NULL)\n        return -1;\n    for(i=0; i < intptr_size[0] - 1; ++i) {\n        *t = svm_csr_predict(model, predict_nodes[i], blas_functions);\n        free(predict_nodes[i]);\n        ++t;\n    }\n    free(predict_nodes);\n    return 0;\n}\n\nint csr_copy_predict_values (npy_intp *data_size, char *data, npy_intp *index_size,\n                char *index, npy_intp *intptr_size, char *intptr, struct svm_csr_model *model,\n                char *dec_values, int nr_class, BlasFunctions *blas_functions) {\n    struct svm_csr_node **predict_nodes;\n    npy_intp i;\n\n    predict_nodes = csr_to_libsvm((double *) data, (int *) index,\n                                  (int *) intptr, intptr_size[0]-1);\n\n    if (predict_nodes == NULL)\n        return -1;\n    for(i=0; i < intptr_size[0] - 1; ++i) {\n        svm_csr_predict_values(model, predict_nodes[i],\n                               ((double *) dec_values) + i*nr_class,\n\t\t\t       blas_functions);\n        free(predict_nodes[i]);\n    }\n    free(predict_nodes);\n\n    return 0;\n}",
                "filename": "sklearn/svm/src/libsvm/libsvm_sparse_helper.c",
                "start_index": 6285,
                "end_index": 8813,
                "start_line": 12,
                "end_line": 471,
                "max_line": 471,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "kernel = \"precomputed\" if callable(self.kernel) else self.kernel\n\n        if kernel == \"precomputed\":\n            # unused but needs to be a float for cython code that ignores\n            # it anyway\n            self._gamma = 0.0\n        elif isinstance(self.gamma, str):\n            if self.gamma == \"scale\":\n                # var = E[X^2] - E[X]^2 if sparse\n                X_var = (X.multiply(X)).mean() - (X.mean()) ** 2 if sparse else X.var()\n                self._gamma = 1.0 / (X.shape[1] * X_var) if X_var != 0 else 1.0\n            elif self.gamma == \"auto\":\n                self._gamma = 1.0 / X.shape[1]\n        elif isinstance(self.gamma, Real):\n            self._gamma = self.gamma\n\n        fit = self._sparse_fit if self._sparse else self._dense_fit\n        if self.verbose:\n            print(\"[LibSVM]\", end=\"\")\n\n        seed = rnd.randint(np.iinfo(\"i\").max)\n        fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\n        # see comment on the other call to np.iinfo in this file\n\n        self.shape_fit_ = X.shape if hasattr(X, \"shape\") else (n_samples,)\n\n        # In binary case, we need to flip the sign of coef, intercept and\n        # decision function. Use self._intercept_ and self._dual_coef_\n        # internally.\n        self._intercept_ = self.intercept_.copy()\n        self._dual_coef_ = self.dual_coef_\n        if self._impl in [\"c_svc\", \"nu_svc\"] and len(self.classes_) == 2:\n            self.intercept_ *= -1\n            self.dual_coef_ = -self.dual_coef_\n\n        dual_coef = self._dual_coef_.data if self._sparse else self._dual_coef_\n        intercept_finiteness = np.isfinite(self._intercept_).all()\n        dual_coef_finiteness = np.isfinite(dual_coef).all()\n        if not (intercept_finiteness and dual_coef_finiteness):\n            raise ValueError(\n                \"The dual coefficients or intercepts are not finite.\"\n                \" The input data may contain large values and need to be\"\n                \" preprocessed.\"\n            )\n\n        # Since, in the case of SVC and NuSVC, the number of models optimized by\n        # libSVM could be greater than one (depending on the input), `n_iter_`\n        # stores an ndarray.\n        # For the other sub-classes (SVR, NuSVR, and OneClassSVM), the number of\n        # models optimized by libSVM is always one, so `n_iter_` stores an\n        # integer.\n        if self._impl in [\"c_svc\", \"nu_svc\"]:\n            self.n_iter_ = self._num_iter\n        else:\n            self.n_iter_ = self._num_iter.item()\n\n        return self",
                "filename": "sklearn/svm/_base.py",
                "start_index": 7989,
                "end_index": 10520,
                "start_line": 229,
                "end_line": 1006,
                "max_line": 1254,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "#include <stdlib.h>\n#include <numpy/arrayobject.h>\n#include \"svm.h\"\n#include \"_svm_cython_blas_helpers.h\"\n\n\n#ifndef MAX\n    #define MAX(x, y) (((x) > (y)) ? (x) : (y))\n#endif\n\n\n/*\n * Convert scipy.sparse.csr to libsvm's sparse data structure\n */\nstruct svm_csr_node **csr_to_libsvm (double *values, int* indices, int* indptr, npy_int n_samples)\n{\n    struct svm_csr_node **sparse, *temp;\n    int i, j=0, k=0, n;\n    sparse = malloc (n_samples * sizeof(struct svm_csr_node *));\n\n    if (sparse == NULL)\n        return NULL;\n\n    for (i=0; i<n_samples; ++i) {\n        n = indptr[i+1] - indptr[i]; /* count elements in row i */\n        temp = malloc ((n+1) * sizeof(struct svm_csr_node));\n\n        if (temp == NULL) {\n            for (j=0; j<i; j++)\n                free(sparse[j]);\n            free(sparse);\n            return NULL;\n        }\n\n        for (j=0; j<n; ++j) {\n            temp[j].value = values[k];\n            temp[j].index = indices[k] + 1; /* libsvm uses 1-based indexing */\n            ++k;\n        }\n        /* set sentinel */\n        temp[n].index = -1;\n        sparse[i] = temp;\n    }\n\n    return sparse;\n}\n\n\n\nstruct svm_parameter * set_parameter(int svm_type, int kernel_type, int degree,\n\t\tdouble gamma, double coef0, double nu, double cache_size, double C,\n\t\tdouble eps, double p, int shrinking, int probability, int nr_weight,\n\t\tchar *weight_label, char *weight, int max_iter, int random_seed)\n{\n    struct svm_parameter *param;\n    param = malloc(sizeof(struct svm_parameter));\n    if (param == NULL) return NULL;\n    param->svm_type = svm_type;\n    param->kernel_type = kernel_type;\n    param->degree = degree;\n    param->coef0 = coef0;\n    param->nu = nu;\n    param->cache_size = cache_size;\n    param->C = C;\n    param->eps = eps;\n    param->p = p;\n    param->shrinking = shrinking;\n    param->probability = probability;\n    param->nr_weight = nr_weight;\n    param->weight_label = (int *) weight_label;\n    param->weight = (double *) weight;\n    param->gamma = gamma;\n    param->max_iter = max_iter;\n    param->random_seed = random_seed;\n    return param;\n}\n\n\n/*\n * Create and return a svm_csr_problem struct from a scipy.sparse.csr matrix. It is\n * up to the user to free resulting structure.\n *\n * TODO: precomputed kernel.\n */\nstruct svm_csr_problem * csr_set_problem (char *values, npy_intp *n_indices,\n\t\tchar *indices, npy_intp *n_indptr, char *indptr, char *Y,\n                char *sample_weight, int kernel_type) {\n\n    struct svm_csr_problem *problem;\n    problem = malloc (sizeof (struct svm_csr_problem));\n    if (problem == NULL) return NULL;\n    problem->l = (int) n_indptr[0] - 1;\n    problem->y = (double *) Y;\n    problem->x = csr_to_libsvm((double *) values, (int *) indices,\n                               (int *) indptr, problem->l);\n    /* should be removed once we implement weighted samples */\n    problem->W = (double *) sample_weight;\n\n    if (problem->x == NULL) {\n        free(problem);\n        return NULL;\n    }\n    return problem;\n}",
                "filename": "sklearn/svm/src/libsvm/libsvm_sparse_helper.c",
                "start_index": 0,
                "end_index": 2988,
                "start_line": 1,
                "end_line": 471,
                "max_line": 471,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "r\"\"\"\n==============================================\nScaling the regularization parameter for SVCs\n==============================================\n\nThe following example illustrates the effect of scaling the\nregularization parameter when using :ref:`svm` for\n:ref:`classification <svm_classification>`.\nFor SVC classification, we are interested in a risk minimization for the\nequation:\n\n\n.. math::\n\n    C \\sum_{i=1, n} \\mathcal{L} (f(x_i), y_i) + \\Omega (w)\n\nwhere\n\n    - :math:`C` is used to set the amount of regularization\n    - :math:`\\mathcal{L}` is a `loss` function of our samples\n      and our model parameters.\n    - :math:`\\Omega` is a `penalty` function of our model parameters\n\nIf we consider the loss function to be the individual error per\nsample, then the data-fit term, or the sum of the error for each sample, will\nincrease as we add more samples. The penalization term, however, will not\nincrease.\n\nWhen using, for example, :ref:`cross validation <cross_validation>`, to\nset the amount of regularization with `C`, there will be a\ndifferent amount of samples between the main problem and the smaller problems\nwithin the folds of the cross validation.\n\nSince our loss function is dependent on the amount of samples, the latter\nwill influence the selected value of `C`.\nThe question that arises is \"How do we optimally adjust C to\naccount for the different amount of training samples?\"\n\nIn the remainder of this example, we will investigate the effect of scaling\nthe value of the regularization parameter `C` in regards to the number of\nsamples for both L1 and L2 penalty. We will generate some synthetic datasets\nthat are appropriate for each type of regularization.\n\"\"\"\n\n# Author: Andreas Mueller <amueller@ais.uni-bonn.de>\n#         Jaques Grobler <jaques.grobler@inria.fr>\n# License: BSD 3 clause\n\n# %%\n# L1-penalty case\n# ---------------\n# In the L1 case, theory says that prediction consistency (i.e. that under\n# given hypothesis, the estimator learned predicts as well as a model knowing\n# the true distribution) is not possible because of the bias of the L1. It\n# does say, however, that model consistency, in terms of finding the right set\n# of non-zero parameters as well as their signs, can be achieved by scaling\n# `C`.\n#\n# We will demonstrate this effect by using a synthetic dataset. This\n# dataset will be sparse, meaning that only a few features will be informative\n# and useful for the model.\nfrom sklearn.datasets import make_classification\n\nn_samples, n_features = 100, 300\nX, y = make_classification(\n    n_samples=n_samples, n_features=n_features, n_informative=5, random_state=1\n)\n\n# %%\n# Now, we can define a linear SVC with the `l1` penalty.\nfrom sklearn.svm import LinearSVC\n\nmodel_l1 = LinearSVC(penalty=\"l1\", loss=\"squared_hinge\", dual=False, tol=1e-3)\n\n# %%\n# We will compute the mean test score for different values of `C`.\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import ShuffleSplit, validation_curve",
                "filename": "examples/svm/plot_svm_scale_c.py",
                "start_index": 0,
                "end_index": 2973,
                "start_line": 1,
                "end_line": 80,
                "max_line": 173,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"Nu-Support Vector Classification.\n\n    Similar to SVC but uses a parameter to control the number of support\n    vectors.\n\n    The implementation is based on libsvm.\n\n    Read more in the :ref:`User Guide <svm_classification>`.\n\n    Parameters\n    ----------\n    nu : float, default=0.5\n        An upper bound on the fraction of margin errors (see :ref:`User Guide\n        <nu_svc>`) and a lower bound of the fraction of support vectors.\n        Should be in the interval (0, 1].\n\n    kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,  \\\n        default='rbf'\n         Specifies the kernel type to be used in the algorithm.\n         If none is given, 'rbf' will be used. If a callable is given it is\n         used to precompute the kernel matrix.\n\n    degree : int, default=3\n        Degree of the polynomial kernel function ('poly').\n        Must be non-negative. Ignored by all other kernels.\n\n    gamma : {'scale', 'auto'} or float, default='scale'\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n        - if ``gamma='scale'`` (default) is passed then it uses\n          1 / (n_features * X.var()) as value of gamma,\n        - if 'auto', uses 1 / n_features\n        - if float, must be non-negative.\n\n        .. versionchanged:: 0.22\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\n\n    coef0 : float, default=0.0\n        Independent term in kernel function.\n        It is only significant in 'poly' and 'sigmoid'.\n\n    shrinking : bool, default=True\n        Whether to use the shrinking heuristic.\n        See the :ref:`User Guide <shrinking_svm>`.\n\n    probability : bool, default=False\n        Whether to enable probability estimates. This must be enabled prior\n        to calling `fit`, will slow down that method as it internally uses\n        5-fold cross-validation, and `predict_proba` may be inconsistent with\n        `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n\n    tol : float, default=1e-3\n        Tolerance for stopping criterion.\n\n    cache_size : float, default=200\n        Specify the size of the kernel cache (in MB).\n\n    class_weight : {dict, 'balanced'}, default=None\n        Set the parameter C of class i to class_weight[i]*C for\n        SVC. If not given, all classes are supposed to have\n        weight one. The \"balanced\" mode uses the values of y to automatically\n        adjust weights inversely proportional to class frequencies as\n        ``n_samples / (n_classes * np.bincount(y))``.\n\n    verbose : bool, default=False\n        Enable verbose output. Note that this setting takes advantage of a\n        per-process runtime setting in libsvm that, if enabled, may not work\n        properly in a multithreaded context.\n\n    max_iter : int, default=-1\n        Hard limit on iterations within solver, or -1 for no limit.\n\n    decision_function_shape : {'ovo', 'ovr'}, default='ovr'\n        Whether to return a one-vs-rest ('ovr') decision function of shape\n        (n_samples, n_classes) as all other classifiers, or the original\n        one-vs-one ('ovo') decision function of libsvm which has shape\n        (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n        ('ovo') is always used as multi-class strategy. The parameter is\n        ignored for binary classification.\n\n        .. versionchanged:: 0.19\n            decision_function_shape is 'ovr' by default.\n\n        .. versionadded:: 0.17\n           *decision_function_shape='ovr'* is recommended.\n\n        .. versionchanged:: 0.17\n           Deprecated *decision_function_shape='ovo' and None*.\n\n    break_ties : bool, default=False\n        If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n        :term:`predict` will break ties according to the confidence values of\n        :term:`decision_function`; otherwise the first class among the tied\n        classes is returned. Please note that breaking ties comes at a\n        relatively high computational cost compared to a simple predict.\n\n        .. versionadded:: 0.22\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the pseudo random number generation for shuffling the data for\n        probability estimates. Ignored when `probability` is False.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    class_weight_ : ndarray of shape (n_classes,)\n        Multipliers of parameter C of each class.\n        Computed based on the ``class_weight`` parameter.\n\n    classes_ : ndarray of shape (n_classes,)\n        The unique classes labels.\n\n    coef_ : ndarray of shape (n_classes * (n_classes -1) / 2, n_features)\n        Weights assigned to the features (coefficients in the primal\n        problem). This is only available in the case of a linear kernel.\n\n        `coef_` is readonly property derived from `dual_coef_` and\n        `support_vectors_`.\n\n    dual_coef_ : ndarray of shape (n_classes - 1, n_SV)\n        Dual coefficients of the support vector in the decision\n        function (see :ref:`sgd_mathematical_formulation`), multiplied by\n        their targets.\n        For multiclass, coefficient for all 1-vs-1 classifiers.\n        The layout of the coefficients in the multiclass case is somewhat\n        non-trivial. See the :ref:`multi-class section of the User Guide\n        <svm_multi_class>` for details.\n\n    fit_status_ : int\n        0 if correctly fitted, 1 if the algorithm did not converge.\n\n    intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n        Constants in decision function.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : ndarray of shape (n_classes * (n_classes - 1) // 2,)\n        Number of iterations run by the optimization routine to fit the model.\n        The shape of this attribute depends on the number of models optimized\n        which in turn depends on the number of classes.\n\n        .. versionadded:: 1.1\n\n    support_ : ndarray of shape (n_SV,)\n        Indices of support vectors.\n\n    support_vectors_ : ndarray of shape (n_SV, n_features)\n        Support vectors.\n\n    n_support_ : ndarray of shape (n_classes,), dtype=int32\n        Number of support vectors for each class.\n\n    fit_status_ : int\n        0 if correctly fitted, 1 if the algorithm did not converge.\n\n    probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n    probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n        If `probability=True`, it corresponds to the parameters learned in\n        Platt scaling to produce probability estimates from decision values.\n        If `probability=False`, it's an empty array. Platt scaling uses the\n        logistic function\n        ``1 / (1 + exp(decision_value * probA_ + probB_))``\n        where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n        more information on the multiclass case and training procedure see\n        section 8 of [1]_.\n\n    shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n        Array dimensions of training vector ``X``.\n\n    See Also\n    --------\n    SVC : Support Vector Machine for classification using libsvm.\n\n    LinearSVC : Scalable linear Support Vector Machine for classification using\n        liblinear.\n\n    References\n    ----------\n    .. [1] `LIBSVM: A Library for Support Vector Machines\n        <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n\n    .. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector\n        Machines and Comparisons to Regularized Likelihood Methods\"\n        <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n    >>> y = np.array([1, 1, 2, 2])\n    >>> from sklearn.pipeline import make_pipeline\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.svm import NuSVC\n    >>> clf = make_pipeline(StandardScaler(), NuSVC())\n    >>> clf.fit(X, y)\n    Pipeline(steps=[('standardscaler', StandardScaler()), ('nusvc', NuSVC())])\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]\n    \"\"\"",
                "filename": "sklearn/svm/_classes.py",
                "start_index": 33596,
                "end_index": 42051,
                "start_line": 881,
                "end_line": 1842,
                "max_line": 1853,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"C-Support Vector Classification.\n\n    The implementation is based on libsvm. The fit time scales at least\n    quadratically with the number of samples and may be impractical\n    beyond tens of thousands of samples. For large datasets\n    consider using :class:`~sklearn.svm.LinearSVC` or\n    :class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a\n    :class:`~sklearn.kernel_approximation.Nystroem` transformer or\n    other :ref:`kernel_approximation`.\n\n    The multiclass support is handled according to a one-vs-one scheme.\n\n    For details on the precise mathematical formulation of the provided\n    kernel functions and how `gamma`, `coef0` and `degree` affect each\n    other, see the corresponding section in the narrative documentation:\n    :ref:`svm_kernels`.\n\n    Read more in the :ref:`User Guide <svm_classification>`.\n\n    Parameters\n    ----------\n    C : float, default=1.0\n        Regularization parameter. The strength of the regularization is\n        inversely proportional to C. Must be strictly positive. The penalty\n        is a squared l2 penalty.\n\n    kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,  \\\n        default='rbf'\n        Specifies the kernel type to be used in the algorithm.\n        If none is given, 'rbf' will be used. If a callable is given it is\n        used to pre-compute the kernel matrix from data matrices; that matrix\n        should be an array of shape ``(n_samples, n_samples)``.\n\n    degree : int, default=3\n        Degree of the polynomial kernel function ('poly').\n        Must be non-negative. Ignored by all other kernels.\n\n    gamma : {'scale', 'auto'} or float, default='scale'\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n        - if ``gamma='scale'`` (default) is passed then it uses\n          1 / (n_features * X.var()) as value of gamma,\n        - if 'auto', uses 1 / n_features\n        - if float, must be non-negative.\n\n        .. versionchanged:: 0.22\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\n\n    coef0 : float, default=0.0\n        Independent term in kernel function.\n        It is only significant in 'poly' and 'sigmoid'.\n\n    shrinking : bool, default=True\n        Whether to use the shrinking heuristic.\n        See the :ref:`User Guide <shrinking_svm>`.\n\n    probability : bool, default=False\n        Whether to enable probability estimates. This must be enabled prior\n        to calling `fit`, will slow down that method as it internally uses\n        5-fold cross-validation, and `predict_proba` may be inconsistent with\n        `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n\n    tol : float, default=1e-3\n        Tolerance for stopping criterion.\n\n    cache_size : float, default=200\n        Specify the size of the kernel cache (in MB).\n\n    class_weight : dict or 'balanced', default=None\n        Set the parameter C of class i to class_weight[i]*C for\n        SVC. If not given, all classes are supposed to have\n        weight one.\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n    verbose : bool, default=False\n        Enable verbose output. Note that this setting takes advantage of a\n        per-process runtime setting in libsvm that, if enabled, may not work\n        properly in a multithreaded context.\n\n    max_iter : int, default=-1\n        Hard limit on iterations within solver, or -1 for no limit.\n\n    decision_function_shape : {'ovo', 'ovr'}, default='ovr'\n        Whether to return a one-vs-rest ('ovr') decision function of shape\n        (n_samples, n_classes) as all other classifiers, or the original\n        one-vs-one ('ovo') decision function of libsvm which has shape\n        (n_samples, n_classes * (n_classes - 1) / 2). However, note that\n        internally, one-vs-one ('ovo') is always used as a multi-class strategy\n        to train models; an ovr matrix is only constructed from the ovo matrix.\n        The parameter is ignored for binary classification.\n\n        .. versionchanged:: 0.19\n            decision_function_shape is 'ovr' by default.\n\n        .. versionadded:: 0.17\n           *decision_function_shape='ovr'* is recommended.\n\n        .. versionchanged:: 0.17\n           Deprecated *decision_function_shape='ovo' and None*.\n\n    break_ties : bool, default=False\n        If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n        :term:`predict` will break ties according to the confidence values of\n        :term:`decision_function`; otherwise the first class among the tied\n        classes is returned. Please note that breaking ties comes at a\n        relatively high computational cost compared to a simple predict.\n\n        .. versionadded:: 0.22\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the pseudo random number generation for shuffling the data for\n        probability estimates. Ignored when `probability` is False.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    class_weight_ : ndarray of shape (n_classes,)\n        Multipliers of parameter C for each class.\n        Computed based on the ``class_weight`` parameter.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)\n        Weights assigned to the features (coefficients in the primal\n        problem). This is only available in the case of a linear kernel.\n\n        `coef_` is a readonly property derived from `dual_coef_` and\n        `support_vectors_`.\n\n    dual_coef_ : ndarray of shape (n_classes -1, n_SV)\n        Dual coefficients of the support vector in the decision\n        function (see :ref:`sgd_mathematical_formulation`), multiplied by\n        their targets.\n        For multiclass, coefficient for all 1-vs-1 classifiers.\n        The layout of the coefficients in the multiclass case is somewhat\n        non-trivial. See the :ref:`multi-class section of the User Guide\n        <svm_multi_class>` for details.\n\n    fit_status_ : int\n        0 if correctly fitted, 1 otherwise (will raise warning)\n\n    intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n        Constants in decision function.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : ndarray of shape (n_classes * (n_classes - 1) // 2,)\n        Number of iterations run by the optimization routine to fit the model.\n        The shape of this attribute depends on the number of models optimized\n        which in turn depends on the number of classes.\n\n        .. versionadded:: 1.1\n\n    support_ : ndarray of shape (n_SV)\n        Indices of support vectors.\n\n    support_vectors_ : ndarray of shape (n_SV, n_features)\n        Support vectors.\n\n    n_support_ : ndarray of shape (n_classes,), dtype=int32\n        Number of support vectors for each class.\n\n    probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n    probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n        If `probability=True`, it corresponds to the parameters learned in\n        Platt scaling to produce probability estimates from decision values.\n        If `probability=False`, it's an empty array. Platt scaling uses the\n        logistic function\n        ``1 / (1 + exp(decision_value * probA_ + probB_))``\n        where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n        more information on the multiclass case and training procedure see\n        section 8 of [1]_.\n\n    shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n        Array dimensions of training vector ``X``.\n\n    See Also\n    --------\n    SVR : Support Vector Machine for Regression implemented using libsvm.\n\n    LinearSVC : Scalable Linear Support Vector Machine for classification\n        implemented using liblinear. Check the See Also section of\n        LinearSVC for more comparison element.\n\n    References\n    ----------\n    .. [1] `LIBSVM: A Library for Support Vector Machines\n        <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n\n    .. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector\n        Machines and Comparisons to Regularized Likelihood Methods\"\n        <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.pipeline import make_pipeline\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n    >>> y = np.array([1, 1, 2, 2])\n    >>> from sklearn.svm import SVC\n    >>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n    >>> clf.fit(X, y)\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('svc', SVC(gamma='auto'))])\n\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]\n    \"\"\"",
                "filename": "sklearn/svm/_classes.py",
                "start_index": 23041,
                "end_index": 32355,
                "start_line": 607,
                "end_line": 1842,
                "max_line": 1853,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "import warnings\nfrom abc import ABCMeta, abstractmethod\nfrom numbers import Integral, Real\n\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom ..base import BaseEstimator, ClassifierMixin, _fit_context\nfrom ..exceptions import ConvergenceWarning, NotFittedError\nfrom ..preprocessing import LabelEncoder\nfrom ..utils import check_array, check_random_state, column_or_1d, compute_class_weight\nfrom ..utils._param_validation import Interval, StrOptions\nfrom ..utils.extmath import safe_sparse_dot\nfrom ..utils.metaestimators import available_if\nfrom ..utils.multiclass import _ovr_decision_function, check_classification_targets\nfrom ..utils.validation import (\n    _check_large_sparse,\n    _check_sample_weight,\n    _num_samples,\n    check_consistent_length,\n    check_is_fitted,\n)\nfrom . import _liblinear as liblinear  # type: ignore\n\n# mypy error: error: Module 'sklearn.svm' has no attribute '_libsvm'\n# (and same for other imports)\nfrom . import _libsvm as libsvm  # type: ignore\nfrom . import _libsvm_sparse as libsvm_sparse  # type: ignore\n\nLIBSVM_IMPL = [\"c_svc\", \"nu_svc\", \"one_class\", \"epsilon_svr\", \"nu_svr\"]\n\n\ndef _one_vs_one_coef(dual_coef, n_support, support_vectors):\n    \"\"\"Generate primal coefficients from dual coefficients\n    for the one-vs-one multi class LibSVM in the case\n    of a linear kernel.\"\"\"\n\n    # get 1vs1 weights for all n*(n-1) classifiers.\n    # this is somewhat messy.\n    # shape of dual_coef_ is nSV * (n_classes -1)\n    # see docs for details\n    n_class = dual_coef.shape[0] + 1\n\n    # XXX we could do preallocation of coef but\n    # would have to take care in the sparse case\n    coef = []\n    sv_locs = np.cumsum(np.hstack([[0], n_support]))\n    for class1 in range(n_class):\n        # SVs for class1:\n        sv1 = support_vectors[sv_locs[class1] : sv_locs[class1 + 1], :]\n        for class2 in range(class1 + 1, n_class):\n            # SVs for class1:\n            sv2 = support_vectors[sv_locs[class2] : sv_locs[class2 + 1], :]\n\n            # dual coef for class1 SVs:\n            alpha1 = dual_coef[class2 - 1, sv_locs[class1] : sv_locs[class1 + 1]]\n            # dual coef for class2 SVs:\n            alpha2 = dual_coef[class1, sv_locs[class2] : sv_locs[class2 + 1]]\n            # build weight for class1 vs class2\n\n            coef.append(safe_sparse_dot(alpha1, sv1) + safe_sparse_dot(alpha2, sv2))\n    return coef",
                "filename": "sklearn/svm/_base.py",
                "start_index": 0,
                "end_index": 2361,
                "start_line": 1,
                "end_line": 1254,
                "max_line": 1254,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_classifiers_one_label(name, classifier_orig):\n    error_string_fit = \"Classifier can't train when only one class is present.\"\n    error_string_predict = \"Classifier can't predict when only one class is present.\"\n    rnd = np.random.RandomState(0)\n    X_train = rnd.uniform(size=(10, 3))\n    X_test = rnd.uniform(size=(10, 3))\n    y = np.ones(10)\n    # catch deprecation warnings\n    with ignore_warnings(category=FutureWarning):\n        classifier = clone(classifier_orig)\n        with raises(\n            ValueError, match=\"class\", may_pass=True, err_msg=error_string_fit\n        ) as cm:\n            classifier.fit(X_train, y)\n\n        if cm.raised_and_matched:\n            # ValueError was raised with proper error message\n            return\n\n        assert_array_equal(classifier.predict(X_test), y, err_msg=error_string_predict)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_classifiers_one_label_sample_weights(name, classifier_orig):\n    \"\"\"Check that classifiers accepting sample_weight fit or throws a ValueError with\n    an explicit message if the problem is reduced to one class.\n    \"\"\"\n    error_fit = (\n        f\"{name} failed when fitted on one label after sample_weight trimming. Error \"\n        \"message is not explicit, it should have 'class'.\"\n    )\n    error_predict = f\"{name} prediction results should only output the remaining class.\"\n    rnd = np.random.RandomState(0)\n    # X should be square for test on SVC with precomputed kernel\n    X_train = rnd.uniform(size=(10, 10))\n    X_test = rnd.uniform(size=(10, 10))\n    y = np.arange(10) % 2\n    sample_weight = y.copy()  # select a single class\n    classifier = clone(classifier_orig)\n\n    if has_fit_parameter(classifier, \"sample_weight\"):\n        match = [r\"\\bclass(es)?\\b\", error_predict]\n        err_type, err_msg = (AssertionError, ValueError), error_fit\n    else:\n        match = r\"\\bsample_weight\\b\"\n        err_type, err_msg = (TypeError, ValueError), None\n\n    with raises(err_type, match=match, may_pass=True, err_msg=err_msg) as cm:\n        classifier.fit(X_train, y, sample_weight=sample_weight)\n        if cm.raised_and_matched:\n            # raise the proper error type with the proper error message\n            return\n        # for estimators that do not fail, they should be able to predict the only\n        # class remaining during fit\n        assert_array_equal(\n            classifier.predict(X_test), np.ones(10), err_msg=error_predict\n        )",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 79371,
                "end_index": 81868,
                "start_line": 181,
                "end_line": 4639,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "def _sparse_fit(self, X, y, sample_weight, solver_type, kernel, random_seed):\n        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")\n        X.sort_indices()\n\n        kernel_type = self._sparse_kernels.index(kernel)\n\n        libsvm_sparse.set_verbosity_wrap(self.verbose)\n\n        (\n            self.support_,\n            self.support_vectors_,\n            dual_coef_data,\n            self.intercept_,\n            self._n_support,\n            self._probA,\n            self._probB,\n            self.fit_status_,\n            self._num_iter,\n        ) = libsvm_sparse.libsvm_sparse_train(\n            X.shape[1],\n            X.data,\n            X.indices,\n            X.indptr,\n            y,\n            solver_type,\n            kernel_type,\n            self.degree,\n            self._gamma,\n            self.coef0,\n            self.tol,\n            self.C,\n            # TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\n            getattr(self, \"_class_weight\", np.empty(0)),\n            sample_weight,\n            self.nu,\n            self.cache_size,\n            self.epsilon,\n            int(self.shrinking),\n            int(self.probability),\n            self.max_iter,\n            random_seed,\n        )\n\n        self._warn_from_fit_status()\n\n        if hasattr(self, \"classes_\"):\n            n_class = len(self.classes_) - 1\n        else:  # regression\n            n_class = 1\n        n_SV = self.support_vectors_.shape[0]\n\n        dual_coef_indices = np.tile(np.arange(n_SV), n_class)\n        if not n_SV:\n            self.dual_coef_ = sp.csr_matrix([])\n        else:\n            dual_coef_indptr = np.arange(\n                0, dual_coef_indices.size + 1, dual_coef_indices.size / n_class\n            )\n            self.dual_coef_ = sp.csr_matrix(\n                (dual_coef_data, dual_coef_indices, dual_coef_indptr), (n_class, n_SV)\n            )\n\n    def predict(self, X):\n        \"\"\"Perform regression on samples in X.\n\n        For an one-class model, +1 (inlier) or -1 (outlier) is returned.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            For kernel=\"precomputed\", the expected shape of X is\n            (n_samples_test, n_samples_train).\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        X = self._validate_for_predict(X)\n        predict = self._sparse_predict if self._sparse else self._dense_predict\n        return predict(X)",
                "filename": "sklearn/svm/_base.py",
                "start_index": 12762,
                "end_index": 15281,
                "start_line": 353,
                "end_line": 433,
                "max_line": 1254,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/svm/_base.py": [
                {
                    "chunk": {
                        "code": "def _sparse_fit(self, X, y, sample_weight, solver_type, kernel, random_seed):\n        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")\n        X.sort_indices()\n\n        kernel_type = self._sparse_kernels.index(kernel)\n\n        libsvm_sparse.set_verbosity_wrap(self.verbose)\n\n        (\n            self.support_,\n            self.support_vectors_,\n            dual_coef_data,\n            self.intercept_,\n            self._n_support,\n            self._probA,\n            self._probB,\n            self.fit_status_,\n            self._num_iter,\n        ) = libsvm_sparse.libsvm_sparse_train(\n            X.shape[1],\n            X.data,\n            X.indices,\n            X.indptr,\n            y,\n            solver_type,\n            kernel_type,\n            self.degree,\n            self._gamma,\n            self.coef0,\n            self.tol,\n            self.C,\n            # TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\n            getattr(self, \"_class_weight\", np.empty(0)),\n            sample_weight,\n            self.nu,\n            self.cache_size,\n            self.epsilon,\n            int(self.shrinking),\n            int(self.probability),\n            self.max_iter,\n            random_seed,\n        )\n\n        self._warn_from_fit_status()\n\n        if hasattr(self, \"classes_\"):\n            n_class = len(self.classes_) - 1\n        else:  # regression\n            n_class = 1\n        n_SV = self.support_vectors_.shape[0]\n\n        dual_coef_indices = np.tile(np.arange(n_SV), n_class)\n        if not n_SV:\n            self.dual_coef_ = sp.csr_matrix([])\n        else:\n            dual_coef_indptr = np.arange(\n                0, dual_coef_indices.size + 1, dual_coef_indices.size / n_class\n            )\n            self.dual_coef_ = sp.csr_matrix(\n                (dual_coef_data, dual_coef_indices, dual_coef_indptr), (n_class, n_SV)\n            )\n\n    def predict(self, X):\n        \"\"\"Perform regression on samples in X.\n\n        For an one-class model, +1 (inlier) or -1 (outlier) is returned.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            For kernel=\"precomputed\", the expected shape of X is\n            (n_samples_test, n_samples_train).\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        X = self._validate_for_predict(X)\n        predict = self._sparse_predict if self._sparse else self._dense_predict\n        return predict(X)",
                        "filename": "sklearn/svm/_base.py",
                        "start_index": 12762,
                        "end_index": 15281,
                        "start_line": 353,
                        "end_line": 433,
                        "max_line": 1254,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the '_sparse_fit' method where the ZeroDivisionError occurs due to division by 'n_class' when 'n_SV' (number of support vectors) is zero."
                },
                {
                    "chunk": {
                        "code": "kernel = \"precomputed\" if callable(self.kernel) else self.kernel\n\n        if kernel == \"precomputed\":\n            # unused but needs to be a float for cython code that ignores\n            # it anyway\n            self._gamma = 0.0\n        elif isinstance(self.gamma, str):\n            if self.gamma == \"scale\":\n                # var = E[X^2] - E[X]^2 if sparse\n                X_var = (X.multiply(X)).mean() - (X.mean()) ** 2 if sparse else X.var()\n                self._gamma = 1.0 / (X.shape[1] * X_var) if X_var != 0 else 1.0\n            elif self.gamma == \"auto\":\n                self._gamma = 1.0 / X.shape[1]\n        elif isinstance(self.gamma, Real):\n            self._gamma = self.gamma\n\n        fit = self._sparse_fit if self._sparse else self._dense_fit\n        if self.verbose:\n            print(\"[LibSVM]\", end=\"\")\n\n        seed = rnd.randint(np.iinfo(\"i\").max)\n        fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\n        # see comment on the other call to np.iinfo in this file\n\n        self.shape_fit_ = X.shape if hasattr(X, \"shape\") else (n_samples,)\n\n        # In binary case, we need to flip the sign of coef, intercept and\n        # decision function. Use self._intercept_ and self._dual_coef_\n        # internally.\n        self._intercept_ = self.intercept_.copy()\n        self._dual_coef_ = self.dual_coef_\n        if self._impl in [\"c_svc\", \"nu_svc\"] and len(self.classes_) == 2:\n            self.intercept_ *= -1\n            self.dual_coef_ = -self.dual_coef_\n\n        dual_coef = self._dual_coef_.data if self._sparse else self._dual_coef_\n        intercept_finiteness = np.isfinite(self._intercept_).all()\n        dual_coef_finiteness = np.isfinite(dual_coef).all()\n        if not (intercept_finiteness and dual_coef_finiteness):\n            raise ValueError(\n                \"The dual coefficients or intercepts are not finite.\"\n                \" The input data may contain large values and need to be\"\n                \" preprocessed.\"\n            )\n\n        # Since, in the case of SVC and NuSVC, the number of models optimized by\n        # libSVM could be greater than one (depending on the input), `n_iter_`\n        # stores an ndarray.\n        # For the other sub-classes (SVR, NuSVR, and OneClassSVM), the number of\n        # models optimized by libSVM is always one, so `n_iter_` stores an\n        # integer.\n        if self._impl in [\"c_svc\", \"nu_svc\"]:\n            self.n_iter_ = self._num_iter\n        else:\n            self.n_iter_ = self._num_iter.item()\n\n        return self",
                        "filename": "sklearn/svm/_base.py",
                        "start_index": 7989,
                        "end_index": 10520,
                        "start_line": 229,
                        "end_line": 1006,
                        "max_line": 1254,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet from '_base.py' includes the call to 'fit' method which leads to the '_sparse_fit' method where the error is raised."
                }
            ]
        },
        "solution_task": "To address the `ZeroDivisionError: float division by zero` issue in the `_sparse_fit` method of the `SVR` class, modify the code as follows:\n\n- In the `sklearn/svm/_base.py` file, locate the `_sparse_fit` method within the `SVR` class or a related class.\n- Update the section where `dual_coef_` is being constructed from `dual_coef_data` to handle the case where `n_class` or `dual_coef_indices.size` is zero to avoid division by zero.\n\nHere is the updated code snippet:\n\n```python\n# sklearn/svm/_base.py\n\n# ... [existing code above] ...\n\nif hasattr(self, \"classes_\"):\n    n_class = len(self.classes_) - 1\nelse:  # regression\n    n_class = 1\nn_SV = self.support_vectors_.shape[0]\n\ndual_coef_indices = np.tile(np.arange(n_SV), n_class)\nif not n_SV or n_class == 0:  # Check if n_SV or n_class is zero to avoid division by zero\n    self.dual_coef_ = sp.csr_matrix((0, 0))  # Create an empty csr_matrix with appropriate shape\nelse:\n    dual_coef_indptr = np.arange(\n        0, dual_coef_indices.size + 1, dual_coef_indices.size // n_class\n    )\n    self.dual_coef_ = sp.csr_matrix(\n        (dual_coef_data, dual_coef_indices, dual_coef_indptr), (n_class, n_SV)\n    )\n\n# ... [existing code below] ...\n```\n\nMake sure to replace the division operator `/` with the floor division operator `//` to ensure an integer result, which is required for the indices of a sparse matrix. Additionally, handle the case where `n_SV` or `n_class` is zero by creating an empty `csr_matrix` with the appropriate shape.",
        "bouncer_response": {
            "score": 8,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE provides a detailed solution to the ZeroDivisionError by suggesting modifications to the _sparse_fit method in the SVR class. It correctly identifies the need to handle the case where n_class or dual_coef_indices.size is zero to prevent division by zero. The RESPONSE includes a code snippet that checks for zero values and creates an empty csr_matrix if necessary, which is a direct and actionable solution to the problem described in the PROMPT. The RESPONSE also correctly advises on replacing the division operator with the floor division operator to ensure an integer result, which is relevant to the construction of sparse matrices. Furthermore, the RESPONSE provides links to the relevant sections of the code where the error occurs and where the changes should be made, which is helpful for locating the issue and applying the fix.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711738994.756089,
        "result": "To address the `ZeroDivisionError: float division by zero` issue in the `_sparse_fit` method of the `SVR` class, modify the code as follows:\n\n- In the `sklearn/svm/_base.py` file, locate the `_sparse_fit` method within the `SVR` class or a related class.\n- Update the section where `dual_coef_` is being constructed from `dual_coef_data` to handle the case where `n_class` or `dual_coef_indices.size` is zero to avoid division by zero.\n\nHere is the updated code snippet:\n\n```python\n# sklearn/svm/_base.py\n\n# ... [existing code above] ...\n\nif hasattr(self, \"classes_\"):\n    n_class = len(self.classes_) - 1\nelse:  # regression\n    n_class = 1\nn_SV = self.support_vectors_.shape[0]\n\ndual_coef_indices = np.tile(np.arange(n_SV), n_class)\nif not n_SV or n_class == 0:  # Check if n_SV or n_class is zero to avoid division by zero\n    self.dual_coef_ = sp.csr_matrix((0, 0))  # Create an empty csr_matrix with appropriate shape\nelse:\n    dual_coef_indptr = np.arange(\n        0, dual_coef_indices.size + 1, dual_coef_indices.size // n_class\n    )\n    self.dual_coef_ = sp.csr_matrix(\n        (dual_coef_data, dual_coef_indices, dual_coef_indptr), (n_class, n_SV)\n    )\n\n# ... [existing code below] ...\n```\n\nMake sure to replace the division operator `/` with the floor division operator `//` to ensure an integer result, which is required for the indices of a sparse matrix. Additionally, handle the case where `n_SV` or `n_class` is zero by creating an empty `csr_matrix` with the appropriate shape.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [svm](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/svm)- [Lines 353 - 433](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/svm/_base.py#L353-L433)\n\n This snippet contains the '_sparse_fit' method where the ZeroDivisionError occurs due to division by 'n_class' when 'n_SV' (number of support vectors) is zero. \n\n\n   - [Lines 229 - 1006](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/svm/_base.py#L229-L1006)\n\n This snippet from '_base.py' includes the call to 'fit' method which leads to the '_sparse_fit' method where the error is raised."
    }
}