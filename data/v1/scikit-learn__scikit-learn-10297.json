{
    "id": "scikit-learn__scikit-learn-10297",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
    "task_id": 3927450988713384520,
    "title": "linear_model.RidgeClassifierCV's Parameter store_cv_values issue",
    "body": "#### Description\r\nParameter store_cv_values error on sklearn.linear_model.RidgeClassifierCV\r\n\r\n#### Steps/Code to Reproduce\r\nimport numpy as np\r\nfrom sklearn import linear_model as lm\r\n\r\n#test database\r\nn = 100\r\nx = np.random.randn(n, 30)\r\ny = np.random.normal(size = n)\r\n\r\nrr = lm.RidgeClassifierCV(alphas = np.arange(0.1, 1000, 0.1), normalize = True, \r\n                                         store_cv_values = True).fit(x, y)\r\n\r\n#### Expected Results\r\nExpected to get the usual ridge regression model output, keeping the cross validation predictions as attribute.\r\n\r\n#### Actual Results\r\nTypeError: __init__() got an unexpected keyword argument 'store_cv_values'\r\n\r\nlm.RidgeClassifierCV actually has no parameter store_cv_values, even though some attributes depends on it.\r\n\r\n#### Versions\r\nWindows-10-10.0.14393-SP0\r\nPython 3.6.3 |Anaconda, Inc.| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]\r\nNumPy 1.13.3\r\nSciPy 0.19.1\r\nScikit-Learn 0.19.1\r\n\r\n\nAdd store_cv_values boolean flag support to RidgeClassifierCV\nAdd store_cv_values support to RidgeClassifierCV - documentation claims that usage of this flag is possible:\n\n> cv_values_ : array, shape = [n_samples, n_alphas] or shape = [n_samples, n_responses, n_alphas], optional\n> Cross-validation values for each alpha (if **store_cv_values**=True and `cv=None`).\n\nWhile actually usage of this flag gives \n\n> TypeError: **init**() got an unexpected keyword argument 'store_cv_values'\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.20.git",
    "requested_commit": "",
    "mayil_response": "- The `store_cv_values` parameter is not supported in the `RidgeClassifierCV` class as of Scikit-Learn version 0.19.1.\n- The `store_cv_values` parameter is only relevant for the `RidgeCV` class, which is for regression, not classification.\n- To resolve the issue, remove the `store_cv_values` parameter from the `RidgeClassifierCV` constructor and any related method calls.\n- If the user needs to store cross-validation values, they will need to manually implement cross-validation using `RidgeClassifier` and store the values themselves, or upgrade to a newer version of Scikit-Learn if it supports this feature for `RidgeClassifierCV`.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/linear_model)- [Lines 796 - 2589](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_ridge.py#L796-L2589)\n\n This snippet includes the __init__ method of RidgeClassifierCV where the 'store_cv_values' parameter is defined, which is directly related to the issue. \n\n\n   - [Lines 2188 - 2579](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_ridge.py#L2188-L2579)\n\n This snippet contains the logic that checks the 'store_cv_values' parameter and raises a ValueError if 'cv' is not None, which is part of the issue. \n\n\n   - [Lines 2401 - 2566](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_ridge.py#L2401-L2566)\n\n This snippet provides the documentation for the RidgeClassifierCV class, which incorrectly states that 'store_cv_values' is a valid parameter. \n\n\n   - [Lines 2315 - 2566](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_ridge.py#L2315-L2566)\n\n This snippet includes the documentation for the 'cv_values_' attribute, which mentions 'store_cv_values' as a condition for its availability.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.000208,
        "snippet_processor": 0.03336,
        "issue_star_creation": 0.03149,
        "issue_star_solver": 0.07856,
        "bouncer": 0.03409
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711742632.438465,
        "relevant_snippets": [
            {
                "code": "shape (n_samples, n_targets, n_alphas), optional\n        Cross-validation values for each alpha (only available if\n        ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been\n        called, this attribute will contain the mean squared errors if\n        `scoring is None` otherwise it will contain standardized per point\n        prediction values.\n\n    coef_ : ndarray of shape (n_features) or (n_targets, n_features)\n        Weight vector(s).\n\n    intercept_ : float or ndarray of shape (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    alpha_ : float or ndarray of shape (n_targets,)\n        Estimated regularization parameter, or, if ``alpha_per_target=True``,\n        the estimated regularization parameter for each target.\n\n    best_score_ : float or ndarray of shape (n_targets,)\n        Score of base estimator with best alpha, or, if\n        ``alpha_per_target=True``, a score for each target.\n\n        .. versionadded:: 0.23\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    Ridge : Ridge regression.\n    RidgeClassifier : Classifier based on ridge regression on {-1, 1} labels.\n    RidgeClassifierCV : Ridge classifier with built-in cross validation.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_diabetes\n    >>> from sklearn.linear_model import RidgeCV\n    >>> X, y = load_diabetes(return_X_y=True)\n    >>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n    >>> clf.score(X, y)\n    0.5166...\n    \"\"\"",
                "filename": "sklearn/linear_model/_ridge.py",
                "start_index": 81834,
                "end_index": 83660,
                "start_line": 2315,
                "end_line": 2566,
                "max_line": 2589,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "if cv is None:\n            estimator = _RidgeGCV(\n                alphas,\n                fit_intercept=self.fit_intercept,\n                scoring=self.scoring,\n                gcv_mode=self.gcv_mode,\n                store_cv_values=self.store_cv_values,\n                is_clf=is_classifier(self),\n                alpha_per_target=self.alpha_per_target,\n            )\n            estimator.fit(X, y, sample_weight=sample_weight)\n            self.alpha_ = estimator.alpha_\n            self.best_score_ = estimator.best_score_\n            if self.store_cv_values:\n                self.cv_values_ = estimator.cv_values_\n        else:\n            if self.store_cv_values:\n                raise ValueError(\"cv!=None and store_cv_values=True are incompatible\")\n            if self.alpha_per_target:\n                raise ValueError(\"cv!=None and alpha_per_target=True are incompatible\")\n\n            parameters = {\"alpha\": alphas}\n            solver = \"sparse_cg\" if sparse.issparse(X) else \"auto\"\n            model = RidgeClassifier if is_classifier(self) else Ridge\n            gs = GridSearchCV(\n                model(\n                    fit_intercept=self.fit_intercept,\n                    solver=solver,\n                ),\n                parameters,\n                cv=cv,\n                scoring=self.scoring,\n            )\n            gs.fit(X, y, sample_weight=sample_weight)\n            estimator = gs.best_estimator_\n            self.alpha_ = gs.best_estimator_.alpha\n            self.best_score_ = gs.best_score_\n\n        self.coef_ = estimator.coef_\n        self.intercept_ = estimator.intercept_\n        self.n_features_in_ = estimator.n_features_in_\n        if hasattr(estimator, \"feature_names_in_\"):\n            self.feature_names_in_ = estimator.feature_names_in_\n\n        return self",
                "filename": "sklearn/linear_model/_ridge.py",
                "start_index": 76333,
                "end_index": 78133,
                "start_line": 2188,
                "end_line": 2579,
                "max_line": 2589,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "_parameter_constraints: dict = {\n        **_BaseRidgeCV._parameter_constraints,\n        \"class_weight\": [dict, StrOptions({\"balanced\"}), None],\n    }\n    for param in (\"gcv_mode\", \"alpha_per_target\"):\n        _parameter_constraints.pop(param)\n\n    def __init__(\n        self,\n        alphas=(0.1, 1.0, 10.0),\n        *,\n        fit_intercept=True,\n        scoring=None,\n        cv=None,\n        class_weight=None,\n        store_cv_values=False,\n    ):\n        super().__init__(\n            alphas=alphas,\n            fit_intercept=fit_intercept,\n            scoring=scoring,\n            cv=cv,\n            store_cv_values=store_cv_values,\n        )\n        self.class_weight = class_weight\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit Ridge classifier with cv.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples\n            and `n_features` is the number of features. When using GCV,\n            will be cast to float64 if necessary.\n\n        y : ndarray of shape (n_samples,)\n            Target values. Will be cast to X's dtype if necessary.\n\n        sample_weight : float or ndarray of shape (n_samples,), default=None\n            Individual weights for each sample. If given a float, every sample\n            will have the same weight.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        # `RidgeClassifier` does not accept \"sag\" or \"saga\" solver and thus support\n        # csr, csc, and coo sparse matrices. By using solver=\"eigen\" we force to accept\n        # all sparse format.\n        X, y, sample_weight, Y = self._prepare_data(X, y, sample_weight, solver=\"eigen\")\n\n        # If cv is None, gcv mode will be used and we used the binarized Y\n        # since y will not be binarized in _RidgeGCV estimator.\n        # If cv is not None, a GridSearchCV with some RidgeClassifier\n        # estimators are used where y will be binarized. Thus, we pass y\n        # instead of the binarized Y.\n        target = Y if self.cv is None else y\n        super().fit(X, target, sample_weight=sample_weight)\n        return self\n\n    def _more_tags(self):\n        return {\n            \"multilabel\": True,\n            \"_xfail_checks\": {\n                \"check_sample_weights_invariance\": (\n                    \"zero sample_weight is not equivalent to removing samples\"\n                ),\n            },\n        }",
                "filename": "sklearn/linear_model/_ridge.py",
                "start_index": 89465,
                "end_index": 92004,
                "start_line": 796,
                "end_line": 2589,
                "max_line": 2589,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"Ridge classifier with built-in cross-validation.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    By default, it performs Leave-One-Out Cross-Validation. Currently,\n    only the n_features > n_samples case is handled efficiently.\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n        Array of alpha values to try.\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``1 / (2C)`` in other linear models such as\n        :class:`~sklearn.linear_model.LogisticRegression` or\n        :class:`~sklearn.svm.LinearSVC`.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    scoring : str, callable, default=None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the efficient Leave-One-Out cross-validation\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    class_weight : dict or 'balanced', default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n    store_cv_values : bool, default=False\n        Flag indicating if the cross-validation values corresponding to\n        each alpha should be stored in the ``cv_values_`` attribute (see\n        below). This flag is only compatible with ``cv=None`` (i.e. using\n        Leave-One-Out Cross-Validation).\n\n    Attributes\n    ----------\n    cv_values_ : ndarray of shape (n_samples, n_targets, n_alphas), optional\n        Cross-validation values for each alpha (only if ``store_cv_values=True`` and\n        ``cv=None``). After ``fit()`` has been called, this attribute will\n        contain the mean squared errors if `scoring is None` otherwise it\n        will contain standardized per point prediction values.\n\n    coef_ : ndarray of shape (1, n_features) or (n_targets, n_features)\n        Coefficient of the features in the decision function.\n\n        ``coef_`` is of shape (1, n_features) when the given problem is binary.\n\n    intercept_ : float or ndarray of shape (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    alpha_ : float\n        Estimated regularization parameter.\n\n    best_score_ : float\n        Score of base estimator with best alpha.\n\n        .. versionadded:: 0.23\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    Ridge : Ridge regression.\n    RidgeClassifier : Ridge classifier.\n    RidgeCV : Ridge regression with built-in cross validation.\n\n    Notes\n    -----\n    For multi-class classification, n_class classifiers are trained in\n    a one-versus-all approach. Concretely, this is implemented by taking\n    advantage of the multi-variate response support in Ridge.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.linear_model import RidgeClassifierCV\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n    >>> clf.score(X, y)\n    0.9630...\n    \"\"\"",
                "filename": "sklearn/linear_model/_ridge.py",
                "start_index": 84931,
                "end_index": 89459,
                "start_line": 2401,
                "end_line": 2566,
                "max_line": 2589,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "class RidgeClassifierCV(_RidgeClassifierMixin, _BaseRidgeCV):",
                "filename": "sklearn/linear_model/_ridge.py",
                "start_index": 84865,
                "end_index": 84926,
                "start_line": 2400,
                "end_line": 2400,
                "max_line": 2589,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"Ridge regression with built-in cross-validation.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    By default, it performs efficient Leave-One-Out Cross-Validation.\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n        Array of alpha values to try.\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``1 / (2C)`` in other linear models such as\n        :class:`~sklearn.linear_model.LogisticRegression` or\n        :class:`~sklearn.svm.LinearSVC`.\n        If using Leave-One-Out cross-validation, alphas must be positive.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    scoring : str, callable, default=None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n        If None, the negative mean squared error if cv is 'auto' or None\n        (i.e. when using leave-one-out cross-validation), and r2 score\n        otherwise.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the efficient Leave-One-Out cross-validation\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`~sklearn.model_selection.StratifiedKFold` is used, else,\n        :class:`~sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    gcv_mode : {'auto', 'svd', 'eigen'}, default='auto'\n        Flag indicating which strategy to use when performing\n        Leave-One-Out Cross-Validation. Options are::\n\n            'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'\n            'svd' : force use of singular value decomposition of X when X is\n                dense, eigenvalue decomposition of X^T.X when X is sparse.\n            'eigen' : force computation via eigendecomposition of X.X^T\n\n        The 'auto' mode is the default and is intended to pick the cheaper\n        option of the two depending on the shape of the training data.\n\n    store_cv_values : bool, default=False\n        Flag indicating if the cross-validation values corresponding to\n        each alpha should be stored in the ``cv_values_`` attribute (see\n        below). This flag is only compatible with ``cv=None`` (i.e. using\n        Leave-One-Out Cross-Validation).\n\n    alpha_per_target : bool, default=False\n        Flag indicating whether to optimize the alpha value (picked from the\n        `alphas` parameter list) for each target separately (for multi-output\n        settings: multiple prediction targets). When set to `True`, after\n        fitting, the `alpha_` attribute will contain a value for each target.\n        When set to `False`, a single alpha is used for all targets.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    cv_values_ : ndarray of shape (n_samples, n_alphas) or \\",
                "filename": "sklearn/linear_model/_ridge.py",
                "start_index": 78203,
                "end_index": 81821,
                "start_line": 2236,
                "end_line": 2314,
                "max_line": 2589,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "# improves the conditioning of the problem and reduces the variance of the\n# estimates. :class:`~sklearn.linear_model.RidgeCV` applies cross validation\n# in order to determine which value of the regularization parameter (`alpha`)\n# is best suited for prediction.\n\nfrom sklearn.linear_model import RidgeCV\n\nalphas = np.logspace(-10, 10, 21)  # alpha values to be chosen from by cross-validation\nmodel = make_pipeline(\n    preprocessor,\n    TransformedTargetRegressor(\n        regressor=RidgeCV(alphas=alphas),\n        func=np.log10,\n        inverse_func=sp.special.exp10,\n    ),\n)\nmodel.fit(X_train, y_train)\n\n# %%\n# First we check which value of :math:`\\alpha` has been selected.\n\nmodel[-1].regressor_.alpha_\n\n# %%\n# Then we check the quality of the predictions.\nmae_train = median_absolute_error(y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = median_absolute_error(y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = plt.subplots(figsize=(5, 5))\ndisplay = PredictionErrorDisplay.from_predictions(\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\nplt.tight_layout()\n\n# %%\n# The ability to reproduce the data of the regularized model is similar to\n# the one of the non-regularized model.\n\ncoefs = pd.DataFrame(\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\nplt.title(\"Ridge model, with regularization, normalized variables\")\nplt.xlabel(\"Raw coefficient values\")\nplt.axvline(x=0, color=\".5\")\nplt.subplots_adjust(left=0.3)\n\n# %%\n# The coefficients are significantly different.\n# AGE and EXPERIENCE coefficients are both positive but they now have less\n# influence on the prediction.\n#\n# The regularization reduces the influence of correlated\n# variables on the model because the weight is shared between the two\n# predictive variables, so neither alone would have strong weights.\n#\n# On the other hand, the weights obtained with regularization are more\n# stable (see the :ref:`ridge_regression` User Guide section). This\n# increased stability is visible from the plot, obtained from data\n# perturbations, in a cross-validation. This plot can be compared with\n# the :ref:`previous one<covariation>`.\n\ncv_model = cross_validate(\n    model,\n    X,\n    y,\n    cv=cv,\n    return_estimator=True,\n    n_jobs=2,\n)\ncoefs = pd.DataFrame(\n    [est[-1].regressor_.coef_ for est in cv_model[\"estimator\"]], columns=feature_names\n)\n\n# %%\nplt.ylabel(\"Age coefficient\")\nplt.xlabel(\"Experience coefficient\")\nplt.grid(True)\nplt.xlim(-0.4, 0.5)\nplt.ylim(-0.4, 0.5)\nplt.scatter(coefs[\"AGE\"], coefs[\"EXPERIENCE\"])\n_ = plt.title(\"Co-variations of coefficients for AGE and EXPERIENCE across folds\")\n\n# %%",
                "filename": "examples/inspection/plot_linear_model_coefficient_interpretation.py",
                "start_index": 17829,
                "end_index": 20820,
                "start_line": 518,
                "end_line": 710,
                "max_line": 757,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "if self.store_cv_values:\n            if len(y.shape) == 1:\n                cv_values_shape = n_samples, n_alphas\n            else:\n                cv_values_shape = n_samples, n_y, n_alphas\n            self.cv_values_ = self.cv_values_.reshape(cv_values_shape)\n\n        return self",
                "filename": "sklearn/linear_model/_ridge.py",
                "start_index": 73394,
                "end_index": 73675,
                "start_line": 2030,
                "end_line": 2579,
                "max_line": 2589,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "class _BaseRidgeCV(LinearModel):",
                "filename": "sklearn/linear_model/_ridge.py",
                "start_index": 73678,
                "end_index": 73710,
                "start_line": 2111,
                "end_line": 2111,
                "max_line": 2589,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "class _RidgeGCV(LinearModel):",
                "filename": "sklearn/linear_model/_ridge.py",
                "start_index": 53569,
                "end_index": 53598,
                "start_line": 1585,
                "end_line": 1585,
                "max_line": 2589,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/linear_model/_ridge.py": [
                {
                    "chunk": {
                        "code": "_parameter_constraints: dict = {\n        **_BaseRidgeCV._parameter_constraints,\n        \"class_weight\": [dict, StrOptions({\"balanced\"}), None],\n    }\n    for param in (\"gcv_mode\", \"alpha_per_target\"):\n        _parameter_constraints.pop(param)\n\n    def __init__(\n        self,\n        alphas=(0.1, 1.0, 10.0),\n        *,\n        fit_intercept=True,\n        scoring=None,\n        cv=None,\n        class_weight=None,\n        store_cv_values=False,\n    ):\n        super().__init__(\n            alphas=alphas,\n            fit_intercept=fit_intercept,\n            scoring=scoring,\n            cv=cv,\n            store_cv_values=store_cv_values,\n        )\n        self.class_weight = class_weight\n\n    @_fit_context(prefer_skip_nested_validation=True)\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit Ridge classifier with cv.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples\n            and `n_features` is the number of features. When using GCV,\n            will be cast to float64 if necessary.\n\n        y : ndarray of shape (n_samples,)\n            Target values. Will be cast to X's dtype if necessary.\n\n        sample_weight : float or ndarray of shape (n_samples,), default=None\n            Individual weights for each sample. If given a float, every sample\n            will have the same weight.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        # `RidgeClassifier` does not accept \"sag\" or \"saga\" solver and thus support\n        # csr, csc, and coo sparse matrices. By using solver=\"eigen\" we force to accept\n        # all sparse format.\n        X, y, sample_weight, Y = self._prepare_data(X, y, sample_weight, solver=\"eigen\")\n\n        # If cv is None, gcv mode will be used and we used the binarized Y\n        # since y will not be binarized in _RidgeGCV estimator.\n        # If cv is not None, a GridSearchCV with some RidgeClassifier\n        # estimators are used where y will be binarized. Thus, we pass y\n        # instead of the binarized Y.\n        target = Y if self.cv is None else y\n        super().fit(X, target, sample_weight=sample_weight)\n        return self\n\n    def _more_tags(self):\n        return {\n            \"multilabel\": True,\n            \"_xfail_checks\": {\n                \"check_sample_weights_invariance\": (\n                    \"zero sample_weight is not equivalent to removing samples\"\n                ),\n            },\n        }",
                        "filename": "sklearn/linear_model/_ridge.py",
                        "start_index": 89465,
                        "end_index": 92004,
                        "start_line": 796,
                        "end_line": 2589,
                        "max_line": 2589,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes the __init__ method of RidgeClassifierCV where the 'store_cv_values' parameter is defined, which is directly related to the issue."
                },
                {
                    "chunk": {
                        "code": "if cv is None:\n            estimator = _RidgeGCV(\n                alphas,\n                fit_intercept=self.fit_intercept,\n                scoring=self.scoring,\n                gcv_mode=self.gcv_mode,\n                store_cv_values=self.store_cv_values,\n                is_clf=is_classifier(self),\n                alpha_per_target=self.alpha_per_target,\n            )\n            estimator.fit(X, y, sample_weight=sample_weight)\n            self.alpha_ = estimator.alpha_\n            self.best_score_ = estimator.best_score_\n            if self.store_cv_values:\n                self.cv_values_ = estimator.cv_values_\n        else:\n            if self.store_cv_values:\n                raise ValueError(\"cv!=None and store_cv_values=True are incompatible\")\n            if self.alpha_per_target:\n                raise ValueError(\"cv!=None and alpha_per_target=True are incompatible\")\n\n            parameters = {\"alpha\": alphas}\n            solver = \"sparse_cg\" if sparse.issparse(X) else \"auto\"\n            model = RidgeClassifier if is_classifier(self) else Ridge\n            gs = GridSearchCV(\n                model(\n                    fit_intercept=self.fit_intercept,\n                    solver=solver,\n                ),\n                parameters,\n                cv=cv,\n                scoring=self.scoring,\n            )\n            gs.fit(X, y, sample_weight=sample_weight)\n            estimator = gs.best_estimator_\n            self.alpha_ = gs.best_estimator_.alpha\n            self.best_score_ = gs.best_score_\n\n        self.coef_ = estimator.coef_\n        self.intercept_ = estimator.intercept_\n        self.n_features_in_ = estimator.n_features_in_\n        if hasattr(estimator, \"feature_names_in_\"):\n            self.feature_names_in_ = estimator.feature_names_in_\n\n        return self",
                        "filename": "sklearn/linear_model/_ridge.py",
                        "start_index": 76333,
                        "end_index": 78133,
                        "start_line": 2188,
                        "end_line": 2579,
                        "max_line": 2589,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the logic that checks the 'store_cv_values' parameter and raises a ValueError if 'cv' is not None, which is part of the issue."
                },
                {
                    "chunk": {
                        "code": "\"\"\"Ridge classifier with built-in cross-validation.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    By default, it performs Leave-One-Out Cross-Validation. Currently,\n    only the n_features > n_samples case is handled efficiently.\n\n    Read more in the :ref:`User Guide <ridge_regression>`.\n\n    Parameters\n    ----------\n    alphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n        Array of alpha values to try.\n        Regularization strength; must be a positive float. Regularization\n        improves the conditioning of the problem and reduces the variance of\n        the estimates. Larger values specify stronger regularization.\n        Alpha corresponds to ``1 / (2C)`` in other linear models such as\n        :class:`~sklearn.linear_model.LogisticRegression` or\n        :class:`~sklearn.svm.LinearSVC`.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    scoring : str, callable, default=None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the efficient Leave-One-Out cross-validation\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n    class_weight : dict or 'balanced', default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n    store_cv_values : bool, default=False\n        Flag indicating if the cross-validation values corresponding to\n        each alpha should be stored in the ``cv_values_`` attribute (see\n        below). This flag is only compatible with ``cv=None`` (i.e. using\n        Leave-One-Out Cross-Validation).\n\n    Attributes\n    ----------\n    cv_values_ : ndarray of shape (n_samples, n_targets, n_alphas), optional\n        Cross-validation values for each alpha (only if ``store_cv_values=True`` and\n        ``cv=None``). After ``fit()`` has been called, this attribute will\n        contain the mean squared errors if `scoring is None` otherwise it\n        will contain standardized per point prediction values.\n\n    coef_ : ndarray of shape (1, n_features) or (n_targets, n_features)\n        Coefficient of the features in the decision function.\n\n        ``coef_`` is of shape (1, n_features) when the given problem is binary.\n\n    intercept_ : float or ndarray of shape (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    alpha_ : float\n        Estimated regularization parameter.\n\n    best_score_ : float\n        Score of base estimator with best alpha.\n\n        .. versionadded:: 0.23\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    Ridge : Ridge regression.\n    RidgeClassifier : Ridge classifier.\n    RidgeCV : Ridge regression with built-in cross validation.\n\n    Notes\n    -----\n    For multi-class classification, n_class classifiers are trained in\n    a one-versus-all approach. Concretely, this is implemented by taking\n    advantage of the multi-variate response support in Ridge.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_breast_cancer\n    >>> from sklearn.linear_model import RidgeClassifierCV\n    >>> X, y = load_breast_cancer(return_X_y=True)\n    >>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n    >>> clf.score(X, y)\n    0.9630...\n    \"\"\"",
                        "filename": "sklearn/linear_model/_ridge.py",
                        "start_index": 84931,
                        "end_index": 89459,
                        "start_line": 2401,
                        "end_line": 2566,
                        "max_line": 2589,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet provides the documentation for the RidgeClassifierCV class, which incorrectly states that 'store_cv_values' is a valid parameter."
                },
                {
                    "chunk": {
                        "code": "shape (n_samples, n_targets, n_alphas), optional\n        Cross-validation values for each alpha (only available if\n        ``store_cv_values=True`` and ``cv=None``). After ``fit()`` has been\n        called, this attribute will contain the mean squared errors if\n        `scoring is None` otherwise it will contain standardized per point\n        prediction values.\n\n    coef_ : ndarray of shape (n_features) or (n_targets, n_features)\n        Weight vector(s).\n\n    intercept_ : float or ndarray of shape (n_targets,)\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    alpha_ : float or ndarray of shape (n_targets,)\n        Estimated regularization parameter, or, if ``alpha_per_target=True``,\n        the estimated regularization parameter for each target.\n\n    best_score_ : float or ndarray of shape (n_targets,)\n        Score of base estimator with best alpha, or, if\n        ``alpha_per_target=True``, a score for each target.\n\n        .. versionadded:: 0.23\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    Ridge : Ridge regression.\n    RidgeClassifier : Classifier based on ridge regression on {-1, 1} labels.\n    RidgeClassifierCV : Ridge classifier with built-in cross validation.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_diabetes\n    >>> from sklearn.linear_model import RidgeCV\n    >>> X, y = load_diabetes(return_X_y=True)\n    >>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n    >>> clf.score(X, y)\n    0.5166...\n    \"\"\"",
                        "filename": "sklearn/linear_model/_ridge.py",
                        "start_index": 81834,
                        "end_index": 83660,
                        "start_line": 2315,
                        "end_line": 2566,
                        "max_line": 2589,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes the documentation for the 'cv_values_' attribute, which mentions 'store_cv_values' as a condition for its availability."
                }
            ]
        },
        "solution_task": "- The `store_cv_values` parameter is not supported in the `RidgeClassifierCV` class as of Scikit-Learn version 0.19.1.\n- The `store_cv_values` parameter is only relevant for the `RidgeCV` class, which is for regression, not classification.\n- To resolve the issue, remove the `store_cv_values` parameter from the `RidgeClassifierCV` constructor and any related method calls.\n- If the user needs to store cross-validation values, they will need to manually implement cross-validation using `RidgeClassifier` and store the values themselves, or upgrade to a newer version of Scikit-Learn if it supports this feature for `RidgeClassifierCV`.",
        "bouncer_response": {
            "score": 8,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE correctly identifies that the 'store_cv_values' parameter is not supported in the 'RidgeClassifierCV' class as of the specified Scikit-Learn version, which is the core issue described in the PROMPT. It provides actionable advice by suggesting the removal of the 'store_cv_values' parameter from the constructor and any related method calls. Additionally, it offers an alternative solution by suggesting manual implementation of cross-validation or upgrading Scikit-Learn if a newer version supports the feature. The RESPONSE also includes links to code snippets that are relevant to the issue, which could be useful for someone looking to understand the problem or contribute to a fix. However, the RESPONSE does not address the discrepancy in the documentation that claims the 'store_cv_values' parameter is possible, which is a part of the PROMPT's issue. Therefore, the RESPONSE is not fully comprehensive in addressing all aspects of the bug.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711742732.919564,
        "result": "- The `store_cv_values` parameter is not supported in the `RidgeClassifierCV` class as of Scikit-Learn version 0.19.1.\n- The `store_cv_values` parameter is only relevant for the `RidgeCV` class, which is for regression, not classification.\n- To resolve the issue, remove the `store_cv_values` parameter from the `RidgeClassifierCV` constructor and any related method calls.\n- If the user needs to store cross-validation values, they will need to manually implement cross-validation using `RidgeClassifier` and store the values themselves, or upgrade to a newer version of Scikit-Learn if it supports this feature for `RidgeClassifierCV`.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/linear_model)- [Lines 796 - 2589](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_ridge.py#L796-L2589)\n\n This snippet includes the __init__ method of RidgeClassifierCV where the 'store_cv_values' parameter is defined, which is directly related to the issue. \n\n\n   - [Lines 2188 - 2579](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_ridge.py#L2188-L2579)\n\n This snippet contains the logic that checks the 'store_cv_values' parameter and raises a ValueError if 'cv' is not None, which is part of the issue. \n\n\n   - [Lines 2401 - 2566](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_ridge.py#L2401-L2566)\n\n This snippet provides the documentation for the RidgeClassifierCV class, which incorrectly states that 'store_cv_values' is a valid parameter. \n\n\n   - [Lines 2315 - 2566](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_ridge.py#L2315-L2566)\n\n This snippet includes the documentation for the 'cv_values_' attribute, which mentions 'store_cv_values' as a condition for its availability."
    }
}