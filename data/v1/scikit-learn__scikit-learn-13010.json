{
    "id": "scikit-learn__scikit-learn-13010",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
    "task_id": 4021812658256913236,
    "title": "StandardScaler fit overflows on float16",
    "body": "#### Description\r\n\r\nWhen using StandardScaler on a large float16 numpy array the mean and std calculation overflows. I can convert the array to a larger precision but when working with a larger dataset the memory saved by using float16 on smaller numbers kind of matter. The error is mostly on numpy. Adding the dtype on the mean/std calculation does it but I'm not sure if that how people here would like to do it.\r\n\r\n#### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.preprocessing import StandardScaler\r\n\r\nsample = np.full([10_000_000, 1], 10.0, dtype=np.float16)\r\nStandardScaler().fit_transform(sample)\r\n```\r\n\r\n#### Expected Results\r\n\r\nThe normalized array\r\n\r\n#### Actual Results\r\n\r\n```\r\n/opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py:36: RuntimeWarning: overflow encountered in reduce\r\n  return umr_sum(a, axis, dtype, out, keepdims, initial)\r\n/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\r\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n/opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py:36: RuntimeWarning: overflow encountered in reduce\r\n  return umr_sum(a, axis, dtype, out, keepdims, initial)\r\n/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:765: RuntimeWarning: invalid value encountered in true_divide\r\n  X /= self.scale_\r\n\r\narray([[nan],\r\n       [nan],\r\n       [nan],\r\n       ...,\r\n       [nan],\r\n       [nan],\r\n       [nan]], dtype=float16)\r\n```\r\n\r\n#### Versions\r\n\r\n```\r\nSystem:\r\n    python: 3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16)  [GCC 7.3.0]\r\nexecutable: /opt/conda/bin/python\r\n   machine: Linux-4.9.0-5-amd64-x86_64-with-debian-9.4\r\n\r\nBLAS:\r\n    macros: SCIPY_MKL_H=None, HAVE_CBLAS=None\r\n  lib_dirs: /opt/conda/lib\r\ncblas_libs: mkl_rt, pthread\r\n\r\nPython deps:\r\n       pip: 18.1\r\nsetuptools: 39.1.0\r\n   sklearn: 0.20.2\r\n     numpy: 1.16.0\r\n     scipy: 1.1.0\r\n    Cython: 0.29.2\r\n    pandas: 0.23.4\r\n```\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.21.git",
    "requested_commit": "",
    "mayil_response": "- Change the data type used in `StandardScaler` to a higher precision such as `np.float32` or `np.float64` before computing the mean and standard deviation to prevent overflow and invalid value errors.\n- Ensure that after scaling, the data is cast back to `np.float16` if memory usage is a concern.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [preprocessing](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/preprocessing)- [Lines 842 - 898](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/preprocessing/_data.py#L842-L898)\n\n This snippet contains the implementation of the online computation of mean and std which is relevant to the overflow issue when fitting the StandardScaler on large float16 arrays. \n\n\n   - [Lines 658 - 3517](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/preprocessing/_data.py#L658-L3517)\n\n This snippet provides the detailed documentation and implementation of the StandardScaler class, including the calculation of the standard score which is directly related to the issue. \n\n\n   - [Lines 657 - 657](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/preprocessing/_data.py#L657-L657)\n\n This snippet shows the class definition of StandardScaler, which is the class where the issue is occurring.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0004185,
        "snippet_processor": 0.08203,
        "issue_star_creation": 0.032729999999999995,
        "issue_star_solver": 0.07929000000000001,
        "bouncer": 0.030320000000000003
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711741138.329422,
        "relevant_snippets": [
            {
                "code": "class StandardScaler(OneToOneFeatureMixin, TransformerMixin, BaseEstimator):",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 21278,
                "end_index": 21354,
                "start_line": 657,
                "end_line": 657,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n=============================\nImportance of Feature Scaling\n=============================\n\nFeature scaling through standardization, also called Z-score normalization, is\nan important preprocessing step for many machine learning algorithms. It\ninvolves rescaling each feature such that it has a standard deviation of 1 and a\nmean of 0.\n\nEven if tree based models are (almost) not affected by scaling, many other\nalgorithms require features to be normalized, often for different reasons: to\nease the convergence (such as a non-penalized logistic regression), to create a\ncompletely different model fit compared to the fit with unscaled data (such as\nKNeighbors models). The latter is demoed on the first part of the present\nexample.\n\nOn the second part of the example we show how Principle Component Analysis (PCA)\nis impacted by normalization of features. To illustrate this, we compare the\nprincipal components found using :class:`~sklearn.decomposition.PCA` on unscaled\ndata with those obatined when using a\n:class:`~sklearn.preprocessing.StandardScaler` to scale data first.\n\nIn the last part of the example we show the effect of the normalization on the\naccuracy of a model trained on PCA-reduced data.\n\n\"\"\"\n\n# Author: Tyler Lanigan <tylerlanigan@gmail.com>\n#         Sebastian Raschka <mail@sebastianraschka.com>\n#         Arturo Amor <david-arturo.amor-quiroz@inria.fr>\n# License: BSD 3 clause\n\n# %%\n# Load and prepare data\n# =====================\n#\n# The dataset used is the :ref:`wine_dataset` available at UCI. This dataset has\n# continuous features that are heterogeneous in scale due to differing\n# properties that they measure (e.g. alcohol content and malic acid).\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX, y = load_wine(return_X_y=True, as_frame=True)\nscaler = StandardScaler().set_output(transform=\"pandas\")\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=42\n)\nscaled_X_train = scaler.fit_transform(X_train)\n\n# %%\n# Effect of rescaling on a k-neighbors models\n# ===========================================\n#\n# For the sake of visualizing the decision boundary of a\n# :class:`~sklearn.neighbors.KNeighborsClassifier`, in this section we select a\n# subset of 2 features that have values with different orders of magnitude.\n#\n# Keep in mind that using a subset of the features to train the model may likely\n# leave out feature with high predictive impact, resulting in a decision\n# boundary that is much worse in comparison to a model trained on the full set\n# of features.\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX_plot = X[[\"proline\", \"hue\"]]\nX_plot_scaled = scaler.fit_transform(X_plot)\nclf = KNeighborsClassifier(n_neighbors=20)",
                "filename": "examples/preprocessing/plot_scaling_importance.py",
                "start_index": 0,
                "end_index": 2897,
                "start_line": 1,
                "end_line": 74,
                "max_line": 256,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "X = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=[\"a\", \"b\", \"c\"])\nscalar = StandardScaler().fit(X)\nscalar.feature_names_in_\n\n# %%\n# The support of :term:`get_feature_names_out` is available for transformers\n# that already had `get_feature_names` and transformers with a one-to-one\n# correspondence between input and output such as\n# :class:`~preprocessing.StandardScaler`. :term:`get_feature_names_out` support\n# will be added to all other transformers in future releases. Additionally,\n# :meth:`compose.ColumnTransformer.get_feature_names_out` is available to\n# combine feature names of its transformers:\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\n\nX = pd.DataFrame({\"pet\": [\"dog\", \"cat\", \"fish\"], \"age\": [3, 7, 1]})\npreprocessor = ColumnTransformer(\n    [\n        (\"numerical\", StandardScaler(), [\"age\"]),\n        (\"categorical\", OneHotEncoder(), [\"pet\"]),\n    ],\n    verbose_feature_names_out=False,\n).fit(X)\n\npreprocessor.get_feature_names_out()\n\n# %%\n# When this ``preprocessor`` is used with a pipeline, the feature names used\n# by the classifier are obtained by slicing and calling\n# :term:`get_feature_names_out`:\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\ny = [1, 0, 1]\npipe = make_pipeline(preprocessor, LogisticRegression())\npipe.fit(X, y)\npipe[:-1].get_feature_names_out()\n\n\n##############################################################################\n# A more flexible plotting API\n# --------------------------------------------------------------------------\n# :class:`metrics.ConfusionMatrixDisplay`,\n# :class:`metrics.PrecisionRecallDisplay`, :class:`metrics.DetCurveDisplay`,\n# and :class:`inspection.PartialDependenceDisplay` now expose two class\n# methods: `from_estimator` and `from_predictions` which allow users to create\n# a plot given the predictions or an estimator. This means the corresponding\n# `plot_*` functions are deprecated. Please check :ref:`example one\n# <sphx_glr_auto_examples_model_selection_plot_confusion_matrix.py>` and\n# :ref:`example two\n# <sphx_glr_auto_examples_classification_plot_digits_classification.py>` for\n# how to use the new plotting functionalities.\n\n##############################################################################\n# Online One-Class SVM\n# --------------------------------------------------------------------------\n# The new class :class:`~linear_model.SGDOneClassSVM` implements an online\n# linear version of the One-Class SVM using a stochastic gradient descent.\n# Combined with kernel approximation techniques,\n# :class:`~linear_model.SGDOneClassSVM` can be used to approximate the solution\n# of a kernelized One-Class SVM, implemented in :class:`~svm.OneClassSVM`, with\n# a fit time complexity linear in the number of samples. Note that the\n# complexity of a kernelized One-Class SVM is at best quadratic in the number\n# of samples. :class:`~linear_model.SGDOneClassSVM` is thus well suited for",
                "filename": "examples/release_highlights/plot_release_highlights_1_0_0.py",
                "start_index": 5998,
                "end_index": 8992,
                "start_line": 151,
                "end_line": 215,
                "max_line": 241,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"Online computation of mean and std on X for later scaling.\n\n        All of X is processed as a single batch. This is intended for cases\n        when :meth:`fit` is not feasible due to very large number of\n        `n_samples` or because X is read from a continuous stream.\n\n        The algorithm for incremental mean and std is given in Equation 1.5a,b\n        in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n        for computing the sample variance: Analysis and recommendations.\"\n        The American Statistician 37.3 (1983): 242-247:\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data used to compute the mean and standard deviation\n            used for later scaling along the features axis.\n\n        y : None\n            Ignored.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Individual weights for each sample.\n\n            .. versionadded:: 0.24\n               parameter *sample_weight* support to StandardScaler.\n\n        Returns\n        -------\n        self : object\n            Fitted scaler.\n        \"\"\"\n        first_call = not hasattr(self, \"n_samples_seen_\")\n        X = self._validate_data(\n            X,\n            accept_sparse=(\"csr\", \"csc\"),\n            dtype=FLOAT_DTYPES,\n            force_all_finite=\"allow-nan\",\n            reset=first_call,\n        )\n        n_features = X.shape[1]\n\n        if sample_weight is not None:\n            sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        # Even in the case of `with_mean=False`, we update the mean anyway\n        # This is needed for the incremental computation of the var\n        # See incr_mean_variance_axis and _incremental_mean_variance_axis\n\n        # if n_samples_seen_ is an integer (i.e. no missing values), we need to\n        # transform it to a NumPy array of shape (n_features,) required by\n        # incr_mean_variance_axis and _incremental_variance_axis\n        dtype = np.int64 if sample_weight is None else X.dtype\n        if not hasattr(self, \"n_samples_seen_\"):\n            self.n_samples_seen_ = np.zeros(n_features, dtype=dtype)\n        elif np.size(self.n_samples_seen_) == 1:\n            self.n_samples_seen_ = np.repeat(self.n_samples_seen_, X.shape[1])\n            self.n_samples_seen_ = self.n_samples_seen_.astype(dtype, copy=False)",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 28197,
                "end_index": 30601,
                "start_line": 842,
                "end_line": 898,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "# compacted to a specific range, [0, 10] for the median income and [0, 6] for\n# the average house occupancy. Note that there are some marginal outliers (some\n# blocks have average occupancy of more than 1200). Therefore, a specific\n# pre-processing can be very beneficial depending of the application. In the\n# following, we present some insights and behaviors of those pre-processing\n# methods in the presence of marginal outliers.\n\nmake_plot(0)\n\n# %%\n# StandardScaler\n# --------------\n#\n# :class:`~sklearn.preprocessing.StandardScaler` removes the mean and scales\n# the data to unit variance. The scaling shrinks the range of the feature\n# values as shown in the left figure below.\n# However, the outliers have an influence when computing the empirical mean and\n# standard deviation. Note in particular that because the outliers on each\n# feature have different magnitudes, the spread of the transformed data on\n# each feature is very different: most of the data lie in the [-2, 4] range for\n# the transformed median income feature while the same data is squeezed in the\n# smaller [-0.2, 0.2] range for the transformed average house occupancy.\n#\n# :class:`~sklearn.preprocessing.StandardScaler` therefore cannot guarantee\n# balanced feature scales in the\n# presence of outliers.\n\nmake_plot(1)\n\n# %%\n# MinMaxScaler\n# ------------\n#\n# :class:`~sklearn.preprocessing.MinMaxScaler` rescales the data set such that\n# all feature values are in\n# the range [0, 1] as shown in the right panel below. However, this scaling\n# compresses all inliers into the narrow range [0, 0.005] for the transformed\n# average house occupancy.\n#\n# Both :class:`~sklearn.preprocessing.StandardScaler` and\n# :class:`~sklearn.preprocessing.MinMaxScaler` are very sensitive to the\n# presence of outliers.\n\nmake_plot(2)\n\n# %%\n# MaxAbsScaler\n# ------------\n#\n# :class:`~sklearn.preprocessing.MaxAbsScaler` is similar to\n# :class:`~sklearn.preprocessing.MinMaxScaler` except that the\n# values are mapped across several ranges depending on whether negative\n# OR positive values are present. If only positive values are present, the\n# range is [0, 1]. If only negative values are present, the range is [-1, 0].\n# If both negative and positive values are present, the range is [-1, 1].\n# On positive only data, both :class:`~sklearn.preprocessing.MinMaxScaler`\n# and :class:`~sklearn.preprocessing.MaxAbsScaler` behave similarly.\n# :class:`~sklearn.preprocessing.MaxAbsScaler` therefore also suffers from\n# the presence of large outliers.\n\nmake_plot(3)\n\n# %%\n# RobustScaler\n# ------------\n#\n# Unlike the previous scalers, the centering and scaling statistics of\n# :class:`~sklearn.preprocessing.RobustScaler`\n# are based on percentiles and are therefore not influenced by a small\n# number of very large marginal outliers. Consequently, the resulting range of\n# the transformed feature values is larger than for the previous scalers and,\n# more importantly, are approximately similar: for both features most of the",
                "filename": "examples/preprocessing/plot_all_scaling.py",
                "start_index": 8518,
                "end_index": 11498,
                "start_line": 258,
                "end_line": 329,
                "max_line": 400,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n=============================================================\nCompare the effect of different scalers on data with outliers\n=============================================================\n\nFeature 0 (median income in a block) and feature 5 (average house occupancy) of\nthe :ref:`california_housing_dataset` have very\ndifferent scales and contain some very large outliers. These two\ncharacteristics lead to difficulties to visualize the data and, more\nimportantly, they can degrade the predictive performance of many machine\nlearning algorithms. Unscaled data can also slow down or even prevent the\nconvergence of many gradient-based estimators.\n\nIndeed many estimators are designed with the assumption that each feature takes\nvalues close to zero or more importantly that all features vary on comparable\nscales. In particular, metric-based and gradient-based estimators often assume\napproximately standardized data (centered features with unit variances). A\nnotable exception are decision tree-based estimators that are robust to\narbitrary scaling of the data.\n\nThis example uses different scalers, transformers, and normalizers to bring the\ndata within a pre-defined range.\n\nScalers are linear (or more precisely affine) transformers and differ from each\nother in the way they estimate the parameters used to shift and scale each\nfeature.\n\n:class:`~sklearn.preprocessing.QuantileTransformer` provides non-linear\ntransformations in which distances\nbetween marginal outliers and inliers are shrunk.\n:class:`~sklearn.preprocessing.PowerTransformer` provides\nnon-linear transformations in which data is mapped to a normal distribution to\nstabilize variance and minimize skewness.\n\nUnlike the previous transformations, normalization refers to a per sample\ntransformation instead of a per feature transformation.\n\nThe following code is a bit verbose, feel free to jump directly to the analysis\nof the results_.\n\n\"\"\"\n\n# Author:  Raghav RV <rvraghav93@gmail.com>\n#          Guillaume Lemaitre <g.lemaitre58@gmail.com>\n#          Thomas Unterthiner\n# License: BSD 3 clause\n\nimport matplotlib as mpl\nimport numpy as np\nfrom matplotlib import cm\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import (\n    MaxAbsScaler,\n    MinMaxScaler,\n    Normalizer,\n    PowerTransformer,\n    QuantileTransformer,\n    RobustScaler,\n    StandardScaler,\n    minmax_scale,\n)\n\ndataset = fetch_california_housing()\nX_full, y_full = dataset.data, dataset.target\nfeature_names = dataset.feature_names\n\nfeature_mapping = {\n    \"MedInc\": \"Median income in block\",\n    \"HouseAge\": \"Median house age in block\",\n    \"AveRooms\": \"Average number of rooms\",\n    \"AveBedrms\": \"Average number of bedrooms\",\n    \"Population\": \"Block population\",\n    \"AveOccup\": \"Average house occupancy\",\n    \"Latitude\": \"House block latitude\",\n    \"Longitude\": \"House block longitude\",\n}\n\n# Take only 2 features to make visualization easier",
                "filename": "examples/preprocessing/plot_all_scaling.py",
                "start_index": 0,
                "end_index": 2958,
                "start_line": 1,
                "end_line": 80,
                "max_line": 400,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "class RobustScaler(OneToOneFeatureMixin, TransformerMixin, BaseEstimator):",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 46115,
                "end_index": 46189,
                "start_line": 1381,
                "end_line": 1381,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"Standardize features by removing the mean and scaling to unit variance.\n\n    The standard score of a sample `x` is calculated as:\n\n        z = (x - u) / s\n\n    where `u` is the mean of the training samples or zero if `with_mean=False`,\n    and `s` is the standard deviation of the training samples or one if\n    `with_std=False`.\n\n    Centering and scaling happen independently on each feature by computing\n    the relevant statistics on the samples in the training set. Mean and\n    standard deviation are then stored to be used on later data using\n    :meth:`transform`.\n\n    Standardization of a dataset is a common requirement for many\n    machine learning estimators: they might behave badly if the\n    individual features do not more or less look like standard normally\n    distributed data (e.g. Gaussian with 0 mean and unit variance).\n\n    For instance many elements used in the objective function of\n    a learning algorithm (such as the RBF kernel of Support Vector\n    Machines or the L1 and L2 regularizers of linear models) assume that\n    all features are centered around 0 and have variance in the same\n    order. If a feature has a variance that is orders of magnitude larger\n    than others, it might dominate the objective function and make the\n    estimator unable to learn from other features correctly as expected.\n\n    This scaler can also be applied to sparse CSR or CSC matrices by passing\n    `with_mean=False` to avoid breaking the sparsity structure of the data.\n\n    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n\n    Parameters\n    ----------\n    copy : bool, default=True\n        If False, try to avoid a copy and do inplace scaling instead.\n        This is not guaranteed to always work inplace; e.g. if the data is\n        not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n        returned.\n\n    with_mean : bool, default=True\n        If True, center the data before scaling.\n        This does not work (and will raise an exception) when attempted on\n        sparse matrices, because centering them entails building a dense\n        matrix which in common use cases is likely to be too large to fit in\n        memory.\n\n    with_std : bool, default=True\n        If True, scale the data to unit variance (or equivalently,\n        unit standard deviation).\n\n    Attributes\n    ----------\n    scale_ : ndarray of shape (n_features,) or None\n        Per feature relative scaling of the data to achieve zero mean and unit\n        variance. Generally this is calculated using `np.sqrt(var_)`. If a\n        variance is zero, we can't achieve unit variance, and the data is left\n        as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n        when `with_std=False`.\n\n        .. versionadded:: 0.17\n           *scale_*\n\n    mean_ : ndarray of shape (n_features,) or None\n        The mean value for each feature in the training set.\n        Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.\n\n    var_ : ndarray of shape (n_features,) or None\n        The variance for each feature in the training set. Used to compute\n        `scale_`. Equal to ``None`` when ``with_mean=False`` and\n        ``with_std=False``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_seen_ : int or ndarray of shape (n_features,)\n        The number of samples processed by the estimator for each feature.\n        If there are no missing samples, the ``n_samples_seen`` will be an\n        integer, otherwise it will be an array of dtype int. If\n        `sample_weights` are used it will be a float (if no missing data)\n        or an array of dtype float that sums the weights seen so far.\n        Will be reset on new calls to fit, but increments across\n        ``partial_fit`` calls.\n\n    See Also\n    --------\n    scale : Equivalent function without the estimator API.\n\n    :class:`~sklearn.decomposition.PCA` : Further removes the linear\n        correlation across features with 'whiten=True'.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in fit, and maintained in\n    transform.\n\n    We use a biased estimator for the standard deviation, equivalent to\n    `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n    affect model performance.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n    >>> scaler = StandardScaler()\n    >>> print(scaler.fit(data))\n    StandardScaler()\n    >>> print(scaler.mean_)\n    [0.5 0.5]\n    >>> print(scaler.transform(data))\n    [[-1. -1.]\n     [-1. -1.]\n     [ 1.  1.]\n     [ 1.  1.]]\n    >>> print(scaler.transform([[2, 2]]))\n    [[3. 3.]]\n    \"\"\"",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 21359,
                "end_index": 26522,
                "start_line": 658,
                "end_line": 3517,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "class MinMaxScaler(OneToOneFeatureMixin, TransformerMixin, BaseEstimator):",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 9738,
                "end_index": 9812,
                "start_line": 277,
                "end_line": 277,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "def check_sample_weights_not_overwritten(name, estimator_orig):\n    # check that estimators don't override the passed sample_weight parameter\n    estimator = clone(estimator_orig)\n    set_random_state(estimator, random_state=0)\n\n    X = np.array(\n        [\n            [1, 3],\n            [1, 3],\n            [1, 3],\n            [1, 3],\n            [2, 1],\n            [2, 1],\n            [2, 1],\n            [2, 1],\n            [3, 3],\n            [3, 3],\n            [3, 3],\n            [3, 3],\n            [4, 1],\n            [4, 1],\n            [4, 1],\n            [4, 1],\n        ],\n        dtype=np.float64,\n    )\n    y = np.array([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2], dtype=int)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    sample_weight_original = np.ones(y.shape[0])\n    sample_weight_original[0] = 10.0\n\n    sample_weight_fit = sample_weight_original.copy()\n\n    estimator.fit(X, y, sample_weight=sample_weight_fit)\n\n    err_msg = f\"{name} overwrote the original `sample_weight` given during fit\"\n    assert_allclose(sample_weight_fit, sample_weight_original, err_msg=err_msg)\n\n\n@ignore_warnings(category=(FutureWarning, UserWarning))\ndef check_dtype_object(name, estimator_orig):\n    # check that estimators treat dtype object as numeric if possible\n    rng = np.random.RandomState(0)\n    X = _enforce_estimator_tags_X(estimator_orig, rng.uniform(size=(40, 10)))\n    X = X.astype(object)\n    tags = _safe_tags(estimator_orig)\n    y = (X[:, 0] * 4).astype(int)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    estimator.fit(X, y)\n    if hasattr(estimator, \"predict\"):\n        estimator.predict(X)\n\n    if hasattr(estimator, \"transform\"):\n        estimator.transform(X)\n\n    with raises(Exception, match=\"Unknown label type\", may_pass=True):\n        estimator.fit(X, y.astype(object))\n\n    if \"string\" not in tags[\"X_types\"]:\n        X[0, 0] = {\"foo\": \"bar\"}\n        msg = \"argument must be a string.* number\"\n        with raises(TypeError, match=msg):\n            estimator.fit(X, y)\n    else:\n        # Estimators supporting string will not call np.asarray to convert the\n        # data to numeric and therefore, the error will not be raised.\n        # Checking for each element dtype in the input array will be costly.\n        # Refer to #11401 for full discussion.\n        estimator.fit(X, y)\n\n\ndef check_complex_data(name, estimator_orig):\n    rng = np.random.RandomState(42)\n    # check that estimators raise an exception on providing complex data\n    X = rng.uniform(size=10) + 1j * rng.uniform(size=10)\n    X = X.reshape(-1, 1)\n\n    # Something both valid for classification and regression\n    y = rng.randint(low=0, high=2, size=10) + 1j\n    estimator = clone(estimator_orig)\n    set_random_state(estimator, random_state=0)\n    with raises(ValueError, match=\"Complex data not supported\"):\n        estimator.fit(X, y)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 44629,
                "end_index": 47525,
                "start_line": 1290,
                "end_line": 4110,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/preprocessing/_data.py": [
                {
                    "chunk": {
                        "code": "\"\"\"Online computation of mean and std on X for later scaling.\n\n        All of X is processed as a single batch. This is intended for cases\n        when :meth:`fit` is not feasible due to very large number of\n        `n_samples` or because X is read from a continuous stream.\n\n        The algorithm for incremental mean and std is given in Equation 1.5a,b\n        in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n        for computing the sample variance: Analysis and recommendations.\"\n        The American Statistician 37.3 (1983): 242-247:\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            The data used to compute the mean and standard deviation\n            used for later scaling along the features axis.\n\n        y : None\n            Ignored.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Individual weights for each sample.\n\n            .. versionadded:: 0.24\n               parameter *sample_weight* support to StandardScaler.\n\n        Returns\n        -------\n        self : object\n            Fitted scaler.\n        \"\"\"\n        first_call = not hasattr(self, \"n_samples_seen_\")\n        X = self._validate_data(\n            X,\n            accept_sparse=(\"csr\", \"csc\"),\n            dtype=FLOAT_DTYPES,\n            force_all_finite=\"allow-nan\",\n            reset=first_call,\n        )\n        n_features = X.shape[1]\n\n        if sample_weight is not None:\n            sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        # Even in the case of `with_mean=False`, we update the mean anyway\n        # This is needed for the incremental computation of the var\n        # See incr_mean_variance_axis and _incremental_mean_variance_axis\n\n        # if n_samples_seen_ is an integer (i.e. no missing values), we need to\n        # transform it to a NumPy array of shape (n_features,) required by\n        # incr_mean_variance_axis and _incremental_variance_axis\n        dtype = np.int64 if sample_weight is None else X.dtype\n        if not hasattr(self, \"n_samples_seen_\"):\n            self.n_samples_seen_ = np.zeros(n_features, dtype=dtype)\n        elif np.size(self.n_samples_seen_) == 1:\n            self.n_samples_seen_ = np.repeat(self.n_samples_seen_, X.shape[1])\n            self.n_samples_seen_ = self.n_samples_seen_.astype(dtype, copy=False)",
                        "filename": "sklearn/preprocessing/_data.py",
                        "start_index": 28197,
                        "end_index": 30601,
                        "start_line": 842,
                        "end_line": 898,
                        "max_line": 3519,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the implementation of the online computation of mean and std which is relevant to the overflow issue when fitting the StandardScaler on large float16 arrays."
                },
                {
                    "chunk": {
                        "code": "\"\"\"Standardize features by removing the mean and scaling to unit variance.\n\n    The standard score of a sample `x` is calculated as:\n\n        z = (x - u) / s\n\n    where `u` is the mean of the training samples or zero if `with_mean=False`,\n    and `s` is the standard deviation of the training samples or one if\n    `with_std=False`.\n\n    Centering and scaling happen independently on each feature by computing\n    the relevant statistics on the samples in the training set. Mean and\n    standard deviation are then stored to be used on later data using\n    :meth:`transform`.\n\n    Standardization of a dataset is a common requirement for many\n    machine learning estimators: they might behave badly if the\n    individual features do not more or less look like standard normally\n    distributed data (e.g. Gaussian with 0 mean and unit variance).\n\n    For instance many elements used in the objective function of\n    a learning algorithm (such as the RBF kernel of Support Vector\n    Machines or the L1 and L2 regularizers of linear models) assume that\n    all features are centered around 0 and have variance in the same\n    order. If a feature has a variance that is orders of magnitude larger\n    than others, it might dominate the objective function and make the\n    estimator unable to learn from other features correctly as expected.\n\n    This scaler can also be applied to sparse CSR or CSC matrices by passing\n    `with_mean=False` to avoid breaking the sparsity structure of the data.\n\n    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n\n    Parameters\n    ----------\n    copy : bool, default=True\n        If False, try to avoid a copy and do inplace scaling instead.\n        This is not guaranteed to always work inplace; e.g. if the data is\n        not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n        returned.\n\n    with_mean : bool, default=True\n        If True, center the data before scaling.\n        This does not work (and will raise an exception) when attempted on\n        sparse matrices, because centering them entails building a dense\n        matrix which in common use cases is likely to be too large to fit in\n        memory.\n\n    with_std : bool, default=True\n        If True, scale the data to unit variance (or equivalently,\n        unit standard deviation).\n\n    Attributes\n    ----------\n    scale_ : ndarray of shape (n_features,) or None\n        Per feature relative scaling of the data to achieve zero mean and unit\n        variance. Generally this is calculated using `np.sqrt(var_)`. If a\n        variance is zero, we can't achieve unit variance, and the data is left\n        as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n        when `with_std=False`.\n\n        .. versionadded:: 0.17\n           *scale_*\n\n    mean_ : ndarray of shape (n_features,) or None\n        The mean value for each feature in the training set.\n        Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.\n\n    var_ : ndarray of shape (n_features,) or None\n        The variance for each feature in the training set. Used to compute\n        `scale_`. Equal to ``None`` when ``with_mean=False`` and\n        ``with_std=False``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_samples_seen_ : int or ndarray of shape (n_features,)\n        The number of samples processed by the estimator for each feature.\n        If there are no missing samples, the ``n_samples_seen`` will be an\n        integer, otherwise it will be an array of dtype int. If\n        `sample_weights` are used it will be a float (if no missing data)\n        or an array of dtype float that sums the weights seen so far.\n        Will be reset on new calls to fit, but increments across\n        ``partial_fit`` calls.\n\n    See Also\n    --------\n    scale : Equivalent function without the estimator API.\n\n    :class:`~sklearn.decomposition.PCA` : Further removes the linear\n        correlation across features with 'whiten=True'.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in fit, and maintained in\n    transform.\n\n    We use a biased estimator for the standard deviation, equivalent to\n    `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n    affect model performance.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\n    Examples\n    --------\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n    >>> scaler = StandardScaler()\n    >>> print(scaler.fit(data))\n    StandardScaler()\n    >>> print(scaler.mean_)\n    [0.5 0.5]\n    >>> print(scaler.transform(data))\n    [[-1. -1.]\n     [-1. -1.]\n     [ 1.  1.]\n     [ 1.  1.]]\n    >>> print(scaler.transform([[2, 2]]))\n    [[3. 3.]]\n    \"\"\"",
                        "filename": "sklearn/preprocessing/_data.py",
                        "start_index": 21359,
                        "end_index": 26522,
                        "start_line": 658,
                        "end_line": 3517,
                        "max_line": 3519,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet provides the detailed documentation and implementation of the StandardScaler class, including the calculation of the standard score which is directly related to the issue."
                },
                {
                    "chunk": {
                        "code": "class StandardScaler(OneToOneFeatureMixin, TransformerMixin, BaseEstimator):",
                        "filename": "sklearn/preprocessing/_data.py",
                        "start_index": 21278,
                        "end_index": 21354,
                        "start_line": 657,
                        "end_line": 657,
                        "max_line": 3519,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the class definition of StandardScaler, which is the class where the issue is occurring."
                }
            ]
        },
        "solution_task": "- Change the data type used in `StandardScaler` to a higher precision such as `np.float32` or `np.float64` before computing the mean and standard deviation to prevent overflow and invalid value errors.\n- Ensure that after scaling, the data is cast back to `np.float16` if memory usage is a concern.",
        "bouncer_response": {
            "score": 7,
            "criteria": "The RESPONSE must provide actionable, relevant guidance to more parts of the PROMPT's described issue to score higher.",
            "supporting_evidence": "The RESPONSE provides a practical solution to the problem by suggesting to change the data type to a higher precision before computing the mean and standard deviation, which is a direct response to the overflow issue described in the PROMPT. It also addresses the concern about memory usage by recommending to cast the data back to np.float16 after scaling. Additionally, the RESPONSE provides links to the relevant code snippets in the sklearn library, which could be useful for someone looking to understand or modify the implementation of StandardScaler. However, the RESPONSE could have been more comprehensive by suggesting a way to modify the StandardScaler code to handle float16 data without overflow, or by providing a workaround that can be applied without changing the data type, which would have made it a perfect solution.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711741334.8691561,
        "result": "- Change the data type used in `StandardScaler` to a higher precision such as `np.float32` or `np.float64` before computing the mean and standard deviation to prevent overflow and invalid value errors.\n- Ensure that after scaling, the data is cast back to `np.float16` if memory usage is a concern.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [preprocessing](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/preprocessing)- [Lines 842 - 898](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/preprocessing/_data.py#L842-L898)\n\n This snippet contains the implementation of the online computation of mean and std which is relevant to the overflow issue when fitting the StandardScaler on large float16 arrays. \n\n\n   - [Lines 658 - 3517](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/preprocessing/_data.py#L658-L3517)\n\n This snippet provides the detailed documentation and implementation of the StandardScaler class, including the calculation of the standard score which is directly related to the issue. \n\n\n   - [Lines 657 - 657](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/preprocessing/_data.py#L657-L657)\n\n This snippet shows the class definition of StandardScaler, which is the class where the issue is occurring."
    }
}