{
    "id": "scikit-learn__scikit-learn-12989",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
    "task_id": 7729196722116942088,
    "title": "`NMF` and `non_negative_factorization` have inconsistent default init",
    "body": "<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn\r\n- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn\r\nFor more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->\r\n`NMF` and `non_negative_factorization` have inconsistent default init. `NMF` has `init=None` while `non_negative_factorization` has `init='random'`.\r\n\r\nSee #11667 \r\n\r\nAs suggested, we could change the default in `non_negative_factorization` with a deprecation process.\r\n\r\n<!--\r\n#### Steps/Code to Reproduce\r\n\r\nExample:\r\n```python\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nfrom sklearn.decomposition import LatentDirichletAllocation\r\n\r\ndocs = [\"Help I have a bug\" for i in range(1000)]\r\n\r\nvectorizer = CountVectorizer(input=docs, analyzer='word')\r\nlda_features = vectorizer.fit_transform(docs)\r\n\r\nlda_model = LatentDirichletAllocation(\r\n    n_topics=10,\r\n    learning_method='online',\r\n    evaluate_every=10,\r\n    n_jobs=4,\r\n)\r\nmodel = lda_model.fit(lda_features)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https://gist.github.com\r\n-->\r\n\r\n<!--\r\n#### Expected Results\r\n Example: No error is thrown. Please paste or describe the expected results.-->\r\n\r\n<!--\r\n#### Actual Results\r\n Please paste or specifically describe the actual output or traceback. -->\r\n\r\n<!--\r\n#### Versions\r\n\r\nPlease run the following snippet and paste the output below.\r\nFor scikit-learn >= 0.20:\r\nimport sklearn; sklearn.show_versions()\r\nFor scikit-learn < 0.20:\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\n-->\r\n\r\n\r\n<!-- Thanks for contributing! -->\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.21.git",
    "requested_commit": "",
    "mayil_response": "- Align the default `init` parameter value of `non_negative_factorization` with `NMF` by setting it to `None`.\n- Implement a deprecation warning in `non_negative_factorization` to inform users of the upcoming change.\n- Modify the default value after a suitable deprecation cycle (typically two versions).\n- Update the `_parameter_constraints` dictionary for `non_negative_factorization` to reflect the new default after deprecation period.\n- Add a conditional check in `non_negative_factorization` to handle the `init=None` case, setting it to `'random'` or `'nndsvda'` based on the condition used in `NMF` (i.e., `'nndsvda'` if `n_components <= min(n_samples, n_features)`, otherwise `'random'`).\n- Ensure that the deprecation warning advises users to explicitly set `init='random'` if they want to preserve the current behavior without any warnings.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [decomposition](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/decomposition)- [Lines 1139 - 1212](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/decomposition/_nmf.py#L1139-L1212)\n\n This snippet shows the __init__ method of the base class for NMF, where the default value for 'init' is set to None. This is relevant as the issue describes an inconsistency with the default 'init' value between 'NMF' and 'non_negative_factorization'. \n\n\n   - [Lines 766 - 2416](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/decomposition/_nmf.py#L766-L2416)\n\n This snippet contains the documentation for the 'NMF' class, which includes the default 'init' parameter. It is relevant for understanding the expected behavior and defaults of the 'NMF' class. \n\n\n   - [Lines 1146 - 2443](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/decomposition/_nmf.py#L1146-L2443)\n\n This snippet shows the __init__ method of the 'NMF' class, where the default value for 'init' is set to None. It is relevant as it directly relates to the issue of inconsistent default 'init' values. \n\n\n   - [Lines 306 - 2361](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/decomposition/_nmf.py#L306-L2361)\n\n This snippet includes the code for handling different 'init' strategies in 'non_negative_factorization'. It is relevant to understand how the 'init' parameter is used and to ensure consistency with the 'NMF' class. \n\n\n   - [Lines 1805 - 2416](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/decomposition/_nmf.py#L1805-L2416)\n\n This snippet contains the documentation for the 'MiniBatchNMF' class, which may also be affected by the default 'init' value inconsistency. It is relevant for checking if similar changes need to be applied here.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0004185,
        "snippet_processor": 0.08203,
        "issue_star_creation": 0.032729999999999995,
        "issue_star_solver": 0.07929000000000001,
        "bouncer": 0.030320000000000003
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711741138.341467,
        "relevant_snippets": [
            {
                "code": "\"\"\"Base class for NMF and MiniBatchNMF.\"\"\"\n\n    # This prevents ``set_split_inverse_transform`` to be generated for the\n    # non-standard ``W`` arg on ``inverse_transform``.\n    # TODO: remove when W is removed in v1.5 for inverse_transform\n    __metadata_request__inverse_transform = {\"W\": metadata_routing.UNUSED}\n\n    _parameter_constraints: dict = {\n        \"n_components\": [\n            Interval(Integral, 1, None, closed=\"left\"),\n            None,\n            StrOptions({\"auto\"}),\n            Hidden(StrOptions({\"warn\"})),\n        ],\n        \"init\": [\n            StrOptions({\"random\", \"nndsvd\", \"nndsvda\", \"nndsvdar\", \"custom\"}),\n            None,\n        ],\n        \"beta_loss\": [\n            StrOptions({\"frobenius\", \"kullback-leibler\", \"itakura-saito\"}),\n            Real,\n        ],\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\n        \"max_iter\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"random_state\": [\"random_state\"],\n        \"alpha_W\": [Interval(Real, 0, None, closed=\"left\")],\n        \"alpha_H\": [Interval(Real, 0, None, closed=\"left\"), StrOptions({\"same\"})],\n        \"l1_ratio\": [Interval(Real, 0, 1, closed=\"both\")],\n        \"verbose\": [\"verbose\"],\n    }\n\n    def __init__(\n        self,\n        n_components=\"warn\",\n        *,\n        init=None,\n        beta_loss=\"frobenius\",\n        tol=1e-4,\n        max_iter=200,\n        random_state=None,\n        alpha_W=0.0,\n        alpha_H=\"same\",\n        l1_ratio=0.0,\n        verbose=0,\n    ):\n        self.n_components = n_components\n        self.init = init\n        self.beta_loss = beta_loss\n        self.tol = tol\n        self.max_iter = max_iter\n        self.random_state = random_state\n        self.alpha_W = alpha_W\n        self.alpha_H = alpha_H\n        self.l1_ratio = l1_ratio\n        self.verbose = verbose\n\n    def _check_params(self, X):\n        # n_components\n        self._n_components = self.n_components\n        if self.n_components == \"warn\":\n            warnings.warn(\n                (\n                    \"The default value of `n_components` will change from `None` to\"\n                    \" `'auto'` in 1.6. Set the value of `n_components` to `None`\"\n                    \" explicitly to supress the warning.\"\n                ),\n                FutureWarning,\n            )\n            self._n_components = None  # Keeping the old default value\n        if self._n_components is None:\n            self._n_components = X.shape[1]\n\n        # beta_loss\n        self._beta_loss = _beta_loss_to_float(self.beta_loss)",
                "filename": "sklearn/decomposition/_nmf.py",
                "start_index": 36069,
                "end_index": 38591,
                "start_line": 1139,
                "end_line": 1212,
                "max_line": 2443,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between X\n        and the dot product WH. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n        matrix X cannot contain zeros. Used only in 'mu' solver.\n\n        .. versionadded:: 0.19\n\n    tol : float, default=1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : int, default=200\n        Maximum number of iterations before timing out.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    alpha_W : float, default=0.0\n        Constant that multiplies the regularization terms of `W`. Set it to zero\n        (default) to have no regularization on `W`.\n\n        .. versionadded:: 1.0\n\n    alpha_H : float or \"same\", default=\"same\"\n        Constant that multiplies the regularization terms of `H`. Set it to zero to\n        have no regularization on `H`. If \"same\" (default), it takes the same value as\n        `alpha_W`.\n\n        .. versionadded:: 1.0\n\n    l1_ratio : float, default=0.0\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n        .. versionadded:: 0.17\n           Regularization parameter *l1_ratio* used in the Coordinate Descent\n           solver.\n\n    verbose : int, default=0\n        Whether to be verbose.\n\n    shuffle : bool, default=False\n        If true, randomize the order of coordinates in the CD solver.\n\n        .. versionadded:: 0.17\n           *shuffle* parameter used in the Coordinate Descent solver.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Factorization matrix, sometimes called 'dictionary'.\n\n    n_components_ : int\n        The number of components. It is same as the `n_components` parameter\n        if it was given. Otherwise, it will be same as the number of\n        features.\n\n    reconstruction_err_ : float\n        Frobenius norm of the matrix difference, or beta-divergence, between\n        the training data ``X`` and the reconstructed data ``WH`` from\n        the fitted model.\n\n    n_iter_ : int\n        Actual number of iterations.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    PCA : Principal component analysis.\n    SparseCoder : Find a sparse representation of data from a fixed,\n        precomputed dictionary.\n    SparsePCA : Sparse Principal Components Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import NMF\n    >>> model = NMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_\n    \"\"\"",
                "filename": "sklearn/decomposition/_nmf.py",
                "start_index": 47016,
                "end_index": 51314,
                "start_line": 766,
                "end_line": 2416,
                "max_line": 2443,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\" Non-negative matrix factorization.\n\"\"\"\n# Author: Vlad Niculae\n#         Lars Buitinck\n#         Mathieu Blondel <mathieu@mblondel.org>\n#         Tom Dupre la Tour\n# License: BSD 3 clause\n\nimport itertools\nimport time\nimport warnings\nfrom abc import ABC\nfrom math import sqrt\nfrom numbers import Integral, Real\n\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy import linalg\n\nfrom .._config import config_context\nfrom ..base import (\n    BaseEstimator,\n    ClassNamePrefixFeaturesOutMixin,\n    TransformerMixin,\n    _fit_context,\n)\nfrom ..exceptions import ConvergenceWarning\nfrom ..utils import check_array, check_random_state, gen_batches, metadata_routing\nfrom ..utils._param_validation import (\n    Hidden,\n    Interval,\n    StrOptions,\n    validate_params,\n)\nfrom ..utils.extmath import randomized_svd, safe_sparse_dot, squared_norm\nfrom ..utils.validation import (\n    check_is_fitted,\n    check_non_negative,\n)\nfrom ._cdnmf_fast import _update_cdnmf_fast\n\nEPSILON = np.finfo(np.float32).eps\n\n\ndef norm(x):\n    \"\"\"Dot product-based Euclidean norm implementation.\n\n    See: http://fa.bianp.net/blog/2011/computing-the-vector-norm/\n\n    Parameters\n    ----------\n    x : array-like\n        Vector for which to compute the norm.\n    \"\"\"\n    return sqrt(squared_norm(x))\n\n\ndef trace_dot(X, Y):\n    \"\"\"Trace of np.dot(X, Y.T).\n\n    Parameters\n    ----------\n    X : array-like\n        First matrix.\n    Y : array-like\n        Second matrix.\n    \"\"\"\n    return np.dot(X.ravel(), Y.ravel())\n\n\ndef _check_init(A, shape, whom):\n    A = check_array(A)\n    if shape[0] != \"auto\" and A.shape[0] != shape[0]:\n        raise ValueError(\n            f\"Array with wrong first dimension passed to {whom}. Expected {shape[0]}, \"\n            f\"but got {A.shape[0]}.\"\n        )\n    if shape[1] != \"auto\" and A.shape[1] != shape[1]:\n        raise ValueError(\n            f\"Array with wrong second dimension passed to {whom}. Expected {shape[1]}, \"\n            f\"but got {A.shape[1]}.\"\n        )\n    check_non_negative(A, whom)\n    if np.max(A) == 0:\n        raise ValueError(f\"Array passed to {whom} is full of zeros.\")",
                "filename": "sklearn/decomposition/_nmf.py",
                "start_index": 0,
                "end_index": 2113,
                "start_line": 1,
                "end_line": 85,
                "max_line": 2443,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "_parameter_constraints: dict = {\n        **_BaseNMF._parameter_constraints,\n        \"solver\": [StrOptions({\"mu\", \"cd\"})],\n        \"shuffle\": [\"boolean\"],\n    }\n\n    def __init__(\n        self,\n        n_components=\"warn\",\n        *,\n        init=None,\n        solver=\"cd\",\n        beta_loss=\"frobenius\",\n        tol=1e-4,\n        max_iter=200,\n        random_state=None,\n        alpha_W=0.0,\n        alpha_H=\"same\",\n        l1_ratio=0.0,\n        verbose=0,\n        shuffle=False,\n    ):\n        super().__init__(\n            n_components=n_components,\n            init=init,\n            beta_loss=beta_loss,\n            tol=tol,\n            max_iter=max_iter,\n            random_state=random_state,\n            alpha_W=alpha_W,\n            alpha_H=alpha_H,\n            l1_ratio=l1_ratio,\n            verbose=verbose,\n        )\n\n        self.solver = solver\n        self.shuffle = shuffle\n\n    def _check_params(self, X):\n        super()._check_params(X)\n\n        # solver\n        if self.solver != \"mu\" and self.beta_loss not in (2, \"frobenius\"):\n            # 'mu' is the only solver that handles other beta losses than 'frobenius'\n            raise ValueError(\n                f\"Invalid beta_loss parameter: solver {self.solver!r} does not handle \"\n                f\"beta_loss = {self.beta_loss!r}\"\n            )\n        if self.solver == \"mu\" and self.init == \"nndsvd\":\n            warnings.warn(\n                (\n                    \"The multiplicative update ('mu') solver cannot update \"\n                    \"zeros present in the initialization, and so leads to \"\n                    \"poorer results when used jointly with init='nndsvd'. \"\n                    \"You may try init='nndsvda' or init='nndsvdar' instead.\"\n                ),\n                UserWarning,\n            )\n\n        return self",
                "filename": "sklearn/decomposition/_nmf.py",
                "start_index": 51320,
                "end_index": 53126,
                "start_line": 1146,
                "end_line": 2443,
                "max_line": 2443,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"Non-Negative Matrix Factorization (NMF).\n\n    Find two non-negative matrices, i.e. matrices with all non-negative elements, (W, H)\n    whose product approximates the non-negative matrix X. This factorization can be used\n    for example for dimensionality reduction, source separation or topic extraction.\n\n    The objective function is:\n\n        .. math::\n\n            L(W, H) &= 0.5 * ||X - WH||_{loss}^2\n\n            &+ alpha\\\\_W * l1\\\\_ratio * n\\\\_features * ||vec(W)||_1\n\n            &+ alpha\\\\_H * l1\\\\_ratio * n\\\\_samples * ||vec(H)||_1\n\n            &+ 0.5 * alpha\\\\_W * (1 - l1\\\\_ratio) * n\\\\_features * ||W||_{Fro}^2\n\n            &+ 0.5 * alpha\\\\_H * (1 - l1\\\\_ratio) * n\\\\_samples * ||H||_{Fro}^2\n\n    Where:\n\n    :math:`||A||_{Fro}^2 = \\\\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n    :math:`||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\n    The generic norm :math:`||X - WH||_{loss}` may represent\n    the Frobenius norm or another supported beta-divergence loss.\n    The choice between options is controlled by the `beta_loss` parameter.\n\n    The regularization terms are scaled by `n_features` for `W` and by `n_samples` for\n    `H` to keep their impact balanced with respect to one another and to the data fit\n    term as independent as possible of the size `n_samples` of the training set.\n\n    The objective function is minimized with an alternating minimization of W\n    and H.\n\n    Note that the transformed data is named W and the components matrix is named H. In\n    the NMF literature, the naming convention is usually the opposite since the data\n    matrix X is transposed.\n\n    Read more in the :ref:`User Guide <NMF>`.\n\n    Parameters\n    ----------\n    n_components : int or {'auto'} or None, default=None\n        Number of components, if n_components is not set all features\n        are kept.\n        If `n_components='auto'`, the number of components is automatically inferred\n        from W or H shapes.\n\n        .. versionchanged:: 1.4\n            Added `'auto'` value.\n\n    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n        Method used to initialize the procedure.\n        Valid options:\n\n        - `None`: 'nndsvda' if n_components <= min(n_samples, n_features),\n          otherwise random.\n\n        - `'random'`: non-negative random matrices, scaled with:\n          `sqrt(X.mean() / n_components)`\n\n        - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n          initialization (better for sparseness)\n\n        - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n          (better when sparsity is not desired)\n\n        - `'nndsvdar'` NNDSVD with zeros filled with small random values\n          (generally faster, less accurate alternative to NNDSVDa\n          for when sparsity is not desired)\n\n        - `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n\n        .. versionchanged:: 1.1\n            When `init=None` and n_components is less than n_samples and n_features\n            defaults to `nndsvda` instead of `nndsvd`.\n\n    solver : {'cd', 'mu'}, default='cd'\n        Numerical solver to use:\n\n        - 'cd' is a Coordinate Descent solver.\n        - 'mu' is a Multiplicative Update solver.\n\n        .. versionadded:: 0.17\n           Coordinate Descent solver.\n\n        .. versionadded:: 0.19\n           Multiplicative Update solver.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler', \\",
                "filename": "sklearn/decomposition/_nmf.py",
                "start_index": 43561,
                "end_index": 47003,
                "start_line": 1365,
                "end_line": 1884,
                "max_line": 2443,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "_parameter_constraints: dict = {\n        **_BaseNMF._parameter_constraints,\n        \"max_no_improvement\": [Interval(Integral, 1, None, closed=\"left\"), None],\n        \"batch_size\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"forget_factor\": [Interval(Real, 0, 1, closed=\"both\")],\n        \"fresh_restarts\": [\"boolean\"],\n        \"fresh_restarts_max_iter\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"transform_max_iter\": [Interval(Integral, 1, None, closed=\"left\"), None],\n    }\n\n    def __init__(\n        self,\n        n_components=\"warn\",\n        *,\n        init=None,\n        batch_size=1024,\n        beta_loss=\"frobenius\",\n        tol=1e-4,\n        max_no_improvement=10,\n        max_iter=200,\n        alpha_W=0.0,\n        alpha_H=\"same\",\n        l1_ratio=0.0,\n        forget_factor=0.7,\n        fresh_restarts=False,\n        fresh_restarts_max_iter=30,\n        transform_max_iter=None,\n        random_state=None,\n        verbose=0,\n    ):\n        super().__init__(\n            n_components=n_components,\n            init=init,\n            beta_loss=beta_loss,\n            tol=tol,\n            max_iter=max_iter,\n            random_state=random_state,\n            alpha_W=alpha_W,\n            alpha_H=alpha_H,\n            l1_ratio=l1_ratio,\n            verbose=verbose,\n        )\n\n        self.max_no_improvement = max_no_improvement\n        self.batch_size = batch_size\n        self.forget_factor = forget_factor\n        self.fresh_restarts = fresh_restarts\n        self.fresh_restarts_max_iter = fresh_restarts_max_iter\n        self.transform_max_iter = transform_max_iter\n\n    def _check_params(self, X):\n        super()._check_params(X)\n\n        # batch_size\n        self._batch_size = min(self.batch_size, X.shape[0])\n\n        # forget_factor\n        self._rho = self.forget_factor ** (self._batch_size / X.shape[0])\n\n        # gamma for Maximization-Minimization (MM) algorithm [Fevotte 2011]\n        if self._beta_loss < 1:\n            self._gamma = 1.0 / (2.0 - self._beta_loss)\n        elif self._beta_loss > 2:\n            self._gamma = 1.0 / (self._beta_loss - 1.0)\n        else:\n            self._gamma = 1.0\n\n        # transform_max_iter\n        self._transform_max_iter = (\n            self.max_iter\n            if self.transform_max_iter is None\n            else self.transform_max_iter\n        )\n\n        return self",
                "filename": "sklearn/decomposition/_nmf.py",
                "start_index": 67335,
                "end_index": 69690,
                "start_line": 1146,
                "end_line": 2443,
                "max_line": 2443,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "if init == \"random\":\n        avg = np.sqrt(X.mean() / n_components)\n        rng = check_random_state(random_state)\n        H = avg * rng.standard_normal(size=(n_components, n_features)).astype(\n            X.dtype, copy=False\n        )\n        W = avg * rng.standard_normal(size=(n_samples, n_components)).astype(\n            X.dtype, copy=False\n        )\n        np.abs(H, out=H)\n        np.abs(W, out=W)\n        return W, H\n\n    # NNDSVD initialization\n    U, S, V = randomized_svd(X, n_components, random_state=random_state)\n    W = np.zeros_like(U)\n    H = np.zeros_like(V)\n\n    # The leading singular triplet is non-negative\n    # so it can be used as is for initialization.\n    W[:, 0] = np.sqrt(S[0]) * np.abs(U[:, 0])\n    H[0, :] = np.sqrt(S[0]) * np.abs(V[0, :])\n\n    for j in range(1, n_components):\n        x, y = U[:, j], V[j, :]\n\n        # extract positive and negative parts of column vectors\n        x_p, y_p = np.maximum(x, 0), np.maximum(y, 0)\n        x_n, y_n = np.abs(np.minimum(x, 0)), np.abs(np.minimum(y, 0))\n\n        # and their norms\n        x_p_nrm, y_p_nrm = norm(x_p), norm(y_p)\n        x_n_nrm, y_n_nrm = norm(x_n), norm(y_n)\n\n        m_p, m_n = x_p_nrm * y_p_nrm, x_n_nrm * y_n_nrm\n\n        # choose update\n        if m_p > m_n:\n            u = x_p / x_p_nrm\n            v = y_p / y_p_nrm\n            sigma = m_p\n        else:\n            u = x_n / x_n_nrm\n            v = y_n / y_n_nrm\n            sigma = m_n\n\n        lbd = np.sqrt(S[j] * sigma)\n        W[:, j] = lbd * u\n        H[j, :] = lbd * v\n\n    W[W < eps] = 0\n    H[H < eps] = 0\n\n    if init == \"nndsvd\":\n        pass\n    elif init == \"nndsvda\":\n        avg = X.mean()\n        W[W == 0] = avg\n        H[H == 0] = avg\n    elif init == \"nndsvdar\":\n        rng = check_random_state(random_state)\n        avg = X.mean()\n        W[W == 0] = abs(avg * rng.standard_normal(size=len(W[W == 0])) / 100)\n        H[H == 0] = abs(avg * rng.standard_normal(size=len(H[H == 0])) / 100)\n    else:\n        raise ValueError(\n            \"Invalid init parameter: got %r instead of one of %r\"\n            % (init, (None, \"random\", \"nndsvd\", \"nndsvda\", \"nndsvdar\"))\n        )\n\n    return W, H",
                "filename": "sklearn/decomposition/_nmf.py",
                "start_index": 9248,
                "end_index": 11409,
                "start_line": 306,
                "end_line": 2361,
                "max_line": 2443,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"Mini-Batch Non-Negative Matrix Factorization (NMF).\n\n    .. versionadded:: 1.1\n\n    Find two non-negative matrices, i.e. matrices with all non-negative elements,\n    (`W`, `H`) whose product approximates the non-negative matrix `X`. This\n    factorization can be used for example for dimensionality reduction, source\n    separation or topic extraction.\n\n    The objective function is:\n\n        .. math::\n\n            L(W, H) &= 0.5 * ||X - WH||_{loss}^2\n\n            &+ alpha\\\\_W * l1\\\\_ratio * n\\\\_features * ||vec(W)||_1\n\n            &+ alpha\\\\_H * l1\\\\_ratio * n\\\\_samples * ||vec(H)||_1\n\n            &+ 0.5 * alpha\\\\_W * (1 - l1\\\\_ratio) * n\\\\_features * ||W||_{Fro}^2\n\n            &+ 0.5 * alpha\\\\_H * (1 - l1\\\\_ratio) * n\\\\_samples * ||H||_{Fro}^2\n\n    Where:\n\n    :math:`||A||_{Fro}^2 = \\\\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n    :math:`||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\n    The generic norm :math:`||X - WH||_{loss}^2` may represent\n    the Frobenius norm or another supported beta-divergence loss.\n    The choice between options is controlled by the `beta_loss` parameter.\n\n    The objective function is minimized with an alternating minimization of `W`\n    and `H`.\n\n    Note that the transformed data is named `W` and the components matrix is\n    named `H`. In the NMF literature, the naming convention is usually the opposite\n    since the data matrix `X` is transposed.\n\n    Read more in the :ref:`User Guide <MiniBatchNMF>`.\n\n    Parameters\n    ----------\n    n_components : int or {'auto'} or None, default=None\n        Number of components, if `n_components` is not set all features\n        are kept.\n        If `n_components='auto'`, the number of components is automatically inferred\n        from W or H shapes.\n\n        .. versionchanged:: 1.4\n            Added `'auto'` value.\n\n    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n        Method used to initialize the procedure.\n        Valid options:\n\n        - `None`: 'nndsvda' if `n_components <= min(n_samples, n_features)`,\n          otherwise random.\n\n        - `'random'`: non-negative random matrices, scaled with:\n          `sqrt(X.mean() / n_components)`\n\n        - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n          initialization (better for sparseness).\n\n        - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n          (better when sparsity is not desired).\n\n        - `'nndsvdar'` NNDSVD with zeros filled with small random values\n          (generally faster, less accurate alternative to NNDSVDa\n          for when sparsity is not desired).\n\n        - `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n\n    batch_size : int, default=1024\n        Number of samples in each mini-batch. Large batch sizes\n        give better long-term convergence at the cost of a slower start.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler', \\\n            'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between `X`\n        and the dot product `WH`. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for `beta_loss <= 0` (or 'itakura-saito'), the input\n        matrix `X` cannot contain zeros.\n\n    tol : float, default=1e-4\n        Control early stopping based on the norm of the differences in `H`\n        between 2 steps. To disable early stopping based on changes in `H`, set\n        `tol` to 0.0.\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini batches\n        that does not yield an improvement on the smoothed cost function.\n        To disable convergence detection based on cost function, set\n        `max_no_improvement` to None.\n\n    max_iter : int, default=200\n        Maximum number of iterations over the complete dataset before\n        timing out.\n\n    alpha_W : float, default=0.0\n        Constant that multiplies the regularization terms of `W`. Set it to zero\n        (default) to have no regularization on `W`.\n\n    alpha_H : float or \"same\", default=\"same\"\n        Constant that multiplies the regularization terms of `H`. Set it to zero to\n        have no regularization on `H`. If \"same\" (default), it takes the same value as\n        `alpha_W`.\n\n    l1_ratio : float, default=0.0\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    forget_factor : float, default=0.7\n        Amount of rescaling of past information. Its value could be 1 with\n        finite datasets. Choosing values < 1 is recommended with online\n        learning as more recent batches will weight more than past batches.\n\n    fresh_restarts : bool, default=False\n        Whether to completely solve for W at each step. Doing fresh restarts will likely\n        lead to a better solution for a same number of iterations but it is much slower.\n\n    fresh_restarts_max_iter : int, default=30\n        Maximum number of iterations when solving for W at each step. Only used when\n        doing fresh restarts. These iterations may be stopped early based on a small\n        change of W controlled by `tol`.\n\n    transform_max_iter : int, default=None\n        Maximum number of iterations when solving for W at transform time.\n        If None, it defaults to `max_iter`.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    verbose : bool, default=False\n        Whether to be verbose.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Factorization matrix, sometimes called 'dictionary'.\n\n    n_components_ : int\n        The number of components. It is same as the `n_components` parameter\n        if it was given. Otherwise, it will be same as the number of\n        features.\n\n    reconstruction_err_ : float\n        Frobenius norm of the matrix difference, or beta-divergence, between\n        the training data `X` and the reconstructed data `WH` from\n        the fitted model.\n\n    n_iter_ : int\n        Actual number of started iterations over the whole dataset.\n\n    n_steps_ : int\n        Number of mini-batches processed.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n    See Also\n    --------\n    NMF : Non-negative matrix factorization.\n    MiniBatchDictionaryLearning : Finds a dictionary that can best be used to represent\n        data using a sparse code.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n    .. [3] :doi:`\"Online algorithms for nonnegative matrix factorization with the\n       Itakura-Saito divergence\" <10.1109/ASPAA.2011.6082314>`\n       Lefevre, A., Bach, F., Fevotte, C. (2011). WASPA.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import MiniBatchNMF\n    >>> model = MiniBatchNMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_\n    \"\"\"",
                "filename": "sklearn/decomposition/_nmf.py",
                "start_index": 59131,
                "end_index": 67329,
                "start_line": 1805,
                "end_line": 2416,
                "max_line": 2443,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "for init_param in init_params:\n            assert (\n                init_param.default != init_param.empty\n            ), \"parameter %s for %s has no default value\" % (\n                init_param.name,\n                type(estimator).__name__,\n            )\n            allowed_types = {\n                str,\n                int,\n                float,\n                bool,\n                tuple,\n                type(None),\n                type,\n            }\n            # Any numpy numeric such as np.int32.\n            allowed_types.update(np.core.numerictypes.allTypes.values())\n\n            allowed_value = (\n                type(init_param.default) in allowed_types\n                or\n                # Although callables are mutable, we accept them as argument\n                # default value and trust that neither the implementation of\n                # the callable nor of the estimator changes the state of the\n                # callable.\n                callable(init_param.default)\n            )\n\n            assert allowed_value, (\n                f\"Parameter '{init_param.name}' of estimator \"\n                f\"'{Estimator.__name__}' is of type \"\n                f\"{type(init_param.default).__name__} which is not allowed. \"\n                f\"'{init_param.name}' must be a callable or must be of type \"\n                f\"{set(type.__name__ for type in allowed_types)}.\"\n            )\n            if init_param.name not in params.keys():\n                # deprecated parameter, not in get_params\n                assert init_param.default is None, (\n                    f\"Estimator parameter '{init_param.name}' of estimator \"\n                    f\"'{Estimator.__name__}' is not returned by get_params. \"\n                    \"If it is deprecated, set its default value to None.\"\n                )\n                continue\n\n            param_value = params[init_param.name]\n            if isinstance(param_value, np.ndarray):\n                assert_array_equal(param_value, init_param.default)\n            else:\n                failure_text = (\n                    f\"Parameter {init_param.name} was mutated on init. All \"\n                    \"parameters must be stored unchanged.\"\n                )\n                if is_scalar_nan(param_value):\n                    # Allows to set default parameters to np.nan\n                    assert param_value is init_param.default, failure_text\n                else:\n                    assert param_value == init_param.default, failure_text",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 122163,
                "end_index": 124660,
                "start_line": 3478,
                "end_line": 3535,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "- :class:`linear_model.LogisticRegression`\n# - :class:`linear_model.GammaRegressor`\n# - :class:`linear_model.PoissonRegressor`\n# - :class:`linear_model.TweedieRegressor`\n\n# %%\n# :class:`~decomposition.MiniBatchNMF`: an online version of NMF\n# --------------------------------------------------------------\n# The new class :class:`~decomposition.MiniBatchNMF` implements a faster but\n# less accurate version of non-negative matrix factorization\n# (:class:`~decomposition.NMF`). :class:`~decomposition.MiniBatchNMF` divides the\n# data into mini-batches and optimizes the NMF model in an online manner by\n# cycling over the mini-batches, making it better suited for large datasets. In\n# particular, it implements `partial_fit`, which can be used for online\n# learning when the data is not readily available from the start, or when the\n# data does not fit into memory.\nimport numpy as np\nfrom sklearn.decomposition import MiniBatchNMF\n\nrng = np.random.RandomState(0)\nn_samples, n_features, n_components = 10, 10, 5\ntrue_W = rng.uniform(size=(n_samples, n_components))\ntrue_H = rng.uniform(size=(n_components, n_features))\nX = true_W @ true_H\n\nnmf = MiniBatchNMF(n_components=n_components, random_state=0)\n\nfor _ in range(10):\n    nmf.partial_fit(X)\n\nW = nmf.transform(X)\nH = nmf.components_\nX_reconstructed = W @ H\n\nprint(\n    f\"relative reconstruction error: \",\n    f\"{np.sum((X - X_reconstructed) ** 2) / np.sum(X**2):.5f}\",\n)\n\n# %%\n# :class:`~cluster.BisectingKMeans`: divide and cluster\n# -----------------------------------------------------\n# The new class :class:`~cluster.BisectingKMeans` is a variant of\n# :class:`~cluster.KMeans`, using divisive hierarchical clustering. Instead of\n# creating all centroids at once, centroids are picked progressively based on a\n# previous clustering: a cluster is split into two new clusters repeatedly\n# until the target number of clusters is reached, giving a hierarchical\n# structure to the clustering.\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans, BisectingKMeans\nimport matplotlib.pyplot as plt\n\nX, _ = make_blobs(n_samples=1000, centers=2, random_state=0)\n\nkm = KMeans(n_clusters=5, random_state=0, n_init=\"auto\").fit(X)\nbisect_km = BisectingKMeans(n_clusters=5, random_state=0).fit(X)\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].scatter(X[:, 0], X[:, 1], s=10, c=km.labels_)\nax[0].scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=20, c=\"r\")\nax[0].set_title(\"KMeans\")\n\nax[1].scatter(X[:, 0], X[:, 1], s=10, c=bisect_km.labels_)\nax[1].scatter(\n    bisect_km.cluster_centers_[:, 0], bisect_km.cluster_centers_[:, 1], s=20, c=\"r\"\n)\n_ = ax[1].set_title(\"BisectingKMeans\")",
                "filename": "examples/release_highlights/plot_release_highlights_1_1_0.py",
                "start_index": 5956,
                "end_index": 8623,
                "start_line": 163,
                "end_line": 229,
                "max_line": 229,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/decomposition/_nmf.py": [
                {
                    "chunk": {
                        "code": "\"\"\"Base class for NMF and MiniBatchNMF.\"\"\"\n\n    # This prevents ``set_split_inverse_transform`` to be generated for the\n    # non-standard ``W`` arg on ``inverse_transform``.\n    # TODO: remove when W is removed in v1.5 for inverse_transform\n    __metadata_request__inverse_transform = {\"W\": metadata_routing.UNUSED}\n\n    _parameter_constraints: dict = {\n        \"n_components\": [\n            Interval(Integral, 1, None, closed=\"left\"),\n            None,\n            StrOptions({\"auto\"}),\n            Hidden(StrOptions({\"warn\"})),\n        ],\n        \"init\": [\n            StrOptions({\"random\", \"nndsvd\", \"nndsvda\", \"nndsvdar\", \"custom\"}),\n            None,\n        ],\n        \"beta_loss\": [\n            StrOptions({\"frobenius\", \"kullback-leibler\", \"itakura-saito\"}),\n            Real,\n        ],\n        \"tol\": [Interval(Real, 0, None, closed=\"left\")],\n        \"max_iter\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"random_state\": [\"random_state\"],\n        \"alpha_W\": [Interval(Real, 0, None, closed=\"left\")],\n        \"alpha_H\": [Interval(Real, 0, None, closed=\"left\"), StrOptions({\"same\"})],\n        \"l1_ratio\": [Interval(Real, 0, 1, closed=\"both\")],\n        \"verbose\": [\"verbose\"],\n    }\n\n    def __init__(\n        self,\n        n_components=\"warn\",\n        *,\n        init=None,\n        beta_loss=\"frobenius\",\n        tol=1e-4,\n        max_iter=200,\n        random_state=None,\n        alpha_W=0.0,\n        alpha_H=\"same\",\n        l1_ratio=0.0,\n        verbose=0,\n    ):\n        self.n_components = n_components\n        self.init = init\n        self.beta_loss = beta_loss\n        self.tol = tol\n        self.max_iter = max_iter\n        self.random_state = random_state\n        self.alpha_W = alpha_W\n        self.alpha_H = alpha_H\n        self.l1_ratio = l1_ratio\n        self.verbose = verbose\n\n    def _check_params(self, X):\n        # n_components\n        self._n_components = self.n_components\n        if self.n_components == \"warn\":\n            warnings.warn(\n                (\n                    \"The default value of `n_components` will change from `None` to\"\n                    \" `'auto'` in 1.6. Set the value of `n_components` to `None`\"\n                    \" explicitly to supress the warning.\"\n                ),\n                FutureWarning,\n            )\n            self._n_components = None  # Keeping the old default value\n        if self._n_components is None:\n            self._n_components = X.shape[1]\n\n        # beta_loss\n        self._beta_loss = _beta_loss_to_float(self.beta_loss)",
                        "filename": "sklearn/decomposition/_nmf.py",
                        "start_index": 36069,
                        "end_index": 38591,
                        "start_line": 1139,
                        "end_line": 1212,
                        "max_line": 2443,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the __init__ method of the base class for NMF, where the default value for 'init' is set to None. This is relevant as the issue describes an inconsistency with the default 'init' value between 'NMF' and 'non_negative_factorization'."
                },
                {
                    "chunk": {
                        "code": "'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between X\n        and the dot product WH. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input\n        matrix X cannot contain zeros. Used only in 'mu' solver.\n\n        .. versionadded:: 0.19\n\n    tol : float, default=1e-4\n        Tolerance of the stopping condition.\n\n    max_iter : int, default=200\n        Maximum number of iterations before timing out.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    alpha_W : float, default=0.0\n        Constant that multiplies the regularization terms of `W`. Set it to zero\n        (default) to have no regularization on `W`.\n\n        .. versionadded:: 1.0\n\n    alpha_H : float or \"same\", default=\"same\"\n        Constant that multiplies the regularization terms of `H`. Set it to zero to\n        have no regularization on `H`. If \"same\" (default), it takes the same value as\n        `alpha_W`.\n\n        .. versionadded:: 1.0\n\n    l1_ratio : float, default=0.0\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n        .. versionadded:: 0.17\n           Regularization parameter *l1_ratio* used in the Coordinate Descent\n           solver.\n\n    verbose : int, default=0\n        Whether to be verbose.\n\n    shuffle : bool, default=False\n        If true, randomize the order of coordinates in the CD solver.\n\n        .. versionadded:: 0.17\n           *shuffle* parameter used in the Coordinate Descent solver.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Factorization matrix, sometimes called 'dictionary'.\n\n    n_components_ : int\n        The number of components. It is same as the `n_components` parameter\n        if it was given. Otherwise, it will be same as the number of\n        features.\n\n    reconstruction_err_ : float\n        Frobenius norm of the matrix difference, or beta-divergence, between\n        the training data ``X`` and the reconstructed data ``WH`` from\n        the fitted model.\n\n    n_iter_ : int\n        Actual number of iterations.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    DictionaryLearning : Find a dictionary that sparsely encodes data.\n    MiniBatchSparsePCA : Mini-batch Sparse Principal Components Analysis.\n    PCA : Principal component analysis.\n    SparseCoder : Find a sparse representation of data from a fixed,\n        precomputed dictionary.\n    SparsePCA : Sparse Principal Components Analysis.\n    TruncatedSVD : Dimensionality reduction using truncated SVD.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import NMF\n    >>> model = NMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_\n    \"\"\"",
                        "filename": "sklearn/decomposition/_nmf.py",
                        "start_index": 47016,
                        "end_index": 51314,
                        "start_line": 766,
                        "end_line": 2416,
                        "max_line": 2443,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the documentation for the 'NMF' class, which includes the default 'init' parameter. It is relevant for understanding the expected behavior and defaults of the 'NMF' class."
                },
                {
                    "chunk": {
                        "code": "_parameter_constraints: dict = {\n        **_BaseNMF._parameter_constraints,\n        \"solver\": [StrOptions({\"mu\", \"cd\"})],\n        \"shuffle\": [\"boolean\"],\n    }\n\n    def __init__(\n        self,\n        n_components=\"warn\",\n        *,\n        init=None,\n        solver=\"cd\",\n        beta_loss=\"frobenius\",\n        tol=1e-4,\n        max_iter=200,\n        random_state=None,\n        alpha_W=0.0,\n        alpha_H=\"same\",\n        l1_ratio=0.0,\n        verbose=0,\n        shuffle=False,\n    ):\n        super().__init__(\n            n_components=n_components,\n            init=init,\n            beta_loss=beta_loss,\n            tol=tol,\n            max_iter=max_iter,\n            random_state=random_state,\n            alpha_W=alpha_W,\n            alpha_H=alpha_H,\n            l1_ratio=l1_ratio,\n            verbose=verbose,\n        )\n\n        self.solver = solver\n        self.shuffle = shuffle\n\n    def _check_params(self, X):\n        super()._check_params(X)\n\n        # solver\n        if self.solver != \"mu\" and self.beta_loss not in (2, \"frobenius\"):\n            # 'mu' is the only solver that handles other beta losses than 'frobenius'\n            raise ValueError(\n                f\"Invalid beta_loss parameter: solver {self.solver!r} does not handle \"\n                f\"beta_loss = {self.beta_loss!r}\"\n            )\n        if self.solver == \"mu\" and self.init == \"nndsvd\":\n            warnings.warn(\n                (\n                    \"The multiplicative update ('mu') solver cannot update \"\n                    \"zeros present in the initialization, and so leads to \"\n                    \"poorer results when used jointly with init='nndsvd'. \"\n                    \"You may try init='nndsvda' or init='nndsvdar' instead.\"\n                ),\n                UserWarning,\n            )\n\n        return self",
                        "filename": "sklearn/decomposition/_nmf.py",
                        "start_index": 51320,
                        "end_index": 53126,
                        "start_line": 1146,
                        "end_line": 2443,
                        "max_line": 2443,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the __init__ method of the 'NMF' class, where the default value for 'init' is set to None. It is relevant as it directly relates to the issue of inconsistent default 'init' values."
                },
                {
                    "chunk": {
                        "code": "if init == \"random\":\n        avg = np.sqrt(X.mean() / n_components)\n        rng = check_random_state(random_state)\n        H = avg * rng.standard_normal(size=(n_components, n_features)).astype(\n            X.dtype, copy=False\n        )\n        W = avg * rng.standard_normal(size=(n_samples, n_components)).astype(\n            X.dtype, copy=False\n        )\n        np.abs(H, out=H)\n        np.abs(W, out=W)\n        return W, H\n\n    # NNDSVD initialization\n    U, S, V = randomized_svd(X, n_components, random_state=random_state)\n    W = np.zeros_like(U)\n    H = np.zeros_like(V)\n\n    # The leading singular triplet is non-negative\n    # so it can be used as is for initialization.\n    W[:, 0] = np.sqrt(S[0]) * np.abs(U[:, 0])\n    H[0, :] = np.sqrt(S[0]) * np.abs(V[0, :])\n\n    for j in range(1, n_components):\n        x, y = U[:, j], V[j, :]\n\n        # extract positive and negative parts of column vectors\n        x_p, y_p = np.maximum(x, 0), np.maximum(y, 0)\n        x_n, y_n = np.abs(np.minimum(x, 0)), np.abs(np.minimum(y, 0))\n\n        # and their norms\n        x_p_nrm, y_p_nrm = norm(x_p), norm(y_p)\n        x_n_nrm, y_n_nrm = norm(x_n), norm(y_n)\n\n        m_p, m_n = x_p_nrm * y_p_nrm, x_n_nrm * y_n_nrm\n\n        # choose update\n        if m_p > m_n:\n            u = x_p / x_p_nrm\n            v = y_p / y_p_nrm\n            sigma = m_p\n        else:\n            u = x_n / x_n_nrm\n            v = y_n / y_n_nrm\n            sigma = m_n\n\n        lbd = np.sqrt(S[j] * sigma)\n        W[:, j] = lbd * u\n        H[j, :] = lbd * v\n\n    W[W < eps] = 0\n    H[H < eps] = 0\n\n    if init == \"nndsvd\":\n        pass\n    elif init == \"nndsvda\":\n        avg = X.mean()\n        W[W == 0] = avg\n        H[H == 0] = avg\n    elif init == \"nndsvdar\":\n        rng = check_random_state(random_state)\n        avg = X.mean()\n        W[W == 0] = abs(avg * rng.standard_normal(size=len(W[W == 0])) / 100)\n        H[H == 0] = abs(avg * rng.standard_normal(size=len(H[H == 0])) / 100)\n    else:\n        raise ValueError(\n            \"Invalid init parameter: got %r instead of one of %r\"\n            % (init, (None, \"random\", \"nndsvd\", \"nndsvda\", \"nndsvdar\"))\n        )\n\n    return W, H",
                        "filename": "sklearn/decomposition/_nmf.py",
                        "start_index": 9248,
                        "end_index": 11409,
                        "start_line": 306,
                        "end_line": 2361,
                        "max_line": 2443,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes the code for handling different 'init' strategies in 'non_negative_factorization'. It is relevant to understand how the 'init' parameter is used and to ensure consistency with the 'NMF' class."
                },
                {
                    "chunk": {
                        "code": "\"\"\"Mini-Batch Non-Negative Matrix Factorization (NMF).\n\n    .. versionadded:: 1.1\n\n    Find two non-negative matrices, i.e. matrices with all non-negative elements,\n    (`W`, `H`) whose product approximates the non-negative matrix `X`. This\n    factorization can be used for example for dimensionality reduction, source\n    separation or topic extraction.\n\n    The objective function is:\n\n        .. math::\n\n            L(W, H) &= 0.5 * ||X - WH||_{loss}^2\n\n            &+ alpha\\\\_W * l1\\\\_ratio * n\\\\_features * ||vec(W)||_1\n\n            &+ alpha\\\\_H * l1\\\\_ratio * n\\\\_samples * ||vec(H)||_1\n\n            &+ 0.5 * alpha\\\\_W * (1 - l1\\\\_ratio) * n\\\\_features * ||W||_{Fro}^2\n\n            &+ 0.5 * alpha\\\\_H * (1 - l1\\\\_ratio) * n\\\\_samples * ||H||_{Fro}^2\n\n    Where:\n\n    :math:`||A||_{Fro}^2 = \\\\sum_{i,j} A_{ij}^2` (Frobenius norm)\n\n    :math:`||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)\n\n    The generic norm :math:`||X - WH||_{loss}^2` may represent\n    the Frobenius norm or another supported beta-divergence loss.\n    The choice between options is controlled by the `beta_loss` parameter.\n\n    The objective function is minimized with an alternating minimization of `W`\n    and `H`.\n\n    Note that the transformed data is named `W` and the components matrix is\n    named `H`. In the NMF literature, the naming convention is usually the opposite\n    since the data matrix `X` is transposed.\n\n    Read more in the :ref:`User Guide <MiniBatchNMF>`.\n\n    Parameters\n    ----------\n    n_components : int or {'auto'} or None, default=None\n        Number of components, if `n_components` is not set all features\n        are kept.\n        If `n_components='auto'`, the number of components is automatically inferred\n        from W or H shapes.\n\n        .. versionchanged:: 1.4\n            Added `'auto'` value.\n\n    init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None\n        Method used to initialize the procedure.\n        Valid options:\n\n        - `None`: 'nndsvda' if `n_components <= min(n_samples, n_features)`,\n          otherwise random.\n\n        - `'random'`: non-negative random matrices, scaled with:\n          `sqrt(X.mean() / n_components)`\n\n        - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)\n          initialization (better for sparseness).\n\n        - `'nndsvda'`: NNDSVD with zeros filled with the average of X\n          (better when sparsity is not desired).\n\n        - `'nndsvdar'` NNDSVD with zeros filled with small random values\n          (generally faster, less accurate alternative to NNDSVDa\n          for when sparsity is not desired).\n\n        - `'custom'`: Use custom matrices `W` and `H` which must both be provided.\n\n    batch_size : int, default=1024\n        Number of samples in each mini-batch. Large batch sizes\n        give better long-term convergence at the cost of a slower start.\n\n    beta_loss : float or {'frobenius', 'kullback-leibler', \\\n            'itakura-saito'}, default='frobenius'\n        Beta divergence to be minimized, measuring the distance between `X`\n        and the dot product `WH`. Note that values different from 'frobenius'\n        (or 2) and 'kullback-leibler' (or 1) lead to significantly slower\n        fits. Note that for `beta_loss <= 0` (or 'itakura-saito'), the input\n        matrix `X` cannot contain zeros.\n\n    tol : float, default=1e-4\n        Control early stopping based on the norm of the differences in `H`\n        between 2 steps. To disable early stopping based on changes in `H`, set\n        `tol` to 0.0.\n\n    max_no_improvement : int, default=10\n        Control early stopping based on the consecutive number of mini batches\n        that does not yield an improvement on the smoothed cost function.\n        To disable convergence detection based on cost function, set\n        `max_no_improvement` to None.\n\n    max_iter : int, default=200\n        Maximum number of iterations over the complete dataset before\n        timing out.\n\n    alpha_W : float, default=0.0\n        Constant that multiplies the regularization terms of `W`. Set it to zero\n        (default) to have no regularization on `W`.\n\n    alpha_H : float or \"same\", default=\"same\"\n        Constant that multiplies the regularization terms of `H`. Set it to zero to\n        have no regularization on `H`. If \"same\" (default), it takes the same value as\n        `alpha_W`.\n\n    l1_ratio : float, default=0.0\n        The regularization mixing parameter, with 0 <= l1_ratio <= 1.\n        For l1_ratio = 0 the penalty is an elementwise L2 penalty\n        (aka Frobenius Norm).\n        For l1_ratio = 1 it is an elementwise L1 penalty.\n        For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n\n    forget_factor : float, default=0.7\n        Amount of rescaling of past information. Its value could be 1 with\n        finite datasets. Choosing values < 1 is recommended with online\n        learning as more recent batches will weight more than past batches.\n\n    fresh_restarts : bool, default=False\n        Whether to completely solve for W at each step. Doing fresh restarts will likely\n        lead to a better solution for a same number of iterations but it is much slower.\n\n    fresh_restarts_max_iter : int, default=30\n        Maximum number of iterations when solving for W at each step. Only used when\n        doing fresh restarts. These iterations may be stopped early based on a small\n        change of W controlled by `tol`.\n\n    transform_max_iter : int, default=None\n        Maximum number of iterations when solving for W at transform time.\n        If None, it defaults to `max_iter`.\n\n    random_state : int, RandomState instance or None, default=None\n        Used for initialisation (when ``init`` == 'nndsvdar' or\n        'random'), and in Coordinate Descent. Pass an int for reproducible\n        results across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    verbose : bool, default=False\n        Whether to be verbose.\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Factorization matrix, sometimes called 'dictionary'.\n\n    n_components_ : int\n        The number of components. It is same as the `n_components` parameter\n        if it was given. Otherwise, it will be same as the number of\n        features.\n\n    reconstruction_err_ : float\n        Frobenius norm of the matrix difference, or beta-divergence, between\n        the training data `X` and the reconstructed data `WH` from\n        the fitted model.\n\n    n_iter_ : int\n        Actual number of started iterations over the whole dataset.\n\n    n_steps_ : int\n        Number of mini-batches processed.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n    See Also\n    --------\n    NMF : Non-negative matrix factorization.\n    MiniBatchDictionaryLearning : Finds a dictionary that can best be used to represent\n        data using a sparse code.\n\n    References\n    ----------\n    .. [1] :doi:`\"Fast local algorithms for large scale nonnegative matrix and tensor\n       factorizations\" <10.1587/transfun.E92.A.708>`\n       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals\n       of electronics, communications and computer sciences 92.3: 708-721, 2009.\n\n    .. [2] :doi:`\"Algorithms for nonnegative matrix factorization with the\n       beta-divergence\" <10.1162/NECO_a_00168>`\n       Fevotte, C., & Idier, J. (2011). Neural Computation, 23(9).\n\n    .. [3] :doi:`\"Online algorithms for nonnegative matrix factorization with the\n       Itakura-Saito divergence\" <10.1109/ASPAA.2011.6082314>`\n       Lefevre, A., Bach, F., Fevotte, C. (2011). WASPA.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    >>> from sklearn.decomposition import MiniBatchNMF\n    >>> model = MiniBatchNMF(n_components=2, init='random', random_state=0)\n    >>> W = model.fit_transform(X)\n    >>> H = model.components_\n    \"\"\"",
                        "filename": "sklearn/decomposition/_nmf.py",
                        "start_index": 59131,
                        "end_index": 67329,
                        "start_line": 1805,
                        "end_line": 2416,
                        "max_line": 2443,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the documentation for the 'MiniBatchNMF' class, which may also be affected by the default 'init' value inconsistency. It is relevant for checking if similar changes need to be applied here."
                }
            ]
        },
        "solution_task": "- Align the default `init` parameter value of `non_negative_factorization` with `NMF` by setting it to `None`.\n- Implement a deprecation warning in `non_negative_factorization` to inform users of the upcoming change.\n- Modify the default value after a suitable deprecation cycle (typically two versions).\n- Update the `_parameter_constraints` dictionary for `non_negative_factorization` to reflect the new default after deprecation period.\n- Add a conditional check in `non_negative_factorization` to handle the `init=None` case, setting it to `'random'` or `'nndsvda'` based on the condition used in `NMF` (i.e., `'nndsvda'` if `n_components <= min(n_samples, n_features)`, otherwise `'random'`).\n- Ensure that the deprecation warning advises users to explicitly set `init='random'` if they want to preserve the current behavior without any warnings.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE provides a detailed and actionable plan to address the inconsistency between the default 'init' parameters in `NMF` and `non_negative_factorization`. It suggests aligning the default values, implementing a deprecation warning, and updating the `_parameter_constraints` dictionary after the deprecation period. Additionally, it advises on handling the `None` case and ensuring users are informed about the change. The RESPONSE also includes relevant code snippets from the scikit-learn repository, which are directly related to the issue at hand. This demonstrates a deep understanding of the problem and the steps needed to resolve it, making it highly valuable for fixing the bug.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711741349.57549,
        "result": "- Align the default `init` parameter value of `non_negative_factorization` with `NMF` by setting it to `None`.\n- Implement a deprecation warning in `non_negative_factorization` to inform users of the upcoming change.\n- Modify the default value after a suitable deprecation cycle (typically two versions).\n- Update the `_parameter_constraints` dictionary for `non_negative_factorization` to reflect the new default after deprecation period.\n- Add a conditional check in `non_negative_factorization` to handle the `init=None` case, setting it to `'random'` or `'nndsvda'` based on the condition used in `NMF` (i.e., `'nndsvda'` if `n_components <= min(n_samples, n_features)`, otherwise `'random'`).\n- Ensure that the deprecation warning advises users to explicitly set `init='random'` if they want to preserve the current behavior without any warnings.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [decomposition](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/decomposition)- [Lines 1139 - 1212](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/decomposition/_nmf.py#L1139-L1212)\n\n This snippet shows the __init__ method of the base class for NMF, where the default value for 'init' is set to None. This is relevant as the issue describes an inconsistency with the default 'init' value between 'NMF' and 'non_negative_factorization'. \n\n\n   - [Lines 766 - 2416](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/decomposition/_nmf.py#L766-L2416)\n\n This snippet contains the documentation for the 'NMF' class, which includes the default 'init' parameter. It is relevant for understanding the expected behavior and defaults of the 'NMF' class. \n\n\n   - [Lines 1146 - 2443](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/decomposition/_nmf.py#L1146-L2443)\n\n This snippet shows the __init__ method of the 'NMF' class, where the default value for 'init' is set to None. It is relevant as it directly relates to the issue of inconsistent default 'init' values. \n\n\n   - [Lines 306 - 2361](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/decomposition/_nmf.py#L306-L2361)\n\n This snippet includes the code for handling different 'init' strategies in 'non_negative_factorization'. It is relevant to understand how the 'init' parameter is used and to ensure consistency with the 'NMF' class. \n\n\n   - [Lines 1805 - 2416](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/decomposition/_nmf.py#L1805-L2416)\n\n This snippet contains the documentation for the 'MiniBatchNMF' class, which may also be affected by the default 'init' value inconsistency. It is relevant for checking if similar changes need to be applied here."
    }
}