{
    "id": "scikit-learn__scikit-learn-12938",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
    "task_id": 8107218162787304635,
    "title": "AttributeError: 'PrettyPrinter' object has no attribute '_indent_at_name'",
    "body": "There's a failing example in #12654, and here's a piece of code causing it:\r\n\r\n```\r\nimport numpy as np\r\nfrom sklearn.datasets import load_digits\r\nfrom sklearn.model_selection import GridSearchCV\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.svm import LinearSVC\r\nfrom sklearn.decomposition import PCA, NMF\r\nfrom sklearn.feature_selection import SelectKBest, chi2\r\n\r\npipe = Pipeline([\r\n    # the reduce_dim stage is populated by the param_grid\r\n    ('reduce_dim', 'passthrough'),\r\n    ('classify', LinearSVC(dual=False, max_iter=10000))\r\n])\r\n\r\nN_FEATURES_OPTIONS = [2, 4, 8]\r\nC_OPTIONS = [1, 10, 100, 1000]\r\nparam_grid = [\r\n    {\r\n        'reduce_dim': [PCA(iterated_power=7), NMF()],\r\n        'reduce_dim__n_components': N_FEATURES_OPTIONS,\r\n        'classify__C': C_OPTIONS\r\n    },\r\n    {\r\n        'reduce_dim': [SelectKBest(chi2)],\r\n        'reduce_dim__k': N_FEATURES_OPTIONS,\r\n        'classify__C': C_OPTIONS\r\n    },\r\n]\r\nreducer_labels = ['PCA', 'NMF', 'KBest(chi2)']\r\n\r\ngrid = GridSearchCV(pipe, cv=5, n_jobs=1, param_grid=param_grid, iid=False)\r\nfrom tempfile import mkdtemp\r\nfrom joblib import Memory\r\n\r\n# Create a temporary folder to store the transformers of the pipeline\r\ncachedir = mkdtemp()\r\nmemory = Memory(location=cachedir, verbose=10)\r\ncached_pipe = Pipeline([('reduce_dim', PCA()),\r\n                        ('classify', LinearSVC(dual=False, max_iter=10000))],\r\n                       memory=memory)\r\n\r\n# This time, a cached pipeline will be used within the grid search\r\ngrid = GridSearchCV(cached_pipe, cv=5, n_jobs=1, param_grid=param_grid,\r\n                    iid=False, error_score='raise')\r\ndigits = load_digits()\r\ngrid.fit(digits.data, digits.target)\r\n```\r\n\r\nWith the stack trace:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<console>\", line 1, in <module>\r\n  File \"/path/to//sklearn/model_selection/_search.py\", line 683, in fit\r\n    self._run_search(evaluate_candidates)\r\n  File \"/path/to//sklearn/model_selection/_search.py\", line 1127, in _run_search\r\n    evaluate_candidates(ParameterGrid(self.param_grid))\r\n  File \"/path/to//sklearn/model_selection/_search.py\", line 672, in evaluate_candidates\r\n    cv.split(X, y, groups)))\r\n  File \"/path/to//sklearn/externals/joblib/parallel.py\", line 917, in __call__\r\n    if self.dispatch_one_batch(iterator):\r\n  File \"/path/to//sklearn/externals/joblib/parallel.py\", line 759, in dispatch_one_batch\r\n    self._dispatch(tasks)\r\n  File \"/path/to//sklearn/externals/joblib/parallel.py\", line 716, in _dispatch\r\n    job = self._backend.apply_async(batch, callback=cb)\r\n  File \"/path/to//sklearn/externals/joblib/_parallel_backends.py\", line 182, in apply_async\r\n    result = ImmediateResult(func)\r\n  File \"/path/to//sklearn/externals/joblib/_parallel_backends.py\", line 549, in __init__\r\n    self.results = batch()\r\n  File \"/path/to//sklearn/externals/joblib/parallel.py\", line 225, in __call__\r\n    for func, args, kwargs in self.items]\r\n  File \"/path/to//sklearn/externals/joblib/parallel.py\", line 225, in <listcomp>\r\n    for func, args, kwargs in self.items]\r\n  File \"/path/to//sklearn/model_selection/_validation.py\", line 511, in _fit_and_score\r\n    estimator.fit(X_train, y_train, **fit_params)\r\n  File \"/path/to//sklearn/pipeline.py\", line 279, in fit\r\n    Xt, fit_params = self._fit(X, y, **fit_params)\r\n  File \"/path/to//sklearn/pipeline.py\", line 244, in _fit\r\n    **fit_params_steps[name])\r\n  File \"/path/to/packages/joblib/memory.py\", line 555, in __call__\r\n    return self._cached_call(args, kwargs)[0]\r\n  File \"/path/to/packages/joblib/memory.py\", line 521, in _cached_call\r\n    out, metadata = self.call(*args, **kwargs)\r\n  File \"/path/to/packages/joblib/memory.py\", line 720, in call\r\n    print(format_call(self.func, args, kwargs))\r\n  File \"/path/to/packages/joblib/func_inspect.py\", line 356, in format_call\r\n    path, signature = format_signature(func, *args, **kwargs)\r\n  File \"/path/to/packages/joblib/func_inspect.py\", line 340, in format_signature\r\n    formatted_arg = _format_arg(arg)\r\n  File \"/path/to/packages/joblib/func_inspect.py\", line 322, in _format_arg\r\n    formatted_arg = pformat(arg, indent=2)\r\n  File \"/path/to/packages/joblib/logger.py\", line 54, in pformat\r\n    out = pprint.pformat(obj, depth=depth, indent=indent)\r\n  File \"/usr/lib64/python3.7/pprint.py\", line 58, in pformat\r\n    compact=compact).pformat(object)\r\n  File \"/usr/lib64/python3.7/pprint.py\", line 144, in pformat\r\n    self._format(object, sio, 0, 0, {}, 0)\r\n  File \"/usr/lib64/python3.7/pprint.py\", line 167, in _format\r\n    p(self, object, stream, indent, allowance, context, level + 1)\r\n  File \"/path/to//sklearn/utils/_pprint.py\", line 175, in _pprint_estimator\r\n    if self._indent_at_name:\r\nAttributeError: 'PrettyPrinter' object has no attribute '_indent_at_name'\r\n```\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.21.git",
    "requested_commit": "",
    "mayil_response": "- The issue is caused by the missing `_indent_at_name` attribute in the `PrettyPrinter` class used within the `_pprint_estimator` function.\n- The `PrettyPrinter` class does not have an `_indent_at_name` attribute in the standard library, so this must be a custom implementation detail within `sklearn`.\n- The solution is to modify the `CustomPrettyPrinter` class to include the `_indent_at_name` attribute or to update the `_pprint_estimator` function to handle the absence of this attribute.\n\nTo resolve the issue:\n- Locate the `CustomPrettyPrinter` class within the `sklearn.utils._pprint` module.\n- Ensure that the `_indent_at_name` attribute is properly initialized in the `__init__` method of the `CustomPrettyPrinter` class.\n- If the attribute is not used or is a remnant of an older version, update the `_pprint_estimator` function to remove any references to `_indent_at_name`.\n- If the `CustomPrettyPrinter` class does not exist, create it by subclassing `PrettyPrinter` from the `pprint` module and adding the `_indent_at_name` attribute as shown in the hypothesized documentation or code for these components.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/utils)- [Lines 1 - 56](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/utils/_pprint.py#L1-L56)\n\n This snippet contains the '_EstimatorPrettyPrinter' class where the AttributeError mentioned in the issue is occurring. The attribute '_indent_at_name' is likely part of this class and needs to be reviewed or fixed.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0004185,
        "snippet_processor": 0.08203,
        "issue_star_creation": 0.032729999999999995,
        "issue_star_solver": 0.07929000000000001,
        "bouncer": 0.030320000000000003
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711741138.4044118,
        "relevant_snippets": [
            {
                "code": "\"\"\"\n=================================================================\nSelecting dimensionality reduction with Pipeline and GridSearchCV\n=================================================================\n\nThis example constructs a pipeline that does dimensionality\nreduction followed by prediction with a support vector\nclassifier. It demonstrates the use of ``GridSearchCV`` and\n``Pipeline`` to optimize over different classes of estimators in a\nsingle CV run -- unsupervised ``PCA`` and ``NMF`` dimensionality\nreductions are compared to univariate feature selection during\nthe grid search.\n\nAdditionally, ``Pipeline`` can be instantiated with the ``memory``\nargument to memoize the transformers within the pipeline, avoiding to fit\nagain the same transformers over and over.\n\nNote that the use of ``memory`` to enable caching becomes interesting when the\nfitting of a transformer is costly.\n\n\"\"\"\n\n# Authors: Robert McGibbon\n#          Joel Nothman\n#          Guillaume Lemaitre\n\n# %%\n# Illustration of ``Pipeline`` and ``GridSearchCV``\n###############################################################################\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.decomposition import NMF, PCA\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import LinearSVC\n\nX, y = load_digits(return_X_y=True)\n\npipe = Pipeline(\n    [\n        (\"scaling\", MinMaxScaler()),\n        # the reduce_dim stage is populated by the param_grid\n        (\"reduce_dim\", \"passthrough\"),\n        (\"classify\", LinearSVC(dual=False, max_iter=10000)),\n    ]\n)\n\nN_FEATURES_OPTIONS = [2, 4, 8]\nC_OPTIONS = [1, 10, 100, 1000]\nparam_grid = [\n    {\n        \"reduce_dim\": [PCA(iterated_power=7), NMF(max_iter=1_000)],\n        \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n    {\n        \"reduce_dim\": [SelectKBest(mutual_info_classif)],\n        \"reduce_dim__k\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n]\nreducer_labels = [\"PCA\", \"NMF\", \"KBest(mutual_info_classif)\"]\n\ngrid = GridSearchCV(pipe, n_jobs=1, param_grid=param_grid)\ngrid.fit(X, y)\n\n# %%\nimport pandas as pd\n\nmean_scores = np.array(grid.cv_results_[\"mean_test_score\"])\n# scores are in the order of param_grid iteration, which is alphabetical\nmean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n# select score for best C\nmean_scores = mean_scores.max(axis=0)\n# create a dataframe to ease plotting\nmean_scores = pd.DataFrame(\n    mean_scores.T, index=N_FEATURES_OPTIONS, columns=reducer_labels\n)\n\nax = mean_scores.plot.bar()\nax.set_title(\"Comparing feature reduction techniques\")\nax.set_xlabel(\"Reduced number of features\")\nax.set_ylabel(\"Digit classification accuracy\")\nax.set_ylim((0, 1))\nax.legend(loc=\"upper left\")\n\nplt.show()\n\n# %%",
                "filename": "examples/compose/plot_compare_reduction.py",
                "start_index": 0,
                "end_index": 2977,
                "start_line": 1,
                "end_line": 126,
                "max_line": 132,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "invalid_names = [\n        (names[::-1], \"Feature names must be in the same order as they were in fit.\"),\n        (\n            [f\"another_prefix_{i}\" for i in range(n_features)],\n            (\n                \"Feature names unseen at fit time:\\n- another_prefix_0\\n-\"\n                \" another_prefix_1\\n\"\n            ),\n        ),\n        (\n            names[:3],\n            f\"Feature names seen at fit time, yet now missing:\\n- {min(names[3:])}\\n\",\n        ),\n    ]\n    params = {\n        key: value\n        for key, value in estimator.get_params().items()\n        if \"early_stopping\" in key\n    }\n    early_stopping_enabled = any(value is True for value in params.values())\n\n    for invalid_name, additional_message in invalid_names:\n        X_bad = pd.DataFrame(X, columns=invalid_name, copy=False)\n\n        expected_msg = re.escape(\n            \"The feature names should match those that were passed during fit.\\n\"\n            f\"{additional_message}\"\n        )\n        for name, method in check_methods:\n            with raises(\n                ValueError, match=expected_msg, err_msg=f\"{name} did not raise\"\n            ):\n                method(X_bad)\n\n        # partial_fit checks on second call\n        # Do not call partial fit if early_stopping is on\n        if not hasattr(estimator, \"partial_fit\") or early_stopping_enabled:\n            continue\n\n        estimator = clone(estimator_orig)\n        if is_classifier(estimator):\n            classes = np.unique(y)\n            estimator.partial_fit(X, y, classes=classes)\n        else:\n            estimator.partial_fit(X, y)\n\n        with raises(ValueError, match=expected_msg):\n            estimator.partial_fit(X_bad, y)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 146663,
                "end_index": 148346,
                "start_line": 4160,
                "end_line": 4207,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"This module contains the _EstimatorPrettyPrinter class used in\nBaseEstimator.__repr__ for pretty-printing estimators\"\"\"\n\n# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,\n# 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018 Python Software Foundation;\n# All Rights Reserved\n\n# Authors: Fred L. Drake, Jr. <fdrake@acm.org> (built-in CPython pprint module)\n#          Nicolas Hug (scikit-learn specific changes)\n\n# License: PSF License version 2 (see below)\n\n# PYTHON SOFTWARE FOUNDATION LICENSE VERSION 2\n# --------------------------------------------\n\n# 1. This LICENSE AGREEMENT is between the Python Software Foundation (\"PSF\"),\n# and the Individual or Organization (\"Licensee\") accessing and otherwise\n# using this software (\"Python\") in source or binary form and its associated\n# documentation.\n\n# 2. Subject to the terms and conditions of this License Agreement, PSF hereby\n# grants Licensee a nonexclusive, royalty-free, world-wide license to\n# reproduce, analyze, test, perform and/or display publicly, prepare\n# derivative works, distribute, and otherwise use Python alone or in any\n# derivative version, provided, however, that PSF's License Agreement and\n# PSF's notice of copyright, i.e., \"Copyright (c) 2001, 2002, 2003, 2004,\n# 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016,\n# 2017, 2018 Python Software Foundation; All Rights Reserved\" are retained in\n# Python alone or in any derivative version prepared by Licensee.\n\n# 3. In the event Licensee prepares a derivative work that is based on or\n# incorporates Python or any part thereof, and wants to make the derivative\n# work available to others as provided herein, then Licensee hereby agrees to\n# include in any such work a brief summary of the changes made to Python.\n\n# 4. PSF is making Python available to Licensee on an \"AS IS\" basis. PSF MAKES\n# NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED. BY WAY OF EXAMPLE, BUT\n# NOT LIMITATION, PSF MAKES NO AND DISCLAIMS ANY REPRESENTATION OR WARRANTY OF\n# MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF\n# PYTHON WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n\n# 5. PSF SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON FOR ANY\n# INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS A RESULT OF\n# MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON, OR ANY DERIVATIVE\n# THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n\n# 6. This License Agreement will automatically terminate upon a material\n# breach of its terms and conditions.\n\n# 7. Nothing in this License Agreement shall be deemed to create any\n# relationship of agency, partnership, or joint venture between PSF and\n# Licensee. This License Agreement does not grant permission to use PSF\n# trademarks or trade name in a trademark sense to endorse or promote products\n# or services of Licensee, or any third party.\n\n# 8. By copying, installing or otherwise using Python, Licensee agrees to be",
                "filename": "sklearn/utils/_pprint.py",
                "start_index": 0,
                "end_index": 2955,
                "start_line": 1,
                "end_line": 56,
                "max_line": 463,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\n\nX, y = load_iris(return_X_y=True)\nestimators = [\n    (\"rf\", RandomForestClassifier(n_estimators=10, random_state=42)),\n    (\"svr\", make_pipeline(StandardScaler(), LinearSVC(random_state=42))),\n]\nclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\nclf.fit(X_train, y_train).score(X_test, y_test)\n\n# %%\n# Permutation-based feature importance\n# ------------------------------------\n#\n# The :func:`inspection.permutation_importance` can be used to get an\n# estimate of the importance of each feature, for any fitted estimator:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\n\nX, y = make_classification(random_state=0, n_features=5, n_informative=3)\nfeature_names = np.array([f\"x_{i}\" for i in range(X.shape[1])])\n\nrf = RandomForestClassifier(random_state=0).fit(X, y)\nresult = permutation_importance(rf, X, y, n_repeats=10, random_state=0, n_jobs=2)\n\nfig, ax = plt.subplots()\nsorted_idx = result.importances_mean.argsort()\nax.boxplot(\n    result.importances[sorted_idx].T, vert=False, labels=feature_names[sorted_idx]\n)\nax.set_title(\"Permutation Importance of each feature\")\nax.set_ylabel(\"Features\")\nfig.tight_layout()\nplt.show()\n\n# %%\n# Native support for missing values for gradient boosting\n# -------------------------------------------------------\n#\n# The :class:`ensemble.HistGradientBoostingClassifier`\n# and :class:`ensemble.HistGradientBoostingRegressor` now have native\n# support for missing values (NaNs). This means that there is no need for\n# imputing data when training or predicting.\n\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nX = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\ny = [0, 0, 1, 1]\n\ngbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\nprint(gbdt.predict(X))\n\n# %%\n# Precomputed sparse nearest neighbors graph\n# ------------------------------------------\n# Most estimators based on nearest neighbors graphs now accept precomputed\n# sparse graphs as input, to reuse the same graph for multiple estimator fits.\n# To use this feature in a pipeline, one can use the `memory` parameter, along\n# with one of the two new transformers,\n# :class:`neighbors.KNeighborsTransformer` and\n# :class:`neighbors.RadiusNeighborsTransformer`. The precomputation\n# can also be performed by custom estimators to use alternative\n# implementations, such as approximate nearest neighbors methods.\n# See more details in the :ref:`User Guide <neighbors_transformer>`.\n\nfrom tempfile import TemporaryDirectory\n\nfrom sklearn.manifold import Isomap\nfrom sklearn.neighbors import KNeighborsTransformer",
                "filename": "examples/release_highlights/plot_release_highlights_0_22_0.py",
                "start_index": 2957,
                "end_index": 5944,
                "start_line": 44,
                "end_line": 161,
                "max_line": 282,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_dont_overwrite_parameters(name, estimator_orig):\n    # check that fit method only changes or sets private attributes\n    if hasattr(estimator_orig.__init__, \"deprecated_original\"):\n        # to not check deprecated classes\n        return\n    estimator = clone(estimator_orig)\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20, 3))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    y = X[:, 0].astype(int)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n    dict_before_fit = estimator.__dict__.copy()\n    estimator.fit(X, y)\n\n    dict_after_fit = estimator.__dict__\n\n    public_keys_after_fit = [\n        key for key in dict_after_fit.keys() if _is_public_parameter(key)\n    ]\n\n    attrs_added_by_fit = [\n        key for key in public_keys_after_fit if key not in dict_before_fit.keys()\n    ]\n\n    # check that fit doesn't add any public attribute\n    assert not attrs_added_by_fit, (\n        \"Estimator adds public attribute(s) during\"\n        \" the fit method.\"\n        \" Estimators are only allowed to add private attributes\"\n        \" either started with _ or ended\"\n        \" with _ but %s added\"\n        % \", \".join(attrs_added_by_fit)\n    )\n\n    # check that fit doesn't change any public attribute\n    attrs_changed_by_fit = [\n        key\n        for key in public_keys_after_fit\n        if (dict_before_fit[key] is not dict_after_fit[key])\n    ]\n\n    assert not attrs_changed_by_fit, (\n        \"Estimator changes public attribute(s) during\"\n        \" the fit method. Estimators are only allowed\"\n        \" to change attributes started\"\n        \" or ended with _, but\"\n        \" %s changed\"\n        % \", \".join(attrs_changed_by_fit)\n    )\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_fit2d_predict1d(name, estimator_orig):\n    # check by fitting a 2d array and predicting with a 1d array\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20, 3))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    y = X[:, 0].astype(int)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n    estimator.fit(X, y)\n\n    for method in [\"predict\", \"transform\", \"decision_function\", \"predict_proba\"]:\n        if hasattr(estimator, method):\n            assert_raise_message(\n                ValueError, \"Reshape your data\", getattr(estimator, method), X[0]\n            )",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 48853,
                "end_index": 51626,
                "start_line": 181,
                "end_line": 4630,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "from sklearn.pipeline import make_pipeline\n\nX, y = make_classification(random_state=0)\n\nwith TemporaryDirectory(prefix=\"sklearn_cache_\") as tmpdir:\n    estimator = make_pipeline(\n        KNeighborsTransformer(n_neighbors=10, mode=\"distance\"),\n        Isomap(n_neighbors=10, metric=\"precomputed\"),\n        memory=tmpdir,\n    )\n    estimator.fit(X)\n\n    # We can decrease the number of neighbors and the graph will not be\n    # recomputed.\n    estimator.set_params(isomap__n_neighbors=5)\n    estimator.fit(X)\n\n# %%\n# KNN Based Imputation\n# ------------------------------------\n# We now support imputation for completing missing values using k-Nearest\n# Neighbors.\n#\n# Each sample's missing values are imputed using the mean value from\n# ``n_neighbors`` nearest neighbors found in the training set. Two samples are\n# close if the features that neither is missing are close.\n# By default, a euclidean distance metric\n# that supports missing values,\n# :func:`~sklearn.metrics.pairwise.nan_euclidean_distances`, is used to find the nearest\n# neighbors.\n#\n# Read more in the :ref:`User Guide <knnimpute>`.\n\nfrom sklearn.impute import KNNImputer\n\nX = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\nimputer = KNNImputer(n_neighbors=2)\nprint(imputer.fit_transform(X))\n\n# %%\n# Tree pruning\n# ------------\n#\n# It is now possible to prune most tree-based estimators once the trees are\n# built. The pruning is based on minimal cost-complexity. Read more in the\n# :ref:`User Guide <minimal_cost_complexity_pruning>` for details.\n\nX, y = make_classification(random_state=0)\n\nrf = RandomForestClassifier(random_state=0, ccp_alpha=0).fit(X, y)\nprint(\n    \"Average number of nodes without pruning {:.1f}\".format(\n        np.mean([e.tree_.node_count for e in rf.estimators_])\n    )\n)\n\nrf = RandomForestClassifier(random_state=0, ccp_alpha=0.05).fit(X, y)\nprint(\n    \"Average number of nodes with pruning {:.1f}\".format(\n        np.mean([e.tree_.node_count for e in rf.estimators_])\n    )\n)\n\n# %%\n# Retrieve dataframes from OpenML\n# -------------------------------\n# :func:`datasets.fetch_openml` can now return pandas dataframe and thus\n# properly handle datasets with heterogeneous data:\n\nfrom sklearn.datasets import fetch_openml\n\ntitanic = fetch_openml(\"titanic\", version=1, as_frame=True, parser=\"pandas\")\nprint(titanic.data.head()[[\"pclass\", \"embarked\"]])\n\n# %%\n# Checking scikit-learn compatibility of an estimator\n# ---------------------------------------------------\n# Developers can check the compatibility of their scikit-learn compatible\n# estimators using :func:`~utils.estimator_checks.check_estimator`. For\n# instance, the ``check_estimator(LinearSVC())`` passes.\n#\n# We now provide a ``pytest`` specific decorator which allows ``pytest``\n# to run all checks independently and report the checks that are failing.\n#\n# ..note::\n#   This entry was slightly updated in version 0.24, where passing classes\n#   isn't supported anymore: pass instances instead.",
                "filename": "examples/release_highlights/plot_release_highlights_0_22_0.py",
                "start_index": 5945,
                "end_index": 8900,
                "start_line": 85,
                "end_line": 248,
                "max_line": 282,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "@ignore_warnings\ndef check_pipeline_consistency(name, estimator_orig):\n    if _safe_tags(estimator_orig, key=\"non_deterministic\"):\n        msg = name + \" is non deterministic\"\n        raise SkipTest(msg)\n\n    # check that make_pipeline(est) gives same score as est\n    X, y = make_blobs(\n        n_samples=30,\n        centers=[[0, 0, 0], [1, 1, 1]],\n        random_state=0,\n        n_features=2,\n        cluster_std=0.1,\n    )\n    X = _enforce_estimator_tags_X(estimator_orig, X, kernel=rbf_kernel)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n    set_random_state(estimator)\n    pipeline = make_pipeline(estimator)\n    estimator.fit(X, y)\n    pipeline.fit(X, y)\n\n    funcs = [\"score\", \"fit_transform\"]\n\n    for func_name in funcs:\n        func = getattr(estimator, func_name, None)\n        if func is not None:\n            func_pipeline = getattr(pipeline, func_name)\n            result = func(X, y)\n            result_pipe = func_pipeline(X, y)\n            assert_allclose_dense_sparse(result, result_pipe)\n\n\n@ignore_warnings\ndef check_fit_score_takes_y(name, estimator_orig):\n    # check that all estimators accept an optional y\n    # in fit and score so they can be used in pipelines\n    rnd = np.random.RandomState(0)\n    n_samples = 30\n    X = rnd.uniform(size=(n_samples, 3))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    y = np.arange(n_samples) % 3\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n    set_random_state(estimator)\n\n    funcs = [\"fit\", \"score\", \"partial_fit\", \"fit_predict\", \"fit_transform\"]\n    for func_name in funcs:\n        func = getattr(estimator, func_name, None)\n        if func is not None:\n            func(X, y)\n            args = [p.name for p in signature(func).parameters.values()]\n            if args[0] == \"self\":\n                # available_if makes methods into functions\n                # with an explicit \"self\", so need to shift arguments\n                args = args[1:]\n            assert args[1] in [\"y\", \"Y\"], (\n                \"Expected y or Y as second argument for method \"\n                \"%s of %s. Got arguments: %r.\"\n                % (func_name, type(estimator).__name__, args)\n            )",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 62859,
                "end_index": 65095,
                "start_line": 181,
                "end_line": 4630,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "try:\n        import pandas as pd\n    except ImportError:\n        raise SkipTest(\n            \"pandas is not installed: not checking column name consistency for pandas\"\n        )\n\n    tags = _safe_tags(estimator_orig)\n    is_supported_X_types = (\n        \"2darray\" in tags[\"X_types\"] or \"categorical\" in tags[\"X_types\"]\n    )\n\n    if not is_supported_X_types or tags[\"no_validation\"]:\n        return\n\n    rng = np.random.RandomState(0)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n\n    X_orig = rng.normal(size=(150, 8))\n\n    X_orig = _enforce_estimator_tags_X(estimator, X_orig)\n    n_samples, n_features = X_orig.shape\n\n    names = np.array([f\"col_{i}\" for i in range(n_features)])\n    X = pd.DataFrame(X_orig, columns=names, copy=False)\n\n    if is_regressor(estimator):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(low=0, high=2, size=n_samples)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    # Check that calling `fit` does not raise any warnings about feature names.\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"error\",\n            message=\"X does not have valid feature names\",\n            category=UserWarning,\n            module=\"sklearn\",\n        )\n        estimator.fit(X, y)\n\n    if not hasattr(estimator, \"feature_names_in_\"):\n        raise ValueError(\n            \"Estimator does not have a feature_names_in_ \"\n            \"attribute after fitting with a dataframe\"\n        )\n    assert isinstance(estimator.feature_names_in_, np.ndarray)\n    assert estimator.feature_names_in_.dtype == object\n    assert_array_equal(estimator.feature_names_in_, names)\n\n    # Only check sklearn estimators for feature_names_in_ in docstring\n    module_name = estimator_orig.__module__\n    if (\n        module_name.startswith(\"sklearn.\")\n        and not (\"test_\" in module_name or module_name.endswith(\"_testing\"))\n        and (\"feature_names_in_\" not in (estimator_orig.__doc__))\n    ):\n        raise ValueError(\n            f\"Estimator {name} does not document its feature_names_in_ attribute\"\n        )\n\n    check_methods = []\n    for method in (\n        \"predict\",\n        \"transform\",\n        \"decision_function\",\n        \"predict_proba\",\n        \"score\",\n        \"score_samples\",\n        \"predict_log_proba\",\n    ):\n        if not hasattr(estimator, method):\n            continue\n\n        callable_method = getattr(estimator, method)\n        if method == \"score\":\n            callable_method = partial(callable_method, y=y)\n        check_methods.append((method, callable_method))\n\n    for _, method in check_methods:\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"error\",\n                message=\"X does not have valid feature names\",\n                category=UserWarning,\n                module=\"sklearn\",\n            )\n            method(X)  # works without UserWarning for valid features",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 143722,
                "end_index": 146657,
                "start_line": 633,
                "end_line": 4158,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "def check_fit_check_is_fitted(name, estimator_orig):\n    # Make sure that estimator doesn't pass check_is_fitted before calling fit\n    # and that passes check_is_fitted once it's fit.\n\n    rng = np.random.RandomState(42)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    if \"warm_start\" in estimator.get_params():\n        estimator.set_params(warm_start=False)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n    if is_regressor(estimator_orig):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(low=0, high=2, size=n_samples)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if not _safe_tags(estimator).get(\"stateless\", False):\n        # stateless estimators (such as FunctionTransformer) are always \"fit\"!\n        try:\n            check_is_fitted(estimator)\n            raise AssertionError(\n                f\"{estimator.__class__.__name__} passes check_is_fitted before being\"\n                \" fit!\"\n            )\n        except NotFittedError:\n            pass\n    estimator.fit(X, y)\n    try:\n        check_is_fitted(estimator)\n    except NotFittedError as e:\n        raise NotFittedError(\n            \"Estimator fails to pass `check_is_fitted` even though it has been fit.\"\n        ) from e\n\n\ndef check_n_features_in(name, estimator_orig):\n    # Make sure that n_features_in_ attribute doesn't exist until fit is\n    # called, and that its value is correct.\n\n    rng = np.random.RandomState(0)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    if \"warm_start\" in estimator.get_params():\n        estimator.set_params(warm_start=False)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n    if is_regressor(estimator_orig):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(low=0, high=2, size=n_samples)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    assert not hasattr(estimator, \"n_features_in_\")\n    estimator.fit(X, y)\n    assert hasattr(estimator, \"n_features_in_\")\n    assert estimator.n_features_in_ == X.shape[1]\n\n\ndef check_requires_y_none(name, estimator_orig):\n    # Make sure that an estimator with requires_y=True fails gracefully when\n    # given y=None\n\n    rng = np.random.RandomState(0)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n\n    expected_err_msgs = (\n        \"requires y to be passed, but the target y is None\",\n        \"Expected array-like (array or non-string sequence), got None\",\n        \"y should be a 1d array\",\n    )\n\n    try:\n        estimator.fit(X, None)\n    except ValueError as ve:\n        if not any(msg in str(ve) for msg in expected_err_msgs):\n            raise ve",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 138164,
                "end_index": 141063,
                "start_line": 3892,
                "end_line": 3980,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "def check_outliers_fit_predict(name, estimator_orig):\n    # Check fit_predict for outlier detectors.\n\n    n_samples = 300\n    X, _ = make_blobs(n_samples=n_samples, random_state=0)\n    X = shuffle(X, random_state=7)\n    n_samples, n_features = X.shape\n    estimator = clone(estimator_orig)\n\n    set_random_state(estimator)\n\n    y_pred = estimator.fit_predict(X)\n    assert y_pred.shape == (n_samples,)\n    assert y_pred.dtype.kind == \"i\"\n    assert_array_equal(np.unique(y_pred), np.array([-1, 1]))\n\n    # check fit_predict = fit.predict when the estimator has both a predict and\n    # a fit_predict method. recall that it is already assumed here that the\n    # estimator has a fit_predict method\n    if hasattr(estimator, \"predict\"):\n        y_pred_2 = estimator.fit(X).predict(X)\n        assert_array_equal(y_pred, y_pred_2)\n\n    if hasattr(estimator, \"contamination\"):\n        # proportion of outliers equal to contamination parameter when not\n        # set to 'auto'\n        expected_outliers = 30\n        contamination = float(expected_outliers) / n_samples\n        estimator.set_params(contamination=contamination)\n        y_pred = estimator.fit_predict(X)\n\n        num_outliers = np.sum(y_pred != 1)\n        # num_outliers should be equal to expected_outliers unless\n        # there are ties in the decision_function values. this can\n        # only be tested for estimators with a decision_function\n        # method\n        if num_outliers != expected_outliers and hasattr(\n            estimator, \"decision_function\"\n        ):\n            decision = estimator.decision_function(X)\n            check_outlier_corruption(num_outliers, expected_outliers, decision)\n\n\ndef check_fit_non_negative(name, estimator_orig):\n    # Check that proper warning is raised for non-negative X\n    # when tag requires_positive_X is present\n    X = np.array([[-1.0, 1], [-1.0, 1]])\n    y = np.array([1, 2])\n    estimator = clone(estimator_orig)\n    with raises(ValueError):\n        estimator.fit(X, y)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 134030,
                "end_index": 136018,
                "start_line": 3781,
                "end_line": 4110,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/utils/_pprint.py": [
                {
                    "chunk": {
                        "code": "\"\"\"This module contains the _EstimatorPrettyPrinter class used in\nBaseEstimator.__repr__ for pretty-printing estimators\"\"\"\n\n# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,\n# 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018 Python Software Foundation;\n# All Rights Reserved\n\n# Authors: Fred L. Drake, Jr. <fdrake@acm.org> (built-in CPython pprint module)\n#          Nicolas Hug (scikit-learn specific changes)\n\n# License: PSF License version 2 (see below)\n\n# PYTHON SOFTWARE FOUNDATION LICENSE VERSION 2\n# --------------------------------------------\n\n# 1. This LICENSE AGREEMENT is between the Python Software Foundation (\"PSF\"),\n# and the Individual or Organization (\"Licensee\") accessing and otherwise\n# using this software (\"Python\") in source or binary form and its associated\n# documentation.\n\n# 2. Subject to the terms and conditions of this License Agreement, PSF hereby\n# grants Licensee a nonexclusive, royalty-free, world-wide license to\n# reproduce, analyze, test, perform and/or display publicly, prepare\n# derivative works, distribute, and otherwise use Python alone or in any\n# derivative version, provided, however, that PSF's License Agreement and\n# PSF's notice of copyright, i.e., \"Copyright (c) 2001, 2002, 2003, 2004,\n# 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016,\n# 2017, 2018 Python Software Foundation; All Rights Reserved\" are retained in\n# Python alone or in any derivative version prepared by Licensee.\n\n# 3. In the event Licensee prepares a derivative work that is based on or\n# incorporates Python or any part thereof, and wants to make the derivative\n# work available to others as provided herein, then Licensee hereby agrees to\n# include in any such work a brief summary of the changes made to Python.\n\n# 4. PSF is making Python available to Licensee on an \"AS IS\" basis. PSF MAKES\n# NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED. BY WAY OF EXAMPLE, BUT\n# NOT LIMITATION, PSF MAKES NO AND DISCLAIMS ANY REPRESENTATION OR WARRANTY OF\n# MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF\n# PYTHON WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n\n# 5. PSF SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON FOR ANY\n# INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS A RESULT OF\n# MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON, OR ANY DERIVATIVE\n# THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n\n# 6. This License Agreement will automatically terminate upon a material\n# breach of its terms and conditions.\n\n# 7. Nothing in this License Agreement shall be deemed to create any\n# relationship of agency, partnership, or joint venture between PSF and\n# Licensee. This License Agreement does not grant permission to use PSF\n# trademarks or trade name in a trademark sense to endorse or promote products\n# or services of Licensee, or any third party.\n\n# 8. By copying, installing or otherwise using Python, Licensee agrees to be",
                        "filename": "sklearn/utils/_pprint.py",
                        "start_index": 0,
                        "end_index": 2955,
                        "start_line": 1,
                        "end_line": 56,
                        "max_line": 463,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the '_EstimatorPrettyPrinter' class where the AttributeError mentioned in the issue is occurring. The attribute '_indent_at_name' is likely part of this class and needs to be reviewed or fixed."
                }
            ]
        },
        "solution_task": "- The issue is caused by the missing `_indent_at_name` attribute in the `PrettyPrinter` class used within the `_pprint_estimator` function.\n- The `PrettyPrinter` class does not have an `_indent_at_name` attribute in the standard library, so this must be a custom implementation detail within `sklearn`.\n- The solution is to modify the `CustomPrettyPrinter` class to include the `_indent_at_name` attribute or to update the `_pprint_estimator` function to handle the absence of this attribute.\n\nTo resolve the issue:\n- Locate the `CustomPrettyPrinter` class within the `sklearn.utils._pprint` module.\n- Ensure that the `_indent_at_name` attribute is properly initialized in the `__init__` method of the `CustomPrettyPrinter` class.\n- If the attribute is not used or is a remnant of an older version, update the `_pprint_estimator` function to remove any references to `_indent_at_name`.\n- If the `CustomPrettyPrinter` class does not exist, create it by subclassing `PrettyPrinter` from the `pprint` module and adding the `_indent_at_name` attribute as shown in the hypothesized documentation or code for these components.",
        "bouncer_response": {
            "score": 8,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE accurately identifies the cause of the AttributeError as a missing attribute in the PrettyPrinter class within the sklearn library. It provides a clear and actionable solution by suggesting to locate and modify the CustomPrettyPrinter class or update the _pprint_estimator function. The RESPONSE also includes a direct reference to the relevant code snippets, which is highly useful for debugging. The RESPONSE does not address all aspects of the bug, such as the potential need to review the entire stack trace to ensure there are no other underlying issues, but it does provide a strong starting point for resolving the primary issue.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711741362.149295,
        "result": "- The issue is caused by the missing `_indent_at_name` attribute in the `PrettyPrinter` class used within the `_pprint_estimator` function.\n- The `PrettyPrinter` class does not have an `_indent_at_name` attribute in the standard library, so this must be a custom implementation detail within `sklearn`.\n- The solution is to modify the `CustomPrettyPrinter` class to include the `_indent_at_name` attribute or to update the `_pprint_estimator` function to handle the absence of this attribute.\n\nTo resolve the issue:\n- Locate the `CustomPrettyPrinter` class within the `sklearn.utils._pprint` module.\n- Ensure that the `_indent_at_name` attribute is properly initialized in the `__init__` method of the `CustomPrettyPrinter` class.\n- If the attribute is not used or is a remnant of an older version, update the `_pprint_estimator` function to remove any references to `_indent_at_name`.\n- If the `CustomPrettyPrinter` class does not exist, create it by subclassing `PrettyPrinter` from the `pprint` module and adding the `_indent_at_name` attribute as shown in the hypothesized documentation or code for these components.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/utils)- [Lines 1 - 56](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/utils/_pprint.py#L1-L56)\n\n This snippet contains the '_EstimatorPrettyPrinter' class where the AttributeError mentioned in the issue is occurring. The attribute '_indent_at_name' is likely part of this class and needs to be reviewed or fixed."
    }
}