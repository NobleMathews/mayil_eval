{
    "id": "scikit-learn__scikit-learn-10870",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
    "task_id": 7721274150615559935,
    "title": "In Gaussian mixtures, when n_init > 1, the lower_bound_ is not always the max",
    "body": "#### Description\r\nIn Gaussian mixtures, when `n_init` is set to any value greater than 1, the `lower_bound_` is not the max lower bound across all initializations, but just the lower bound of the last initialization.\r\n\r\nThe bug can be fixed by adding the following line just before `return self` in `BaseMixture.fit()`:\r\n\r\n```python\r\nself.lower_bound_ = max_lower_bound\r\n```\r\n\r\nThe test that should have caught this bug is `test_init()` in `mixture/tests/test_gaussian_mixture.py`, but it just does a single test, so it had a 50% chance of missing the issue. It should be updated to try many random states.\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nimport numpy as np\r\nfrom sklearn.mixture import GaussianMixture\r\n\r\nX = np.random.rand(1000, 10)\r\nfor random_state in range(100):\r\n    gm1 = GaussianMixture(n_components=2, n_init=1, random_state=random_state).fit(X)\r\n    gm2 = GaussianMixture(n_components=2, n_init=10, random_state=random_state).fit(X)\r\n    assert gm2.lower_bound_ > gm1.lower_bound_, random_state\r\n```\r\n\r\n#### Expected Results\r\nNo error.\r\n\r\n#### Actual Results\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\nAssertionError: 4\r\n```\r\n\r\n#### Versions\r\n\r\n```\r\n>>> import platform; print(platform.platform())\r\nDarwin-17.4.0-x86_64-i386-64bit\r\n>>> import sys; print(\"Python\", sys.version)\r\nPython 3.6.4 (default, Dec 21 2017, 20:33:21)\r\n[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.38)]\r\n>>> import numpy; print(\"NumPy\", numpy.__version__)\r\nNumPy 1.14.2\r\n>>> import scipy; print(\"SciPy\", scipy.__version__)\r\nSciPy 1.0.0\r\n>>> import sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\nScikit-Learn 0.19.1\r\n```\nIn Gaussian mixtures, when n_init > 1, the lower_bound_ is not always the max\n#### Description\r\nIn Gaussian mixtures, when `n_init` is set to any value greater than 1, the `lower_bound_` is not the max lower bound across all initializations, but just the lower bound of the last initialization.\r\n\r\nThe bug can be fixed by adding the following line just before `return self` in `BaseMixture.fit()`:\r\n\r\n```python\r\nself.lower_bound_ = max_lower_bound\r\n```\r\n\r\nThe test that should have caught this bug is `test_init()` in `mixture/tests/test_gaussian_mixture.py`, but it just does a single test, so it had a 50% chance of missing the issue. It should be updated to try many random states.\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nimport numpy as np\r\nfrom sklearn.mixture import GaussianMixture\r\n\r\nX = np.random.rand(1000, 10)\r\nfor random_state in range(100):\r\n    gm1 = GaussianMixture(n_components=2, n_init=1, random_state=random_state).fit(X)\r\n    gm2 = GaussianMixture(n_components=2, n_init=10, random_state=random_state).fit(X)\r\n    assert gm2.lower_bound_ > gm1.lower_bound_, random_state\r\n```\r\n\r\n#### Expected Results\r\nNo error.\r\n\r\n#### Actual Results\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 4, in <module>\r\nAssertionError: 4\r\n```\r\n\r\n#### Versions\r\n\r\n```\r\n>>> import platform; print(platform.platform())\r\nDarwin-17.4.0-x86_64-i386-64bit\r\n>>> import sys; print(\"Python\", sys.version)\r\nPython 3.6.4 (default, Dec 21 2017, 20:33:21)\r\n[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.38)]\r\n>>> import numpy; print(\"NumPy\", numpy.__version__)\r\nNumPy 1.14.2\r\n>>> import scipy; print(\"SciPy\", scipy.__version__)\r\nSciPy 1.0.0\r\n>>> import sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\nScikit-Learn 0.19.1\r\n```\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.20.git",
    "requested_commit": "",
    "mayil_response": "- In `sklearn/mixture/_gaussian_mixture.py`, update the `fit` method of the `GaussianMixture` class to store the maximum lower bound across all initializations:\n\n```python\nclass GaussianMixture(BaseMixture):\n    # ...\n    def fit(self, X, y=None):\n        # ...\n        max_lower_bound = -np.infty\n        self.converged_ = False\n\n        for init in range(self.n_init):\n            # ...\n            # Current lower bound computation\n            current_lower_bound = ...\n            # ...\n            if current_lower_bound > max_lower_bound:\n                max_lower_bound = current_lower_bound\n                # Store other parameters if necessary\n                # ...\n\n        self.lower_bound_ = max_lower_bound\n        return self\n```\n\n- In `mixture/tests/test_gaussian_mixture.py`, update the `test_init` method to try multiple random states:\n\n```python\nclass TestGaussianMixture:\n    def test_init(self):\n        # ...\n        random_states = [np.random.RandomState(i) for i in range(10)]\n        for random_state in random_states:\n            # Create a GaussianMixture instance with the random_state\n            # Perform fitting and other necessary assertions\n            # ...\n```\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [mixture](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/mixture)- [Lines 638 - 797](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/mixture/_gaussian_mixture.py#L638-L797)\n\n This snippet contains the '__init__' method of the 'GaussianMixture' class, which includes the 'n_init' parameter that is directly related to the issue. \n\n\n   - [Lines 455 - 455](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/mixture/_gaussian_mixture.py#L455-L455)\n\n This snippet marks the beginning of the 'GaussianMixture' class definition, which is relevant to the issue as the bug is within this class. \n\n\n   - [Lines 1 - 96](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/mixture/_gaussian_mixture.py#L1-L96)\n\n This snippet contains helper functions used by the 'GaussianMixture' class to check the parameters. It might be necessary to review these functions to ensure that the 'max_lower_bound' is being calculated and used correctly.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.000641,
        "snippet_processor": 0.08109000000000001,
        "issue_star_creation": 0.027979999999999998,
        "issue_star_solver": 0.06982000000000001,
        "bouncer": 0.03111
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711742308.281728,
        "relevant_snippets": [
            {
                "code": "\"\"\"\n============================================================\nEmpirical evaluation of the impact of k-means initialization\n============================================================\n\nEvaluate the ability of k-means initializations strategies to make\nthe algorithm convergence robust, as measured by the relative standard\ndeviation of the inertia of the clustering (i.e. the sum of squared\ndistances to the nearest cluster center).\n\nThe first plot shows the best inertia reached for each combination\nof the model (``KMeans`` or ``MiniBatchKMeans``), and the init method\n(``init=\"random\"`` or ``init=\"k-means++\"``) for increasing values of the\n``n_init`` parameter that controls the number of initializations.\n\nThe second plot demonstrates one single run of the ``MiniBatchKMeans``\nestimator using a ``init=\"random\"`` and ``n_init=1``. This run leads to\na bad convergence (local optimum), with estimated centers stuck\nbetween ground truth clusters.\n\nThe dataset used for evaluation is a 2D grid of isotropic Gaussian\nclusters widely spaced.\n\n\"\"\"\n\n# Author: Olivier Grisel <olivier.grisel@ensta.org>\n# License: BSD 3 clause\n\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\nfrom sklearn.utils import check_random_state, shuffle\n\nrandom_state = np.random.RandomState(0)\n\n# Number of run (with randomly generated dataset) for each strategy so as\n# to be able to compute an estimate of the standard deviation\nn_runs = 5\n\n# k-means models can do several random inits so as to be able to trade\n# CPU time for convergence robustness\nn_init_range = np.array([1, 5, 10, 15, 20])\n\n# Datasets generation parameters\nn_samples_per_center = 100\ngrid_size = 3\nscale = 0.1\nn_clusters = grid_size**2\n\n\ndef make_data(random_state, n_samples_per_center, grid_size, scale):\n    random_state = check_random_state(random_state)\n    centers = np.array([[i, j] for i in range(grid_size) for j in range(grid_size)])\n    n_clusters_true, n_features = centers.shape\n\n    noise = random_state.normal(\n        scale=scale, size=(n_samples_per_center, centers.shape[1])\n    )\n\n    X = np.concatenate([c + noise for c in centers])\n    y = np.concatenate([[i] * n_samples_per_center for i in range(n_clusters_true)])\n    return shuffle(X, y, random_state=random_state)\n\n\n# Part 1: Quantitative evaluation of various init methods\n\n\nplt.figure()\nplots = []\nlegends = []\n\ncases = [\n    (KMeans, \"k-means++\", {}, \"^-\"),\n    (KMeans, \"random\", {}, \"o-\"),\n    (MiniBatchKMeans, \"k-means++\", {\"max_no_improvement\": 3}, \"x-\"),\n    (MiniBatchKMeans, \"random\", {\"max_no_improvement\": 3, \"init_size\": 500}, \"d-\"),\n]",
                "filename": "examples/cluster/plot_kmeans_stability_low_dim_dense.py",
                "start_index": 0,
                "end_index": 2657,
                "start_line": 1,
                "end_line": 122,
                "max_line": 132,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"\n==========================\nGMM Initialization Methods\n==========================\n\nExamples of the different methods of initialization in Gaussian Mixture Models\n\nSee :ref:`gmm` for more information on the estimator.\n\nHere we generate some sample data with four easy to identify clusters. The\npurpose of this example is to show the four different methods for the\ninitialization parameter *init_param*.\n\nThe four initializations are *kmeans* (default), *random*, *random_from_data* and\n*k-means++*.\n\nOrange diamonds represent the initialization centers for the gmm generated by\nthe *init_param*. The rest of the data is represented as crosses and the\ncolouring represents the eventual associated classification after the GMM has\nfinished.\n\nThe numbers in the top right of each subplot represent the number of\niterations taken for the GaussianMixture to converge and the relative time\ntaken for the initialization part of the algorithm to run. The shorter\ninitialization times tend to have a greater number of iterations to converge.\n\nThe initialization time is the ratio of the time taken for that method versus\nthe time taken for the default *kmeans* method. As you can see all three\nalternative methods take less time to initialize when compared to *kmeans*.\n\nIn this example, when initialized with *random_from_data* or *random* the model takes\nmore iterations to converge. Here *k-means++* does a good job of both low\ntime to initialize and low number of GaussianMixture iterations to converge.\n\"\"\"\n\n\n# Author: Gordon Walsh <gordon.p.walsh@gmail.com>\n# Data generation code from Jake Vanderplas <vanderplas@astro.washington.edu>\n\nfrom timeit import default_timer as timer\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets._samples_generator import make_blobs\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.utils.extmath import row_norms\n\nprint(__doc__)\n\n# Generate some data\n\nX, y_true = make_blobs(n_samples=4000, centers=4, cluster_std=0.60, random_state=0)\nX = X[:, ::-1]\n\nn_samples = 4000\nn_components = 4\nx_squared_norms = row_norms(X, squared=True)\n\n\ndef get_initial_means(X, init_params, r):\n    # Run a GaussianMixture with max_iter=0 to output the initialization means\n    gmm = GaussianMixture(\n        n_components=4, init_params=init_params, tol=1e-9, max_iter=0, random_state=r\n    ).fit(X)\n    return gmm.means_\n\n\nmethods = [\"kmeans\", \"random_from_data\", \"k-means++\", \"random\"]\ncolors = [\"navy\", \"turquoise\", \"cornflowerblue\", \"darkorange\"]\ntimes_init = {}\nrelative_times = {}\n\nplt.figure(figsize=(4 * len(methods) // 2, 6))\nplt.subplots_adjust(\n    bottom=0.1, top=0.9, hspace=0.15, wspace=0.05, left=0.05, right=0.95\n)",
                "filename": "examples/mixture/plot_gmm_init.py",
                "start_index": 0,
                "end_index": 2675,
                "start_line": 1,
                "end_line": 111,
                "max_line": 111,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "_parameter_constraints: dict = {\n        **BaseMixture._parameter_constraints,\n        \"covariance_type\": [StrOptions({\"full\", \"tied\", \"diag\", \"spherical\"})],\n        \"weights_init\": [\"array-like\", None],\n        \"means_init\": [\"array-like\", None],\n        \"precisions_init\": [\"array-like\", None],\n    }\n\n    def __init__(\n        self,\n        n_components=1,\n        *,\n        covariance_type=\"full\",\n        tol=1e-3,\n        reg_covar=1e-6,\n        max_iter=100,\n        n_init=1,\n        init_params=\"kmeans\",\n        weights_init=None,\n        means_init=None,\n        precisions_init=None,\n        random_state=None,\n        warm_start=False,\n        verbose=0,\n        verbose_interval=10,\n    ):\n        super().__init__(\n            n_components=n_components,\n            tol=tol,\n            reg_covar=reg_covar,\n            max_iter=max_iter,\n            n_init=n_init,\n            init_params=init_params,\n            random_state=random_state,\n            warm_start=warm_start,\n            verbose=verbose,\n            verbose_interval=verbose_interval,\n        )\n\n        self.covariance_type = covariance_type\n        self.weights_init = weights_init\n        self.means_init = means_init\n        self.precisions_init = precisions_init\n\n    def _check_parameters(self, X):\n        \"\"\"Check the Gaussian mixture parameters are well defined.\"\"\"\n        _, n_features = X.shape\n\n        if self.weights_init is not None:\n            self.weights_init = _check_weights(self.weights_init, self.n_components)\n\n        if self.means_init is not None:\n            self.means_init = _check_means(\n                self.means_init, self.n_components, n_features\n            )\n\n        if self.precisions_init is not None:\n            self.precisions_init = _check_precisions(\n                self.precisions_init,\n                self.covariance_type,\n                self.n_components,\n                n_features,\n            )",
                "filename": "sklearn/mixture/_gaussian_mixture.py",
                "start_index": 22157,
                "end_index": 24091,
                "start_line": 638,
                "end_line": 797,
                "max_line": 851,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"Gaussian Mixture Model.\"\"\"\n\n# Author: Wei Xue <xuewei4d@gmail.com>\n# Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom scipy import linalg\n\nfrom ..utils import check_array\nfrom ..utils._param_validation import StrOptions\nfrom ..utils.extmath import row_norms\nfrom ._base import BaseMixture, _check_shape\n\n###############################################################################\n# Gaussian mixture shape checkers used by the GaussianMixture class\n\n\ndef _check_weights(weights, n_components):\n    \"\"\"Check the user provided 'weights'.\n\n    Parameters\n    ----------\n    weights : array-like of shape (n_components,)\n        The proportions of components of each mixture.\n\n    n_components : int\n        Number of components.\n\n    Returns\n    -------\n    weights : array, shape (n_components,)\n    \"\"\"\n    weights = check_array(weights, dtype=[np.float64, np.float32], ensure_2d=False)\n    _check_shape(weights, (n_components,), \"weights\")\n\n    # check range\n    if any(np.less(weights, 0.0)) or any(np.greater(weights, 1.0)):\n        raise ValueError(\n            \"The parameter 'weights' should be in the range \"\n            \"[0, 1], but got max value %.5f, min value %.5f\"\n            % (np.min(weights), np.max(weights))\n        )\n\n    # check normalization\n    if not np.allclose(np.abs(1.0 - np.sum(weights)), 0.0):\n        raise ValueError(\n            \"The parameter 'weights' should be normalized, but got sum(weights) = %.5f\"\n            % np.sum(weights)\n        )\n    return weights\n\n\ndef _check_means(means, n_components, n_features):\n    \"\"\"Validate the provided 'means'.\n\n    Parameters\n    ----------\n    means : array-like of shape (n_components, n_features)\n        The centers of the current components.\n\n    n_components : int\n        Number of components.\n\n    n_features : int\n        Number of features.\n\n    Returns\n    -------\n    means : array, (n_components, n_features)\n    \"\"\"\n    means = check_array(means, dtype=[np.float64, np.float32], ensure_2d=False)\n    _check_shape(means, (n_components, n_features), \"means\")\n    return means\n\n\ndef _check_precision_positivity(precision, covariance_type):\n    \"\"\"Check a precision vector is positive-definite.\"\"\"\n    if np.any(np.less_equal(precision, 0.0)):\n        raise ValueError(\"'%s precision' should be positive\" % covariance_type)\n\n\ndef _check_precision_matrix(precision, covariance_type):\n    \"\"\"Check a precision matrix is symmetric and positive-definite.\"\"\"\n    if not (\n        np.allclose(precision, precision.T) and np.all(linalg.eigvalsh(precision) > 0.0)\n    ):\n        raise ValueError(\n            \"'%s precision' should be symmetric, positive-definite\" % covariance_type\n        )\n\n\ndef _check_precisions_full(precisions, covariance_type):\n    \"\"\"Check the precision matrices are symmetric and positive-definite.\"\"\"\n    for prec in precisions:\n        _check_precision_matrix(prec, covariance_type)",
                "filename": "sklearn/mixture/_gaussian_mixture.py",
                "start_index": 0,
                "end_index": 2959,
                "start_line": 1,
                "end_line": 96,
                "max_line": 851,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "default='dirichlet_process'\n        String describing the type of the weight concentration prior.\n\n    weight_concentration_prior : float or None, default=None\n        The dirichlet concentration of each component on the weight\n        distribution (Dirichlet). This is commonly called gamma in the\n        literature. The higher concentration puts more mass in\n        the center and will lead to more components being active, while a lower\n        concentration parameter will lead to more mass at the edge of the\n        mixture weights simplex. The value of the parameter must be greater\n        than 0. If it is None, it's set to ``1. / n_components``.\n\n    mean_precision_prior : float or None, default=None\n        The precision prior on the mean distribution (Gaussian).\n        Controls the extent of where means can be placed. Larger\n        values concentrate the cluster means around `mean_prior`.\n        The value of the parameter must be greater than 0.\n        If it is None, it is set to 1.\n\n    mean_prior : array-like, shape (n_features,), default=None\n        The prior on the mean distribution (Gaussian).\n        If it is None, it is set to the mean of X.\n\n    degrees_of_freedom_prior : float or None, default=None\n        The prior of the number of degrees of freedom on the covariance\n        distributions (Wishart). If it is None, it's set to `n_features`.\n\n    covariance_prior : float or array-like, default=None\n        The prior on the covariance distribution (Wishart).\n        If it is None, the emiprical covariance prior is initialized using the\n        covariance of X. The shape depends on `covariance_type`::\n\n                (n_features, n_features) if 'full',\n                (n_features, n_features) if 'tied',\n                (n_features)             if 'diag',\n                float                    if 'spherical'\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given to the method chosen to initialize the\n        parameters (see `init_params`).\n        In addition, it controls the generation of random samples from the\n        fitted distribution (see the method `sample`).\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    warm_start : bool, default=False\n        If 'warm_start' is True, the solution of the last fitting is used as\n        initialization for the next call of fit(). This can speed up\n        convergence when fit is called several times on similar problems.\n        See :term:`the Glossary <warm_start>`.\n\n    verbose : int, default=0\n        Enable verbose output. If 1 then it prints the current\n        initialization and each iteration step. If greater than 1 then\n        it prints also the log probability and the time needed\n        for each step.\n\n    verbose_interval : int, default=10\n        Number of iteration done before the next print.\n\n    Attributes\n    ----------\n    weights_ : array-like of shape (n_components,)\n        The weights of each mixture components.\n\n    means_ : array-like of shape (n_components, n_features)\n        The mean of each mixture component.\n\n    covariances_ : array-like\n        The covariance of each mixture component.\n        The shape depends on `covariance_type`::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_ : array-like\n        The precision matrices for each component in the mixture. A precision\n        matrix is the inverse of a covariance matrix. A covariance matrix is\n        symmetric positive definite so the mixture of Gaussian can be\n        equivalently parameterized by the precision matrices. Storing the\n        precision matrices instead of the covariance matrices makes it more\n        efficient to compute the log-likelihood of new samples at test time.\n        The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    precisions_cholesky_ : array-like\n        The cholesky decomposition of the precision matrices of each mixture\n        component. A precision matrix is the inverse of a covariance matrix.\n        A covariance matrix is symmetric positive definite so the mixture of\n        Gaussian can be equivalently parameterized by the precision matrices.\n        Storing the precision matrices instead of the covariance matrices makes\n        it more efficient to compute the log-likelihood of new samples at test\n        time. The shape depends on ``covariance_type``::\n\n            (n_components,)                        if 'spherical',\n            (n_features, n_features)               if 'tied',\n            (n_components, n_features)             if 'diag',\n            (n_components, n_features, n_features) if 'full'\n\n    converged_ : bool\n        True when convergence was reached in fit(), False otherwise.\n\n    n_iter_ : int\n        Number of step used by the best fit of inference to reach the\n        convergence.\n\n    lower_bound_ : float\n        Lower bound value on the model evidence (of the training data) of the\n        best fit of inference.\n\n    weight_concentration_prior_ : tuple or float\n        The dirichlet concentration of each component on the weight\n        distribution (Dirichlet). The type depends on\n        ``weight_concentration_prior_type``::\n\n            (float, float) if 'dirichlet_process' (Beta parameters),\n            float          if 'dirichlet_distribution' (Dirichlet parameters).\n\n        The higher concentration puts more mass in\n        the center and will lead to more components being active, while a lower\n        concentration parameter will lead to more mass at the edge of the\n        simplex.\n\n    weight_concentration_ : array-like of shape (n_components,)\n        The dirichlet concentration of each component on the weight\n        distribution (Dirichlet).\n\n    mean_precision_prior_ : float\n        The precision prior on the mean distribution (Gaussian).\n        Controls the extent of where means can be placed.\n        Larger values concentrate the cluster means around `mean_prior`.\n        If mean_precision_prior is set to None, `mean_precision_prior_` is set\n        to 1.\n\n    mean_precision_ : array-like of shape (n_components,)\n        The precision of each components on the mean distribution (Gaussian).\n\n    mean_prior_ : array-like of shape (n_features,)\n        The prior on the mean distribution (Gaussian).\n\n    degrees_of_freedom_prior_ : float\n        The prior of the number of degrees of freedom on the covariance\n        distributions (Wishart).\n\n    degrees_of_freedom_ : array-like of shape (n_components,)\n        The number of degrees of freedom of each components in the model.\n\n    covariance_prior_ : float or array-like\n        The prior on the covariance distribution (Wishart).\n        The shape depends on `covariance_type`::\n\n            (n_features, n_features) if 'full',\n            (n_features, n_features) if 'tied',\n            (n_features)             if 'diag',\n            float                    if 'spherical'\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    GaussianMixture : Finite Gaussian mixture fit with EM.\n\n    References\n    ----------\n\n    .. [1] `Bishop, Christopher M. (2006). \"Pattern recognition and machine\n       learning\". Vol. 4 No. 4. New York: Springer.\n       <https://www.springer.com/kr/book/9780387310732>`_\n\n    .. [2] `Hagai Attias. (2000). \"A Variational Bayesian Framework for\n       Graphical Models\". In Advances in Neural Information Processing\n       Systems 12.\n       <https://citeseerx.ist.psu.edu/doc_view/pid/ee844fd96db7041a9681b5a18bff008912052c7e>`_\n\n    .. [3] `Blei, David M. and Michael I. Jordan. (2006). \"Variational\n       inference for Dirichlet process mixtures\". Bayesian analysis 1.1\n       <https://www.cs.princeton.edu/courses/archive/fall11/cos597C/reading/BleiJordan2005.pdf>`_\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.mixture import BayesianGaussianMixture\n    >>> X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [12, 4], [10, 7]])\n    >>> bgm = BayesianGaussianMixture(n_components=2, random_state=42).fit(X)\n    >>> bgm.means_\n    array([[2.49... , 2.29...],\n           [8.45..., 4.52... ]])\n    >>> bgm.predict([[0, 0], [9, 3]])\n    array([0, 1])\n    \"\"\"",
                "filename": "sklearn/mixture/_bayesian_mixture.py",
                "start_index": 5297,
                "end_index": 14301,
                "start_line": 142,
                "end_line": 799,
                "max_line": 888,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "for factory, init, params, format in cases:\n    print(\"Evaluation of %s with %s init\" % (factory.__name__, init))\n    inertia = np.empty((len(n_init_range), n_runs))\n\n    for run_id in range(n_runs):\n        X, y = make_data(run_id, n_samples_per_center, grid_size, scale)\n        for i, n_init in enumerate(n_init_range):\n            km = factory(\n                n_clusters=n_clusters,\n                init=init,\n                random_state=run_id,\n                n_init=n_init,\n                **params,\n            ).fit(X)\n            inertia[i, run_id] = km.inertia_\n    p = plt.errorbar(\n        n_init_range, inertia.mean(axis=1), inertia.std(axis=1), fmt=format\n    )\n    plots.append(p[0])\n    legends.append(\"%s with %s init\" % (factory.__name__, init))\n\nplt.xlabel(\"n_init\")\nplt.ylabel(\"inertia\")\nplt.legend(plots, legends)\nplt.title(\"Mean inertia for various k-means init across %d runs\" % n_runs)\n\n# Part 2: Qualitative visual inspection of the convergence\n\nX, y = make_data(random_state, n_samples_per_center, grid_size, scale)\nkm = MiniBatchKMeans(\n    n_clusters=n_clusters, init=\"random\", n_init=1, random_state=random_state\n).fit(X)\n\nplt.figure()\nfor k in range(n_clusters):\n    my_members = km.labels_ == k\n    color = cm.nipy_spectral(float(k) / n_clusters, 1)\n    plt.plot(X[my_members, 0], X[my_members, 1], \".\", c=color)\n    cluster_center = km.cluster_centers_[k]\n    plt.plot(\n        cluster_center[0],\n        cluster_center[1],\n        \"o\",\n        markerfacecolor=color,\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n    plt.title(\n        \"Example cluster allocation with a single random init\\nwith MiniBatchKMeans\"\n    )\n\nplt.show()",
                "filename": "examples/cluster/plot_kmeans_stability_low_dim_dense.py",
                "start_index": 2659,
                "end_index": 4336,
                "start_line": 81,
                "end_line": 132,
                "max_line": 132,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "class BayesianGaussianMixture(BaseMixture):",
                "filename": "sklearn/mixture/_bayesian_mixture.py",
                "start_index": 2170,
                "end_index": 2213,
                "start_line": 74,
                "end_line": 74,
                "max_line": 888,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "class GaussianMixture(BaseMixture):",
                "filename": "sklearn/mixture/_gaussian_mixture.py",
                "start_index": 14622,
                "end_index": 14657,
                "start_line": 455,
                "end_line": 455,
                "max_line": 851,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "def _estimate_log_prob(self, X):\n        _, n_features = X.shape\n        # We remove `n_features * np.log(self.degrees_of_freedom_)` because\n        # the precision matrix is normalized\n        log_gauss = _estimate_log_gaussian_prob(\n            X, self.means_, self.precisions_cholesky_, self.covariance_type\n        ) - 0.5 * n_features * np.log(self.degrees_of_freedom_)\n\n        log_lambda = n_features * np.log(2.0) + np.sum(\n            digamma(\n                0.5\n                * (self.degrees_of_freedom_ - np.arange(0, n_features)[:, np.newaxis])\n            ),\n            0,\n        )\n\n        return log_gauss + 0.5 * (log_lambda - n_features / self.mean_precision_)\n\n    def _compute_lower_bound(self, log_resp, log_prob_norm):\n        \"\"\"Estimate the lower bound of the model.\n\n        The lower bound on the likelihood (of the training data with respect to\n        the model) is used to detect the convergence and has to increase at\n        each iteration.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n\n        log_resp : array, shape (n_samples, n_components)\n            Logarithm of the posterior probabilities (or responsibilities) of\n            the point of each sample in X.\n\n        log_prob_norm : float\n            Logarithm of the probability of each sample in X.\n\n        Returns\n        -------\n        lower_bound : float\n        \"\"\"\n        # Contrary to the original formula, we have done some simplification\n        # and removed all the constant terms.\n        (n_features,) = self.mean_prior_.shape\n\n        # We removed `.5 * n_features * np.log(self.degrees_of_freedom_)`\n        # because the precision matrix is normalized.\n        log_det_precisions_chol = _compute_log_det_cholesky(\n            self.precisions_cholesky_, self.covariance_type, n_features\n        ) - 0.5 * n_features * np.log(self.degrees_of_freedom_)\n\n        if self.covariance_type == \"tied\":\n            log_wishart = self.n_components * np.float64(\n                _log_wishart_norm(\n                    self.degrees_of_freedom_, log_det_precisions_chol, n_features\n                )\n            )\n        else:\n            log_wishart = np.sum(\n                _log_wishart_norm(\n                    self.degrees_of_freedom_, log_det_precisions_chol, n_features\n                )\n            )\n\n        if self.weight_concentration_prior_type == \"dirichlet_process\":\n            log_norm_weight = -np.sum(\n                betaln(self.weight_concentration_[0], self.weight_concentration_[1])\n            )\n        else:\n            log_norm_weight = _log_dirichlet_norm(self.weight_concentration_)\n\n        return (\n            -np.sum(np.exp(log_resp) * log_resp)\n            - log_wishart\n            - log_norm_weight\n            - 0.5 * n_features * np.sum(np.log(self.mean_precision_))\n        )",
                "filename": "sklearn/mixture/_bayesian_mixture.py",
                "start_index": 28849,
                "end_index": 31715,
                "start_line": 760,
                "end_line": 886,
                "max_line": 888,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"Bayesian Gaussian Mixture Model.\"\"\"\n# Author: Wei Xue <xuewei4d@gmail.com>\n#         Thierry Guillemot <thierry.guillemot.work@gmail.com>\n# License: BSD 3 clause\n\nimport math\nfrom numbers import Real\n\nimport numpy as np\nfrom scipy.special import betaln, digamma, gammaln\n\nfrom ..utils import check_array\nfrom ..utils._param_validation import Interval, StrOptions\nfrom ._base import BaseMixture, _check_shape\nfrom ._gaussian_mixture import (\n    _check_precision_matrix,\n    _check_precision_positivity,\n    _compute_log_det_cholesky,\n    _compute_precision_cholesky,\n    _estimate_gaussian_parameters,\n    _estimate_log_gaussian_prob,\n)\n\n\ndef _log_dirichlet_norm(dirichlet_concentration):\n    \"\"\"Compute the log of the Dirichlet distribution normalization term.\n\n    Parameters\n    ----------\n    dirichlet_concentration : array-like of shape (n_samples,)\n        The parameters values of the Dirichlet distribution.\n\n    Returns\n    -------\n    log_dirichlet_norm : float\n        The log normalization of the Dirichlet distribution.\n    \"\"\"\n    return gammaln(np.sum(dirichlet_concentration)) - np.sum(\n        gammaln(dirichlet_concentration)\n    )\n\n\ndef _log_wishart_norm(degrees_of_freedom, log_det_precisions_chol, n_features):\n    \"\"\"Compute the log of the Wishart distribution normalization term.\n\n    Parameters\n    ----------\n    degrees_of_freedom : array-like of shape (n_components,)\n        The number of degrees of freedom on the covariance Wishart\n        distributions.\n\n    log_det_precision_chol : array-like of shape (n_components,)\n         The determinant of the precision matrix for each component.\n\n    n_features : int\n        The number of features.\n\n    Return\n    ------\n    log_wishart_norm : array-like of shape (n_components,)\n        The log normalization of the Wishart distribution.\n    \"\"\"\n    # To simplify the computation we have removed the np.log(np.pi) term\n    return -(\n        degrees_of_freedom * log_det_precisions_chol\n        + degrees_of_freedom * n_features * 0.5 * math.log(2.0)\n        + np.sum(\n            gammaln(0.5 * (degrees_of_freedom - np.arange(n_features)[:, np.newaxis])),\n            0,\n        )\n    )",
                "filename": "sklearn/mixture/_bayesian_mixture.py",
                "start_index": 0,
                "end_index": 2167,
                "start_line": 1,
                "end_line": 886,
                "max_line": 888,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/mixture/_gaussian_mixture.py": [
                {
                    "chunk": {
                        "code": "_parameter_constraints: dict = {\n        **BaseMixture._parameter_constraints,\n        \"covariance_type\": [StrOptions({\"full\", \"tied\", \"diag\", \"spherical\"})],\n        \"weights_init\": [\"array-like\", None],\n        \"means_init\": [\"array-like\", None],\n        \"precisions_init\": [\"array-like\", None],\n    }\n\n    def __init__(\n        self,\n        n_components=1,\n        *,\n        covariance_type=\"full\",\n        tol=1e-3,\n        reg_covar=1e-6,\n        max_iter=100,\n        n_init=1,\n        init_params=\"kmeans\",\n        weights_init=None,\n        means_init=None,\n        precisions_init=None,\n        random_state=None,\n        warm_start=False,\n        verbose=0,\n        verbose_interval=10,\n    ):\n        super().__init__(\n            n_components=n_components,\n            tol=tol,\n            reg_covar=reg_covar,\n            max_iter=max_iter,\n            n_init=n_init,\n            init_params=init_params,\n            random_state=random_state,\n            warm_start=warm_start,\n            verbose=verbose,\n            verbose_interval=verbose_interval,\n        )\n\n        self.covariance_type = covariance_type\n        self.weights_init = weights_init\n        self.means_init = means_init\n        self.precisions_init = precisions_init\n\n    def _check_parameters(self, X):\n        \"\"\"Check the Gaussian mixture parameters are well defined.\"\"\"\n        _, n_features = X.shape\n\n        if self.weights_init is not None:\n            self.weights_init = _check_weights(self.weights_init, self.n_components)\n\n        if self.means_init is not None:\n            self.means_init = _check_means(\n                self.means_init, self.n_components, n_features\n            )\n\n        if self.precisions_init is not None:\n            self.precisions_init = _check_precisions(\n                self.precisions_init,\n                self.covariance_type,\n                self.n_components,\n                n_features,\n            )",
                        "filename": "sklearn/mixture/_gaussian_mixture.py",
                        "start_index": 22157,
                        "end_index": 24091,
                        "start_line": 638,
                        "end_line": 797,
                        "max_line": 851,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the '__init__' method of the 'GaussianMixture' class, which includes the 'n_init' parameter that is directly related to the issue."
                },
                {
                    "chunk": {
                        "code": "class GaussianMixture(BaseMixture):",
                        "filename": "sklearn/mixture/_gaussian_mixture.py",
                        "start_index": 14622,
                        "end_index": 14657,
                        "start_line": 455,
                        "end_line": 455,
                        "max_line": 851,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet marks the beginning of the 'GaussianMixture' class definition, which is relevant to the issue as the bug is within this class."
                },
                {
                    "chunk": {
                        "code": "\"\"\"Gaussian Mixture Model.\"\"\"\n\n# Author: Wei Xue <xuewei4d@gmail.com>\n# Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom scipy import linalg\n\nfrom ..utils import check_array\nfrom ..utils._param_validation import StrOptions\nfrom ..utils.extmath import row_norms\nfrom ._base import BaseMixture, _check_shape\n\n###############################################################################\n# Gaussian mixture shape checkers used by the GaussianMixture class\n\n\ndef _check_weights(weights, n_components):\n    \"\"\"Check the user provided 'weights'.\n\n    Parameters\n    ----------\n    weights : array-like of shape (n_components,)\n        The proportions of components of each mixture.\n\n    n_components : int\n        Number of components.\n\n    Returns\n    -------\n    weights : array, shape (n_components,)\n    \"\"\"\n    weights = check_array(weights, dtype=[np.float64, np.float32], ensure_2d=False)\n    _check_shape(weights, (n_components,), \"weights\")\n\n    # check range\n    if any(np.less(weights, 0.0)) or any(np.greater(weights, 1.0)):\n        raise ValueError(\n            \"The parameter 'weights' should be in the range \"\n            \"[0, 1], but got max value %.5f, min value %.5f\"\n            % (np.min(weights), np.max(weights))\n        )\n\n    # check normalization\n    if not np.allclose(np.abs(1.0 - np.sum(weights)), 0.0):\n        raise ValueError(\n            \"The parameter 'weights' should be normalized, but got sum(weights) = %.5f\"\n            % np.sum(weights)\n        )\n    return weights\n\n\ndef _check_means(means, n_components, n_features):\n    \"\"\"Validate the provided 'means'.\n\n    Parameters\n    ----------\n    means : array-like of shape (n_components, n_features)\n        The centers of the current components.\n\n    n_components : int\n        Number of components.\n\n    n_features : int\n        Number of features.\n\n    Returns\n    -------\n    means : array, (n_components, n_features)\n    \"\"\"\n    means = check_array(means, dtype=[np.float64, np.float32], ensure_2d=False)\n    _check_shape(means, (n_components, n_features), \"means\")\n    return means\n\n\ndef _check_precision_positivity(precision, covariance_type):\n    \"\"\"Check a precision vector is positive-definite.\"\"\"\n    if np.any(np.less_equal(precision, 0.0)):\n        raise ValueError(\"'%s precision' should be positive\" % covariance_type)\n\n\ndef _check_precision_matrix(precision, covariance_type):\n    \"\"\"Check a precision matrix is symmetric and positive-definite.\"\"\"\n    if not (\n        np.allclose(precision, precision.T) and np.all(linalg.eigvalsh(precision) > 0.0)\n    ):\n        raise ValueError(\n            \"'%s precision' should be symmetric, positive-definite\" % covariance_type\n        )\n\n\ndef _check_precisions_full(precisions, covariance_type):\n    \"\"\"Check the precision matrices are symmetric and positive-definite.\"\"\"\n    for prec in precisions:\n        _check_precision_matrix(prec, covariance_type)",
                        "filename": "sklearn/mixture/_gaussian_mixture.py",
                        "start_index": 0,
                        "end_index": 2959,
                        "start_line": 1,
                        "end_line": 96,
                        "max_line": 851,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains helper functions used by the 'GaussianMixture' class to check the parameters. It might be necessary to review these functions to ensure that the 'max_lower_bound' is being calculated and used correctly."
                }
            ]
        },
        "solution_task": "- In `sklearn/mixture/_gaussian_mixture.py`, update the `fit` method of the `GaussianMixture` class to store the maximum lower bound across all initializations:\n\n```python\nclass GaussianMixture(BaseMixture):\n    # ...\n    def fit(self, X, y=None):\n        # ...\n        max_lower_bound = -np.infty\n        self.converged_ = False\n\n        for init in range(self.n_init):\n            # ...\n            # Current lower bound computation\n            current_lower_bound = ...\n            # ...\n            if current_lower_bound > max_lower_bound:\n                max_lower_bound = current_lower_bound\n                # Store other parameters if necessary\n                # ...\n\n        self.lower_bound_ = max_lower_bound\n        return self\n```\n\n- In `mixture/tests/test_gaussian_mixture.py`, update the `test_init` method to try multiple random states:\n\n```python\nclass TestGaussianMixture:\n    def test_init(self):\n        # ...\n        random_states = [np.random.RandomState(i) for i in range(10)]\n        for random_state in random_states:\n            # Create a GaussianMixture instance with the random_state\n            # Perform fitting and other necessary assertions\n            # ...\n```",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE provides a detailed and accurate solution to the bug described in the PROMPT. It includes the exact code modification needed in the 'fit' method of the 'GaussianMixture' class to ensure that 'lower_bound_' is the maximum lower bound across all initializations. Additionally, it suggests an update to the 'test_init' method to include multiple random states, which aligns with the PROMPT's suggestion to improve the test coverage. The RESPONSE also provides links to the relevant code snippets in the GitHub repository, which is helpful for locating the code that needs to be changed. The RESPONSE is comprehensive and addresses both the bug fix and the test improvement, which are the two main issues outlined in the PROMPT.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711742505.545554,
        "result": "- In `sklearn/mixture/_gaussian_mixture.py`, update the `fit` method of the `GaussianMixture` class to store the maximum lower bound across all initializations:\n\n```python\nclass GaussianMixture(BaseMixture):\n    # ...\n    def fit(self, X, y=None):\n        # ...\n        max_lower_bound = -np.infty\n        self.converged_ = False\n\n        for init in range(self.n_init):\n            # ...\n            # Current lower bound computation\n            current_lower_bound = ...\n            # ...\n            if current_lower_bound > max_lower_bound:\n                max_lower_bound = current_lower_bound\n                # Store other parameters if necessary\n                # ...\n\n        self.lower_bound_ = max_lower_bound\n        return self\n```\n\n- In `mixture/tests/test_gaussian_mixture.py`, update the `test_init` method to try multiple random states:\n\n```python\nclass TestGaussianMixture:\n    def test_init(self):\n        # ...\n        random_states = [np.random.RandomState(i) for i in range(10)]\n        for random_state in random_states:\n            # Create a GaussianMixture instance with the random_state\n            # Perform fitting and other necessary assertions\n            # ...\n```\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [mixture](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/mixture)- [Lines 638 - 797](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/mixture/_gaussian_mixture.py#L638-L797)\n\n This snippet contains the '__init__' method of the 'GaussianMixture' class, which includes the 'n_init' parameter that is directly related to the issue. \n\n\n   - [Lines 455 - 455](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/mixture/_gaussian_mixture.py#L455-L455)\n\n This snippet marks the beginning of the 'GaussianMixture' class definition, which is relevant to the issue as the bug is within this class. \n\n\n   - [Lines 1 - 96](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/mixture/_gaussian_mixture.py#L1-L96)\n\n This snippet contains helper functions used by the 'GaussianMixture' class to check the parameters. It might be necessary to review these functions to ensure that the 'max_lower_bound' is being calculated and used correctly."
    }
}