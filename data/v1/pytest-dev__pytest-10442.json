{
    "id": "pytest-dev__pytest-10442",
    "repo_name": "testbed/pytest-dev__pytest__7.2",
    "task_id": 7901572461041850932,
    "title": "Keep temporary directory for failing tests only",
    "body": "Currently pytest will keep the last 3 _root_ temporary directories for each testing session, for all tests.\r\n\r\nThis proposal is to change this behavior to only keep the directories for failed tests, instead of for all tests. This would save significant disk space for test suites which manipulate large amounts of data in the temporary directories.\r\n\r\nThe behavior of keeping the last 3 root temporary directories would be kept unchanged.\r\n\r\n\r\nFrom: https://github.com/pytest-dev/pytest/issues/8036#issuecomment-742567384\r\n\r\nEDIT: after some discussion, the full proposal is defined in https://github.com/pytest-dev/pytest/issues/8141#issuecomment-1278960826.\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/pytest-dev__pytest__7.2.git",
    "requested_commit": "",
    "mayil_response": "To implement the feature of keeping temporary directories only for failing tests in pytest, you can follow these steps:\n\n- Identify the pytest hook or fixture that handles the creation and cleanup of temporary directories. This is likely to be part of the `tmpdir` or `tmp_path` fixture.\n- Modify the fixture to tag or mark the temporary directories associated with failing tests. This could be done by intercepting the test outcome and, if a test fails, storing a reference to the temporary directory.\n- Adjust the cleanup logic to only remove temporary directories that are not marked as associated with failed tests. Ensure that the last 3 root temporary directories are still kept, as per the existing behavior.\n- Add a configuration option to enable or disable this new behavior, as some users may still want to keep temporary directories for all tests for debugging purposes.\n- Write tests for the new feature to ensure it works as expected and does not introduce regressions.\n- Update the documentation to describe the new feature and how to configure it.\n\nHere is a high-level overview of the changes in pseudo-code:\n\n```python\n# In the tmpdir fixture or equivalent\n@pytest.fixture\ndef tmpdir(request):\n    temp_dir = create_temporary_directory()\n    request.addfinalizer(lambda: cleanup_temp_dir(temp_dir, request.node))\n    return temp_dir\n\ndef cleanup_temp_dir(temp_dir, node):\n    if should_preserve_temp_dir(node):\n        mark_temp_dir(temp_dir)\n    else:\n        remove_temp_dir(temp_dir)\n\ndef should_preserve_temp_dir(node):\n    # Logic to determine if the temp dir should be preserved\n    # Check if the test failed and if the feature is enabled\n    return node.rep_call.failed and is_feature_enabled()\n\n# In the session or test run teardown\ndef cleanup_session_temp_dirs():\n    for temp_dir in get_all_temp_dirs():\n        if not is_marked_temp_dir(temp_dir):\n            remove_temp_dir(temp_dir)\n        else:\n            if more_than_three_marked_dirs():\n                remove_oldest_marked_temp_dir()\n\n# Configuration option\ndef pytest_addoption(parser):\n    parser.addoption(\"--keep-failed-temp-dirs\", action=\"store_true\", help=\"Keep temporary directories only for failed tests\")\n\n# Usage of the configuration option\ndef is_feature_enabled():\n    return pytest.config.getoption(\"--keep-failed-temp-dirs\")\n```\n\nThis is a simplified version of what the actual implementation might look like. The actual implementation would need to handle edge cases, integrate with the existing pytest configuration system, and follow the internal architecture of pytest.\n\nFor further research, you can look into the following pytest documentation and source code:\n- Pytest fixtures: https://docs.pytest.org/en/stable/fixture.html\n- Pytest hooks: https://docs.pytest.org/en/stable/reference.html#hooks\n- Pytest configuration: https://docs.pytest.org/en/stable/reference.html#configuration-options\n\nNote: The provided code snippets from the codebase do not seem to be directly relevant to the implementation of this feature.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.000239,
        "snippet_processor": 0.03833,
        "issue_star_creation": 0.025259999999999998,
        "issue_star_solver": 0.03222,
        "bouncer": 0.02645
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711733684.3993542,
        "relevant_snippets": [
            {
                "code": "blank_issues_enabled: false\ncontact_links:\n  - name: \u2753 Support Question\n    url: https://github.com/pytest-dev/pytest/discussions\n    about: Use GitHub's new Discussions feature for questions",
                "filename": ".github/ISSUE_TEMPLATE/config.yml",
                "start_index": 0,
                "end_index": 191,
                "start_line": 1,
                "end_line": 5,
                "max_line": 5,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.2",
                "sha": ""
            },
            {
                "code": "rtd:\n  project: pytest",
                "filename": ".github/config.yml",
                "start_index": 0,
                "end_index": 22,
                "start_line": 1,
                "end_line": 2,
                "max_line": 2,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.2",
                "sha": ""
            },
            {
                "code": "# shim for pylib going away\n# if pylib is installed this file will get skipped\n# (`py/__init__.py` has higher precedence)\nimport sys\n\nimport _pytest._py.error as error\nimport _pytest._py.path as path\n\nsys.modules[\"py.error\"] = error\nsys.modules[\"py.path\"] = path",
                "filename": "src/py.py",
                "start_index": 0,
                "end_index": 262,
                "start_line": 1,
                "end_line": 10,
                "max_line": 10,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.2",
                "sha": ""
            },
            {
                "code": "import pytest\n\n\n@pytest.fixture(scope=\"module\", params=range(966))\ndef foo(request):\n    return request.param\n\n\ndef test_it(foo):\n    pass\n\n\ndef test_it2(foo):\n    pass",
                "filename": "bench/manyparam.py",
                "start_index": 0,
                "end_index": 168,
                "start_line": 1,
                "end_line": 14,
                "max_line": 14,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.2",
                "sha": ""
            },
            {
                "code": "import pytest\n\nSKIP = True\n\n\n@pytest.mark.parametrize(\"x\", range(5000))\ndef test_foo(x):\n    if SKIP:\n        pytest.skip(\"heh\")",
                "filename": "bench/skip.py",
                "start_index": 0,
                "end_index": 128,
                "start_line": 1,
                "end_line": 9,
                "max_line": 9,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.2",
                "sha": ""
            },
            {
                "code": "import json\nfrom pathlib import Path\n\nimport requests\n\nissues_url = \"https://api.github.com/repos/pytest-dev/pytest/issues\"\n\n\ndef get_issues():\n    issues = []\n    url = issues_url\n    while 1:\n        get_data = {\"state\": \"all\"}\n        r = requests.get(url, params=get_data)\n        data = r.json()\n        if r.status_code == 403:\n            # API request limit exceeded\n            print(data[\"message\"])\n            exit(1)\n        issues.extend(data)\n\n        # Look for next page\n        links = requests.utils.parse_header_links(r.headers[\"Link\"])\n        another_page = False\n        for link in links:\n            if link[\"rel\"] == \"next\":\n                url = link[\"url\"]\n                another_page = True\n        if not another_page:\n            return issues\n\n\ndef main(args):\n    cachefile = Path(args.cache)\n    if not cachefile.exists() or args.refresh:\n        issues = get_issues()\n        cachefile.write_text(json.dumps(issues), \"utf-8\")\n    else:\n        issues = json.loads(cachefile.read_text(\"utf-8\"))\n\n    open_issues = [x for x in issues if x[\"state\"] == \"open\"]\n\n    open_issues.sort(key=lambda x: x[\"number\"])\n    report(open_issues)\n\n\ndef _get_kind(issue):\n    labels = [label[\"name\"] for label in issue[\"labels\"]]\n    for key in (\"bug\", \"enhancement\", \"proposal\"):\n        if key in labels:\n            return key\n    return \"issue\"\n\n\ndef report(issues):\n    for issue in issues:\n        title = issue[\"title\"]\n        # body = issue[\"body\"]\n        kind = _get_kind(issue)\n        status = issue[\"state\"]\n        number = issue[\"number\"]\n        link = \"https://github.com/pytest-dev/pytest/issues/%s/\" % number\n        print(\"----\")\n        print(status, kind, link)\n        print(title)\n        # print()\n        # lines = body.split(\"\\n\")\n        # print(\"\\n\".join(lines[:3]))\n        # if len(lines) > 3 or len(body) > 240:\n        #    print(\"...\")\n    print(\"\\n\\nFound %s open issues\" % len(issues))\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser(\"process bitbucket issues\")\n    parser.add_argument(\n        \"--refresh\", action=\"store_true\", help=\"invalidate cache, refresh issues\"\n    )\n    parser.add_argument(\n        \"--cache\", action=\"store\", default=\"issues.json\", help=\"cache file\"\n    )\n    args = parser.parse_args()\n    main(args)",
                "filename": "extra/get_issues.py",
                "start_index": 0,
                "end_index": 2321,
                "start_line": 1,
                "end_line": 85,
                "max_line": 85,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.2",
                "sha": ""
            },
            {
                "code": "version: 2\nupdates:\n- package-ecosystem: pip\n  directory: \"/testing/plugins_integration\"\n  schedule:\n    interval: weekly\n    time: \"03:00\"\n  open-pull-requests-limit: 10\n  allow:\n  - dependency-type: direct\n  - dependency-type: indirect\n- package-ecosystem: github-actions\n  directory: /\n  schedule:\n    interval: weekly\n    time: \"03:00\"\n  open-pull-requests-limit: 10",
                "filename": ".github/dependabot.yml",
                "start_index": 0,
                "end_index": 370,
                "start_line": 1,
                "end_line": 17,
                "max_line": 17,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.2",
                "sha": ""
            },
            {
                "code": "import datetime\nimport pathlib\nimport re\nfrom textwrap import dedent\nfrom textwrap import indent\n\nimport packaging.version\nimport platformdirs\nimport tabulate\nimport wcwidth\nfrom requests_cache import CachedResponse\nfrom requests_cache import CachedSession\nfrom requests_cache import OriginalResponse\nfrom requests_cache import SQLiteCache\nfrom tqdm import tqdm\n\n\nFILE_HEAD = r\"\"\"\n.. Note this file is autogenerated by scripts/update-plugin-list.py - usually weekly via github action\n\n.. _plugin-list:\n\nPlugin List\n===========\n\nPyPI projects that match \"pytest-\\*\" are considered plugins and are listed\nautomatically together with a manually-maintained list in `the source\ncode <https://github.com/pytest-dev/pytest/blob/main/scripts/update-plugin-list.py>`_.\nPackages classified as inactive are excluded.\n\n.. The following conditional uses a different format for this list when\n   creating a PDF, because otherwise the table gets far too wide for the\n   page.\n\n\"\"\"\nDEVELOPMENT_STATUS_CLASSIFIERS = (\n    \"Development Status :: 1 - Planning\",\n    \"Development Status :: 2 - Pre-Alpha\",\n    \"Development Status :: 3 - Alpha\",\n    \"Development Status :: 4 - Beta\",\n    \"Development Status :: 5 - Production/Stable\",\n    \"Development Status :: 6 - Mature\",\n    \"Development Status :: 7 - Inactive\",\n)\nADDITIONAL_PROJECTS = {  # set of additional projects to consider as plugins\n    \"logassert\",\n    \"nuts\",\n}\n\n\ndef escape_rst(text: str) -> str:\n    \"\"\"Rudimentary attempt to escape special RST characters to appear as\n    plain text.\"\"\"\n    text = (\n        text.replace(\"*\", \"\\\\*\")\n        .replace(\"<\", \"\\\\<\")\n        .replace(\">\", \"\\\\>\")\n        .replace(\"`\", \"\\\\`\")\n    )\n    text = re.sub(r\"_\\b\", \"\", text)\n    return text\n\n\ndef project_response_with_refresh(\n    session: CachedSession, name: str, last_serial: int\n) -> OriginalResponse | CachedResponse:\n    \"\"\"Get a http cached pypi project\n\n    force refresh in case of last serial mismatch\n    \"\"\"\n\n    response = session.get(f\"https://pypi.org/pypi/{name}/json\")\n    if int(response.headers.get(\"X-PyPI-Last-Serial\", -1)) != last_serial:\n        response = session.get(f\"https://pypi.org/pypi/{name}/json\", refresh=True)\n    return response\n\n\ndef get_session() -> CachedSession:\n    \"\"\"Configures the requests-cache session\"\"\"\n    cache_path = platformdirs.user_cache_path(\"pytest-plugin-list\")\n    cache_path.mkdir(exist_ok=True, parents=True)\n    cache_file = cache_path.joinpath(\"http_cache.sqlite3\")\n    return CachedSession(backend=SQLiteCache(cache_file))\n\n\ndef pytest_plugin_projects_from_pypi(session: CachedSession) -> dict[str, int]:\n    response = session.get(\n        \"https://pypi.org/simple\",\n        headers={\"Accept\": \"application/vnd.pypi.simple.v1+json\"},\n        refresh=True,\n    )\n    return {\n        name: p[\"_last-serial\"]\n        for p in response.json()[\"projects\"]\n        if (name := p[\"name\"]).startswith(\"pytest-\") or name in ADDITIONAL_PROJECTS\n    }",
                "filename": "scripts/update-plugin-list.py",
                "start_index": 0,
                "end_index": 2939,
                "start_line": 1,
                "end_line": 155,
                "max_line": 195,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.2",
                "sha": ""
            },
            {
                "code": "import sys\n\nif __name__ == \"__main__\":\n    import cProfile\n    import pytest  # NOQA\n    import pstats\n\n    script = sys.argv[1:] if len(sys.argv) > 1 else [\"empty.py\"]\n    cProfile.run(\"pytest.cmdline.main(%r)\" % script, \"prof\")\n    p = pstats.Stats(\"prof\")\n    p.strip_dirs()\n    p.sort_stats(\"cumulative\")\n    print(p.print_stats(500))",
                "filename": "bench/bench.py",
                "start_index": 0,
                "end_index": 338,
                "start_line": 1,
                "end_line": 13,
                "max_line": 13,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.2",
                "sha": ""
            },
            {
                "code": "def iter_plugins():\n    session = get_session()\n    name_2_serial = pytest_plugin_projects_from_pypi(session)\n\n    for name, last_serial in tqdm(name_2_serial.items(), smoothing=0):\n        response = project_response_with_refresh(session, name, last_serial)\n        if response.status_code == 404:\n            # Some packages, like pytest-azurepipelines42, are included in https://pypi.org/simple\n            # but return 404 on the JSON API. Skip.\n            continue\n        response.raise_for_status()\n        info = response.json()[\"info\"]\n        if \"Development Status :: 7 - Inactive\" in info[\"classifiers\"]:\n            continue\n        for classifier in DEVELOPMENT_STATUS_CLASSIFIERS:\n            if classifier in info[\"classifiers\"]:\n                status = classifier[22:]\n                break\n        else:\n            status = \"N/A\"\n        requires = \"N/A\"\n        if info[\"requires_dist\"]:\n            for requirement in info[\"requires_dist\"]:\n                if re.match(r\"pytest(?![-.\\w])\", requirement):\n                    requires = requirement\n                    break\n\n        def version_sort_key(version_string):\n            \"\"\"\n            Return the sort key for the given version string\n            returned by the API.\n            \"\"\"\n            try:\n                return packaging.version.parse(version_string)\n            except packaging.version.InvalidVersion:\n                # Use a hard-coded pre-release version.\n                return packaging.version.Version(\"0.0.0alpha\")\n\n        releases = response.json()[\"releases\"]\n        for release in sorted(releases, key=version_sort_key, reverse=True):\n            if releases[release]:\n                release_date = datetime.date.fromisoformat(\n                    releases[release][-1][\"upload_time_iso_8601\"].split(\"T\")[0]\n                )\n                last_release = release_date.strftime(\"%b %d, %Y\")\n                break\n        name = f':pypi:`{info[\"name\"]}`'\n        summary = \"\"\n        if info[\"summary\"]:\n            summary = escape_rst(info[\"summary\"].replace(\"\\n\", \"\"))\n        yield {\n            \"name\": name,\n            \"summary\": summary.strip(),\n            \"last release\": last_release,\n            \"status\": status,\n            \"requires\": requires,\n        }\n\n\ndef plugin_definitions(plugins):\n    \"\"\"Return RST for the plugin list that fits better on a vertical page.\"\"\"\n\n    for plugin in plugins:\n        yield dedent(\n            f\"\"\"\n            {plugin['name']}\n               *last release*: {plugin[\"last release\"]},\n               *status*: {plugin[\"status\"]},\n               *requires*: {plugin[\"requires\"]}\n\n               {plugin[\"summary\"]}\n            \"\"\"\n        )",
                "filename": "scripts/update-plugin-list.py",
                "start_index": 2942,
                "end_index": 5644,
                "start_line": 99,
                "end_line": 171,
                "max_line": 195,
                "git_instance": "github",
                "repo_name": "testbed/pytest-dev__pytest__7.2",
                "sha": ""
            }
        ],
        "classification": "feature request",
        "snippet_processor_task": {},
        "solution_task": "To implement the feature of keeping temporary directories only for failing tests in pytest, you can follow these steps:\n\n- Identify the pytest hook or fixture that handles the creation and cleanup of temporary directories. This is likely to be part of the `tmpdir` or `tmp_path` fixture.\n- Modify the fixture to tag or mark the temporary directories associated with failing tests. This could be done by intercepting the test outcome and, if a test fails, storing a reference to the temporary directory.\n- Adjust the cleanup logic to only remove temporary directories that are not marked as associated with failed tests. Ensure that the last 3 root temporary directories are still kept, as per the existing behavior.\n- Add a configuration option to enable or disable this new behavior, as some users may still want to keep temporary directories for all tests for debugging purposes.\n- Write tests for the new feature to ensure it works as expected and does not introduce regressions.\n- Update the documentation to describe the new feature and how to configure it.\n\nHere is a high-level overview of the changes in pseudo-code:\n\n```python\n# In the tmpdir fixture or equivalent\n@pytest.fixture\ndef tmpdir(request):\n    temp_dir = create_temporary_directory()\n    request.addfinalizer(lambda: cleanup_temp_dir(temp_dir, request.node))\n    return temp_dir\n\ndef cleanup_temp_dir(temp_dir, node):\n    if should_preserve_temp_dir(node):\n        mark_temp_dir(temp_dir)\n    else:\n        remove_temp_dir(temp_dir)\n\ndef should_preserve_temp_dir(node):\n    # Logic to determine if the temp dir should be preserved\n    # Check if the test failed and if the feature is enabled\n    return node.rep_call.failed and is_feature_enabled()\n\n# In the session or test run teardown\ndef cleanup_session_temp_dirs():\n    for temp_dir in get_all_temp_dirs():\n        if not is_marked_temp_dir(temp_dir):\n            remove_temp_dir(temp_dir)\n        else:\n            if more_than_three_marked_dirs():\n                remove_oldest_marked_temp_dir()\n\n# Configuration option\ndef pytest_addoption(parser):\n    parser.addoption(\"--keep-failed-temp-dirs\", action=\"store_true\", help=\"Keep temporary directories only for failed tests\")\n\n# Usage of the configuration option\ndef is_feature_enabled():\n    return pytest.config.getoption(\"--keep-failed-temp-dirs\")\n```\n\nThis is a simplified version of what the actual implementation might look like. The actual implementation would need to handle edge cases, integrate with the existing pytest configuration system, and follow the internal architecture of pytest.\n\nFor further research, you can look into the following pytest documentation and source code:\n- Pytest fixtures: https://docs.pytest.org/en/stable/fixture.html\n- Pytest hooks: https://docs.pytest.org/en/stable/reference.html#hooks\n- Pytest configuration: https://docs.pytest.org/en/stable/reference.html#configuration-options\n\nNote: The provided code snippets from the codebase do not seem to be directly relevant to the implementation of this feature.",
        "bouncer_response": {
            "score": 8,
            "criteria": "The VALUE score should reflect how well the RESPONSE addresses the specific needs and context presented in the PROMPT, regardless of the completeness of the PROMPT itself. RESPONSES that are highly relevant, offering clear and comprehensive suggestions or solutions for the described feature request, should score between 7 and 9.",
            "supporting_evidence": "The RESPONSE provides a clear and detailed plan for implementing the feature of keeping temporary directories only for failing tests in pytest. It identifies the necessary steps, such as modifying the fixture to tag directories associated with failing tests and adjusting the cleanup logic. The RESPONSE also suggests adding a configuration option for enabling or disabling the feature, which is a thoughtful addition for user flexibility. Furthermore, it includes pseudo-code to illustrate the proposed changes and points to relevant pytest documentation for further research. The RESPONSE does not address all aspects of the feature request, such as the specifics of how to ensure the last 3 root temporary directories are kept, which prevents it from scoring a 10.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711733763.1183062,
        "result": "To implement the feature of keeping temporary directories only for failing tests in pytest, you can follow these steps:\n\n- Identify the pytest hook or fixture that handles the creation and cleanup of temporary directories. This is likely to be part of the `tmpdir` or `tmp_path` fixture.\n- Modify the fixture to tag or mark the temporary directories associated with failing tests. This could be done by intercepting the test outcome and, if a test fails, storing a reference to the temporary directory.\n- Adjust the cleanup logic to only remove temporary directories that are not marked as associated with failed tests. Ensure that the last 3 root temporary directories are still kept, as per the existing behavior.\n- Add a configuration option to enable or disable this new behavior, as some users may still want to keep temporary directories for all tests for debugging purposes.\n- Write tests for the new feature to ensure it works as expected and does not introduce regressions.\n- Update the documentation to describe the new feature and how to configure it.\n\nHere is a high-level overview of the changes in pseudo-code:\n\n```python\n# In the tmpdir fixture or equivalent\n@pytest.fixture\ndef tmpdir(request):\n    temp_dir = create_temporary_directory()\n    request.addfinalizer(lambda: cleanup_temp_dir(temp_dir, request.node))\n    return temp_dir\n\ndef cleanup_temp_dir(temp_dir, node):\n    if should_preserve_temp_dir(node):\n        mark_temp_dir(temp_dir)\n    else:\n        remove_temp_dir(temp_dir)\n\ndef should_preserve_temp_dir(node):\n    # Logic to determine if the temp dir should be preserved\n    # Check if the test failed and if the feature is enabled\n    return node.rep_call.failed and is_feature_enabled()\n\n# In the session or test run teardown\ndef cleanup_session_temp_dirs():\n    for temp_dir in get_all_temp_dirs():\n        if not is_marked_temp_dir(temp_dir):\n            remove_temp_dir(temp_dir)\n        else:\n            if more_than_three_marked_dirs():\n                remove_oldest_marked_temp_dir()\n\n# Configuration option\ndef pytest_addoption(parser):\n    parser.addoption(\"--keep-failed-temp-dirs\", action=\"store_true\", help=\"Keep temporary directories only for failed tests\")\n\n# Usage of the configuration option\ndef is_feature_enabled():\n    return pytest.config.getoption(\"--keep-failed-temp-dirs\")\n```\n\nThis is a simplified version of what the actual implementation might look like. The actual implementation would need to handle edge cases, integrate with the existing pytest configuration system, and follow the internal architecture of pytest.\n\nFor further research, you can look into the following pytest documentation and source code:\n- Pytest fixtures: https://docs.pytest.org/en/stable/fixture.html\n- Pytest hooks: https://docs.pytest.org/en/stable/reference.html#hooks\n- Pytest configuration: https://docs.pytest.org/en/stable/reference.html#configuration-options\n\nNote: The provided code snippets from the codebase do not seem to be directly relevant to the implementation of this feature."
    }
}