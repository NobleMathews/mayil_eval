{
    "id": "scikit-learn__scikit-learn-10558",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
    "task_id": 696915689547603271,
    "title": "Deprecate Imputer with axis=1",
    "body": "After having tried to deal with a few issues related to extending `Imputer` behaviour, I believe we should be removing the `axis` parameter from `Imputer`.\r\n\r\n* It seems a strange feature to support in a machine learning context, except perhaps where the features represent something like a time series.\r\n* It is not stateful and can be performed with a FunctionTransformer. (We could even provide a `row_impute` function, if we felt it necessary, which would roughly be defined as `def row_impute(X, **kwargs): return Imputer(**kwargs).fit_transform(X.T).T`.)\r\n* It complicates the implementation, which already has a bunch of weird edge-cases (to handle sparse data with missing indicated by 0 which is an inefficient use of a sparse data structure; and to handle non-NaN missingness indicators), unnecessarily.\r\n* It is often nonsensical to extend further features to the `axis=1` case.\r\n\r\nDo others agree?\n[MRG+1] Deprecate ``Imputer.axis`` argument\n\r\n#### Reference Issue\r\nFixes: #9463\r\n\r\n#### What does this implement/fix? Explain your changes.\r\nDeprecated the argument `axis` on the `Imputer` class.\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.20.git",
    "requested_commit": "",
    "mayil_response": "To deprecate the `axis` parameter from the `Imputer` class in scikit-learn, follow these steps:\n\n- Identify the `Imputer` class in the codebase. It seems that the `Imputer` class is not directly provided in the provided snippets, but you might be referring to `SimpleImputer` or a similar class that has an `axis` parameter.\n- Add a deprecation warning in the `__init__` method of the `Imputer` class (or the relevant class with the `axis` parameter).\n- Set the default value of the `axis` parameter to `None` or another value that indicates deprecation.\n- In the `fit`, `transform`, and `fit_transform` methods, check if `axis` is not `None` (or the deprecation value) and issue a deprecation warning if it is used.\n- Update the documentation to reflect the deprecation.\n- Remove any internal logic that uses the `axis` parameter, as it will no longer be supported.\n- Optionally, provide a utility function `row_impute` as suggested in the ticket.\n\nHere is an example of how you might modify the `SimpleImputer` class to deprecate the `axis` parameter:\n\n```python\nfrom sklearn.utils.validation import check_is_fitted\nimport warnings\n\nclass SimpleImputer(_BaseImputer):\n    # Add the axis parameter with a default value indicating deprecation\n    def __init__(self, missing_values=np.nan, strategy=\"mean\", fill_value=None, verbose=0, copy=True, add_indicator=False, axis=None):\n        super().__init__(missing_values=missing_values, add_indicator=add_indicator)\n        self.strategy = strategy\n        self.fill_value = fill_value\n        self.verbose = verbose\n        self.copy = copy\n        self.axis = axis  # Deprecated parameter\n\n    def fit(self, X, y=None):\n        # Deprecation warning for the axis parameter\n        if self.axis is not None:\n            warnings.warn(\n                \"The 'axis' parameter is deprecated and will be removed in version 1.2. \"\n                \"Use the 'row_impute' function instead.\",\n                FutureWarning\n            )\n        # Rest of the fit method implementation...\n        # ...\n\n    def transform(self, X):\n        check_is_fitted(self)\n        # Deprecation warning for the axis parameter\n        if self.axis is not None:\n            warnings.warn(\n                \"The 'axis' parameter is deprecated and will be removed in version 1.2. \"\n                \"Use the 'row_impute' function instead.\",\n                FutureWarning\n            )\n        # Rest of the transform method implementation...\n        # ...\n\n# Utility function as suggested in the ticket\ndef row_impute(X, **kwargs):\n    return SimpleImputer(**kwargs).fit_transform(X.T).T\n```\n\nRemember to update the documentation accordingly and remove any test cases that involve the deprecated `axis` parameter. After the deprecation period, you can completely remove the `axis` parameter from the class signature and any related logic.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [impute](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/impute)- [Lines 71 - 136](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/impute/_base.py#L71-L136)\n\n This snippet is from the file where the `Imputer` class is defined, which is directly related to the issue of deprecating the `axis` parameter. \n\n\n   - [Lines 390 - 596](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/impute/_base.py#L390-L596)\n\n This snippet is also from the `_base.py` file and contains logic for the `Imputer` class, which may need to be reviewed or modified in relation to the deprecation of the `axis` parameter.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [impute](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/impute)- [Lines 1 - 24](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/impute/__init__.py#L1-L24)\n\n This snippet is from the `__init__.py` file in the `impute` module and could be relevant for ensuring that the deprecation message or documentation is updated accordingly.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.000641,
        "snippet_processor": 0.08109000000000001,
        "issue_star_creation": 0.027979999999999998,
        "issue_star_solver": 0.06982000000000001,
        "bouncer": 0.03111
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711742308.429642,
        "relevant_snippets": [
            {
                "code": "# flake8: noqa\n\"\"\"\n=======================================\nRelease Highlights for scikit-learn 1.0\n=======================================\n\n.. currentmodule:: sklearn\n\nWe are very pleased to announce the release of scikit-learn 1.0! The library\nhas been stable for quite some time, releasing version 1.0 is recognizing that\nand signalling it to our users. This release does not include any breaking\nchanges apart from the usual two-release deprecation cycle. For the future, we\ndo our best to keep this pattern.\n\nThis release includes some new key features as well as many improvements and\nbug fixes. We detail below a few of the major features of this release. **For\nan exhaustive list of all the changes**, please refer to the :ref:`release\nnotes <changes_1_0>`.\n\nTo install the latest version (with pip)::\n\n    pip install --upgrade scikit-learn\n\nor with conda::\n\n    conda install -c conda-forge scikit-learn\n\n\"\"\"\n\n##############################################################################\n# Keyword and positional arguments\n# ---------------------------------------------------------\n# The scikit-learn API exposes many functions and methods which have many input\n# parameters. For example, before this release, one could instantiate a\n# :class:`~ensemble.HistGradientBoostingRegressor` as::\n#\n#         HistGradientBoostingRegressor(\"squared_error\", 0.1, 100, 31, None,\n#             20, 0.0, 255, None, None, False, \"auto\", \"loss\", 0.1, 10, 1e-7,\n#             0, None)\n#\n# Understanding the above code requires the reader to go to the API\n# documentation and to check each and every parameter for its position and\n# its meaning. To improve the readability of code written based on scikit-learn,\n# now users have to provide most parameters with their names, as keyword\n# arguments, instead of positional arguments. For example, the above code would\n# be::\n#\n#     HistGradientBoostingRegressor(\n#         loss=\"squared_error\",\n#         learning_rate=0.1,\n#         max_iter=100,\n#         max_leaf_nodes=31,\n#         max_depth=None,\n#         min_samples_leaf=20,\n#         l2_regularization=0.0,\n#         max_bins=255,\n#         categorical_features=None,\n#         monotonic_cst=None,\n#         warm_start=False,\n#         early_stopping=\"auto\",\n#         scoring=\"loss\",\n#         validation_fraction=0.1,\n#         n_iter_no_change=10,\n#         tol=1e-7,\n#         verbose=0,\n#         random_state=None,\n#     )\n#\n# which is much more readable. Positional arguments have been deprecated since\n# version 0.23 and will now raise a ``TypeError``. A limited number of\n# positional arguments are still allowed in some cases, for example in\n# :class:`~decomposition.PCA`, where ``PCA(10)`` is still allowed, but ``PCA(10,\n# False)`` is not allowed.\n\n##############################################################################\n# Spline Transformers\n# ---------------------------------------------------------\n# One way to add nonlinear terms to a dataset's feature set is to generate",
                "filename": "examples/release_highlights/plot_release_highlights_1_0_0.py",
                "start_index": 0,
                "end_index": 2999,
                "start_line": 1,
                "end_line": 78,
                "max_line": 241,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "class _BaseImputer(TransformerMixin, BaseEstimator):\n    \"\"\"Base class for all imputers.\n\n    It adds automatically support for `add_indicator`.\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"missing_values\": [MissingValues()],\n        \"add_indicator\": [\"boolean\"],\n        \"keep_empty_features\": [\"boolean\"],\n    }\n\n    def __init__(\n        self, *, missing_values=np.nan, add_indicator=False, keep_empty_features=False\n    ):\n        self.missing_values = missing_values\n        self.add_indicator = add_indicator\n        self.keep_empty_features = keep_empty_features\n\n    def _fit_indicator(self, X):\n        \"\"\"Fit a MissingIndicator.\"\"\"\n        if self.add_indicator:\n            self.indicator_ = MissingIndicator(\n                missing_values=self.missing_values, error_on_new=False\n            )\n            self.indicator_._fit(X, precomputed=True)\n        else:\n            self.indicator_ = None\n\n    def _transform_indicator(self, X):\n        \"\"\"Compute the indicator mask.'\n\n        Note that X must be the original data as passed to the imputer before\n        any imputation, since imputation may be done inplace in some cases.\n        \"\"\"\n        if self.add_indicator:\n            if not hasattr(self, \"indicator_\"):\n                raise ValueError(\n                    \"Make sure to call _fit_indicator before _transform_indicator\"\n                )\n            return self.indicator_.transform(X)\n\n    def _concatenate_indicator(self, X_imputed, X_indicator):\n        \"\"\"Concatenate indicator mask with the imputed data.\"\"\"\n        if not self.add_indicator:\n            return X_imputed\n\n        hstack = sp.hstack if sp.issparse(X_imputed) else np.hstack\n        if X_indicator is None:\n            raise ValueError(\n                \"Data from the missing indicator are not provided. Call \"\n                \"_fit_indicator and _transform_indicator in the imputer \"\n                \"implementation.\"\n            )\n\n        return hstack((X_imputed, X_indicator))\n\n    def _concatenate_indicator_feature_names_out(self, names, input_features):\n        if not self.add_indicator:\n            return names\n\n        indicator_names = self.indicator_.get_feature_names_out(input_features)\n        return np.concatenate([names, indicator_names])\n\n    def _more_tags(self):\n        return {\"allow_nan\": is_scalar_nan(self.missing_values)}",
                "filename": "sklearn/impute/_base.py",
                "start_index": 2536,
                "end_index": 4899,
                "start_line": 71,
                "end_line": 136,
                "max_line": 1053,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "from sklearn.pipeline import make_pipeline\n\nX, y = make_classification(random_state=0)\n\nwith TemporaryDirectory(prefix=\"sklearn_cache_\") as tmpdir:\n    estimator = make_pipeline(\n        KNeighborsTransformer(n_neighbors=10, mode=\"distance\"),\n        Isomap(n_neighbors=10, metric=\"precomputed\"),\n        memory=tmpdir,\n    )\n    estimator.fit(X)\n\n    # We can decrease the number of neighbors and the graph will not be\n    # recomputed.\n    estimator.set_params(isomap__n_neighbors=5)\n    estimator.fit(X)\n\n# %%\n# KNN Based Imputation\n# ------------------------------------\n# We now support imputation for completing missing values using k-Nearest\n# Neighbors.\n#\n# Each sample's missing values are imputed using the mean value from\n# ``n_neighbors`` nearest neighbors found in the training set. Two samples are\n# close if the features that neither is missing are close.\n# By default, a euclidean distance metric\n# that supports missing values,\n# :func:`~sklearn.metrics.pairwise.nan_euclidean_distances`, is used to find the nearest\n# neighbors.\n#\n# Read more in the :ref:`User Guide <knnimpute>`.\n\nfrom sklearn.impute import KNNImputer\n\nX = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\nimputer = KNNImputer(n_neighbors=2)\nprint(imputer.fit_transform(X))\n\n# %%\n# Tree pruning\n# ------------\n#\n# It is now possible to prune most tree-based estimators once the trees are\n# built. The pruning is based on minimal cost-complexity. Read more in the\n# :ref:`User Guide <minimal_cost_complexity_pruning>` for details.\n\nX, y = make_classification(random_state=0)\n\nrf = RandomForestClassifier(random_state=0, ccp_alpha=0).fit(X, y)\nprint(\n    \"Average number of nodes without pruning {:.1f}\".format(\n        np.mean([e.tree_.node_count for e in rf.estimators_])\n    )\n)\n\nrf = RandomForestClassifier(random_state=0, ccp_alpha=0.05).fit(X, y)\nprint(\n    \"Average number of nodes with pruning {:.1f}\".format(\n        np.mean([e.tree_.node_count for e in rf.estimators_])\n    )\n)\n\n# %%\n# Retrieve dataframes from OpenML\n# -------------------------------\n# :func:`datasets.fetch_openml` can now return pandas dataframe and thus\n# properly handle datasets with heterogeneous data:\n\nfrom sklearn.datasets import fetch_openml\n\ntitanic = fetch_openml(\"titanic\", version=1, as_frame=True, parser=\"pandas\")\nprint(titanic.data.head()[[\"pclass\", \"embarked\"]])\n\n# %%\n# Checking scikit-learn compatibility of an estimator\n# ---------------------------------------------------\n# Developers can check the compatibility of their scikit-learn compatible\n# estimators using :func:`~utils.estimator_checks.check_estimator`. For\n# instance, the ``check_estimator(LinearSVC())`` passes.\n#\n# We now provide a ``pytest`` specific decorator which allows ``pytest``\n# to run all checks independently and report the checks that are failing.\n#\n# ..note::\n#   This entry was slightly updated in version 0.24, where passing classes\n#   isn't supported anymore: pass instances instead.",
                "filename": "examples/release_highlights/plot_release_highlights_0_22_0.py",
                "start_index": 5945,
                "end_index": 8900,
                "start_line": 85,
                "end_line": 248,
                "max_line": 282,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "def _deprecate_normalize(normalize, estimator_name):\n    \"\"\"Normalize is to be deprecated from linear models and a use of\n    a pipeline with a StandardScaler is to be recommended instead.\n    Here the appropriate message is selected to be displayed to the user\n    depending on the default normalize value (as it varies between the linear\n    models and normalize value selected by the user).\n\n    Parameters\n    ----------\n    normalize : bool,\n        normalize value passed by the user\n\n    estimator_name : str\n        name of the linear estimator which calls this function.\n        The name will be used for writing the deprecation warnings\n\n    Returns\n    -------\n    normalize : bool,\n        normalize value which should further be used by the estimator at this\n        stage of the depreciation process\n\n    Notes\n    -----\n    This function should be completely removed in 1.4.\n    \"\"\"\n\n    if normalize not in [True, False, \"deprecated\"]:\n        raise ValueError(\n            \"Leave 'normalize' to its default value or set it to True or False\"\n        )\n\n    if normalize == \"deprecated\":\n        _normalize = False\n    else:\n        _normalize = normalize\n\n    pipeline_msg = (\n        \"If you wish to scale the data, use Pipeline with a StandardScaler \"\n        \"in a preprocessing stage. To reproduce the previous behavior:\\n\\n\"\n        \"from sklearn.pipeline import make_pipeline\\n\\n\"\n        \"model = make_pipeline(StandardScaler(with_mean=False), \"\n        f\"{estimator_name}())\\n\\n\"\n        \"If you wish to pass a sample_weight parameter, you need to pass it \"\n        \"as a fit parameter to each step of the pipeline as follows:\\n\\n\"\n        \"kwargs = {s[0] + '__sample_weight': sample_weight for s \"\n        \"in model.steps}\\n\"\n        \"model.fit(X, y, **kwargs)\\n\\n\"\n    )\n\n    alpha_msg = \"\"\n    if \"LassoLars\" in estimator_name:\n        alpha_msg = \"Set parameter alpha to: original_alpha * np.sqrt(n_samples). \"\n\n    if normalize != \"deprecated\" and normalize:\n        warnings.warn(\n            \"'normalize' was deprecated in version 1.2 and will be removed in 1.4.\\n\"\n            + pipeline_msg\n            + alpha_msg,\n            FutureWarning,\n        )\n    elif not normalize:\n        warnings.warn(\n            (\n                \"'normalize' was deprecated in version 1.2 and will be \"\n                \"removed in 1.4. \"\n                \"Please leave the normalize parameter to its default value to \"\n                \"silence this warning. The default behavior of this estimator \"\n                \"is to not do any normalization. If normalization is needed \"\n                \"please use sklearn.preprocessing.StandardScaler instead.\"\n            ),\n            FutureWarning,\n        )\n\n    return _normalize",
                "filename": "sklearn/linear_model/_base.py",
                "start_index": 1810,
                "end_index": 4552,
                "start_line": 59,
                "end_line": 133,
                "max_line": 920,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "if sp.issparse(X):\n            if self.missing_values == 0:\n                raise ValueError(\n                    \"Imputation not possible when missing_values \"\n                    \"== 0 and input is sparse. Provide a dense \"\n                    \"array instead.\"\n                )\n            else:\n                # if no invalid statistics are found, use the mask computed\n                # before, else recompute mask\n                if valid_statistics_indexes is None:\n                    mask = missing_mask.data\n                else:\n                    mask = _get_mask(X.data, self.missing_values)\n                indexes = np.repeat(\n                    np.arange(len(X.indptr) - 1, dtype=int), np.diff(X.indptr)\n                )[mask]\n\n                X.data[mask] = valid_statistics[indexes].astype(X.dtype, copy=False)\n        else:\n            # use mask computed before eliminating invalid mask\n            if valid_statistics_indexes is None:\n                mask_valid_features = missing_mask\n            else:\n                mask_valid_features = missing_mask[:, valid_statistics_indexes]\n            n_missing = np.sum(mask_valid_features, axis=0)\n            values = np.repeat(valid_statistics, n_missing)\n            coordinates = np.where(mask_valid_features.transpose())[::-1]\n\n            X[coordinates] = values\n\n        X_indicator = super()._transform_indicator(missing_mask)\n\n        return super()._concatenate_indicator(X, X_indicator)",
                "filename": "sklearn/impute/_base.py",
                "start_index": 21022,
                "end_index": 22490,
                "start_line": 390,
                "end_line": 596,
                "max_line": 1053,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "# flake8: noqa\n\"\"\"\n=======================================\nRelease Highlights for scikit-learn 1.1\n=======================================\n\n.. currentmodule:: sklearn\n\nWe are pleased to announce the release of scikit-learn 1.1! Many bug fixes\nand improvements were added, as well as some new key features. We detail\nbelow a few of the major features of this release. **For an exhaustive list of\nall the changes**, please refer to the :ref:`release notes <changes_1_1>`.\n\nTo install the latest version (with pip)::\n\n    pip install --upgrade scikit-learn\n\nor with conda::\n\n    conda install -c conda-forge scikit-learn\n\n\"\"\"\n\n# %%\n# Quantile loss in :class:`ensemble.HistGradientBoostingRegressor`\n# ----------------------------------------------------------------\n# :class:`~ensemble.HistGradientBoostingRegressor` can model quantiles with\n# `loss=\"quantile\"` and the new parameter `quantile`.\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simple regression function for X * cos(X)\nrng = np.random.RandomState(42)\nX_1d = np.linspace(0, 10, num=2000)\nX = X_1d.reshape(-1, 1)\ny = X_1d * np.cos(X_1d) + rng.normal(scale=X_1d / 3)\n\nquantiles = [0.95, 0.5, 0.05]\nparameters = dict(loss=\"quantile\", max_bins=32, max_iter=50)\nhist_quantiles = {\n    f\"quantile={quantile:.2f}\": HistGradientBoostingRegressor(\n        **parameters, quantile=quantile\n    ).fit(X, y)\n    for quantile in quantiles\n}\n\nfig, ax = plt.subplots()\nax.plot(X_1d, y, \"o\", alpha=0.5, markersize=1)\nfor quantile, hist in hist_quantiles.items():\n    ax.plot(X_1d, hist.predict(X), label=quantile)\n_ = ax.legend(loc=\"lower left\")\n\n\n# %%\n# `get_feature_names_out` Available in all Transformers\n# -----------------------------------------------------\n# :term:`get_feature_names_out` is now available in all Transformers. This enables\n# :class:`~pipeline.Pipeline` to construct the output feature names for more complex\n# pipelines:\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import LogisticRegression\n\nX, y = fetch_openml(\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n)\nnumeric_features = [\"age\", \"fare\"]\nnumeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\ncategorical_features = [\"embarked\", \"pclass\"]\n\npreprocessor = ColumnTransformer(\n    [\n        (\"num\", numeric_transformer, numeric_features),\n        (\n            \"cat\",\n            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n            categorical_features,\n        ),\n    ],\n    verbose_feature_names_out=False,\n)\nlog_reg = make_pipeline(preprocessor, SelectKBest(k=7), LogisticRegression())\nlog_reg.fit(X, y)\n\n\n# %%",
                "filename": "examples/release_highlights/plot_release_highlights_1_1_0.py",
                "start_index": 0,
                "end_index": 2954,
                "start_line": 1,
                "end_line": 202,
                "max_line": 229,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"Transformers for missing value imputation\"\"\"\nimport typing\n\nfrom ._base import MissingIndicator, SimpleImputer\nfrom ._knn import KNNImputer\n\nif typing.TYPE_CHECKING:\n    # Avoid errors in type checkers (e.g. mypy) for experimental estimators.\n    # TODO: remove this check once the estimator is no longer experimental.\n    from ._iterative import IterativeImputer  # noqa\n\n__all__ = [\"MissingIndicator\", \"SimpleImputer\", \"KNNImputer\"]\n\n\n# TODO: remove this check once the estimator is no longer experimental.\ndef __getattr__(name):\n    if name == \"IterativeImputer\":\n        raise ImportError(\n            f\"{name} is experimental and the API might change without any \"\n            \"deprecation cycle. To use it, you need to explicitly import \"\n            \"enable_iterative_imputer:\\n\"\n            \"from sklearn.experimental import enable_iterative_imputer\"\n        )\n    raise AttributeError(f\"module {__name__} has no attribute {name}\")",
                "filename": "sklearn/impute/__init__.py",
                "start_index": 0,
                "end_index": 942,
                "start_line": 1,
                "end_line": 24,
                "max_line": 24,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "import warnings\nfrom collections import namedtuple\nfrom numbers import Integral, Real\nfrom time import time\n\nimport numpy as np\nfrom scipy import stats\n\nfrom ..base import _fit_context, clone\nfrom ..exceptions import ConvergenceWarning\nfrom ..preprocessing import normalize\nfrom ..utils import (\n    _safe_assign,\n    _safe_indexing,\n    check_array,\n    check_random_state,\n    is_scalar_nan,\n)\nfrom ..utils._mask import _get_mask\nfrom ..utils._param_validation import HasMethods, Interval, StrOptions\nfrom ..utils.validation import FLOAT_DTYPES, _check_feature_names_in, check_is_fitted\nfrom ._base import SimpleImputer, _BaseImputer, _check_inputs_dtype\n\n_ImputerTriplet = namedtuple(\n    \"_ImputerTriplet\", [\"feat_idx\", \"neighbor_feat_idx\", \"estimator\"]\n)\n\n\ndef _assign_where(X1, X2, cond):\n    \"\"\"Assign X2 to X1 where cond is True.\n\n    Parameters\n    ----------\n    X1 : ndarray or dataframe of shape (n_samples, n_features)\n        Data.\n\n    X2 : ndarray of shape (n_samples, n_features)\n        Data to be assigned.\n\n    cond : ndarray of shape (n_samples, n_features)\n        Boolean mask to assign data.\n    \"\"\"\n    if hasattr(X1, \"mask\"):  # pandas dataframes\n        X1.mask(cond=cond, other=X2, inplace=True)\n    else:  # ndarrays\n        X1[cond] = X2[cond]",
                "filename": "sklearn/impute/_iterative.py",
                "start_index": 0,
                "end_index": 1273,
                "start_line": 1,
                "end_line": 46,
                "max_line": 901,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "num_proc = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n\ncat_proc = make_pipeline(\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n    OneHotEncoder(handle_unknown=\"ignore\"),\n)\n\npreprocessor = make_column_transformer(\n    (num_proc, (\"feat1\", \"feat3\")), (cat_proc, (\"feat0\", \"feat2\"))\n)\n\nclf = make_pipeline(preprocessor, LogisticRegression())\nclf\n\n##############################################################################\n# Scalability and stability improvements to KMeans\n# ------------------------------------------------\n# The :class:`~sklearn.cluster.KMeans` estimator was entirely re-worked, and it\n# is now significantly faster and more stable. In addition, the Elkan algorithm\n# is now compatible with sparse matrices. The estimator uses OpenMP based\n# parallelism instead of relying on joblib, so the `n_jobs` parameter has no\n# effect anymore. For more details on how to control the number of threads,\n# please refer to our :ref:`parallelism` notes.\nimport scipy\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import completeness_score\n\nrng = np.random.RandomState(0)\nX, y = make_blobs(random_state=rng)\nX = scipy.sparse.csr_matrix(X)\nX_train, X_test, _, y_test = train_test_split(X, y, random_state=rng)\nkmeans = KMeans(n_init=\"auto\").fit(X_train)\nprint(completeness_score(kmeans.predict(X_test), y_test))\n\n##############################################################################\n# Improvements to the histogram-based Gradient Boosting estimators\n# ----------------------------------------------------------------\n# Various improvements were made to\n# :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and\n# :class:`~sklearn.ensemble.HistGradientBoostingRegressor`. On top of the\n# Poisson loss mentioned above, these estimators now support :ref:`sample\n# weights <sw_hgbdt>`. Also, an automatic early-stopping criterion was added:\n# early-stopping is enabled by default when the number of samples exceeds 10k.\n# Finally, users can now define :ref:`monotonic constraints\n# <monotonic_cst_gbdt>` to constrain the predictions based on the variations of\n# specific features. In the following example, we construct a target that is\n# generally positively correlated with the first feature, with some noise.\n# Applying monotoinc constraints allows the prediction to capture the global\n# effect of the first feature, instead of fitting the noise.\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# from sklearn.inspection import plot_partial_dependence\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nn_samples = 500\nrng = np.random.RandomState(0)\nX = rng.randn(n_samples, 2)\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\ny = 5 * X[:, 0] + np.sin(10 * np.pi * X[:, 0]) - noise",
                "filename": "examples/release_highlights/plot_release_highlights_0_23_0.py",
                "start_index": 2976,
                "end_index": 5971,
                "start_line": 74,
                "end_line": 138,
                "max_line": 189,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "X = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=[\"a\", \"b\", \"c\"])\nscalar = StandardScaler().fit(X)\nscalar.feature_names_in_\n\n# %%\n# The support of :term:`get_feature_names_out` is available for transformers\n# that already had `get_feature_names` and transformers with a one-to-one\n# correspondence between input and output such as\n# :class:`~preprocessing.StandardScaler`. :term:`get_feature_names_out` support\n# will be added to all other transformers in future releases. Additionally,\n# :meth:`compose.ColumnTransformer.get_feature_names_out` is available to\n# combine feature names of its transformers:\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\n\nX = pd.DataFrame({\"pet\": [\"dog\", \"cat\", \"fish\"], \"age\": [3, 7, 1]})\npreprocessor = ColumnTransformer(\n    [\n        (\"numerical\", StandardScaler(), [\"age\"]),\n        (\"categorical\", OneHotEncoder(), [\"pet\"]),\n    ],\n    verbose_feature_names_out=False,\n).fit(X)\n\npreprocessor.get_feature_names_out()\n\n# %%\n# When this ``preprocessor`` is used with a pipeline, the feature names used\n# by the classifier are obtained by slicing and calling\n# :term:`get_feature_names_out`:\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\ny = [1, 0, 1]\npipe = make_pipeline(preprocessor, LogisticRegression())\npipe.fit(X, y)\npipe[:-1].get_feature_names_out()\n\n\n##############################################################################\n# A more flexible plotting API\n# --------------------------------------------------------------------------\n# :class:`metrics.ConfusionMatrixDisplay`,\n# :class:`metrics.PrecisionRecallDisplay`, :class:`metrics.DetCurveDisplay`,\n# and :class:`inspection.PartialDependenceDisplay` now expose two class\n# methods: `from_estimator` and `from_predictions` which allow users to create\n# a plot given the predictions or an estimator. This means the corresponding\n# `plot_*` functions are deprecated. Please check :ref:`example one\n# <sphx_glr_auto_examples_model_selection_plot_confusion_matrix.py>` and\n# :ref:`example two\n# <sphx_glr_auto_examples_classification_plot_digits_classification.py>` for\n# how to use the new plotting functionalities.\n\n##############################################################################\n# Online One-Class SVM\n# --------------------------------------------------------------------------\n# The new class :class:`~linear_model.SGDOneClassSVM` implements an online\n# linear version of the One-Class SVM using a stochastic gradient descent.\n# Combined with kernel approximation techniques,\n# :class:`~linear_model.SGDOneClassSVM` can be used to approximate the solution\n# of a kernelized One-Class SVM, implemented in :class:`~svm.OneClassSVM`, with\n# a fit time complexity linear in the number of samples. Note that the\n# complexity of a kernelized One-Class SVM is at best quadratic in the number\n# of samples. :class:`~linear_model.SGDOneClassSVM` is thus well suited for",
                "filename": "examples/release_highlights/plot_release_highlights_1_0_0.py",
                "start_index": 5998,
                "end_index": 8992,
                "start_line": 151,
                "end_line": 215,
                "max_line": 241,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            }
        ],
        "classification": "feature request",
        "snippet_processor_task": {
            "sklearn/impute/_base.py": [
                {
                    "chunk": {
                        "code": "class _BaseImputer(TransformerMixin, BaseEstimator):\n    \"\"\"Base class for all imputers.\n\n    It adds automatically support for `add_indicator`.\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"missing_values\": [MissingValues()],\n        \"add_indicator\": [\"boolean\"],\n        \"keep_empty_features\": [\"boolean\"],\n    }\n\n    def __init__(\n        self, *, missing_values=np.nan, add_indicator=False, keep_empty_features=False\n    ):\n        self.missing_values = missing_values\n        self.add_indicator = add_indicator\n        self.keep_empty_features = keep_empty_features\n\n    def _fit_indicator(self, X):\n        \"\"\"Fit a MissingIndicator.\"\"\"\n        if self.add_indicator:\n            self.indicator_ = MissingIndicator(\n                missing_values=self.missing_values, error_on_new=False\n            )\n            self.indicator_._fit(X, precomputed=True)\n        else:\n            self.indicator_ = None\n\n    def _transform_indicator(self, X):\n        \"\"\"Compute the indicator mask.'\n\n        Note that X must be the original data as passed to the imputer before\n        any imputation, since imputation may be done inplace in some cases.\n        \"\"\"\n        if self.add_indicator:\n            if not hasattr(self, \"indicator_\"):\n                raise ValueError(\n                    \"Make sure to call _fit_indicator before _transform_indicator\"\n                )\n            return self.indicator_.transform(X)\n\n    def _concatenate_indicator(self, X_imputed, X_indicator):\n        \"\"\"Concatenate indicator mask with the imputed data.\"\"\"\n        if not self.add_indicator:\n            return X_imputed\n\n        hstack = sp.hstack if sp.issparse(X_imputed) else np.hstack\n        if X_indicator is None:\n            raise ValueError(\n                \"Data from the missing indicator are not provided. Call \"\n                \"_fit_indicator and _transform_indicator in the imputer \"\n                \"implementation.\"\n            )\n\n        return hstack((X_imputed, X_indicator))\n\n    def _concatenate_indicator_feature_names_out(self, names, input_features):\n        if not self.add_indicator:\n            return names\n\n        indicator_names = self.indicator_.get_feature_names_out(input_features)\n        return np.concatenate([names, indicator_names])\n\n    def _more_tags(self):\n        return {\"allow_nan\": is_scalar_nan(self.missing_values)}",
                        "filename": "sklearn/impute/_base.py",
                        "start_index": 2536,
                        "end_index": 4899,
                        "start_line": 71,
                        "end_line": 136,
                        "max_line": 1053,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet is from the file where the `Imputer` class is defined, which is directly related to the issue of deprecating the `axis` parameter."
                },
                {
                    "chunk": {
                        "code": "if sp.issparse(X):\n            if self.missing_values == 0:\n                raise ValueError(\n                    \"Imputation not possible when missing_values \"\n                    \"== 0 and input is sparse. Provide a dense \"\n                    \"array instead.\"\n                )\n            else:\n                # if no invalid statistics are found, use the mask computed\n                # before, else recompute mask\n                if valid_statistics_indexes is None:\n                    mask = missing_mask.data\n                else:\n                    mask = _get_mask(X.data, self.missing_values)\n                indexes = np.repeat(\n                    np.arange(len(X.indptr) - 1, dtype=int), np.diff(X.indptr)\n                )[mask]\n\n                X.data[mask] = valid_statistics[indexes].astype(X.dtype, copy=False)\n        else:\n            # use mask computed before eliminating invalid mask\n            if valid_statistics_indexes is None:\n                mask_valid_features = missing_mask\n            else:\n                mask_valid_features = missing_mask[:, valid_statistics_indexes]\n            n_missing = np.sum(mask_valid_features, axis=0)\n            values = np.repeat(valid_statistics, n_missing)\n            coordinates = np.where(mask_valid_features.transpose())[::-1]\n\n            X[coordinates] = values\n\n        X_indicator = super()._transform_indicator(missing_mask)\n\n        return super()._concatenate_indicator(X, X_indicator)",
                        "filename": "sklearn/impute/_base.py",
                        "start_index": 21022,
                        "end_index": 22490,
                        "start_line": 390,
                        "end_line": 596,
                        "max_line": 1053,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet is also from the `_base.py` file and contains logic for the `Imputer` class, which may need to be reviewed or modified in relation to the deprecation of the `axis` parameter."
                }
            ],
            "sklearn/impute/__init__.py": [
                {
                    "chunk": {
                        "code": "\"\"\"Transformers for missing value imputation\"\"\"\nimport typing\n\nfrom ._base import MissingIndicator, SimpleImputer\nfrom ._knn import KNNImputer\n\nif typing.TYPE_CHECKING:\n    # Avoid errors in type checkers (e.g. mypy) for experimental estimators.\n    # TODO: remove this check once the estimator is no longer experimental.\n    from ._iterative import IterativeImputer  # noqa\n\n__all__ = [\"MissingIndicator\", \"SimpleImputer\", \"KNNImputer\"]\n\n\n# TODO: remove this check once the estimator is no longer experimental.\ndef __getattr__(name):\n    if name == \"IterativeImputer\":\n        raise ImportError(\n            f\"{name} is experimental and the API might change without any \"\n            \"deprecation cycle. To use it, you need to explicitly import \"\n            \"enable_iterative_imputer:\\n\"\n            \"from sklearn.experimental import enable_iterative_imputer\"\n        )\n    raise AttributeError(f\"module {__name__} has no attribute {name}\")",
                        "filename": "sklearn/impute/__init__.py",
                        "start_index": 0,
                        "end_index": 942,
                        "start_line": 1,
                        "end_line": 24,
                        "max_line": 24,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet is from the `__init__.py` file in the `impute` module and could be relevant for ensuring that the deprecation message or documentation is updated accordingly."
                }
            ]
        },
        "solution_task": "To deprecate the `axis` parameter from the `Imputer` class in scikit-learn, follow these steps:\n\n- Identify the `Imputer` class in the codebase. It seems that the `Imputer` class is not directly provided in the provided snippets, but you might be referring to `SimpleImputer` or a similar class that has an `axis` parameter.\n- Add a deprecation warning in the `__init__` method of the `Imputer` class (or the relevant class with the `axis` parameter).\n- Set the default value of the `axis` parameter to `None` or another value that indicates deprecation.\n- In the `fit`, `transform`, and `fit_transform` methods, check if `axis` is not `None` (or the deprecation value) and issue a deprecation warning if it is used.\n- Update the documentation to reflect the deprecation.\n- Remove any internal logic that uses the `axis` parameter, as it will no longer be supported.\n- Optionally, provide a utility function `row_impute` as suggested in the ticket.\n\nHere is an example of how you might modify the `SimpleImputer` class to deprecate the `axis` parameter:\n\n```python\nfrom sklearn.utils.validation import check_is_fitted\nimport warnings\n\nclass SimpleImputer(_BaseImputer):\n    # Add the axis parameter with a default value indicating deprecation\n    def __init__(self, missing_values=np.nan, strategy=\"mean\", fill_value=None, verbose=0, copy=True, add_indicator=False, axis=None):\n        super().__init__(missing_values=missing_values, add_indicator=add_indicator)\n        self.strategy = strategy\n        self.fill_value = fill_value\n        self.verbose = verbose\n        self.copy = copy\n        self.axis = axis  # Deprecated parameter\n\n    def fit(self, X, y=None):\n        # Deprecation warning for the axis parameter\n        if self.axis is not None:\n            warnings.warn(\n                \"The 'axis' parameter is deprecated and will be removed in version 1.2. \"\n                \"Use the 'row_impute' function instead.\",\n                FutureWarning\n            )\n        # Rest of the fit method implementation...\n        # ...\n\n    def transform(self, X):\n        check_is_fitted(self)\n        # Deprecation warning for the axis parameter\n        if self.axis is not None:\n            warnings.warn(\n                \"The 'axis' parameter is deprecated and will be removed in version 1.2. \"\n                \"Use the 'row_impute' function instead.\",\n                FutureWarning\n            )\n        # Rest of the transform method implementation...\n        # ...\n\n# Utility function as suggested in the ticket\ndef row_impute(X, **kwargs):\n    return SimpleImputer(**kwargs).fit_transform(X.T).T\n```\n\nRemember to update the documentation accordingly and remove any test cases that involve the deprecated `axis` parameter. After the deprecation period, you can completely remove the `axis` parameter from the class signature and any related logic.",
        "bouncer_response": {
            "score": 7,
            "criteria": "RESPONSE must provide actionable, relevant guidance to more parts of the PROMPT's described issue to score higher.",
            "supporting_evidence": "The RESPONSE provides a comprehensive guide on how to deprecate the `axis` parameter from the `Imputer` class, which is the main issue described in the PROMPT. It includes actionable steps such as adding a deprecation warning, setting the default value of the `axis` parameter to `None`, updating the documentation, and providing an example code snippet. The RESPONSE also suggests providing a utility function `row_impute` as mentioned in the PROMPT. Additionally, it references specific code snippets from a GitHub repository that may need to be reviewed or modified in relation to the deprecation, which is highly relevant to the task at hand. However, the RESPONSE assumes the existence of an `Imputer` class, which is not explicitly mentioned in the PROMPT, and it does not address the broader context of why the feature is being deprecated, such as the strange feature support in a machine learning context or the complications in implementation. Therefore, while the RESPONSE is highly relevant and provides clear guidance, it does not fully address all aspects of the described feature request.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711742508.646854,
        "result": "To deprecate the `axis` parameter from the `Imputer` class in scikit-learn, follow these steps:\n\n- Identify the `Imputer` class in the codebase. It seems that the `Imputer` class is not directly provided in the provided snippets, but you might be referring to `SimpleImputer` or a similar class that has an `axis` parameter.\n- Add a deprecation warning in the `__init__` method of the `Imputer` class (or the relevant class with the `axis` parameter).\n- Set the default value of the `axis` parameter to `None` or another value that indicates deprecation.\n- In the `fit`, `transform`, and `fit_transform` methods, check if `axis` is not `None` (or the deprecation value) and issue a deprecation warning if it is used.\n- Update the documentation to reflect the deprecation.\n- Remove any internal logic that uses the `axis` parameter, as it will no longer be supported.\n- Optionally, provide a utility function `row_impute` as suggested in the ticket.\n\nHere is an example of how you might modify the `SimpleImputer` class to deprecate the `axis` parameter:\n\n```python\nfrom sklearn.utils.validation import check_is_fitted\nimport warnings\n\nclass SimpleImputer(_BaseImputer):\n    # Add the axis parameter with a default value indicating deprecation\n    def __init__(self, missing_values=np.nan, strategy=\"mean\", fill_value=None, verbose=0, copy=True, add_indicator=False, axis=None):\n        super().__init__(missing_values=missing_values, add_indicator=add_indicator)\n        self.strategy = strategy\n        self.fill_value = fill_value\n        self.verbose = verbose\n        self.copy = copy\n        self.axis = axis  # Deprecated parameter\n\n    def fit(self, X, y=None):\n        # Deprecation warning for the axis parameter\n        if self.axis is not None:\n            warnings.warn(\n                \"The 'axis' parameter is deprecated and will be removed in version 1.2. \"\n                \"Use the 'row_impute' function instead.\",\n                FutureWarning\n            )\n        # Rest of the fit method implementation...\n        # ...\n\n    def transform(self, X):\n        check_is_fitted(self)\n        # Deprecation warning for the axis parameter\n        if self.axis is not None:\n            warnings.warn(\n                \"The 'axis' parameter is deprecated and will be removed in version 1.2. \"\n                \"Use the 'row_impute' function instead.\",\n                FutureWarning\n            )\n        # Rest of the transform method implementation...\n        # ...\n\n# Utility function as suggested in the ticket\ndef row_impute(X, **kwargs):\n    return SimpleImputer(**kwargs).fit_transform(X.T).T\n```\n\nRemember to update the documentation accordingly and remove any test cases that involve the deprecated `axis` parameter. After the deprecation period, you can completely remove the `axis` parameter from the class signature and any related logic.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [impute](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/impute)- [Lines 71 - 136](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/impute/_base.py#L71-L136)\n\n This snippet is from the file where the `Imputer` class is defined, which is directly related to the issue of deprecating the `axis` parameter. \n\n\n   - [Lines 390 - 596](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/impute/_base.py#L390-L596)\n\n This snippet is also from the `_base.py` file and contains logic for the `Imputer` class, which may need to be reviewed or modified in relation to the deprecation of the `axis` parameter.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [impute](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/impute)- [Lines 1 - 24](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/impute/__init__.py#L1-L24)\n\n This snippet is from the `__init__.py` file in the `impute` module and could be relevant for ensuring that the deprecation message or documentation is updated accordingly."
    }
}