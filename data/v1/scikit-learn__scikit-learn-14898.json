{
    "id": "scikit-learn__scikit-learn-14898",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
    "task_id": 7730343140649421415,
    "title": "Documentation section 3.3.1.1 has incorrect description of brier_score_loss",
    "body": "In the documentation, section 3.3.1.1. \"Common cases: predefined values\" includes the remark\r\n\r\n> All scorer objects follow the convention that higher return values are better than lower return values. \r\n\r\nAs far as I can tell, this is true for all of the listed metrics, **except** the `brier_score_loss`. In the case of `brier_score_loss`, a _lower loss value is better._ This is because `brier_score_loss` measures the mean-square difference between a predicted probability and a categorical outcome; the Brier score is _minimized_ at 0.0 because all summands are either `(0 - 0) ^ 2=0` or `(1 -1) ^ 2=0` when the model is making perfect predictions. On the other hand, the Brier score is _maximized_ at 1.0 when all predictions are **opposite** the correct label, as all summands are either `(0 - 1)^2=1` or `(1 - 0)^2=1`.\r\n\r\nTherefore, the definition of the `brier_score_loss` is not consistent with the quotation from section 3.3.1.1. \r\n\r\nI suggest making 2 changes to relieve this confusion.\r\n\r\n1. Implement a function `neg_brier_score_loss` which simply negates the value of `brier_score_loss`; this is a direct analogy to what is done in the case of `neg_log_loss`. A better model has a lower value of log-loss (categorical cross-entropy loss), therefore a larger value of the _negative_ log-loss implies a better model. Naturally, the same is true for Brier score, where it is also the case that a better model is assigned a lower loss.\r\n\r\n2. Remove reference to `brier_score_loss` from section 3.3.1.1. Brier score is useful in lots of ways; however, because it does not have the property that a larger value implies a better model, it seems confusing to mention it in the context of section 3.3.1.1. References to `brier_score_loss` can be replaced with `neg_brier_score_loss`, which has the property that better models have large values, just like accuracy, ROC AUC and the rest of the listed metrics.\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.22.git",
    "requested_commit": "",
    "mayil_response": "- Update the documentation in section 3.3.1.1 to correctly describe the behavior of `brier_score_loss`.\n- Implement `neg_brier_score_loss` function that returns the negative Brier score loss.\n- Replace references to `brier_score_loss` with `neg_brier_score_loss` in section 3.3.1.1 of the documentation.\n- Add `neg_brier_score_loss` to `sklearn/metrics/_scorer.py` as a scorer object using `make_scorer` with `greater_is_better=True`.\n\n```python\n# sklearn/metrics/_classification.py\ndef neg_brier_score_loss(y_true, y_prob, *, pos_label=None, sample_weight=None):\n    \"\"\"\n    Calculate the negative Brier score loss.\n\n    Parameters:\n    - y_true : array-like of shape (n_samples,)\n        True labels.\n    - y_prob : array-like of shape (n_samples,)\n        Predicted probabilities.\n    - pos_label : int or str, default=None\n        The label of the positive class.\n    - sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n    Returns:\n    - score : float\n        The negative Brier score loss.\n    \"\"\"\n    return -brier_score_loss(y_true, y_prob, pos_label=pos_label, sample_weight=sample_weight)\n\n# sklearn/metrics/_scorer.py\nneg_brier_score_loss_scorer = make_scorer(neg_brier_score_loss, greater_is_better=True, needs_proba=True)\n```\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [metrics](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/metrics)- [Lines 743 - 822](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/metrics/_scorer.py#L743-L822)\n\n This snippet defines the scorers for various metrics, including the `neg_brier_score_scorer` which is relevant to the issue as it suggests the implementation of a negated version of the `brier_score_loss`.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [metrics](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/metrics)- [Lines 3084 - 3159](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/metrics/_classification.py#L3084-L3159)\n\n This snippet contains the detailed documentation and implementation of the `brier_score_loss` function, which is directly related to the issue as it explains the behavior and usage of the metric that is inconsistent with the documentation section mentioned in the issue. \n\n\n   - [Lines 3083 - 3083](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/metrics/_classification.py#L3083-L3083)\n\n This snippet shows the function signature of `brier_score_loss`, which is relevant to the issue as it is the function that needs to be negated to create `neg_brier_score_loss`.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0003105,
        "snippet_processor": 0.11024,
        "issue_star_creation": 0.03695,
        "issue_star_solver": 0.1084,
        "bouncer": 0.03046
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711738738.7377658,
        "relevant_snippets": [
            {
                "code": "prob_pos_sigmoid = clf_sigmoid.predict_proba(X_test)[:, 1]\n\nprint(\"Brier score losses: (the smaller the better)\")\n\nclf_score = brier_score_loss(y_test, prob_pos_clf, sample_weight=sw_test)\nprint(\"No calibration: %1.3f\" % clf_score)\n\nclf_isotonic_score = brier_score_loss(y_test, prob_pos_isotonic, sample_weight=sw_test)\nprint(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\n\nclf_sigmoid_score = brier_score_loss(y_test, prob_pos_sigmoid, sample_weight=sw_test)\nprint(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)\n\n# %%\n# Plot data and the predicted probabilities\n# -----------------------------------------\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nplt.figure()\ny_unique = np.unique(y)\ncolors = cm.rainbow(np.linspace(0.0, 1.0, y_unique.size))\nfor this_y, color in zip(y_unique, colors):\n    this_X = X_train[y_train == this_y]\n    this_sw = sw_train[y_train == this_y]\n    plt.scatter(\n        this_X[:, 0],\n        this_X[:, 1],\n        s=this_sw * 50,\n        c=color[np.newaxis, :],\n        alpha=0.5,\n        edgecolor=\"k\",\n        label=\"Class %s\" % this_y,\n    )\nplt.legend(loc=\"best\")\nplt.title(\"Data\")\n\nplt.figure()\n\norder = np.lexsort((prob_pos_clf,))\nplt.plot(prob_pos_clf[order], \"r\", label=\"No calibration (%1.3f)\" % clf_score)\nplt.plot(\n    prob_pos_isotonic[order],\n    \"g\",\n    linewidth=3,\n    label=\"Isotonic calibration (%1.3f)\" % clf_isotonic_score,\n)\nplt.plot(\n    prob_pos_sigmoid[order],\n    \"b\",\n    linewidth=3,\n    label=\"Sigmoid calibration (%1.3f)\" % clf_sigmoid_score,\n)\nplt.plot(\n    np.linspace(0, y_test.size, 51)[1::2],\n    y_test[order].reshape(25, -1).mean(1),\n    \"k\",\n    linewidth=3,\n    label=r\"Empirical\",\n)\nplt.ylim([-0.05, 1.05])\nplt.xlabel(\"Instances sorted according to predicted probability (uncalibrated GNB)\")\nplt.ylabel(\"P(y=1)\")\nplt.legend(loc=\"upper left\")\nplt.title(\"Gaussian naive Bayes probabilities\")\n\nplt.show()",
                "filename": "examples/calibration/plot_calibration.py",
                "start_index": 2990,
                "end_index": 4892,
                "start_line": 78,
                "end_line": 144,
                "max_line": 144,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "#   one away from these values. Because predictions are restricted to the\n#   interval [0,1], errors caused by variance tend to be one- sided near zero\n#   and one. For example, if a model should predict p = 0 for a case, the only\n#   way bagging can achieve this is if all bagged trees predict zero. If we add\n#   noise to the trees that bagging is averaging over, this noise will cause\n#   some trees to predict values larger than 0 for this case, thus moving the\n#   average prediction of the bagged ensemble away from 0. We observe this\n#   effect most strongly with random forests because the base-level trees\n#   trained with random forests have relatively high variance due to feature\n#   subsetting.\" As a result, the calibration curve shows a characteristic\n#   sigmoid shape, indicating that the classifier is under-confident\n#   and could return probabilities closer to 0 or 1.\n#\n# * To show the performance of :class:`~sklearn.svm.LinearSVC`, we naively\n#   scale the output of the :term:`decision_function` into [0, 1] by applying\n#   min-max scaling, since SVC does not output probabilities by default.\n#   :class:`~sklearn.svm.LinearSVC` shows an\n#   even more sigmoid curve than the\n#   :class:`~sklearn.ensemble.RandomForestClassifier`, which is typical for\n#   maximum-margin methods [1]_ as they focus on difficult to classify samples\n#   that are close to the decision boundary (the support vectors).\n#\n# References\n# ----------\n#\n# .. [1] `Predicting Good Probabilities with Supervised Learning\n#        <https://dl.acm.org/doi/pdf/10.1145/1102351.1102430>`_,\n#        A. Niculescu-Mizil & R. Caruana, ICML 2005\n# .. [2] `Beyond independence: Conditions for the optimality of the simple\n#        bayesian classifier\n#        <https://www.ics.uci.edu/~pazzani/Publications/mlc96-pedro.pdf>`_\n#        Domingos, P., & Pazzani, M., Proc. 13th Intl. Conf. Machine Learning.\n#        1996.\n# .. [3] `Obtaining calibrated probability estimates from decision trees and\n#        naive Bayesian classifiers\n#        <https://citeseerx.ist.psu.edu/doc_view/pid/4f67a122ec3723f08ad5cbefecad119b432b3304>`_\n#        Zadrozny, Bianca, and Charles Elkan. Icml. Vol. 1. 2001.",
                "filename": "examples/calibration/plot_compare_calibration.py",
                "start_index": 5970,
                "end_index": 8151,
                "start_line": 174,
                "end_line": 210,
                "max_line": 210,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "def brier_score_loss(y_true, y_prob, *, sample_weight=None, pos_label=None):",
                "filename": "sklearn/metrics/_classification.py",
                "start_index": 113662,
                "end_index": 113738,
                "start_line": 3083,
                "end_line": 3083,
                "max_line": 3182,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "# Standard regression scores\nexplained_variance_scorer = make_scorer(explained_variance_score)\nr2_scorer = make_scorer(r2_score)\nmax_error_scorer = make_scorer(max_error, greater_is_better=False)\nneg_mean_squared_error_scorer = make_scorer(mean_squared_error, greater_is_better=False)\nneg_mean_squared_log_error_scorer = make_scorer(\n    mean_squared_log_error, greater_is_better=False\n)\nneg_mean_absolute_error_scorer = make_scorer(\n    mean_absolute_error, greater_is_better=False\n)\nneg_mean_absolute_percentage_error_scorer = make_scorer(\n    mean_absolute_percentage_error, greater_is_better=False\n)\nneg_median_absolute_error_scorer = make_scorer(\n    median_absolute_error, greater_is_better=False\n)\nneg_root_mean_squared_error_scorer = make_scorer(\n    mean_squared_error, greater_is_better=False, squared=False\n)\nneg_mean_poisson_deviance_scorer = make_scorer(\n    mean_poisson_deviance, greater_is_better=False\n)\n\nneg_mean_gamma_deviance_scorer = make_scorer(\n    mean_gamma_deviance, greater_is_better=False\n)\n\n# Standard Classification Scores\naccuracy_scorer = make_scorer(accuracy_score)\nbalanced_accuracy_scorer = make_scorer(balanced_accuracy_score)\nmatthews_corrcoef_scorer = make_scorer(matthews_corrcoef)\n\n\ndef positive_likelihood_ratio(y_true, y_pred):\n    return class_likelihood_ratios(y_true, y_pred)[0]\n\n\ndef negative_likelihood_ratio(y_true, y_pred):\n    return class_likelihood_ratios(y_true, y_pred)[1]\n\n\npositive_likelihood_ratio_scorer = make_scorer(positive_likelihood_ratio)\nneg_negative_likelihood_ratio_scorer = make_scorer(\n    negative_likelihood_ratio, greater_is_better=False\n)\n\n# Score functions that need decision values\ntop_k_accuracy_scorer = make_scorer(\n    top_k_accuracy_score, greater_is_better=True, needs_threshold=True\n)\nroc_auc_scorer = make_scorer(\n    roc_auc_score, greater_is_better=True, needs_threshold=True\n)\naverage_precision_scorer = make_scorer(average_precision_score, needs_threshold=True)\nroc_auc_ovo_scorer = make_scorer(roc_auc_score, needs_proba=True, multi_class=\"ovo\")\nroc_auc_ovo_weighted_scorer = make_scorer(\n    roc_auc_score, needs_proba=True, multi_class=\"ovo\", average=\"weighted\"\n)\nroc_auc_ovr_scorer = make_scorer(roc_auc_score, needs_proba=True, multi_class=\"ovr\")\nroc_auc_ovr_weighted_scorer = make_scorer(\n    roc_auc_score, needs_proba=True, multi_class=\"ovr\", average=\"weighted\"\n)\n\n# Score function for probabilistic classification\nneg_log_loss_scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\nneg_brier_score_scorer = make_scorer(\n    brier_score_loss, greater_is_better=False, needs_proba=True\n)\nbrier_score_loss_scorer = make_scorer(\n    brier_score_loss, greater_is_better=False, needs_proba=True\n)\n\n\n# Clustering scores\nadjusted_rand_scorer = make_scorer(adjusted_rand_score)\nrand_scorer = make_scorer(rand_score)\nhomogeneity_scorer = make_scorer(homogeneity_score)\ncompleteness_scorer = make_scorer(completeness_score)\nv_measure_scorer = make_scorer(v_measure_score)",
                "filename": "sklearn/metrics/_scorer.py",
                "start_index": 25306,
                "end_index": 28281,
                "start_line": 743,
                "end_line": 822,
                "max_line": 956,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"Compute the Brier score loss.\n\n    The smaller the Brier score loss, the better, hence the naming with \"loss\".\n    The Brier score measures the mean squared difference between the predicted\n    probability and the actual outcome. The Brier score always\n    takes on a value between zero and one, since this is the largest\n    possible difference between a predicted probability (which must be\n    between zero and one) and the actual outcome (which can take on values\n    of only 0 and 1). It can be decomposed as the sum of refinement loss and\n    calibration loss.\n\n    The Brier score is appropriate for binary and categorical outcomes that\n    can be structured as true or false, but is inappropriate for ordinal\n    variables which can take on three or more values (this is because the\n    Brier score assumes that all possible outcomes are equivalently\n    \"distant\" from one another). Which label is considered to be the positive\n    label is controlled via the parameter `pos_label`, which defaults to\n    the greater label unless `y_true` is all 0 or all -1, in which case\n    `pos_label` defaults to 1.\n\n    Read more in the :ref:`User Guide <brier_score_loss>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,)\n        True targets.\n\n    y_prob : array-like of shape (n_samples,)\n        Probabilities of the positive class.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n    pos_label : int, float, bool or str, default=None\n        Label of the positive class. `pos_label` will be inferred in the\n        following manner:\n\n        * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;\n        * else if `y_true` contains string, an error will be raised and\n          `pos_label` should be explicitly specified;\n        * otherwise, `pos_label` defaults to the greater label,\n          i.e. `np.unique(y_true)[-1]`.\n\n    Returns\n    -------\n    score : float\n        Brier score loss.\n\n    References\n    ----------\n    .. [1] `Wikipedia entry for the Brier score\n            <https://en.wikipedia.org/wiki/Brier_score>`_.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.metrics import brier_score_loss\n    >>> y_true = np.array([0, 1, 1, 0])\n    >>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n    >>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    >>> brier_score_loss(y_true, y_prob)\n    0.037...\n    >>> brier_score_loss(y_true, 1-y_prob, pos_label=0)\n    0.037...\n    >>> brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n    0.037...\n    >>> brier_score_loss(y_true, np.array(y_prob) > 0.5)\n    0.0\n    \"\"\"\n    y_true = column_or_1d(y_true)\n    y_prob = column_or_1d(y_prob)\n    assert_all_finite(y_true)\n    assert_all_finite(y_prob)\n    check_consistent_length(y_true, y_prob, sample_weight)\n\n    y_type = type_of_target(y_true, input_name=\"y_true\")",
                "filename": "sklearn/metrics/_classification.py",
                "start_index": 113743,
                "end_index": 116653,
                "start_line": 3084,
                "end_line": 3159,
                "max_line": 3182,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"Metrics to assess performance on classification task given class prediction.\n\nFunctions named as ``*_score`` return a scalar value to maximize: the higher\nthe better.\n\nFunction named as ``*_error`` or ``*_loss`` return a scalar value to minimize:\nthe lower the better.\n\"\"\"\n\n# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Arnaud Joly <a.joly@ulg.ac.be>\n#          Jochen Wersdorfer <jochen@wersdoerfer.de>\n#          Lars Buitinck\n#          Joel Nothman <joel.nothman@gmail.com>\n#          Noel Dawe <noel@dawe.me>\n#          Jatin Shah <jatindshah@gmail.com>\n#          Saurabh Jha <saurabh.jhaa@gmail.com>\n#          Bernardo Stein <bernardovstein@gmail.com>\n#          Shangwu Yao <shangwuyao@gmail.com>\n#          Michal Karbownik <michakarbownik@gmail.com>\n# License: BSD 3 clause\n\n\nimport warnings\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy.sparse import coo_matrix, csr_matrix\nfrom scipy.special import xlogy\n\nfrom ..exceptions import UndefinedMetricWarning\nfrom ..preprocessing import LabelBinarizer, LabelEncoder\nfrom ..utils import (\n    assert_all_finite,\n    check_array,\n    check_consistent_length,\n    column_or_1d,\n)\nfrom ..utils._array_api import _union1d, _weighted_sum, get_namespace\nfrom ..utils._param_validation import Interval, Options, StrOptions, validate_params\nfrom ..utils.extmath import _nanaverage\nfrom ..utils.multiclass import type_of_target, unique_labels\nfrom ..utils.sparsefuncs import count_nonzero\nfrom ..utils.validation import _check_pos_label_consistency, _num_samples\n\n\ndef _check_zero_division(zero_division):\n    if isinstance(zero_division, str) and zero_division == \"warn\":\n        return np.float64(0.0)\n    elif isinstance(zero_division, (int, float)) and zero_division in [0, 1]:\n        return np.float64(zero_division)\n    else:  # np.isnan(zero_division)\n        return np.nan",
                "filename": "sklearn/metrics/_classification.py",
                "start_index": 0,
                "end_index": 1980,
                "start_line": 1,
                "end_line": 55,
                "max_line": 3182,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "for i, (_, name) in enumerate(clf_list):\n    row, col = grid_positions[i]\n    ax = fig.add_subplot(gs[row, col])\n\n    ax.hist(\n        calibration_displays[name].y_prob,\n        range=(0, 1),\n        bins=10,\n        label=name,\n        color=colors(i),\n    )\n    ax.set(title=name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\n\nplt.tight_layout()\nplt.show()\n\n# %%\n# :class:`~sklearn.svm.LinearSVC` shows the opposite\n# behavior to :class:`~sklearn.naive_bayes.GaussianNB`; the calibration\n# curve has a sigmoid shape, which is typical for an under-confident\n# classifier. In the case of :class:`~sklearn.svm.LinearSVC`, this is caused\n# by the margin property of the hinge loss, which focuses on samples that are\n# close to the decision boundary (support vectors). Samples that are far\n# away from the decision boundary do not impact the hinge loss. It thus makes\n# sense that :class:`~sklearn.svm.LinearSVC` does not try to separate samples\n# in the high confidence region regions. This leads to flatter calibration\n# curves near 0 and 1 and is empirically shown with a variety of datasets\n# in Niculescu-Mizil & Caruana [1]_.\n#\n# Both kinds of calibration (sigmoid and isotonic) can fix this issue and\n# yield similar results.\n#\n# As before, we show the :ref:`brier_score_loss`, :ref:`log_loss`,\n# :ref:`precision, recall, F1 score <precision_recall_f_measure_metrics>` and\n# :ref:`ROC AUC <roc_metrics>`.\n\nscores = defaultdict(list)\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    y_prob = clf.predict_proba(X_test)\n    y_pred = clf.predict(X_test)\n    scores[\"Classifier\"].append(name)\n\n    for metric in [brier_score_loss, log_loss, roc_auc_score]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_prob[:, 1]))\n\n    for metric in [precision_score, recall_score, f1_score]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_pred))\n\n    score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n    score_df.round(decimals=3)\n\nscore_df\n\n# %%\n# As with :class:`~sklearn.naive_bayes.GaussianNB` above, calibration improves\n# both :ref:`brier_score_loss` and :ref:`log_loss` but does not alter the\n# prediction accuracy measures (precision, recall and F1 score) much.\n#\n# Summary\n# -------\n#\n# Parametric sigmoid calibration can deal with situations where the calibration\n# curve of the base classifier is sigmoid (e.g., for\n# :class:`~sklearn.svm.LinearSVC`) but not where it is transposed-sigmoid\n# (e.g., :class:`~sklearn.naive_bayes.GaussianNB`). Non-parametric\n# isotonic calibration can deal with both situations but may require more\n# data to produce good results.\n#\n# References\n# ----------\n#\n# .. [1] `Predicting Good Probabilities with Supervised Learning\n#        <https://dl.acm.org/doi/pdf/10.1145/1102351.1102430>`_,\n#        A. Niculescu-Mizil & R. Caruana, ICML 2005",
                "filename": "examples/calibration/plot_calibration_curve.py",
                "start_index": 8607,
                "end_index": 11607,
                "start_line": 103,
                "end_line": 336,
                "max_line": 336,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"Metrics to assess performance on classification task given scores.\n\nFunctions named as ``*_score`` return a scalar value to maximize: the higher\nthe better.\n\nFunction named as ``*_error`` or ``*_loss`` return a scalar value to minimize:\nthe lower the better.\n\"\"\"\n\n# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Arnaud Joly <a.joly@ulg.ac.be>\n#          Jochen Wersdorfer <jochen@wersdoerfer.de>\n#          Lars Buitinck\n#          Joel Nothman <joel.nothman@gmail.com>\n#          Noel Dawe <noel@dawe.me>\n#          Michal Karbownik <michakarbownik@gmail.com>\n# License: BSD 3 clause\n\n\nimport warnings\nfrom functools import partial\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, issparse\nfrom scipy.stats import rankdata\n\nfrom ..exceptions import UndefinedMetricWarning\nfrom ..preprocessing import label_binarize\nfrom ..utils import (\n    assert_all_finite,\n    check_array,\n    check_consistent_length,\n    column_or_1d,\n)\nfrom ..utils._encode import _encode, _unique\nfrom ..utils._param_validation import Interval, StrOptions, validate_params\nfrom ..utils.extmath import stable_cumsum\nfrom ..utils.multiclass import type_of_target\nfrom ..utils.sparsefuncs import count_nonzero\nfrom ..utils.validation import _check_pos_label_consistency, _check_sample_weight\nfrom ._base import _average_binary_score, _average_multiclass_ovo_score",
                "filename": "sklearn/metrics/_ranking.py",
                "start_index": 0,
                "end_index": 1507,
                "start_line": 1,
                "end_line": 44,
                "max_line": 1995,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "for i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    display = CalibrationDisplay.from_estimator(\n        clf,\n        X_test,\n        y_test,\n        n_bins=10,\n        name=name,\n        ax=ax_calibration_curve,\n        color=colors(i),\n    )\n    calibration_displays[name] = display\n\nax_calibration_curve.grid()\nax_calibration_curve.set_title(\"Calibration plots (Naive Bayes)\")\n\n# Add histogram\ngrid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]\nfor i, (_, name) in enumerate(clf_list):\n    row, col = grid_positions[i]\n    ax = fig.add_subplot(gs[row, col])\n\n    ax.hist(\n        calibration_displays[name].y_prob,\n        range=(0, 1),\n        bins=10,\n        label=name,\n        color=colors(i),\n    )\n    ax.set(title=name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\n\nplt.tight_layout()\nplt.show()\n\n# %%\n# Uncalibrated :class:`~sklearn.naive_bayes.GaussianNB` is poorly calibrated\n# because of\n# the redundant features which violate the assumption of feature-independence\n# and result in an overly confident classifier, which is indicated by the\n# typical transposed-sigmoid curve. Calibration of the probabilities of\n# :class:`~sklearn.naive_bayes.GaussianNB` with :ref:`isotonic` can fix\n# this issue as can be seen from the nearly diagonal calibration curve.\n# :ref:`Sigmoid regression <sigmoid_regressor>` also improves calibration\n# slightly,\n# albeit not as strongly as the non-parametric isotonic regression. This can be\n# attributed to the fact that we have plenty of calibration data such that the\n# greater flexibility of the non-parametric model can be exploited.\n#\n# Below we will make a quantitative analysis considering several classification\n# metrics: :ref:`brier_score_loss`, :ref:`log_loss`,\n# :ref:`precision, recall, F1 score <precision_recall_f_measure_metrics>` and\n# :ref:`ROC AUC <roc_metrics>`.\n\nfrom collections import defaultdict\n\nimport pandas as pd\n\nfrom sklearn.metrics import (\n    brier_score_loss,\n    f1_score,\n    log_loss,\n    precision_score,\n    recall_score,\n    roc_auc_score,\n)\n\nscores = defaultdict(list)\nfor i, (clf, name) in enumerate(clf_list):\n    clf.fit(X_train, y_train)\n    y_prob = clf.predict_proba(X_test)\n    y_pred = clf.predict(X_test)\n    scores[\"Classifier\"].append(name)\n\n    for metric in [brier_score_loss, log_loss, roc_auc_score]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_prob[:, 1]))\n\n    for metric in [precision_score, recall_score, f1_score]:\n        score_name = metric.__name__.replace(\"_\", \" \").replace(\"score\", \"\").capitalize()\n        scores[score_name].append(metric(y_test, y_pred))\n\n    score_df = pd.DataFrame(scores).set_index(\"Classifier\")\n    score_df.round(decimals=3)\n\nscore_df\n\n# %%\n# Notice that although calibration improves the :ref:`brier_score_loss` (a\n# metric composed\n# of calibration term and refinement term) and :ref:`log_loss`, it does not",
                "filename": "examples/calibration/plot_calibration_curve.py",
                "start_index": 2839,
                "end_index": 5812,
                "start_line": 85,
                "end_line": 174,
                "max_line": 336,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "# Indeed, :class:`~sklearn.linear_model.LinearRegression` is a least squares\n# approach minimizing the mean squared error (MSE) between the training and\n# predicted targets. In contrast,\n# :class:`~sklearn.linear_model.QuantileRegressor` with `quantile=0.5`\n# minimizes the mean absolute error (MAE) instead.\n#\n# Let's first compute the training errors of such models in terms of mean\n# squared error and mean absolute error. We will use the asymmetric Pareto\n# distributed target to make it more interesting as mean and median are not\n# equal.\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nlinear_regression = LinearRegression()\nquantile_regression = QuantileRegressor(quantile=0.5, alpha=0, solver=solver)\n\ny_pred_lr = linear_regression.fit(X, y_pareto).predict(X)\ny_pred_qr = quantile_regression.fit(X, y_pareto).predict(X)\n\nprint(f\"\"\"Training error (in-sample performance)\n    {linear_regression.__class__.__name__}:\n    MAE = {mean_absolute_error(y_pareto, y_pred_lr):.3f}\n    MSE = {mean_squared_error(y_pareto, y_pred_lr):.3f}\n    {quantile_regression.__class__.__name__}:\n    MAE = {mean_absolute_error(y_pareto, y_pred_qr):.3f}\n    MSE = {mean_squared_error(y_pareto, y_pred_qr):.3f}\n    \"\"\")\n\n# %%\n# On the training set, we see that MAE is lower for\n# :class:`~sklearn.linear_model.QuantileRegressor` than\n# :class:`~sklearn.linear_model.LinearRegression`. In contrast to that, MSE is\n# lower for :class:`~sklearn.linear_model.LinearRegression` than\n# :class:`~sklearn.linear_model.QuantileRegressor`. These results confirms that\n# MAE is the loss minimized by :class:`~sklearn.linear_model.QuantileRegressor`\n# while MSE is the loss minimized\n# :class:`~sklearn.linear_model.LinearRegression`.\n#\n# We can make a similar evaluation by looking at the test error obtained by\n# cross-validation.\nfrom sklearn.model_selection import cross_validate\n\ncv_results_lr = cross_validate(\n    linear_regression,\n    X,\n    y_pareto,\n    cv=3,\n    scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\"],\n)\ncv_results_qr = cross_validate(\n    quantile_regression,\n    X,\n    y_pareto,\n    cv=3,\n    scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\"],\n)\nprint(f\"\"\"Test error (cross-validated performance)\n    {linear_regression.__class__.__name__}:\n    MAE = {-cv_results_lr[\"test_neg_mean_absolute_error\"].mean():.3f}\n    MSE = {-cv_results_lr[\"test_neg_mean_squared_error\"].mean():.3f}\n    {quantile_regression.__class__.__name__}:\n    MAE = {-cv_results_qr[\"test_neg_mean_absolute_error\"].mean():.3f}\n    MSE = {-cv_results_qr[\"test_neg_mean_squared_error\"].mean():.3f}\n    \"\"\")\n\n# %%\n# We reach similar conclusions on the out-of-sample evaluation.",
                "filename": "examples/linear_model/plot_quantile_regression.py",
                "start_index": 8841,
                "end_index": 11578,
                "start_line": 245,
                "end_line": 311,
                "max_line": 311,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/metrics/_scorer.py": [
                {
                    "chunk": {
                        "code": "# Standard regression scores\nexplained_variance_scorer = make_scorer(explained_variance_score)\nr2_scorer = make_scorer(r2_score)\nmax_error_scorer = make_scorer(max_error, greater_is_better=False)\nneg_mean_squared_error_scorer = make_scorer(mean_squared_error, greater_is_better=False)\nneg_mean_squared_log_error_scorer = make_scorer(\n    mean_squared_log_error, greater_is_better=False\n)\nneg_mean_absolute_error_scorer = make_scorer(\n    mean_absolute_error, greater_is_better=False\n)\nneg_mean_absolute_percentage_error_scorer = make_scorer(\n    mean_absolute_percentage_error, greater_is_better=False\n)\nneg_median_absolute_error_scorer = make_scorer(\n    median_absolute_error, greater_is_better=False\n)\nneg_root_mean_squared_error_scorer = make_scorer(\n    mean_squared_error, greater_is_better=False, squared=False\n)\nneg_mean_poisson_deviance_scorer = make_scorer(\n    mean_poisson_deviance, greater_is_better=False\n)\n\nneg_mean_gamma_deviance_scorer = make_scorer(\n    mean_gamma_deviance, greater_is_better=False\n)\n\n# Standard Classification Scores\naccuracy_scorer = make_scorer(accuracy_score)\nbalanced_accuracy_scorer = make_scorer(balanced_accuracy_score)\nmatthews_corrcoef_scorer = make_scorer(matthews_corrcoef)\n\n\ndef positive_likelihood_ratio(y_true, y_pred):\n    return class_likelihood_ratios(y_true, y_pred)[0]\n\n\ndef negative_likelihood_ratio(y_true, y_pred):\n    return class_likelihood_ratios(y_true, y_pred)[1]\n\n\npositive_likelihood_ratio_scorer = make_scorer(positive_likelihood_ratio)\nneg_negative_likelihood_ratio_scorer = make_scorer(\n    negative_likelihood_ratio, greater_is_better=False\n)\n\n# Score functions that need decision values\ntop_k_accuracy_scorer = make_scorer(\n    top_k_accuracy_score, greater_is_better=True, needs_threshold=True\n)\nroc_auc_scorer = make_scorer(\n    roc_auc_score, greater_is_better=True, needs_threshold=True\n)\naverage_precision_scorer = make_scorer(average_precision_score, needs_threshold=True)\nroc_auc_ovo_scorer = make_scorer(roc_auc_score, needs_proba=True, multi_class=\"ovo\")\nroc_auc_ovo_weighted_scorer = make_scorer(\n    roc_auc_score, needs_proba=True, multi_class=\"ovo\", average=\"weighted\"\n)\nroc_auc_ovr_scorer = make_scorer(roc_auc_score, needs_proba=True, multi_class=\"ovr\")\nroc_auc_ovr_weighted_scorer = make_scorer(\n    roc_auc_score, needs_proba=True, multi_class=\"ovr\", average=\"weighted\"\n)\n\n# Score function for probabilistic classification\nneg_log_loss_scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\nneg_brier_score_scorer = make_scorer(\n    brier_score_loss, greater_is_better=False, needs_proba=True\n)\nbrier_score_loss_scorer = make_scorer(\n    brier_score_loss, greater_is_better=False, needs_proba=True\n)\n\n\n# Clustering scores\nadjusted_rand_scorer = make_scorer(adjusted_rand_score)\nrand_scorer = make_scorer(rand_score)\nhomogeneity_scorer = make_scorer(homogeneity_score)\ncompleteness_scorer = make_scorer(completeness_score)\nv_measure_scorer = make_scorer(v_measure_score)",
                        "filename": "sklearn/metrics/_scorer.py",
                        "start_index": 25306,
                        "end_index": 28281,
                        "start_line": 743,
                        "end_line": 822,
                        "max_line": 956,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet defines the scorers for various metrics, including the `neg_brier_score_scorer` which is relevant to the issue as it suggests the implementation of a negated version of the `brier_score_loss`."
                }
            ],
            "sklearn/metrics/_classification.py": [
                {
                    "chunk": {
                        "code": "\"\"\"Compute the Brier score loss.\n\n    The smaller the Brier score loss, the better, hence the naming with \"loss\".\n    The Brier score measures the mean squared difference between the predicted\n    probability and the actual outcome. The Brier score always\n    takes on a value between zero and one, since this is the largest\n    possible difference between a predicted probability (which must be\n    between zero and one) and the actual outcome (which can take on values\n    of only 0 and 1). It can be decomposed as the sum of refinement loss and\n    calibration loss.\n\n    The Brier score is appropriate for binary and categorical outcomes that\n    can be structured as true or false, but is inappropriate for ordinal\n    variables which can take on three or more values (this is because the\n    Brier score assumes that all possible outcomes are equivalently\n    \"distant\" from one another). Which label is considered to be the positive\n    label is controlled via the parameter `pos_label`, which defaults to\n    the greater label unless `y_true` is all 0 or all -1, in which case\n    `pos_label` defaults to 1.\n\n    Read more in the :ref:`User Guide <brier_score_loss>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,)\n        True targets.\n\n    y_prob : array-like of shape (n_samples,)\n        Probabilities of the positive class.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n    pos_label : int, float, bool or str, default=None\n        Label of the positive class. `pos_label` will be inferred in the\n        following manner:\n\n        * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;\n        * else if `y_true` contains string, an error will be raised and\n          `pos_label` should be explicitly specified;\n        * otherwise, `pos_label` defaults to the greater label,\n          i.e. `np.unique(y_true)[-1]`.\n\n    Returns\n    -------\n    score : float\n        Brier score loss.\n\n    References\n    ----------\n    .. [1] `Wikipedia entry for the Brier score\n            <https://en.wikipedia.org/wiki/Brier_score>`_.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.metrics import brier_score_loss\n    >>> y_true = np.array([0, 1, 1, 0])\n    >>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n    >>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    >>> brier_score_loss(y_true, y_prob)\n    0.037...\n    >>> brier_score_loss(y_true, 1-y_prob, pos_label=0)\n    0.037...\n    >>> brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n    0.037...\n    >>> brier_score_loss(y_true, np.array(y_prob) > 0.5)\n    0.0\n    \"\"\"\n    y_true = column_or_1d(y_true)\n    y_prob = column_or_1d(y_prob)\n    assert_all_finite(y_true)\n    assert_all_finite(y_prob)\n    check_consistent_length(y_true, y_prob, sample_weight)\n\n    y_type = type_of_target(y_true, input_name=\"y_true\")",
                        "filename": "sklearn/metrics/_classification.py",
                        "start_index": 113743,
                        "end_index": 116653,
                        "start_line": 3084,
                        "end_line": 3159,
                        "max_line": 3182,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the detailed documentation and implementation of the `brier_score_loss` function, which is directly related to the issue as it explains the behavior and usage of the metric that is inconsistent with the documentation section mentioned in the issue."
                },
                {
                    "chunk": {
                        "code": "def brier_score_loss(y_true, y_prob, *, sample_weight=None, pos_label=None):",
                        "filename": "sklearn/metrics/_classification.py",
                        "start_index": 113662,
                        "end_index": 113738,
                        "start_line": 3083,
                        "end_line": 3083,
                        "max_line": 3182,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the function signature of `brier_score_loss`, which is relevant to the issue as it is the function that needs to be negated to create `neg_brier_score_loss`."
                }
            ]
        },
        "solution_task": "- Update the documentation in section 3.3.1.1 to correctly describe the behavior of `brier_score_loss`.\n- Implement `neg_brier_score_loss` function that returns the negative Brier score loss.\n- Replace references to `brier_score_loss` with `neg_brier_score_loss` in section 3.3.1.1 of the documentation.\n- Add `neg_brier_score_loss` to `sklearn/metrics/_scorer.py` as a scorer object using `make_scorer` with `greater_is_better=True`.\n\n```python\n# sklearn/metrics/_classification.py\ndef neg_brier_score_loss(y_true, y_prob, *, pos_label=None, sample_weight=None):\n    \"\"\"\n    Calculate the negative Brier score loss.\n\n    Parameters:\n    - y_true : array-like of shape (n_samples,)\n        True labels.\n    - y_prob : array-like of shape (n_samples,)\n        Predicted probabilities.\n    - pos_label : int or str, default=None\n        The label of the positive class.\n    - sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n    Returns:\n    - score : float\n        The negative Brier score loss.\n    \"\"\"\n    return -brier_score_loss(y_true, y_prob, pos_label=pos_label, sample_weight=sample_weight)\n\n# sklearn/metrics/_scorer.py\nneg_brier_score_loss_scorer = make_scorer(neg_brier_score_loss, greater_is_better=True, needs_proba=True)\n```",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE provides a clear and actionable set of steps to address the issue raised in the PROMPT. It suggests updating the documentation to accurately describe the behavior of `brier_score_loss`, which directly addresses the first concern about the incorrect description in the documentation. The RESPONSE also proposes the implementation of a `neg_brier_score_loss` function, which aligns with the suggestion in the PROMPT to create a negated version of the Brier score loss to maintain consistency with other metrics. Furthermore, the RESPONSE includes the replacement of references to `brier_score_loss` with `neg_brier_score_loss` in the documentation, which is a logical step following the implementation of the new function. Lastly, the RESPONSE provides a code snippet to add the `neg_brier_score_loss` to the scorer objects, which is a necessary step to make the new metric available for use. The inclusion of code snippets and references to the relevant sections of the codebase adds significant value to the RESPONSE, making it highly actionable for developers looking to resolve the issue.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711738954.422504,
        "result": "- Update the documentation in section 3.3.1.1 to correctly describe the behavior of `brier_score_loss`.\n- Implement `neg_brier_score_loss` function that returns the negative Brier score loss.\n- Replace references to `brier_score_loss` with `neg_brier_score_loss` in section 3.3.1.1 of the documentation.\n- Add `neg_brier_score_loss` to `sklearn/metrics/_scorer.py` as a scorer object using `make_scorer` with `greater_is_better=True`.\n\n```python\n# sklearn/metrics/_classification.py\ndef neg_brier_score_loss(y_true, y_prob, *, pos_label=None, sample_weight=None):\n    \"\"\"\n    Calculate the negative Brier score loss.\n\n    Parameters:\n    - y_true : array-like of shape (n_samples,)\n        True labels.\n    - y_prob : array-like of shape (n_samples,)\n        Predicted probabilities.\n    - pos_label : int or str, default=None\n        The label of the positive class.\n    - sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n    Returns:\n    - score : float\n        The negative Brier score loss.\n    \"\"\"\n    return -brier_score_loss(y_true, y_prob, pos_label=pos_label, sample_weight=sample_weight)\n\n# sklearn/metrics/_scorer.py\nneg_brier_score_loss_scorer = make_scorer(neg_brier_score_loss, greater_is_better=True, needs_proba=True)\n```\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [metrics](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/metrics)- [Lines 743 - 822](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/metrics/_scorer.py#L743-L822)\n\n This snippet defines the scorers for various metrics, including the `neg_brier_score_scorer` which is relevant to the issue as it suggests the implementation of a negated version of the `brier_score_loss`.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [metrics](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/metrics)- [Lines 3084 - 3159](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/metrics/_classification.py#L3084-L3159)\n\n This snippet contains the detailed documentation and implementation of the `brier_score_loss` function, which is directly related to the issue as it explains the behavior and usage of the metric that is inconsistent with the documentation section mentioned in the issue. \n\n\n   - [Lines 3083 - 3083](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/metrics/_classification.py#L3083-L3083)\n\n This snippet shows the function signature of `brier_score_loss`, which is relevant to the issue as it is the function that needs to be negated to create `neg_brier_score_loss`."
    }
}