{
    "id": "psf__requests-4356",
    "repo_name": "testbed/psf__requests__2.18",
    "task_id": 460186882206467715,
    "title": "Misleading exception with invalid protocol in proxy variable",
    "body": "When the value of `https_proxy` or `HTTPS_PROXY` variable(s) accidentally miss one '/' in the protocol, a traceback is thrown to the user which doesn't pin point that the issue is with the proxy configuration.\r\n\r\n## Expected Result\r\n\r\nA better exception\r\n\r\n## Actual Result\r\n\r\nAn exception which doesn't pin point exactly what went wrong.\r\n\r\n## Reproduction Steps\r\n```\r\n(req2) nwani@dockerub01:~/requests$ export https_proxy=http:/my.proxy.com:3128\r\n(req2) nwani@dockerub01:~/requests$ python -c \"import requests; requests.get('https://google.com')\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/nehaljwani/requests/requests/api.py\", line 72, in get\r\n    return request('get', url, params=params, **kwargs)\r\n  File \"/home/nehaljwani/requests/requests/api.py\", line 58, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/home/nehaljwani/requests/requests/sessions.py\", line 508, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/home/nehaljwani/requests/requests/sessions.py\", line 618, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/home/nehaljwani/requests/requests/adapters.py\", line 440, in send\r\n    timeout=timeout\r\n  File \"/home/nehaljwani/m3/envs/req2/lib/python3.6/site-packages/urllib3-1.22-py3.6.egg/urllib3/connectionpool.py\", line 595, in urlopen\r\n    self._prepare_proxy(conn)\r\n  File \"/home/nehaljwani/m3/envs/req2/lib/python3.6/site-packages/urllib3-1.22-py3.6.egg/urllib3/connectionpool.py\", line 816, in _prepare_proxy\r\n    conn.connect()\r\n  File \"/home/nehaljwani/m3/envs/req2/lib/python3.6/site-packages/urllib3-1.22-py3.6.egg/urllib3/connection.py\", line 284, in connect\r\n    conn = self._new_conn()\r\n  File \"/home/nehaljwani/m3/envs/req2/lib/python3.6/site-packages/urllib3-1.22-py3.6.egg/urllib3/connection.py\", line 141, in _new_conn\r\n    (self.host, self.port), self.timeout, **extra_kw)\r\n  File \"/home/nehaljwani/m3/envs/req2/lib/python3.6/site-packages/urllib3-1.22-py3.6.egg/urllib3/util/connection.py\", line 51, in create_connection\r\n    if host.startswith('['):\r\nAttributeError: 'NoneType' object has no attribute 'startswith'\r\n```\r\n\r\n## System Information\r\n\r\n```\r\n(req2) nwani@dockerub01:~/requests$ python -m requests.help\r\n{\r\n  \"chardet\": {\r\n    \"version\": \"3.0.4\"\r\n  },\r\n  \"cryptography\": {\r\n    \"version\": \"\"\r\n  },\r\n  \"idna\": {\r\n    \"version\": \"2.6\"\r\n  },\r\n  \"implementation\": {\r\n    \"name\": \"CPython\",\r\n    \"version\": \"3.6.3\"\r\n  },\r\n  \"platform\": {\r\n    \"release\": \"4.4.0-93-generic\",\r\n    \"system\": \"Linux\"\r\n  },\r\n  \"pyOpenSSL\": {\r\n    \"openssl_version\": \"\",\r\n    \"version\": null\r\n  },\r\n  \"requests\": {\r\n    \"version\": \"2.18.4\"\r\n  },\r\n  \"system_ssl\": {\r\n    \"version\": \"100020cf\"\r\n  },\r\n  \"urllib3\": {\r\n    \"version\": \"1.22\"\r\n  },\r\n  \"using_pyopenssl\": false\r\n}\r\n```\r\n\r\nI am not sure what is the correct place to fix this. Should the fix/check be in requests, urllib3, or urlparse?\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/psf__requests__2.18.git",
    "requested_commit": "",
    "mayil_response": "- Add a check in `urllib3.connectionpool.HTTPConnectionPool._prepare_proxy` to validate the proxy URL format and raise a more informative exception if it is invalid.\n- Update the `InvalidProxyURL` exception in `requests.exceptions` to provide a clearer error message when the proxy URL is invalid.\n- Ensure that the proxy URL includes the scheme (e.g., `http://` or `https://`) and the trailing slash if necessary.\n- Use the `InvalidProxyURL` exception within `urllib3.util.connection.create_connection` or `urllib3.connectionpool.HTTPConnectionPool._prepare_proxy` when an invalid proxy URL is detected.\n\nExample of a possible solution:\n\n```python\n# requests/exceptions.py\nclass InvalidProxyURL(InvalidURL):\n    \"\"\"The proxy URL provided is invalid. Ensure the URL starts with http:// or https:// and is a valid proxy address.\"\"\"\n\n# urllib3/connectionpool.py\nclass HTTPConnectionPool:\n    # ...\n    def _prepare_proxy(self, conn):\n        proxy = self.proxy\n        if proxy:\n            if not proxy.startswith(('http://', 'https://')):\n                raise InvalidProxyURL(f\"The proxy URL is invalid: {proxy}. Ensure the URL starts with http:// or https://.\")\n            # Existing code to prepare the proxy...\n        # ...\n```\n\nThis solution ensures that the proxy URL is checked for the correct format before attempting a connection and provides a more informative error message to the user if the proxy URL is invalid.\n\n### Code snippets to check\n\n-  \u2192 [requests](https://github.com/testbed/psf__requests__2.18/tree//requests)- [Lines 1 - 106](https://github.com/testbed/psf__requests__2.18/blob//requests/exceptions.py#L1-L106)\n\n This file contains definitions for exceptions in the requests library. A new exception or modification of an existing one, such as InvalidProxyURL, may be needed to provide a clearer error message when the proxy URL is malformed.\n-  \u2192 [requests](https://github.com/testbed/psf__requests__2.18/tree//requests)- [Lines 832 - 942](https://github.com/testbed/psf__requests__2.18/blob//requests/utils.py#L832-L942)\n\n This file contains the proxy selection logic. It may need to be modified to validate the proxy URL format and raise a more descriptive exception when the format is incorrect.\n-  \u2192 [requests](https://github.com/testbed/psf__requests__2.18/tree//requests)- [Lines 370 - 432](https://github.com/testbed/psf__requests__2.18/blob//requests/adapters.py#L370-L432)\n\n This file contains the logic for preparing the request URL, which includes handling proxies. It may be necessary to review and potentially modify how proxy URLs are processed to ensure that malformed proxy URLs are handled correctly.\n-  \u2192 [requests](https://github.com/testbed/psf__requests__2.18/tree//requests)- [Lines 283 - 332](https://github.com/testbed/psf__requests__2.18/blob//requests/sessions.py#L283-L332)\n\n This file contains methods for rebuilding proxy information. It may be relevant to check if additional validation is needed here to catch malformed proxy URLs before a request is sent.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0005665,
        "snippet_processor": 0.06871000000000001,
        "issue_star_creation": 0.03444,
        "issue_star_solver": 0.06586,
        "bouncer": 0.031259999999999996
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711760746.692829,
        "relevant_snippets": [
            {
                "code": "\"\"\"\nrequests.exceptions\n~~~~~~~~~~~~~~~~~~~\n\nThis module contains the set of Requests' exceptions.\n\"\"\"\nfrom urllib3.exceptions import HTTPError as BaseHTTPError\n\nfrom .compat import JSONDecodeError as CompatJSONDecodeError\n\n\nclass RequestException(IOError):\n    \"\"\"There was an ambiguous exception that occurred while handling your\n    request.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize RequestException with `request` and `response` objects.\"\"\"\n        response = kwargs.pop(\"response\", None)\n        self.response = response\n        self.request = kwargs.pop(\"request\", None)\n        if response is not None and not self.request and hasattr(response, \"request\"):\n            self.request = self.response.request\n        super().__init__(*args, **kwargs)\n\n\nclass InvalidJSONError(RequestException):\n    \"\"\"A JSON error occurred.\"\"\"\n\n\nclass JSONDecodeError(InvalidJSONError, CompatJSONDecodeError):\n    \"\"\"Couldn't decode the text into json\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Construct the JSONDecodeError instance first with all\n        args. Then use it's args to construct the IOError so that\n        the json specific args aren't used as IOError specific args\n        and the error message from JSONDecodeError is preserved.\n        \"\"\"\n        CompatJSONDecodeError.__init__(self, *args)\n        InvalidJSONError.__init__(self, *self.args, **kwargs)\n\n\nclass HTTPError(RequestException):\n    \"\"\"An HTTP error occurred.\"\"\"\n\n\nclass ConnectionError(RequestException):\n    \"\"\"A Connection error occurred.\"\"\"\n\n\nclass ProxyError(ConnectionError):\n    \"\"\"A proxy error occurred.\"\"\"\n\n\nclass SSLError(ConnectionError):\n    \"\"\"An SSL error occurred.\"\"\"\n\n\nclass Timeout(RequestException):\n    \"\"\"The request timed out.\n\n    Catching this error will catch both\n    :exc:`~requests.exceptions.ConnectTimeout` and\n    :exc:`~requests.exceptions.ReadTimeout` errors.\n    \"\"\"\n\n\nclass ConnectTimeout(ConnectionError, Timeout):\n    \"\"\"The request timed out while trying to connect to the remote server.\n\n    Requests that produced this error are safe to retry.\n    \"\"\"\n\n\nclass ReadTimeout(Timeout):\n    \"\"\"The server did not send any data in the allotted amount of time.\"\"\"\n\n\nclass URLRequired(RequestException):\n    \"\"\"A valid URL is required to make a request.\"\"\"\n\n\nclass TooManyRedirects(RequestException):\n    \"\"\"Too many redirects.\"\"\"\n\n\nclass MissingSchema(RequestException, ValueError):\n    \"\"\"The URL scheme (e.g. http or https) is missing.\"\"\"\n\n\nclass InvalidSchema(RequestException, ValueError):\n    \"\"\"The URL scheme provided is either invalid or unsupported.\"\"\"\n\n\nclass InvalidURL(RequestException, ValueError):\n    \"\"\"The URL provided was somehow invalid.\"\"\"\n\n\nclass InvalidHeader(RequestException, ValueError):\n    \"\"\"The header value provided was somehow invalid.\"\"\"\n\n\nclass InvalidProxyURL(InvalidURL):\n    \"\"\"The proxy URL provided is invalid.\"\"\"",
                "filename": "requests/exceptions.py",
                "start_index": 0,
                "end_index": 2907,
                "start_line": 1,
                "end_line": 106,
                "max_line": 141,
                "git_instance": "github",
                "repo_name": "testbed/psf__requests__2.18",
                "sha": ""
            },
            {
                "code": "#   __\n#  /__)  _  _     _   _ _/   _\n# / (   (- (/ (/ (- _)  /  _)\n#          /\n\n\"\"\"\nRequests HTTP Library\n~~~~~~~~~~~~~~~~~~~~~\n\nRequests is an HTTP library, written in Python, for human beings.\nBasic GET usage:\n\n   >>> import requests\n   >>> r = requests.get('https://www.python.org')\n   >>> r.status_code\n   200\n   >>> b'Python is a programming language' in r.content\n   True\n\n... or POST:\n\n   >>> payload = dict(key1='value1', key2='value2')\n   >>> r = requests.post('https://httpbin.org/post', data=payload)\n   >>> print(r.text)\n   {\n     ...\n     \"form\": {\n       \"key1\": \"value1\",\n       \"key2\": \"value2\"\n     },\n     ...\n   }\n\nThe other HTTP methods are supported - see `requests.api`. Full documentation\nis at <https://requests.readthedocs.io>.\n\n:copyright: (c) 2017 by Kenneth Reitz.\n:license: Apache 2.0, see LICENSE for more details.\n\"\"\"\n\nimport warnings\n\nimport urllib3\n\nfrom .exceptions import RequestsDependencyWarning\n\ntry:\n    from charset_normalizer import __version__ as charset_normalizer_version\nexcept ImportError:\n    charset_normalizer_version = None\n\ntry:\n    from chardet import __version__ as chardet_version\nexcept ImportError:\n    chardet_version = None\n\n\ndef check_compatibility(urllib3_version, chardet_version, charset_normalizer_version):\n    urllib3_version = urllib3_version.split(\".\")\n    assert urllib3_version != [\"dev\"]  # Verify urllib3 isn't installed from git.\n\n    # Sometimes, urllib3 only reports its version as 16.1.\n    if len(urllib3_version) == 2:\n        urllib3_version.append(\"0\")\n\n    # Check urllib3 for compatibility.\n    major, minor, patch = urllib3_version  # noqa: F811\n    major, minor, patch = int(major), int(minor), int(patch)\n    # urllib3 >= 1.21.1\n    assert major >= 1\n    if major == 1:\n        assert minor >= 21\n\n    # Check charset_normalizer for compatibility.\n    if chardet_version:\n        major, minor, patch = chardet_version.split(\".\")[:3]\n        major, minor, patch = int(major), int(minor), int(patch)\n        # chardet_version >= 3.0.2, < 6.0.0\n        assert (3, 0, 2) <= (major, minor, patch) < (6, 0, 0)\n    elif charset_normalizer_version:\n        major, minor, patch = charset_normalizer_version.split(\".\")[:3]\n        major, minor, patch = int(major), int(minor), int(patch)\n        # charset_normalizer >= 2.0.0 < 4.0.0\n        assert (2, 0, 0) <= (major, minor, patch) < (4, 0, 0)\n    else:\n        raise Exception(\"You need either charset_normalizer or chardet installed\")\n\n\ndef _check_cryptography(cryptography_version):\n    # cryptography < 1.3.4\n    try:\n        cryptography_version = list(map(int, cryptography_version.split(\".\")))\n    except ValueError:\n        return\n\n    if cryptography_version < [1, 3, 4]:\n        warning = \"Old version of cryptography ({}) may cause slowdown.\".format(\n            cryptography_version\n        )\n        warnings.warn(warning, RequestsDependencyWarning)\n\n\n# Check imported dependencies for compatibility.",
                "filename": "requests/__init__.py",
                "start_index": 0,
                "end_index": 2941,
                "start_line": 1,
                "end_line": 103,
                "max_line": 180,
                "git_instance": "github",
                "repo_name": "testbed/psf__requests__2.18",
                "sha": ""
            },
            {
                "code": "def select_proxy(url, proxies):\n    \"\"\"Select a proxy for the url, if applicable.\n\n    :param url: The url being for the request\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    \"\"\"\n    proxies = proxies or {}\n    urlparts = urlparse(url)\n    if urlparts.hostname is None:\n        return proxies.get(urlparts.scheme, proxies.get(\"all\"))\n\n    proxy_keys = [\n        urlparts.scheme + \"://\" + urlparts.hostname,\n        urlparts.scheme,\n        \"all://\" + urlparts.hostname,\n        \"all\",\n    ]\n    proxy = None\n    for proxy_key in proxy_keys:\n        if proxy_key in proxies:\n            proxy = proxies[proxy_key]\n            break\n\n    return proxy\n\n\ndef resolve_proxies(request, proxies, trust_env=True):\n    \"\"\"This method takes proxy information from a request and configuration\n    input to resolve a mapping of target proxies. This will consider settings\n    such a NO_PROXY to strip proxy configurations.\n\n    :param request: Request or PreparedRequest\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    :param trust_env: Boolean declaring whether to trust environment configs\n\n    :rtype: dict\n    \"\"\"\n    proxies = proxies if proxies is not None else {}\n    url = request.url\n    scheme = urlparse(url).scheme\n    no_proxy = proxies.get(\"no_proxy\")\n    new_proxies = proxies.copy()\n\n    if trust_env and not should_bypass_proxies(url, no_proxy=no_proxy):\n        environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n\n        proxy = environ_proxies.get(scheme, environ_proxies.get(\"all\"))\n\n        if proxy:\n            new_proxies.setdefault(scheme, proxy)\n    return new_proxies\n\n\ndef default_user_agent(name=\"python-requests\"):\n    \"\"\"\n    Return a string representing the default user agent.\n\n    :rtype: str\n    \"\"\"\n    return f\"{name}/{__version__}\"\n\n\ndef default_headers():\n    \"\"\"\n    :rtype: requests.structures.CaseInsensitiveDict\n    \"\"\"\n    return CaseInsensitiveDict(\n        {\n            \"User-Agent\": default_user_agent(),\n            \"Accept-Encoding\": DEFAULT_ACCEPT_ENCODING,\n            \"Accept\": \"*/*\",\n            \"Connection\": \"keep-alive\",\n        }\n    )\n\n\ndef parse_header_links(value):\n    \"\"\"Return a list of parsed link headers proxies.\n\n    i.e. Link: <http:/.../front.jpeg>; rel=front; type=\"image/jpeg\",<http://.../back.jpeg>; rel=back;type=\"image/jpeg\"\n\n    :rtype: list\n    \"\"\"\n\n    links = []\n\n    replace_chars = \" '\\\"\"\n\n    value = value.strip(replace_chars)\n    if not value:\n        return links\n\n    for val in re.split(\", *<\", value):\n        try:\n            url, params = val.split(\";\", 1)\n        except ValueError:\n            url, params = val, \"\"\n\n        link = {\"url\": url.strip(\"<> '\\\"\")}\n\n        for param in params.split(\";\"):\n            try:\n                key, value = param.split(\"=\")\n            except ValueError:\n                break\n\n            link[key.strip(replace_chars)] = value.strip(replace_chars)\n\n        links.append(link)\n\n    return links",
                "filename": "requests/utils.py",
                "start_index": 25594,
                "end_index": 28594,
                "start_line": 832,
                "end_line": 942,
                "max_line": 1090,
                "git_instance": "github",
                "repo_name": "testbed/psf__requests__2.18",
                "sha": ""
            },
            {
                "code": "class ChunkedEncodingError(RequestException):\n    \"\"\"The server declared chunked encoding but sent an invalid chunk.\"\"\"\n\n\nclass ContentDecodingError(RequestException, BaseHTTPError):\n    \"\"\"Failed to decode response content.\"\"\"\n\n\nclass StreamConsumedError(RequestException, TypeError):\n    \"\"\"The content for this response was already consumed.\"\"\"\n\n\nclass RetryError(RequestException):\n    \"\"\"Custom retries logic failed\"\"\"\n\n\nclass UnrewindableBodyError(RequestException):\n    \"\"\"Requests encountered an error when trying to rewind a body.\"\"\"\n\n\n# Warnings\n\n\nclass RequestsWarning(Warning):\n    \"\"\"Base warning for Requests.\"\"\"\n\n\nclass FileModeWarning(RequestsWarning, DeprecationWarning):\n    \"\"\"A file was opened in text mode, but Requests determined its binary length.\"\"\"\n\n\nclass RequestsDependencyWarning(RequestsWarning):\n    \"\"\"An imported dependency doesn't match the expected version range.\"\"\"",
                "filename": "requests/exceptions.py",
                "start_index": 2910,
                "end_index": 3810,
                "start_line": 109,
                "end_line": 141,
                "max_line": 141,
                "git_instance": "github",
                "repo_name": "testbed/psf__requests__2.18",
                "sha": ""
            },
            {
                "code": "def request_url(self, request, proxies):\n        \"\"\"Obtain the url to use when making the final request.\n\n        If the message is being sent through a HTTP proxy, the full URL has to\n        be used. Otherwise, we should only use the path portion of the URL.\n\n        This should not be called from user code, and is only exposed for use\n        when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n        :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs.\n        :rtype: str\n        \"\"\"\n        proxy = select_proxy(request.url, proxies)\n        scheme = urlparse(request.url).scheme\n\n        is_proxied_http_request = proxy and scheme != \"https\"\n        using_socks_proxy = False\n        if proxy:\n            proxy_scheme = urlparse(proxy).scheme.lower()\n            using_socks_proxy = proxy_scheme.startswith(\"socks\")\n\n        url = request.path_url\n        if is_proxied_http_request and not using_socks_proxy:\n            url = urldefragauth(request.url)\n\n        return url\n\n    def add_headers(self, request, **kwargs):\n        \"\"\"Add any headers needed by the connection. As of v2.0 this does\n        nothing by default, but is left for overriding by users that subclass\n        the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        This should not be called from user code, and is only exposed for use\n        when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` to add headers to.\n        :param kwargs: The keyword arguments from the call to send().\n        \"\"\"\n        pass\n\n    def proxy_headers(self, proxy):\n        \"\"\"Returns a dictionary of the headers to add to any request sent\n        through a proxy. This works with urllib3 magic to ensure that they are\n        correctly sent to the proxy, rather than in a tunnelled request if\n        CONNECT is being used.\n\n        This should not be called from user code, and is only exposed for use\n        when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param proxy: The url of the proxy being used for this request.\n        :rtype: dict\n        \"\"\"\n        headers = {}\n        username, password = get_auth_from_url(proxy)\n\n        if username:\n            headers[\"Proxy-Authorization\"] = _basic_auth_str(username, password)\n\n        return headers",
                "filename": "requests/adapters.py",
                "start_index": 12968,
                "end_index": 15487,
                "start_line": 370,
                "end_line": 432,
                "max_line": 538,
                "git_instance": "github",
                "repo_name": "testbed/psf__requests__2.18",
                "sha": ""
            },
            {
                "code": "def rebuild_auth(self, prepared_request, response):\n        \"\"\"When being redirected we may want to strip authentication from the\n        request to avoid leaking credentials. This method intelligently removes\n        and reapplies authentication where possible to avoid credential loss.\n        \"\"\"\n        headers = prepared_request.headers\n        url = prepared_request.url\n\n        if \"Authorization\" in headers and self.should_strip_auth(\n            response.request.url, url\n        ):\n            # If we get redirected to a new host, we should strip out any\n            # authentication headers.\n            del headers[\"Authorization\"]\n\n        # .netrc might have more auth for us on our new host.\n        new_auth = get_netrc_auth(url) if self.trust_env else None\n        if new_auth is not None:\n            prepared_request.prepare_auth(new_auth)\n\n    def rebuild_proxies(self, prepared_request, proxies):\n        \"\"\"This method re-evaluates the proxy configuration by considering the\n        environment variables. If we are redirected to a URL covered by\n        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing\n        proxy keys for this URL (in case they were stripped by a previous\n        redirect).\n\n        This method also replaces the Proxy-Authorization header where\n        necessary.\n\n        :rtype: dict\n        \"\"\"\n        headers = prepared_request.headers\n        scheme = urlparse(prepared_request.url).scheme\n        new_proxies = resolve_proxies(prepared_request, proxies, self.trust_env)\n\n        if \"Proxy-Authorization\" in headers:\n            del headers[\"Proxy-Authorization\"]\n\n        try:\n            username, password = get_auth_from_url(new_proxies[scheme])\n        except KeyError:\n            username, password = None, None\n\n        # urllib3 handles proxy authorization for us in the standard adapter.\n        # Avoid appending this to TLS tunneled requests where it may be leaked.\n        if not scheme.startswith(\"https\") and username and password:\n            headers[\"Proxy-Authorization\"] = _basic_auth_str(username, password)\n\n        return new_proxies",
                "filename": "requests/sessions.py",
                "start_index": 10248,
                "end_index": 12379,
                "start_line": 283,
                "end_line": 332,
                "max_line": 835,
                "git_instance": "github",
                "repo_name": "testbed/psf__requests__2.18",
                "sha": ""
            },
            {
                "code": "prepared_request = req.copy()\n\n            # Update history and keep track of redirects.\n            # resp.history must ignore the original request in this loop\n            hist.append(resp)\n            resp.history = hist[1:]\n\n            try:\n                resp.content  # Consume socket so it can be released\n            except (ChunkedEncodingError, ContentDecodingError, RuntimeError):\n                resp.raw.read(decode_content=False)\n\n            if len(resp.history) >= self.max_redirects:\n                raise TooManyRedirects(\n                    f\"Exceeded {self.max_redirects} redirects.\", response=resp\n                )\n\n            # Release the connection back into the pool.\n            resp.close()\n\n            # Handle redirection without scheme (see: RFC 1808 Section 4)\n            if url.startswith(\"//\"):\n                parsed_rurl = urlparse(resp.url)\n                url = \":\".join([to_native_string(parsed_rurl.scheme), url])\n\n            # Normalize url case and attach previous fragment if needed (RFC 7231 7.1.2)\n            parsed = urlparse(url)\n            if parsed.fragment == \"\" and previous_fragment:\n                parsed = parsed._replace(fragment=previous_fragment)\n            elif parsed.fragment:\n                previous_fragment = parsed.fragment\n            url = parsed.geturl()\n\n            # Facilitate relative 'location' headers, as allowed by RFC 7231.\n            # (e.g. '/path/to/resource' instead of 'http://domain.tld/path/to/resource')\n            # Compliant with RFC3986, we percent encode the url.\n            if not parsed.netloc:\n                url = urljoin(resp.url, requote_uri(url))\n            else:\n                url = requote_uri(url)\n\n            prepared_request.url = to_native_string(url)\n\n            self.rebuild_method(prepared_request, resp)\n\n            # https://github.com/psf/requests/issues/1084\n            if resp.status_code not in (\n                codes.temporary_redirect,\n                codes.permanent_redirect,\n            ):\n                # https://github.com/psf/requests/issues/3490\n                purged_headers = (\"Content-Length\", \"Content-Type\", \"Transfer-Encoding\")\n                for header in purged_headers:\n                    prepared_request.headers.pop(header, None)\n                prepared_request.body = None\n\n            headers = prepared_request.headers\n            headers.pop(\"Cookie\", None)\n\n            # Extract any cookies sent on the response to the cookiejar\n            # in the new request. Because we've mutated our copied prepared\n            # request, use the old one that we haven't yet touched.\n            extract_cookies_to_jar(prepared_request._cookies, req, resp.raw)\n            merge_cookies(prepared_request._cookies, self.cookies)\n            prepared_request.prepare_cookies(prepared_request._cookies)\n\n            # Rebuild auth and proxy information.\n            proxies = self.rebuild_proxies(prepared_request, proxies)",
                "filename": "requests/sessions.py",
                "start_index": 5959,
                "end_index": 8935,
                "start_line": 178,
                "end_line": 245,
                "max_line": 835,
                "git_instance": "github",
                "repo_name": "testbed/psf__requests__2.18",
                "sha": ""
            },
            {
                "code": "try:\n    check_compatibility(\n        urllib3.__version__, chardet_version, charset_normalizer_version\n    )\nexcept (AssertionError, ValueError):\n    warnings.warn(\n        \"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n        \"version!\".format(\n            urllib3.__version__, chardet_version, charset_normalizer_version\n        ),\n        RequestsDependencyWarning,\n    )\n\n# Attempt to enable urllib3's fallback for SNI support\n# if the standard library doesn't support SNI or the\n# 'ssl' library isn't available.\ntry:\n    try:\n        import ssl\n    except ImportError:\n        ssl = None\n\n    if not getattr(ssl, \"HAS_SNI\", False):\n        from urllib3.contrib import pyopenssl\n\n        pyopenssl.inject_into_urllib3()\n\n        # Check cryptography version\n        from cryptography import __version__ as cryptography_version\n\n        _check_cryptography(cryptography_version)\nexcept ImportError:\n    pass\n\n# urllib3's DependencyWarnings should be silenced.\nfrom urllib3.exceptions import DependencyWarning\n\nwarnings.simplefilter(\"ignore\", DependencyWarning)\n\n# Set default logging handler to avoid \"No handler found\" warnings.\nimport logging\nfrom logging import NullHandler\n\nfrom . import packages, utils\nfrom .__version__ import (\n    __author__,\n    __author_email__,\n    __build__,\n    __cake__,\n    __copyright__,\n    __description__,\n    __license__,\n    __title__,\n    __url__,\n    __version__,\n)\nfrom .api import delete, get, head, options, patch, post, put, request\nfrom .exceptions import (\n    ConnectionError,\n    ConnectTimeout,\n    FileModeWarning,\n    HTTPError,\n    JSONDecodeError,\n    ReadTimeout,\n    RequestException,\n    Timeout,\n    TooManyRedirects,\n    URLRequired,\n)\nfrom .models import PreparedRequest, Request, Response\nfrom .sessions import Session, session\nfrom .status_codes import codes\n\nlogging.getLogger(__name__).addHandler(NullHandler())\n\n# FileModeWarnings go off per the default.\nwarnings.simplefilter(\"default\", FileModeWarning, append=True)",
                "filename": "requests/__init__.py",
                "start_index": 2942,
                "end_index": 4962,
                "start_line": 47,
                "end_line": 180,
                "max_line": 180,
                "git_instance": "github",
                "repo_name": "testbed/psf__requests__2.18",
                "sha": ""
            },
            {
                "code": "# .-. .-. .-. . . .-. .-. .-. .-.\n# |(  |-  |.| | | |-  `-.  |  `-.\n# ' ' `-' `-`.`-' `-' `-'  '  `-'\n\n__title__ = \"requests\"\n__description__ = \"Python HTTP for Humans.\"\n__url__ = \"https://requests.readthedocs.io\"\n__version__ = \"2.31.0\"\n__build__ = 0x023100\n__author__ = \"Kenneth Reitz\"\n__author_email__ = \"me@kennethreitz.org\"\n__license__ = \"Apache 2.0\"\n__copyright__ = \"Copyright Kenneth Reitz\"\n__cake__ = \"\\u2728 \\U0001f370 \\u2728\"",
                "filename": "requests/__version__.py",
                "start_index": 0,
                "end_index": 434,
                "start_line": 1,
                "end_line": 14,
                "max_line": 14,
                "git_instance": "github",
                "repo_name": "testbed/psf__requests__2.18",
                "sha": ""
            },
            {
                "code": "import sys\n\ntry:\n    import chardet\nexcept ImportError:\n    import warnings\n\n    import charset_normalizer as chardet\n\n    warnings.filterwarnings(\"ignore\", \"Trying to detect\", module=\"charset_normalizer\")\n\n# This code exists for backwards compatibility reasons.\n# I don't like it either. Just look the other way. :)\n\nfor package in (\"urllib3\", \"idna\"):\n    locals()[package] = __import__(package)\n    # This traversal is apparently necessary such that the identities are\n    # preserved (requests.packages.urllib3.* is urllib3.*)\n    for mod in list(sys.modules):\n        if mod == package or mod.startswith(f\"{package}.\"):\n            sys.modules[f\"requests.packages.{mod}\"] = sys.modules[mod]\n\ntarget = chardet.__name__\nfor mod in list(sys.modules):\n    if mod == target or mod.startswith(f\"{target}.\"):\n        target = target.replace(target, \"chardet\")\n        sys.modules[f\"requests.packages.{target}\"] = sys.modules[mod]\n# Kinda cool, though, right?",
                "filename": "requests/packages.py",
                "start_index": 0,
                "end_index": 956,
                "start_line": 1,
                "end_line": 28,
                "max_line": 28,
                "git_instance": "github",
                "repo_name": "testbed/psf__requests__2.18",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "requests/exceptions.py": [
                {
                    "chunk": {
                        "code": "\"\"\"\nrequests.exceptions\n~~~~~~~~~~~~~~~~~~~\n\nThis module contains the set of Requests' exceptions.\n\"\"\"\nfrom urllib3.exceptions import HTTPError as BaseHTTPError\n\nfrom .compat import JSONDecodeError as CompatJSONDecodeError\n\n\nclass RequestException(IOError):\n    \"\"\"There was an ambiguous exception that occurred while handling your\n    request.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Initialize RequestException with `request` and `response` objects.\"\"\"\n        response = kwargs.pop(\"response\", None)\n        self.response = response\n        self.request = kwargs.pop(\"request\", None)\n        if response is not None and not self.request and hasattr(response, \"request\"):\n            self.request = self.response.request\n        super().__init__(*args, **kwargs)\n\n\nclass InvalidJSONError(RequestException):\n    \"\"\"A JSON error occurred.\"\"\"\n\n\nclass JSONDecodeError(InvalidJSONError, CompatJSONDecodeError):\n    \"\"\"Couldn't decode the text into json\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Construct the JSONDecodeError instance first with all\n        args. Then use it's args to construct the IOError so that\n        the json specific args aren't used as IOError specific args\n        and the error message from JSONDecodeError is preserved.\n        \"\"\"\n        CompatJSONDecodeError.__init__(self, *args)\n        InvalidJSONError.__init__(self, *self.args, **kwargs)\n\n\nclass HTTPError(RequestException):\n    \"\"\"An HTTP error occurred.\"\"\"\n\n\nclass ConnectionError(RequestException):\n    \"\"\"A Connection error occurred.\"\"\"\n\n\nclass ProxyError(ConnectionError):\n    \"\"\"A proxy error occurred.\"\"\"\n\n\nclass SSLError(ConnectionError):\n    \"\"\"An SSL error occurred.\"\"\"\n\n\nclass Timeout(RequestException):\n    \"\"\"The request timed out.\n\n    Catching this error will catch both\n    :exc:`~requests.exceptions.ConnectTimeout` and\n    :exc:`~requests.exceptions.ReadTimeout` errors.\n    \"\"\"\n\n\nclass ConnectTimeout(ConnectionError, Timeout):\n    \"\"\"The request timed out while trying to connect to the remote server.\n\n    Requests that produced this error are safe to retry.\n    \"\"\"\n\n\nclass ReadTimeout(Timeout):\n    \"\"\"The server did not send any data in the allotted amount of time.\"\"\"\n\n\nclass URLRequired(RequestException):\n    \"\"\"A valid URL is required to make a request.\"\"\"\n\n\nclass TooManyRedirects(RequestException):\n    \"\"\"Too many redirects.\"\"\"\n\n\nclass MissingSchema(RequestException, ValueError):\n    \"\"\"The URL scheme (e.g. http or https) is missing.\"\"\"\n\n\nclass InvalidSchema(RequestException, ValueError):\n    \"\"\"The URL scheme provided is either invalid or unsupported.\"\"\"\n\n\nclass InvalidURL(RequestException, ValueError):\n    \"\"\"The URL provided was somehow invalid.\"\"\"\n\n\nclass InvalidHeader(RequestException, ValueError):\n    \"\"\"The header value provided was somehow invalid.\"\"\"\n\n\nclass InvalidProxyURL(InvalidURL):\n    \"\"\"The proxy URL provided is invalid.\"\"\"",
                        "filename": "requests/exceptions.py",
                        "start_index": 0,
                        "end_index": 2907,
                        "start_line": 1,
                        "end_line": 106,
                        "max_line": 141,
                        "git_instance": "github",
                        "repo_name": "testbed/psf__requests__2.18",
                        "sha": ""
                    },
                    "reason_for_relevance": "This file contains definitions for exceptions in the requests library. A new exception or modification of an existing one, such as InvalidProxyURL, may be needed to provide a clearer error message when the proxy URL is malformed."
                }
            ],
            "requests/utils.py": [
                {
                    "chunk": {
                        "code": "def select_proxy(url, proxies):\n    \"\"\"Select a proxy for the url, if applicable.\n\n    :param url: The url being for the request\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    \"\"\"\n    proxies = proxies or {}\n    urlparts = urlparse(url)\n    if urlparts.hostname is None:\n        return proxies.get(urlparts.scheme, proxies.get(\"all\"))\n\n    proxy_keys = [\n        urlparts.scheme + \"://\" + urlparts.hostname,\n        urlparts.scheme,\n        \"all://\" + urlparts.hostname,\n        \"all\",\n    ]\n    proxy = None\n    for proxy_key in proxy_keys:\n        if proxy_key in proxies:\n            proxy = proxies[proxy_key]\n            break\n\n    return proxy\n\n\ndef resolve_proxies(request, proxies, trust_env=True):\n    \"\"\"This method takes proxy information from a request and configuration\n    input to resolve a mapping of target proxies. This will consider settings\n    such a NO_PROXY to strip proxy configurations.\n\n    :param request: Request or PreparedRequest\n    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n    :param trust_env: Boolean declaring whether to trust environment configs\n\n    :rtype: dict\n    \"\"\"\n    proxies = proxies if proxies is not None else {}\n    url = request.url\n    scheme = urlparse(url).scheme\n    no_proxy = proxies.get(\"no_proxy\")\n    new_proxies = proxies.copy()\n\n    if trust_env and not should_bypass_proxies(url, no_proxy=no_proxy):\n        environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n\n        proxy = environ_proxies.get(scheme, environ_proxies.get(\"all\"))\n\n        if proxy:\n            new_proxies.setdefault(scheme, proxy)\n    return new_proxies\n\n\ndef default_user_agent(name=\"python-requests\"):\n    \"\"\"\n    Return a string representing the default user agent.\n\n    :rtype: str\n    \"\"\"\n    return f\"{name}/{__version__}\"\n\n\ndef default_headers():\n    \"\"\"\n    :rtype: requests.structures.CaseInsensitiveDict\n    \"\"\"\n    return CaseInsensitiveDict(\n        {\n            \"User-Agent\": default_user_agent(),\n            \"Accept-Encoding\": DEFAULT_ACCEPT_ENCODING,\n            \"Accept\": \"*/*\",\n            \"Connection\": \"keep-alive\",\n        }\n    )\n\n\ndef parse_header_links(value):\n    \"\"\"Return a list of parsed link headers proxies.\n\n    i.e. Link: <http:/.../front.jpeg>; rel=front; type=\"image/jpeg\",<http://.../back.jpeg>; rel=back;type=\"image/jpeg\"\n\n    :rtype: list\n    \"\"\"\n\n    links = []\n\n    replace_chars = \" '\\\"\"\n\n    value = value.strip(replace_chars)\n    if not value:\n        return links\n\n    for val in re.split(\", *<\", value):\n        try:\n            url, params = val.split(\";\", 1)\n        except ValueError:\n            url, params = val, \"\"\n\n        link = {\"url\": url.strip(\"<> '\\\"\")}\n\n        for param in params.split(\";\"):\n            try:\n                key, value = param.split(\"=\")\n            except ValueError:\n                break\n\n            link[key.strip(replace_chars)] = value.strip(replace_chars)\n\n        links.append(link)\n\n    return links",
                        "filename": "requests/utils.py",
                        "start_index": 25594,
                        "end_index": 28594,
                        "start_line": 832,
                        "end_line": 942,
                        "max_line": 1090,
                        "git_instance": "github",
                        "repo_name": "testbed/psf__requests__2.18",
                        "sha": ""
                    },
                    "reason_for_relevance": "This file contains the proxy selection logic. It may need to be modified to validate the proxy URL format and raise a more descriptive exception when the format is incorrect."
                }
            ],
            "requests/adapters.py": [
                {
                    "chunk": {
                        "code": "def request_url(self, request, proxies):\n        \"\"\"Obtain the url to use when making the final request.\n\n        If the message is being sent through a HTTP proxy, the full URL has to\n        be used. Otherwise, we should only use the path portion of the URL.\n\n        This should not be called from user code, and is only exposed for use\n        when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n        :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs.\n        :rtype: str\n        \"\"\"\n        proxy = select_proxy(request.url, proxies)\n        scheme = urlparse(request.url).scheme\n\n        is_proxied_http_request = proxy and scheme != \"https\"\n        using_socks_proxy = False\n        if proxy:\n            proxy_scheme = urlparse(proxy).scheme.lower()\n            using_socks_proxy = proxy_scheme.startswith(\"socks\")\n\n        url = request.path_url\n        if is_proxied_http_request and not using_socks_proxy:\n            url = urldefragauth(request.url)\n\n        return url\n\n    def add_headers(self, request, **kwargs):\n        \"\"\"Add any headers needed by the connection. As of v2.0 this does\n        nothing by default, but is left for overriding by users that subclass\n        the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        This should not be called from user code, and is only exposed for use\n        when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` to add headers to.\n        :param kwargs: The keyword arguments from the call to send().\n        \"\"\"\n        pass\n\n    def proxy_headers(self, proxy):\n        \"\"\"Returns a dictionary of the headers to add to any request sent\n        through a proxy. This works with urllib3 magic to ensure that they are\n        correctly sent to the proxy, rather than in a tunnelled request if\n        CONNECT is being used.\n\n        This should not be called from user code, and is only exposed for use\n        when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param proxy: The url of the proxy being used for this request.\n        :rtype: dict\n        \"\"\"\n        headers = {}\n        username, password = get_auth_from_url(proxy)\n\n        if username:\n            headers[\"Proxy-Authorization\"] = _basic_auth_str(username, password)\n\n        return headers",
                        "filename": "requests/adapters.py",
                        "start_index": 12968,
                        "end_index": 15487,
                        "start_line": 370,
                        "end_line": 432,
                        "max_line": 538,
                        "git_instance": "github",
                        "repo_name": "testbed/psf__requests__2.18",
                        "sha": ""
                    },
                    "reason_for_relevance": "This file contains the logic for preparing the request URL, which includes handling proxies. It may be necessary to review and potentially modify how proxy URLs are processed to ensure that malformed proxy URLs are handled correctly."
                }
            ],
            "requests/sessions.py": [
                {
                    "chunk": {
                        "code": "def rebuild_auth(self, prepared_request, response):\n        \"\"\"When being redirected we may want to strip authentication from the\n        request to avoid leaking credentials. This method intelligently removes\n        and reapplies authentication where possible to avoid credential loss.\n        \"\"\"\n        headers = prepared_request.headers\n        url = prepared_request.url\n\n        if \"Authorization\" in headers and self.should_strip_auth(\n            response.request.url, url\n        ):\n            # If we get redirected to a new host, we should strip out any\n            # authentication headers.\n            del headers[\"Authorization\"]\n\n        # .netrc might have more auth for us on our new host.\n        new_auth = get_netrc_auth(url) if self.trust_env else None\n        if new_auth is not None:\n            prepared_request.prepare_auth(new_auth)\n\n    def rebuild_proxies(self, prepared_request, proxies):\n        \"\"\"This method re-evaluates the proxy configuration by considering the\n        environment variables. If we are redirected to a URL covered by\n        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing\n        proxy keys for this URL (in case they were stripped by a previous\n        redirect).\n\n        This method also replaces the Proxy-Authorization header where\n        necessary.\n\n        :rtype: dict\n        \"\"\"\n        headers = prepared_request.headers\n        scheme = urlparse(prepared_request.url).scheme\n        new_proxies = resolve_proxies(prepared_request, proxies, self.trust_env)\n\n        if \"Proxy-Authorization\" in headers:\n            del headers[\"Proxy-Authorization\"]\n\n        try:\n            username, password = get_auth_from_url(new_proxies[scheme])\n        except KeyError:\n            username, password = None, None\n\n        # urllib3 handles proxy authorization for us in the standard adapter.\n        # Avoid appending this to TLS tunneled requests where it may be leaked.\n        if not scheme.startswith(\"https\") and username and password:\n            headers[\"Proxy-Authorization\"] = _basic_auth_str(username, password)\n\n        return new_proxies",
                        "filename": "requests/sessions.py",
                        "start_index": 10248,
                        "end_index": 12379,
                        "start_line": 283,
                        "end_line": 332,
                        "max_line": 835,
                        "git_instance": "github",
                        "repo_name": "testbed/psf__requests__2.18",
                        "sha": ""
                    },
                    "reason_for_relevance": "This file contains methods for rebuilding proxy information. It may be relevant to check if additional validation is needed here to catch malformed proxy URLs before a request is sent."
                }
            ]
        },
        "solution_task": "- Add a check in `urllib3.connectionpool.HTTPConnectionPool._prepare_proxy` to validate the proxy URL format and raise a more informative exception if it is invalid.\n- Update the `InvalidProxyURL` exception in `requests.exceptions` to provide a clearer error message when the proxy URL is invalid.\n- Ensure that the proxy URL includes the scheme (e.g., `http://` or `https://`) and the trailing slash if necessary.\n- Use the `InvalidProxyURL` exception within `urllib3.util.connection.create_connection` or `urllib3.connectionpool.HTTPConnectionPool._prepare_proxy` when an invalid proxy URL is detected.\n\nExample of a possible solution:\n\n```python\n# requests/exceptions.py\nclass InvalidProxyURL(InvalidURL):\n    \"\"\"The proxy URL provided is invalid. Ensure the URL starts with http:// or https:// and is a valid proxy address.\"\"\"\n\n# urllib3/connectionpool.py\nclass HTTPConnectionPool:\n    # ...\n    def _prepare_proxy(self, conn):\n        proxy = self.proxy\n        if proxy:\n            if not proxy.startswith(('http://', 'https://')):\n                raise InvalidProxyURL(f\"The proxy URL is invalid: {proxy}. Ensure the URL starts with http:// or https://.\")\n            # Existing code to prepare the proxy...\n        # ...\n```\n\nThis solution ensures that the proxy URL is checked for the correct format before attempting a connection and provides a more informative error message to the user if the proxy URL is invalid.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE provides a detailed and actionable solution to the problem of misleading exceptions when an invalid proxy URL is set. It suggests adding a check in the `urllib3.connectionpool.HTTPConnectionPool._prepare_proxy` method to validate the proxy URL format and raise a more informative exception if it is invalid. It also recommends updating the `InvalidProxyURL` exception in `requests.exceptions` to provide a clearer error message. The RESPONSE includes an example of how to implement the solution, which is directly relevant to the PROMPT's issue. The RESPONSE also provides links to the relevant code snippets that may need to be checked or modified, which is useful for developers looking to implement the fix. The only reason it does not score a 10 is that it does not explicitly mention which library (requests, urllib3, or urlparse) should be responsible for the fix, which was a question raised in the PROMPT.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711760833.817626,
        "result": "- Add a check in `urllib3.connectionpool.HTTPConnectionPool._prepare_proxy` to validate the proxy URL format and raise a more informative exception if it is invalid.\n- Update the `InvalidProxyURL` exception in `requests.exceptions` to provide a clearer error message when the proxy URL is invalid.\n- Ensure that the proxy URL includes the scheme (e.g., `http://` or `https://`) and the trailing slash if necessary.\n- Use the `InvalidProxyURL` exception within `urllib3.util.connection.create_connection` or `urllib3.connectionpool.HTTPConnectionPool._prepare_proxy` when an invalid proxy URL is detected.\n\nExample of a possible solution:\n\n```python\n# requests/exceptions.py\nclass InvalidProxyURL(InvalidURL):\n    \"\"\"The proxy URL provided is invalid. Ensure the URL starts with http:// or https:// and is a valid proxy address.\"\"\"\n\n# urllib3/connectionpool.py\nclass HTTPConnectionPool:\n    # ...\n    def _prepare_proxy(self, conn):\n        proxy = self.proxy\n        if proxy:\n            if not proxy.startswith(('http://', 'https://')):\n                raise InvalidProxyURL(f\"The proxy URL is invalid: {proxy}. Ensure the URL starts with http:// or https://.\")\n            # Existing code to prepare the proxy...\n        # ...\n```\n\nThis solution ensures that the proxy URL is checked for the correct format before attempting a connection and provides a more informative error message to the user if the proxy URL is invalid.\n\n### Code snippets to check\n\n-  \u2192 [requests](https://github.com/testbed/psf__requests__2.18/tree//requests)- [Lines 1 - 106](https://github.com/testbed/psf__requests__2.18/blob//requests/exceptions.py#L1-L106)\n\n This file contains definitions for exceptions in the requests library. A new exception or modification of an existing one, such as InvalidProxyURL, may be needed to provide a clearer error message when the proxy URL is malformed.\n-  \u2192 [requests](https://github.com/testbed/psf__requests__2.18/tree//requests)- [Lines 832 - 942](https://github.com/testbed/psf__requests__2.18/blob//requests/utils.py#L832-L942)\n\n This file contains the proxy selection logic. It may need to be modified to validate the proxy URL format and raise a more descriptive exception when the format is incorrect.\n-  \u2192 [requests](https://github.com/testbed/psf__requests__2.18/tree//requests)- [Lines 370 - 432](https://github.com/testbed/psf__requests__2.18/blob//requests/adapters.py#L370-L432)\n\n This file contains the logic for preparing the request URL, which includes handling proxies. It may be necessary to review and potentially modify how proxy URLs are processed to ensure that malformed proxy URLs are handled correctly.\n-  \u2192 [requests](https://github.com/testbed/psf__requests__2.18/tree//requests)- [Lines 283 - 332](https://github.com/testbed/psf__requests__2.18/blob//requests/sessions.py#L283-L332)\n\n This file contains methods for rebuilding proxy information. It may be relevant to check if additional validation is needed here to catch malformed proxy URLs before a request is sent."
    }
}