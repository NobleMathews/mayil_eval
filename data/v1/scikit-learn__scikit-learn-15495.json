{
    "id": "scikit-learn__scikit-learn-15495",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
    "task_id": 2210989863649669923,
    "title": "Use _check_sample_weight to consistently validate sample_weight",
    "body": "We recently introduced `utils.validation._check_sample_weight` which returns a validated `sample_weight` array.\r\n\r\nWe should use it consistently throughout the code base, instead of relying on custom and adhoc checks like `check_consistent_lenght` or `check_array` (which are now handled by `_check_sample_weight`).\r\n\r\n\r\n\r\nHere's a list of the estimators/functions that could make use of it (mostly in `fit` or `partial_fit`):\r\n\r\n- [x] CalibratedClassifierCV\r\n- [x] DBSCAN\r\n- [x] DummyClassifier\r\n- [x] DummyRegressor\r\n- [x] BaseBagging\r\n- [x] BaseForest\r\n- [x] BaseGradientBoosting\r\n- [x] IsotonicRegression\r\n- [x] KernelRidge\r\n- [x] GaussianNB\r\n- [x] BaseDiscreteNB\r\n- [x] KernelDensity\r\n- [x] BaseDecisionTree\r\n\r\n(I left-out the linear_model module because it seems more involved there)\r\n\r\nCould be a decent sprint issue @amueller ?\r\n\r\nTo know where a given class is defined, use e.g. `git grep -n \"class DBSCAN\"`\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.22.git",
    "requested_commit": "",
    "mayil_response": "- Locate the `fit` or `partial_fit` methods in the classes `CalibratedClassifierCV`, `DBSCAN`, `DummyClassifier`, `DummyRegressor`, `BaseBagging`, `BaseForest`, `BaseGradientBoosting`, `IsotonicRegression`, `KernelRidge`, `GaussianNB`, `BaseDiscreteNB`, `KernelDensity`, and `BaseDecisionTree`.\n- Import `_check_sample_weight` from `sklearn.utils.validation` in each of these classes if not already imported.\n- Replace any ad-hoc checks for `sample_weight` with a call to `_check_sample_weight(sample_weight, X)` in the `fit` or `partial_fit` methods.\n- Ensure that the `sample_weight` parameter is passed to `_check_sample_weight` before any further processing in the `fit` or `partial_fit` methods.\n- Remove any now redundant imports of `check_consistent_length` and `check_array` if they were only used for validating `sample_weight`.\n- Do not make changes to modules within the `linear_model` module, as it is excluded from this task.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/utils)- [Lines 1789 - 1861](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/validation.py#L1789-L1861)\n\n This snippet contains the implementation of '_check_sample_weight' which is the method that needs to be used consistently across the codebase for validating 'sample_weight'.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/utils)- [Lines 181 - 1181](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/estimator_checks.py#L181-L1181)\n\n This snippet includes tests that check if estimators accept 'sample_weight' of various types. It may need to be updated to use '_check_sample_weight' for consistency. \n\n\n   - [Lines 181 - 1218](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/estimator_checks.py#L181-L1218)\n\n This snippet tests the behavior of estimators when 'sample_weight' has a mismatched shape. It's relevant because it may need to be updated to use '_check_sample_weight'. \n\n\n   - [Lines 181 - 1287](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/estimator_checks.py#L181-L1287)\n\n This snippet tests the invariance of estimators to 'sample_weight'. It's relevant because it may need to be updated to use '_check_sample_weight'.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0003105,
        "snippet_processor": 0.11024,
        "issue_star_creation": 0.03695,
        "issue_star_solver": 0.1084,
        "bouncer": 0.03046
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711738738.52704,
        "relevant_snippets": [
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_sample_weights_pandas_series(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type pandas.Series in the 'fit' function.\n    estimator = clone(estimator_orig)\n    try:\n        import pandas as pd\n\n        X = np.array(\n            [\n                [1, 1],\n                [1, 2],\n                [1, 3],\n                [1, 4],\n                [2, 1],\n                [2, 2],\n                [2, 3],\n                [2, 4],\n                [3, 1],\n                [3, 2],\n                [3, 3],\n                [3, 4],\n            ]\n        )\n        X = pd.DataFrame(_enforce_estimator_tags_X(estimator_orig, X), copy=False)\n        y = pd.Series([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2])\n        weights = pd.Series([1] * 12)\n        if _safe_tags(estimator, key=\"multioutput_only\"):\n            y = pd.DataFrame(y, copy=False)\n        try:\n            estimator.fit(X, y, sample_weight=weights)\n        except ValueError:\n            raise ValueError(\n                \"Estimator {0} raises error if \"\n                \"'sample_weight' parameter is of \"\n                \"type pandas.Series\".format(name)\n            )\n    except ImportError:\n        raise SkipTest(\n            \"pandas is not installed: not testing for \"\n            \"input of type pandas.Series to class weight.\"\n        )\n\n\n@ignore_warnings(category=(FutureWarning))\ndef check_sample_weights_not_an_array(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type _NotAnArray in the 'fit' function.\n    estimator = clone(estimator_orig)\n    X = np.array(\n        [\n            [1, 1],\n            [1, 2],\n            [1, 3],\n            [1, 4],\n            [2, 1],\n            [2, 2],\n            [2, 3],\n            [2, 4],\n            [3, 1],\n            [3, 2],\n            [3, 3],\n            [3, 4],\n        ]\n    )\n    X = _NotAnArray(_enforce_estimator_tags_X(estimator_orig, X))\n    y = _NotAnArray([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2])\n    weights = _NotAnArray([1] * 12)\n    if _safe_tags(estimator, key=\"multioutput_only\"):\n        y = _NotAnArray(y.data.reshape(-1, 1))\n    estimator.fit(X, y, sample_weight=weights)\n\n\n@ignore_warnings(category=(FutureWarning))\ndef check_sample_weights_list(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type list in the 'fit' function.\n    estimator = clone(estimator_orig)\n    rnd = np.random.RandomState(0)\n    n_samples = 30\n    X = _enforce_estimator_tags_X(estimator_orig, rnd.uniform(size=(n_samples, 3)))\n    y = np.arange(n_samples) % 3\n    y = _enforce_estimator_tags_y(estimator, y)\n    sample_weight = [3] * n_samples\n    # Test that estimators don't raise any exception\n    estimator.fit(X, y, sample_weight=sample_weight)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 38580,
                "end_index": 41430,
                "start_line": 181,
                "end_line": 1181,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "def check_sample_weights_not_overwritten(name, estimator_orig):\n    # check that estimators don't override the passed sample_weight parameter\n    estimator = clone(estimator_orig)\n    set_random_state(estimator, random_state=0)\n\n    X = np.array(\n        [\n            [1, 3],\n            [1, 3],\n            [1, 3],\n            [1, 3],\n            [2, 1],\n            [2, 1],\n            [2, 1],\n            [2, 1],\n            [3, 3],\n            [3, 3],\n            [3, 3],\n            [3, 3],\n            [4, 1],\n            [4, 1],\n            [4, 1],\n            [4, 1],\n        ],\n        dtype=np.float64,\n    )\n    y = np.array([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2], dtype=int)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    sample_weight_original = np.ones(y.shape[0])\n    sample_weight_original[0] = 10.0\n\n    sample_weight_fit = sample_weight_original.copy()\n\n    estimator.fit(X, y, sample_weight=sample_weight_fit)\n\n    err_msg = f\"{name} overwrote the original `sample_weight` given during fit\"\n    assert_allclose(sample_weight_fit, sample_weight_original, err_msg=err_msg)\n\n\n@ignore_warnings(category=(FutureWarning, UserWarning))\ndef check_dtype_object(name, estimator_orig):\n    # check that estimators treat dtype object as numeric if possible\n    rng = np.random.RandomState(0)\n    X = _enforce_estimator_tags_X(estimator_orig, rng.uniform(size=(40, 10)))\n    X = X.astype(object)\n    tags = _safe_tags(estimator_orig)\n    y = (X[:, 0] * 4).astype(int)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    estimator.fit(X, y)\n    if hasattr(estimator, \"predict\"):\n        estimator.predict(X)\n\n    if hasattr(estimator, \"transform\"):\n        estimator.transform(X)\n\n    with raises(Exception, match=\"Unknown label type\", may_pass=True):\n        estimator.fit(X, y.astype(object))\n\n    if \"string\" not in tags[\"X_types\"]:\n        X[0, 0] = {\"foo\": \"bar\"}\n        msg = \"argument must be a string.* number\"\n        with raises(TypeError, match=msg):\n            estimator.fit(X, y)\n    else:\n        # Estimators supporting string will not call np.asarray to convert the\n        # data to numeric and therefore, the error will not be raised.\n        # Checking for each element dtype in the input array will be costly.\n        # Refer to #11401 for full discussion.\n        estimator.fit(X, y)\n\n\ndef check_complex_data(name, estimator_orig):\n    rng = np.random.RandomState(42)\n    # check that estimators raise an exception on providing complex data\n    X = rng.uniform(size=10) + 1j * rng.uniform(size=10)\n    X = X.reshape(-1, 1)\n\n    # Something both valid for classification and regression\n    y = rng.randint(low=0, high=2, size=10) + 1j\n    estimator = clone(estimator_orig)\n    set_random_state(estimator, random_state=0)\n    with raises(ValueError, match=\"Complex data not supported\"):\n        estimator.fit(X, y)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 44629,
                "end_index": 47525,
                "start_line": 1290,
                "end_line": 4110,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_sample_weights_shape(name, estimator_orig):\n    # check that estimators raise an error if sample_weight\n    # shape mismatches the input\n    estimator = clone(estimator_orig)\n    X = np.array(\n        [\n            [1, 3],\n            [1, 3],\n            [1, 3],\n            [1, 3],\n            [2, 1],\n            [2, 1],\n            [2, 1],\n            [2, 1],\n            [3, 3],\n            [3, 3],\n            [3, 3],\n            [3, 3],\n            [4, 1],\n            [4, 1],\n            [4, 1],\n            [4, 1],\n        ]\n    )\n    y = np.array([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2])\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    estimator.fit(X, y, sample_weight=np.ones(len(y)))\n\n    with raises(ValueError):\n        estimator.fit(X, y, sample_weight=np.ones(2 * len(y)))\n\n    with raises(ValueError):\n        estimator.fit(X, y, sample_weight=np.ones((len(y), 2)))",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 41433,
                "end_index": 42380,
                "start_line": 181,
                "end_line": 1218,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_classifiers_one_label(name, classifier_orig):\n    error_string_fit = \"Classifier can't train when only one class is present.\"\n    error_string_predict = \"Classifier can't predict when only one class is present.\"\n    rnd = np.random.RandomState(0)\n    X_train = rnd.uniform(size=(10, 3))\n    X_test = rnd.uniform(size=(10, 3))\n    y = np.ones(10)\n    # catch deprecation warnings\n    with ignore_warnings(category=FutureWarning):\n        classifier = clone(classifier_orig)\n        with raises(\n            ValueError, match=\"class\", may_pass=True, err_msg=error_string_fit\n        ) as cm:\n            classifier.fit(X_train, y)\n\n        if cm.raised_and_matched:\n            # ValueError was raised with proper error message\n            return\n\n        assert_array_equal(classifier.predict(X_test), y, err_msg=error_string_predict)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_classifiers_one_label_sample_weights(name, classifier_orig):\n    \"\"\"Check that classifiers accepting sample_weight fit or throws a ValueError with\n    an explicit message if the problem is reduced to one class.\n    \"\"\"\n    error_fit = (\n        f\"{name} failed when fitted on one label after sample_weight trimming. Error \"\n        \"message is not explicit, it should have 'class'.\"\n    )\n    error_predict = f\"{name} prediction results should only output the remaining class.\"\n    rnd = np.random.RandomState(0)\n    # X should be square for test on SVC with precomputed kernel\n    X_train = rnd.uniform(size=(10, 10))\n    X_test = rnd.uniform(size=(10, 10))\n    y = np.arange(10) % 2\n    sample_weight = y.copy()  # select a single class\n    classifier = clone(classifier_orig)\n\n    if has_fit_parameter(classifier, \"sample_weight\"):\n        match = [r\"\\bclass(es)?\\b\", error_predict]\n        err_type, err_msg = (AssertionError, ValueError), error_fit\n    else:\n        match = r\"\\bsample_weight\\b\"\n        err_type, err_msg = (TypeError, ValueError), None\n\n    with raises(err_type, match=match, may_pass=True, err_msg=err_msg) as cm:\n        classifier.fit(X_train, y, sample_weight=sample_weight)\n        if cm.raised_and_matched:\n            # raise the proper error type with the proper error message\n            return\n        # for estimators that do not fail, they should be able to predict the only\n        # class remaining during fit\n        assert_array_equal(\n            classifier.predict(X_test), np.ones(10), err_msg=error_predict\n        )",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 79371,
                "end_index": 81868,
                "start_line": 181,
                "end_line": 4639,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_class_weight_classifiers(name, classifier_orig):\n    if _safe_tags(classifier_orig, key=\"binary_only\"):\n        problems = [2]\n    else:\n        problems = [2, 3]\n\n    for n_centers in problems:\n        # create a very noisy dataset\n        X, y = make_blobs(centers=n_centers, random_state=0, cluster_std=20)\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.5, random_state=0\n        )\n\n        # can't use gram_if_pairwise() here, setting up gram matrix manually\n        if _safe_tags(classifier_orig, key=\"pairwise\"):\n            X_test = rbf_kernel(X_test, X_train)\n            X_train = rbf_kernel(X_train, X_train)\n\n        n_centers = len(np.unique(y_train))\n\n        if n_centers == 2:\n            class_weight = {0: 1000, 1: 0.0001}\n        else:\n            class_weight = {0: 1000, 1: 0.0001, 2: 0.0001}\n\n        classifier = clone(classifier_orig).set_params(class_weight=class_weight)\n        if hasattr(classifier, \"n_iter\"):\n            classifier.set_params(n_iter=100)\n        if hasattr(classifier, \"max_iter\"):\n            classifier.set_params(max_iter=1000)\n        if hasattr(classifier, \"min_weight_fraction_leaf\"):\n            classifier.set_params(min_weight_fraction_leaf=0.01)\n        if hasattr(classifier, \"n_iter_no_change\"):\n            classifier.set_params(n_iter_no_change=20)\n\n        set_random_state(classifier)\n        classifier.fit(X_train, y_train)\n        y_pred = classifier.predict(X_test)\n        # XXX: Generally can use 0.89 here. On Windows, LinearSVC gets\n        #      0.88 (Issue #9111)\n        if not _safe_tags(classifier_orig, key=\"poor_score\"):\n            assert np.mean(y_pred == 0) > 0.87\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_class_weight_balanced_classifiers(\n    name, classifier_orig, X_train, y_train, X_test, y_test, weights\n):\n    classifier = clone(classifier_orig)\n    if hasattr(classifier, \"n_iter\"):\n        classifier.set_params(n_iter=100)\n    if hasattr(classifier, \"max_iter\"):\n        classifier.set_params(max_iter=1000)\n\n    set_random_state(classifier)\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n\n    classifier.set_params(class_weight=\"balanced\")\n    classifier.fit(X_train, y_train)\n    y_pred_balanced = classifier.predict(X_test)\n    assert f1_score(y_test, y_pred_balanced, average=\"weighted\") > f1_score(\n        y_test, y_pred, average=\"weighted\"\n    )",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 110227,
                "end_index": 112707,
                "start_line": 181,
                "end_line": 4639,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_sample_weights_invariance(name, estimator_orig, kind=\"ones\"):\n    # For kind=\"ones\" check that the estimators yield same results for\n    # unit weights and no weights\n    # For kind=\"zeros\" check that setting sample_weight to 0 is equivalent\n    # to removing corresponding samples.\n    estimator1 = clone(estimator_orig)\n    estimator2 = clone(estimator_orig)\n    set_random_state(estimator1, random_state=0)\n    set_random_state(estimator2, random_state=0)\n\n    X1 = np.array(\n        [\n            [1, 3],\n            [1, 3],\n            [1, 3],\n            [1, 3],\n            [2, 1],\n            [2, 1],\n            [2, 1],\n            [2, 1],\n            [3, 3],\n            [3, 3],\n            [3, 3],\n            [3, 3],\n            [4, 1],\n            [4, 1],\n            [4, 1],\n            [4, 1],\n        ],\n        dtype=np.float64,\n    )\n    y1 = np.array([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2], dtype=int)\n\n    if kind == \"ones\":\n        X2 = X1\n        y2 = y1\n        sw2 = np.ones(shape=len(y1))\n        err_msg = (\n            f\"For {name} sample_weight=None is not equivalent to sample_weight=ones\"\n        )\n    elif kind == \"zeros\":\n        # Construct a dataset that is very different to (X, y) if weights\n        # are disregarded, but identical to (X, y) given weights.\n        X2 = np.vstack([X1, X1 + 1])\n        y2 = np.hstack([y1, 3 - y1])\n        sw2 = np.ones(shape=len(y1) * 2)\n        sw2[len(y1) :] = 0\n        X2, y2, sw2 = shuffle(X2, y2, sw2, random_state=0)\n\n        err_msg = (\n            f\"For {name}, a zero sample_weight is not equivalent to removing the sample\"\n        )\n    else:  # pragma: no cover\n        raise ValueError\n\n    y1 = _enforce_estimator_tags_y(estimator1, y1)\n    y2 = _enforce_estimator_tags_y(estimator2, y2)\n\n    estimator1.fit(X1, y=y1, sample_weight=None)\n    estimator2.fit(X2, y=y2, sample_weight=sw2)\n\n    for method in [\"predict\", \"predict_proba\", \"decision_function\", \"transform\"]:\n        if hasattr(estimator_orig, method):\n            X_pred1 = getattr(estimator1, method)(X1)\n            X_pred2 = getattr(estimator2, method)(X1)\n            assert_allclose_dense_sparse(X_pred1, X_pred2, err_msg=err_msg)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 42383,
                "end_index": 44626,
                "start_line": 181,
                "end_line": 1287,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "def _check_sample_weight(\n    sample_weight, X, dtype=None, copy=False, only_non_negative=False\n):\n    \"\"\"Validate sample weights.\n\n    Note that passing sample_weight=None will output an array of ones.\n    Therefore, in some cases, you may want to protect the call with:\n    if sample_weight is not None:\n        sample_weight = _check_sample_weight(...)\n\n    Parameters\n    ----------\n    sample_weight : {ndarray, Number or None}, shape (n_samples,)\n        Input sample weights.\n\n    X : {ndarray, list, sparse matrix}\n        Input data.\n\n    only_non_negative : bool, default=False,\n        Whether or not the weights are expected to be non-negative.\n\n        .. versionadded:: 1.0\n\n    dtype : dtype, default=None\n        dtype of the validated `sample_weight`.\n        If None, and the input `sample_weight` is an array, the dtype of the\n        input is preserved; otherwise an array with the default numpy dtype\n        is be allocated.  If `dtype` is not one of `float32`, `float64`,\n        `None`, the output will be of dtype `float64`.\n\n    copy : bool, default=False\n        If True, a copy of sample_weight will be created.\n\n    Returns\n    -------\n    sample_weight : ndarray of shape (n_samples,)\n        Validated sample weight. It is guaranteed to be \"C\" contiguous.\n    \"\"\"\n    n_samples = _num_samples(X)\n\n    if dtype is not None and dtype not in [np.float32, np.float64]:\n        dtype = np.float64\n\n    if sample_weight is None:\n        sample_weight = np.ones(n_samples, dtype=dtype)\n    elif isinstance(sample_weight, numbers.Number):\n        sample_weight = np.full(n_samples, sample_weight, dtype=dtype)\n    else:\n        if dtype is None:\n            dtype = [np.float64, np.float32]\n        sample_weight = check_array(\n            sample_weight,\n            accept_sparse=False,\n            ensure_2d=False,\n            dtype=dtype,\n            order=\"C\",\n            copy=copy,\n            input_name=\"sample_weight\",\n        )\n        if sample_weight.ndim != 1:\n            raise ValueError(\"Sample weights must be 1D array or scalar\")\n\n        if sample_weight.shape != (n_samples,):\n            raise ValueError(\n                \"sample_weight.shape == {}, expected {}!\".format(\n                    sample_weight.shape, (n_samples,)\n                )\n            )\n\n    if only_non_negative:\n        check_non_negative(sample_weight, \"`sample_weight`\")\n\n    return sample_weight",
                "filename": "sklearn/utils/validation.py",
                "start_index": 63891,
                "end_index": 66306,
                "start_line": 1789,
                "end_line": 1861,
                "max_line": 2282,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_methods_sample_order_invariance(name, estimator_orig):\n    # check that method gives invariant results if applied\n    # on a subset with different sample order\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20, 3))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    y = X[:, 0].astype(np.int64)\n    if _safe_tags(estimator_orig, key=\"binary_only\"):\n        y[y == 2] = 1\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 2\n\n    set_random_state(estimator, 1)\n    estimator.fit(X, y)\n\n    idx = np.random.permutation(X.shape[0])\n\n    for method in [\n        \"predict\",\n        \"transform\",\n        \"decision_function\",\n        \"score_samples\",\n        \"predict_proba\",\n    ]:\n        msg = (\n            \"{method} of {name} is not invariant when applied to a dataset\"\n            \"with different sample order.\"\n        ).format(method=method, name=name)\n\n        if hasattr(estimator, method):\n            assert_allclose_dense_sparse(\n                getattr(estimator, method)(X)[idx],\n                getattr(estimator, method)(X[idx]),\n                atol=1e-9,\n                err_msg=msg,\n            )\n\n\n@ignore_warnings\ndef check_fit2d_1sample(name, estimator_orig):\n    # Check that fitting a 2d array with only one sample either works or\n    # returns an informative message. The error message should either mention\n    # the number of samples or the number of classes.\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(1, 10))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n\n    y = X[:, 0].astype(int)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n\n    # min_cluster_size cannot be less than the data size for OPTICS.\n    if name == \"OPTICS\":\n        estimator.set_params(min_samples=1.0)\n\n    # perplexity cannot be more than the number of samples for TSNE.\n    if name == \"TSNE\":\n        estimator.set_params(perplexity=0.5)\n\n    msgs = [\n        \"1 sample\",\n        \"n_samples = 1\",\n        \"n_samples=1\",\n        \"one sample\",\n        \"1 class\",\n        \"one class\",\n    ]\n\n    with raises(ValueError, match=msgs, may_pass=True):\n        estimator.fit(X, y)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 53411,
                "end_index": 55996,
                "start_line": 181,
                "end_line": 4110,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_class_weight_balanced_linear_classifier(name, Classifier):\n    \"\"\"Test class weights with non-contiguous class labels.\"\"\"\n    # this is run on classes, not instances, though this should be changed\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-0.8, -1.0], [1.0, 1.0], [1.0, 0.0]])\n    y = np.array([1, 1, 1, -1, -1])\n\n    classifier = Classifier()\n\n    if hasattr(classifier, \"n_iter\"):\n        # This is a very small dataset, default n_iter are likely to prevent\n        # convergence\n        classifier.set_params(n_iter=1000)\n    if hasattr(classifier, \"max_iter\"):\n        classifier.set_params(max_iter=1000)\n    if hasattr(classifier, \"cv\"):\n        classifier.set_params(cv=3)\n    set_random_state(classifier)\n\n    # Let the model compute the class frequencies\n    classifier.set_params(class_weight=\"balanced\")\n    coef_balanced = classifier.fit(X, y).coef_.copy()\n\n    # Count each label occurrence to reweight manually\n    n_samples = len(y)\n    n_classes = float(len(np.unique(y)))\n\n    class_weight = {\n        1: n_samples / (np.sum(y == 1) * n_classes),\n        -1: n_samples / (np.sum(y == -1) * n_classes),\n    }\n    classifier.set_params(class_weight=class_weight)\n    coef_manual = classifier.fit(X, y).coef_.copy()\n\n    assert_allclose(\n        coef_balanced,\n        coef_manual,\n        err_msg=\"Classifier %s is not computing class_weight=balanced properly.\" % name,\n    )\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_estimators_overwrite_params(name, estimator_orig):\n    X, y = make_blobs(random_state=0, n_samples=21)\n    X = _enforce_estimator_tags_X(estimator_orig, X, kernel=rbf_kernel)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    set_random_state(estimator)\n\n    # Make a physical copy of the original estimator parameters before fitting.\n    params = estimator.get_params()\n    original_params = deepcopy(params)\n\n    # Fit the model\n    estimator.fit(X, y)\n\n    # Compare the state of the model parameters with the original parameters\n    new_params = estimator.get_params()\n    for param_name, original_value in original_params.items():\n        new_value = new_params[param_name]\n\n        # We should never change or mutate the internal state of input\n        # parameters by default. To check this we use the joblib.hash function\n        # that introspects recursively any subobjects to compute a checksum.\n        # The only exception to this rule of immutable constructor parameters\n        # is possible RandomState instance but in this check we explicitly\n        # fixed the random_state params recursively to be integer seeds.\n        assert joblib.hash(new_value) == joblib.hash(original_value), (\n            \"Estimator %s should not change or mutate \"\n            \" the parameter %s from %s to %s during fit.\"\n            % (name, param_name, original_value, new_value)\n        )",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 112710,
                "end_index": 115635,
                "start_line": 181,
                "end_line": 4639,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "gbdt_no_cst = HistGradientBoostingRegressor().fit(X, y)\ngbdt_cst = HistGradientBoostingRegressor(monotonic_cst=[1, 0]).fit(X, y)\n\n# plot_partial_dependence has been removed in version 1.2. From 1.2, use\n# PartialDependenceDisplay instead.\n# disp = plot_partial_dependence(\ndisp = PartialDependenceDisplay.from_estimator(\n    gbdt_no_cst,\n    X,\n    features=[0],\n    feature_names=[\"feature 0\"],\n    line_kw={\"linewidth\": 4, \"label\": \"unconstrained\", \"color\": \"tab:blue\"},\n)\n# plot_partial_dependence(\nPartialDependenceDisplay.from_estimator(\n    gbdt_cst,\n    X,\n    features=[0],\n    line_kw={\"linewidth\": 4, \"label\": \"constrained\", \"color\": \"tab:orange\"},\n    ax=disp.axes_,\n)\ndisp.axes_[0, 0].plot(\n    X[:, 0], y, \"o\", alpha=0.5, zorder=-1, label=\"samples\", color=\"tab:green\"\n)\ndisp.axes_[0, 0].set_ylim(-3, 3)\ndisp.axes_[0, 0].set_xlim(-1, 1)\nplt.legend()\nplt.show()\n\n##############################################################################\n# Sample-weight support for Lasso and ElasticNet\n# ----------------------------------------------\n# The two linear regressors :class:`~sklearn.linear_model.Lasso` and\n# :class:`~sklearn.linear_model.ElasticNet` now support sample weights.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import Lasso\nimport numpy as np\n\nn_samples, n_features = 1000, 20\nrng = np.random.RandomState(0)\nX, y = make_regression(n_samples, n_features, random_state=rng)\nsample_weight = rng.rand(n_samples)\nX_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(\n    X, y, sample_weight, random_state=rng\n)\nreg = Lasso()\nreg.fit(X_train, y_train, sample_weight=sw_train)\nprint(reg.score(X_test, y_test, sw_test))",
                "filename": "examples/release_highlights/plot_release_highlights_0_23_0.py",
                "start_index": 5973,
                "end_index": 7708,
                "start_line": 140,
                "end_line": 189,
                "max_line": 189,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            }
        ],
        "classification": "other",
        "snippet_processor_task": {
            "sklearn/utils/validation.py": [
                {
                    "chunk": {
                        "code": "def _check_sample_weight(\n    sample_weight, X, dtype=None, copy=False, only_non_negative=False\n):\n    \"\"\"Validate sample weights.\n\n    Note that passing sample_weight=None will output an array of ones.\n    Therefore, in some cases, you may want to protect the call with:\n    if sample_weight is not None:\n        sample_weight = _check_sample_weight(...)\n\n    Parameters\n    ----------\n    sample_weight : {ndarray, Number or None}, shape (n_samples,)\n        Input sample weights.\n\n    X : {ndarray, list, sparse matrix}\n        Input data.\n\n    only_non_negative : bool, default=False,\n        Whether or not the weights are expected to be non-negative.\n\n        .. versionadded:: 1.0\n\n    dtype : dtype, default=None\n        dtype of the validated `sample_weight`.\n        If None, and the input `sample_weight` is an array, the dtype of the\n        input is preserved; otherwise an array with the default numpy dtype\n        is be allocated.  If `dtype` is not one of `float32`, `float64`,\n        `None`, the output will be of dtype `float64`.\n\n    copy : bool, default=False\n        If True, a copy of sample_weight will be created.\n\n    Returns\n    -------\n    sample_weight : ndarray of shape (n_samples,)\n        Validated sample weight. It is guaranteed to be \"C\" contiguous.\n    \"\"\"\n    n_samples = _num_samples(X)\n\n    if dtype is not None and dtype not in [np.float32, np.float64]:\n        dtype = np.float64\n\n    if sample_weight is None:\n        sample_weight = np.ones(n_samples, dtype=dtype)\n    elif isinstance(sample_weight, numbers.Number):\n        sample_weight = np.full(n_samples, sample_weight, dtype=dtype)\n    else:\n        if dtype is None:\n            dtype = [np.float64, np.float32]\n        sample_weight = check_array(\n            sample_weight,\n            accept_sparse=False,\n            ensure_2d=False,\n            dtype=dtype,\n            order=\"C\",\n            copy=copy,\n            input_name=\"sample_weight\",\n        )\n        if sample_weight.ndim != 1:\n            raise ValueError(\"Sample weights must be 1D array or scalar\")\n\n        if sample_weight.shape != (n_samples,):\n            raise ValueError(\n                \"sample_weight.shape == {}, expected {}!\".format(\n                    sample_weight.shape, (n_samples,)\n                )\n            )\n\n    if only_non_negative:\n        check_non_negative(sample_weight, \"`sample_weight`\")\n\n    return sample_weight",
                        "filename": "sklearn/utils/validation.py",
                        "start_index": 63891,
                        "end_index": 66306,
                        "start_line": 1789,
                        "end_line": 1861,
                        "max_line": 2282,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the implementation of '_check_sample_weight' which is the method that needs to be used consistently across the codebase for validating 'sample_weight'."
                }
            ],
            "sklearn/utils/estimator_checks.py": [
                {
                    "chunk": {
                        "code": "@ignore_warnings(category=FutureWarning)\ndef check_sample_weights_pandas_series(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type pandas.Series in the 'fit' function.\n    estimator = clone(estimator_orig)\n    try:\n        import pandas as pd\n\n        X = np.array(\n            [\n                [1, 1],\n                [1, 2],\n                [1, 3],\n                [1, 4],\n                [2, 1],\n                [2, 2],\n                [2, 3],\n                [2, 4],\n                [3, 1],\n                [3, 2],\n                [3, 3],\n                [3, 4],\n            ]\n        )\n        X = pd.DataFrame(_enforce_estimator_tags_X(estimator_orig, X), copy=False)\n        y = pd.Series([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2])\n        weights = pd.Series([1] * 12)\n        if _safe_tags(estimator, key=\"multioutput_only\"):\n            y = pd.DataFrame(y, copy=False)\n        try:\n            estimator.fit(X, y, sample_weight=weights)\n        except ValueError:\n            raise ValueError(\n                \"Estimator {0} raises error if \"\n                \"'sample_weight' parameter is of \"\n                \"type pandas.Series\".format(name)\n            )\n    except ImportError:\n        raise SkipTest(\n            \"pandas is not installed: not testing for \"\n            \"input of type pandas.Series to class weight.\"\n        )\n\n\n@ignore_warnings(category=(FutureWarning))\ndef check_sample_weights_not_an_array(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type _NotAnArray in the 'fit' function.\n    estimator = clone(estimator_orig)\n    X = np.array(\n        [\n            [1, 1],\n            [1, 2],\n            [1, 3],\n            [1, 4],\n            [2, 1],\n            [2, 2],\n            [2, 3],\n            [2, 4],\n            [3, 1],\n            [3, 2],\n            [3, 3],\n            [3, 4],\n        ]\n    )\n    X = _NotAnArray(_enforce_estimator_tags_X(estimator_orig, X))\n    y = _NotAnArray([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2])\n    weights = _NotAnArray([1] * 12)\n    if _safe_tags(estimator, key=\"multioutput_only\"):\n        y = _NotAnArray(y.data.reshape(-1, 1))\n    estimator.fit(X, y, sample_weight=weights)\n\n\n@ignore_warnings(category=(FutureWarning))\ndef check_sample_weights_list(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type list in the 'fit' function.\n    estimator = clone(estimator_orig)\n    rnd = np.random.RandomState(0)\n    n_samples = 30\n    X = _enforce_estimator_tags_X(estimator_orig, rnd.uniform(size=(n_samples, 3)))\n    y = np.arange(n_samples) % 3\n    y = _enforce_estimator_tags_y(estimator, y)\n    sample_weight = [3] * n_samples\n    # Test that estimators don't raise any exception\n    estimator.fit(X, y, sample_weight=sample_weight)",
                        "filename": "sklearn/utils/estimator_checks.py",
                        "start_index": 38580,
                        "end_index": 41430,
                        "start_line": 181,
                        "end_line": 1181,
                        "max_line": 4639,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes tests that check if estimators accept 'sample_weight' of various types. It may need to be updated to use '_check_sample_weight' for consistency."
                },
                {
                    "chunk": {
                        "code": "@ignore_warnings(category=FutureWarning)\ndef check_sample_weights_shape(name, estimator_orig):\n    # check that estimators raise an error if sample_weight\n    # shape mismatches the input\n    estimator = clone(estimator_orig)\n    X = np.array(\n        [\n            [1, 3],\n            [1, 3],\n            [1, 3],\n            [1, 3],\n            [2, 1],\n            [2, 1],\n            [2, 1],\n            [2, 1],\n            [3, 3],\n            [3, 3],\n            [3, 3],\n            [3, 3],\n            [4, 1],\n            [4, 1],\n            [4, 1],\n            [4, 1],\n        ]\n    )\n    y = np.array([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2])\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    estimator.fit(X, y, sample_weight=np.ones(len(y)))\n\n    with raises(ValueError):\n        estimator.fit(X, y, sample_weight=np.ones(2 * len(y)))\n\n    with raises(ValueError):\n        estimator.fit(X, y, sample_weight=np.ones((len(y), 2)))",
                        "filename": "sklearn/utils/estimator_checks.py",
                        "start_index": 41433,
                        "end_index": 42380,
                        "start_line": 181,
                        "end_line": 1218,
                        "max_line": 4639,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet tests the behavior of estimators when 'sample_weight' has a mismatched shape. It's relevant because it may need to be updated to use '_check_sample_weight'."
                },
                {
                    "chunk": {
                        "code": "@ignore_warnings(category=FutureWarning)\ndef check_sample_weights_invariance(name, estimator_orig, kind=\"ones\"):\n    # For kind=\"ones\" check that the estimators yield same results for\n    # unit weights and no weights\n    # For kind=\"zeros\" check that setting sample_weight to 0 is equivalent\n    # to removing corresponding samples.\n    estimator1 = clone(estimator_orig)\n    estimator2 = clone(estimator_orig)\n    set_random_state(estimator1, random_state=0)\n    set_random_state(estimator2, random_state=0)\n\n    X1 = np.array(\n        [\n            [1, 3],\n            [1, 3],\n            [1, 3],\n            [1, 3],\n            [2, 1],\n            [2, 1],\n            [2, 1],\n            [2, 1],\n            [3, 3],\n            [3, 3],\n            [3, 3],\n            [3, 3],\n            [4, 1],\n            [4, 1],\n            [4, 1],\n            [4, 1],\n        ],\n        dtype=np.float64,\n    )\n    y1 = np.array([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2], dtype=int)\n\n    if kind == \"ones\":\n        X2 = X1\n        y2 = y1\n        sw2 = np.ones(shape=len(y1))\n        err_msg = (\n            f\"For {name} sample_weight=None is not equivalent to sample_weight=ones\"\n        )\n    elif kind == \"zeros\":\n        # Construct a dataset that is very different to (X, y) if weights\n        # are disregarded, but identical to (X, y) given weights.\n        X2 = np.vstack([X1, X1 + 1])\n        y2 = np.hstack([y1, 3 - y1])\n        sw2 = np.ones(shape=len(y1) * 2)\n        sw2[len(y1) :] = 0\n        X2, y2, sw2 = shuffle(X2, y2, sw2, random_state=0)\n\n        err_msg = (\n            f\"For {name}, a zero sample_weight is not equivalent to removing the sample\"\n        )\n    else:  # pragma: no cover\n        raise ValueError\n\n    y1 = _enforce_estimator_tags_y(estimator1, y1)\n    y2 = _enforce_estimator_tags_y(estimator2, y2)\n\n    estimator1.fit(X1, y=y1, sample_weight=None)\n    estimator2.fit(X2, y=y2, sample_weight=sw2)\n\n    for method in [\"predict\", \"predict_proba\", \"decision_function\", \"transform\"]:\n        if hasattr(estimator_orig, method):\n            X_pred1 = getattr(estimator1, method)(X1)\n            X_pred2 = getattr(estimator2, method)(X1)\n            assert_allclose_dense_sparse(X_pred1, X_pred2, err_msg=err_msg)",
                        "filename": "sklearn/utils/estimator_checks.py",
                        "start_index": 42383,
                        "end_index": 44626,
                        "start_line": 181,
                        "end_line": 1287,
                        "max_line": 4639,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet tests the invariance of estimators to 'sample_weight'. It's relevant because it may need to be updated to use '_check_sample_weight'."
                }
            ]
        },
        "solution_task": "- Locate the `fit` or `partial_fit` methods in the classes `CalibratedClassifierCV`, `DBSCAN`, `DummyClassifier`, `DummyRegressor`, `BaseBagging`, `BaseForest`, `BaseGradientBoosting`, `IsotonicRegression`, `KernelRidge`, `GaussianNB`, `BaseDiscreteNB`, `KernelDensity`, and `BaseDecisionTree`.\n- Import `_check_sample_weight` from `sklearn.utils.validation` in each of these classes if not already imported.\n- Replace any ad-hoc checks for `sample_weight` with a call to `_check_sample_weight(sample_weight, X)` in the `fit` or `partial_fit` methods.\n- Ensure that the `sample_weight` parameter is passed to `_check_sample_weight` before any further processing in the `fit` or `partial_fit` methods.\n- Remove any now redundant imports of `check_consistent_length` and `check_array` if they were only used for validating `sample_weight`.\n- Do not make changes to modules within the `linear_model` module, as it is excluded from this task.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE provides a clear and actionable step-by-step guide to replace ad-hoc checks for `sample_weight` with the new `_check_sample_weight` function in the specified classes. It correctly identifies the need to locate the `fit` or `partial_fit` methods, import the new function, replace the old checks, and remove redundant imports. The RESPONSE also correctly advises not to make changes to the `linear_model` module, which is consistent with the PROMPT's instructions. Additionally, the RESPONSE includes relevant code snippets from the repository that may need to be reviewed or updated, which is useful for developers working on this task. The RESPONSE does not address all aspects of the described bug, such as specific implementation details or potential edge cases that may arise during the refactoring process, which is why it does not receive a perfect score.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711738939.847451,
        "result": "- Locate the `fit` or `partial_fit` methods in the classes `CalibratedClassifierCV`, `DBSCAN`, `DummyClassifier`, `DummyRegressor`, `BaseBagging`, `BaseForest`, `BaseGradientBoosting`, `IsotonicRegression`, `KernelRidge`, `GaussianNB`, `BaseDiscreteNB`, `KernelDensity`, and `BaseDecisionTree`.\n- Import `_check_sample_weight` from `sklearn.utils.validation` in each of these classes if not already imported.\n- Replace any ad-hoc checks for `sample_weight` with a call to `_check_sample_weight(sample_weight, X)` in the `fit` or `partial_fit` methods.\n- Ensure that the `sample_weight` parameter is passed to `_check_sample_weight` before any further processing in the `fit` or `partial_fit` methods.\n- Remove any now redundant imports of `check_consistent_length` and `check_array` if they were only used for validating `sample_weight`.\n- Do not make changes to modules within the `linear_model` module, as it is excluded from this task.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/utils)- [Lines 1789 - 1861](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/validation.py#L1789-L1861)\n\n This snippet contains the implementation of '_check_sample_weight' which is the method that needs to be used consistently across the codebase for validating 'sample_weight'.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/utils)- [Lines 181 - 1181](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/estimator_checks.py#L181-L1181)\n\n This snippet includes tests that check if estimators accept 'sample_weight' of various types. It may need to be updated to use '_check_sample_weight' for consistency. \n\n\n   - [Lines 181 - 1218](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/estimator_checks.py#L181-L1218)\n\n This snippet tests the behavior of estimators when 'sample_weight' has a mismatched shape. It's relevant because it may need to be updated to use '_check_sample_weight'. \n\n\n   - [Lines 181 - 1287](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/estimator_checks.py#L181-L1287)\n\n This snippet tests the invariance of estimators to 'sample_weight'. It's relevant because it may need to be updated to use '_check_sample_weight'."
    }
}