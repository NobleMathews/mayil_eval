{
    "id": "scikit-learn__scikit-learn-26242",
    "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
    "task_id": 5462362706792268199,
    "title": "AdaBoost: deprecation of \"base_estimator\" does not handle \"base_estimator=None\" setting properly",
    "body": "### Describe the bug\r\n\r\nScikit-learn 1.2 deprecated `AdaBoostClassifier` 's `base_estimator` in favour of `estimator` (see #23819). Because there are also validators in place, old code that explicitly defined `base_estimator=None` stopped working.\r\n\r\nA solution that fixes the deprecation is to add a possible `None` to a list allowed values in `_parameter_constraints`; I will do that in a PR.\r\n\r\n### Steps/Code to Reproduce\r\n\r\n```\r\nfrom sklearn.ensemble import AdaBoostClassifier\r\nclf = AdaBoostClassifier(base_estimator=None)\r\nclf.fit([[1]], [0])\r\n```\r\n\r\n### Expected Results\r\n\r\nNo error is thrown.\r\n\r\n### Actual Results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/Users/marko/opt/miniconda3/envs/orange310/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py\", line 124, in fit\r\n    self._validate_params()\r\n  File \"/Users/marko/opt/miniconda3/envs/orange310/lib/python3.10/site-packages/sklearn/base.py\", line 600, in _validate_params\r\n    validate_parameter_constraints(\r\n  File \"/Users/marko/opt/miniconda3/envs/orange310/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\r\n    raise InvalidParameterError(\r\nsklearn.utils._param_validation.InvalidParameterError: The 'base_estimator' parameter of AdaBoostClassifier must be an object implementing 'fit' and 'predict' or a str among {'deprecated'}. Got None instead.\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nsklearn: 1.2.2; others are not important\r\n```\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__1.3.git",
    "requested_commit": "",
    "mayil_response": "- Update the `_parameter_constraints` dictionary in the `BaseWeightBoosting` class to allow `None` as a valid value for `base_estimator`.\n- Ensure that the `AdaBoostClassifier` class constructor and `_validate_estimator` method handle the `base_estimator=None` case correctly, without raising an `InvalidParameterError`.\n- Deprecate the `base_estimator` parameter in favor of the `estimator` parameter in the `AdaBoostClassifier` class.\n- Modify the `AdaBoostClassifier` class to set `estimator_` attribute to a default `DecisionTreeClassifier(max_depth=1)` when `base_estimator` is `None`.\n- Remove any checks that raise `InvalidParameterError` when `base_estimator` is `None` in the `AdaBoostClassifier` class.\n- Update the `fit` method of the `AdaBoostClassifier` to handle `base_estimator=None` by setting the `estimator_` attribute to the default estimator if not already set.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn) \u2192 [ensemble](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn/ensemble)- [Lines 60 - 1199](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/ensemble/_weight_boosting.py#L60-L1199)\n\n This snippet contains the '_parameter_constraints' dictionary where the 'base_estimator' parameter is defined. It needs to be updated to allow 'None' as a valid value. \n\n\n   - [Lines 66 - 1199](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/ensemble/_weight_boosting.py#L66-L1199)\n\n This snippet shows the '__init__' method of 'AdaBoostClassifier' where 'base_estimator' is set to 'deprecated'. It may need to be reviewed to ensure that the deprecation is handled correctly when 'None' is passed. \n\n\n   - [Lines 342 - 1261](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/ensemble/_weight_boosting.py#L342-L1261)\n\n This snippet provides the documentation for 'AdaBoostClassifier', which includes information about the 'base_estimator' parameter. It may need to be updated to reflect the changes made to handle 'None' as a valid value.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0003495,
        "snippet_processor": 0.04503,
        "issue_star_creation": 0.0381,
        "issue_star_solver": 0.06981000000000001,
        "bouncer": 0.02748
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711737893.002263,
        "relevant_snippets": [
            {
                "code": "_parameter_constraints: dict = {\n        **BaseWeightBoosting._parameter_constraints,\n        \"algorithm\": [StrOptions({\"SAMME\", \"SAMME.R\"})],\n    }\n\n    def __init__(\n        self,\n        estimator=None,\n        *,\n        n_estimators=50,\n        learning_rate=1.0,\n        algorithm=\"SAMME.R\",\n        random_state=None,\n        base_estimator=\"deprecated\",\n    ):\n        super().__init__(\n            estimator=estimator,\n            n_estimators=n_estimators,\n            learning_rate=learning_rate,\n            random_state=random_state,\n            base_estimator=base_estimator,\n        )\n\n        self.algorithm = algorithm\n\n    def _validate_estimator(self):\n        \"\"\"Check the estimator and set the estimator_ attribute.\"\"\"\n        super()._validate_estimator(default=DecisionTreeClassifier(max_depth=1))\n\n        #  SAMME-R requires predict_proba-enabled base estimators\n        if self.algorithm == \"SAMME.R\":\n            if not hasattr(self.estimator_, \"predict_proba\"):\n                raise TypeError(\n                    \"AdaBoostClassifier with algorithm='SAMME.R' requires \"\n                    \"that the weak learner supports the calculation of class \"\n                    \"probabilities with a predict_proba method.\\n\"\n                    \"Please change the base estimator or set \"\n                    \"algorithm='SAMME' instead.\"\n                )\n        if not has_fit_parameter(self.estimator_, \"sample_weight\"):\n            raise ValueError(\n                f\"{self.estimator.__class__.__name__} doesn't support sample_weight.\"\n            )",
                "filename": "sklearn/ensemble/_weight_boosting.py",
                "start_index": 17106,
                "end_index": 18678,
                "start_line": 66,
                "end_line": 1199,
                "max_line": 1266,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "\"\"\"Base class for AdaBoost estimators.\n\n    Warning: This class should not be used directly. Use derived classes\n    instead.\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"estimator\": [HasMethods([\"fit\", \"predict\"]), None],\n        \"n_estimators\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"learning_rate\": [Interval(Real, 0, None, closed=\"neither\")],\n        \"random_state\": [\"random_state\"],\n        \"base_estimator\": [\n            HasMethods([\"fit\", \"predict\"]),\n            StrOptions({\"deprecated\"}),\n            None,\n        ],\n    }\n\n    @abstractmethod\n    def __init__(\n        self,\n        estimator=None,\n        *,\n        n_estimators=50,\n        estimator_params=tuple(),\n        learning_rate=1.0,\n        random_state=None,\n        base_estimator=\"deprecated\",\n    ):\n        super().__init__(\n            estimator=estimator,\n            n_estimators=n_estimators,\n            estimator_params=estimator_params,\n            base_estimator=base_estimator,\n        )\n\n        self.learning_rate = learning_rate\n        self.random_state = random_state\n\n    def _check_X(self, X):\n        # Only called to validate X in non-fit methods, therefore reset=False\n        return self._validate_data(\n            X,\n            accept_sparse=[\"csr\", \"csc\"],\n            ensure_2d=True,\n            allow_nd=True,\n            dtype=None,\n            reset=False,\n        )",
                "filename": "sklearn/ensemble/_weight_boosting.py",
                "start_index": 1654,
                "end_index": 3056,
                "start_line": 60,
                "end_line": 1199,
                "max_line": 1266,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "@_fit_context(\n        # AdaBoost*.estimator is not validated yet\n        prefer_skip_nested_validation=False\n    )",
                "filename": "sklearn/ensemble/_weight_boosting.py",
                "start_index": 3062,
                "end_index": 3177,
                "start_line": 110,
                "end_line": 1199,
                "max_line": 1266,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_classifiers_one_label(name, classifier_orig):\n    error_string_fit = \"Classifier can't train when only one class is present.\"\n    error_string_predict = \"Classifier can't predict when only one class is present.\"\n    rnd = np.random.RandomState(0)\n    X_train = rnd.uniform(size=(10, 3))\n    X_test = rnd.uniform(size=(10, 3))\n    y = np.ones(10)\n    # catch deprecation warnings\n    with ignore_warnings(category=FutureWarning):\n        classifier = clone(classifier_orig)\n        with raises(\n            ValueError, match=\"class\", may_pass=True, err_msg=error_string_fit\n        ) as cm:\n            classifier.fit(X_train, y)\n\n        if cm.raised_and_matched:\n            # ValueError was raised with proper error message\n            return\n\n        assert_array_equal(classifier.predict(X_test), y, err_msg=error_string_predict)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_classifiers_one_label_sample_weights(name, classifier_orig):\n    \"\"\"Check that classifiers accepting sample_weight fit or throws a ValueError with\n    an explicit message if the problem is reduced to one class.\n    \"\"\"\n    error_fit = (\n        f\"{name} failed when fitted on one label after sample_weight trimming. Error \"\n        \"message is not explicit, it should have 'class'.\"\n    )\n    error_predict = f\"{name} prediction results should only output the remaining class.\"\n    rnd = np.random.RandomState(0)\n    # X should be square for test on SVC with precomputed kernel\n    X_train = rnd.uniform(size=(10, 10))\n    X_test = rnd.uniform(size=(10, 10))\n    y = np.arange(10) % 2\n    sample_weight = y.copy()  # select a single class\n    classifier = clone(classifier_orig)\n\n    if has_fit_parameter(classifier, \"sample_weight\"):\n        match = [r\"\\bclass(es)?\\b\", error_predict]\n        err_type, err_msg = (AssertionError, ValueError), error_fit\n    else:\n        match = r\"\\bsample_weight\\b\"\n        err_type, err_msg = (TypeError, ValueError), None\n\n    with raises(err_type, match=match, may_pass=True, err_msg=err_msg) as cm:\n        classifier.fit(X_train, y, sample_weight=sample_weight)\n        if cm.raised_and_matched:\n            # raise the proper error type with the proper error message\n            return\n        # for estimators that do not fail, they should be able to predict the only\n        # class remaining during fit\n        assert_array_equal(\n            classifier.predict(X_test), np.ones(10), err_msg=error_predict\n        )",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 79371,
                "end_index": 81868,
                "start_line": 181,
                "end_line": 4639,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "\"\"\"Weight Boosting.\n\nThis module contains weight boosting estimators for both classification and\nregression.\n\nThe module structure is the following:\n\n- The `BaseWeightBoosting` base class implements a common ``fit`` method\n  for all the estimators in the module. Regression and classification\n  only differ from each other in the loss function that is optimized.\n\n- :class:`~sklearn.ensemble.AdaBoostClassifier` implements adaptive boosting\n  (AdaBoost-SAMME) for classification problems.\n\n- :class:`~sklearn.ensemble.AdaBoostRegressor` implements adaptive boosting\n  (AdaBoost.R2) for regression problems.\n\"\"\"\n\n# Authors: Noel Dawe <noel@dawe.me>\n#          Gilles Louppe <g.louppe@gmail.com>\n#          Hamzeh Alsalhi <ha258@cornell.edu>\n#          Arnaud Joly <arnaud.v.joly@gmail.com>\n#\n# License: BSD 3 clause\n\nimport warnings\nfrom abc import ABCMeta, abstractmethod\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy.special import xlogy\n\nfrom ..base import (\n    ClassifierMixin,\n    RegressorMixin,\n    _fit_context,\n    is_classifier,\n    is_regressor,\n)\nfrom ..metrics import accuracy_score, r2_score\nfrom ..tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom ..utils import _safe_indexing, check_random_state\nfrom ..utils._param_validation import HasMethods, Interval, StrOptions\nfrom ..utils.extmath import softmax, stable_cumsum\nfrom ..utils.validation import (\n    _check_sample_weight,\n    _num_samples,\n    check_is_fitted,\n    has_fit_parameter,\n)\nfrom ._base import BaseEnsemble\n\n__all__ = [\n    \"AdaBoostClassifier\",\n    \"AdaBoostRegressor\",\n]",
                "filename": "sklearn/ensemble/_weight_boosting.py",
                "start_index": 0,
                "end_index": 1588,
                "start_line": 1,
                "end_line": 1218,
                "max_line": 1266,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "\"\"\"An AdaBoost classifier.\n\n    An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n    classifier on the original dataset and then fits additional copies of the\n    classifier on the same dataset but where the weights of incorrectly\n    classified instances are adjusted such that subsequent classifiers focus\n    more on difficult cases.\n\n    This class implements the algorithm known as AdaBoost-SAMME [2].\n\n    Read more in the :ref:`User Guide <adaboost>`.\n\n    .. versionadded:: 0.14\n\n    Parameters\n    ----------\n    estimator : object, default=None\n        The base estimator from which the boosted ensemble is built.\n        Support for sample weighting is required, as well as proper\n        ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n        the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n        initialized with `max_depth=1`.\n\n        .. versionadded:: 1.2\n           `base_estimator` was renamed to `estimator`.\n\n    n_estimators : int, default=50\n        The maximum number of estimators at which boosting is terminated.\n        In case of perfect fit, the learning procedure is stopped early.\n        Values must be in the range `[1, inf)`.\n\n    learning_rate : float, default=1.0\n        Weight applied to each classifier at each boosting iteration. A higher\n        learning rate increases the contribution of each classifier. There is\n        a trade-off between the `learning_rate` and `n_estimators` parameters.\n        Values must be in the range `(0.0, inf)`.\n\n    algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'\n        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n        ``estimator`` must support calculation of class probabilities.\n        If 'SAMME' then use the SAMME discrete boosting algorithm.\n        The SAMME.R algorithm typically converges faster than SAMME,\n        achieving a lower test error with fewer boosting iterations.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given at each `estimator` at each\n        boosting iteration.\n        Thus, it is only used when `estimator` exposes a `random_state`.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    base_estimator : object, default=None\n        The base estimator from which the boosted ensemble is built.\n        Support for sample weighting is required, as well as proper\n        ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n        the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n        initialized with `max_depth=1`.\n\n        .. deprecated:: 1.2\n            `base_estimator` is deprecated and will be removed in 1.4.\n            Use `estimator` instead.\n\n    Attributes\n    ----------\n    estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n        .. versionadded:: 1.2\n           `base_estimator_` was renamed to `estimator_`.\n\n    base_estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n        .. deprecated:: 1.2\n            `base_estimator_` is deprecated and will be removed in 1.4.\n            Use `estimator_` instead.\n\n    estimators_ : list of classifiers\n        The collection of fitted sub-estimators.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    n_classes_ : int\n        The number of classes.\n\n    estimator_weights_ : ndarray of floats\n        Weights for each estimator in the boosted ensemble.\n\n    estimator_errors_ : ndarray of floats\n        Classification error for each estimator in the boosted\n        ensemble.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances if supported by the\n        ``estimator`` (when based on decision trees).\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    AdaBoostRegressor : An AdaBoost regressor that begins by fitting a\n        regressor on the original dataset and then fits additional copies of\n        the regressor on the same dataset but where the weights of instances\n        are adjusted according to the error of the current prediction.\n\n    GradientBoostingClassifier : GB builds an additive model in a forward\n        stage-wise fashion. Regression trees are fit on the negative gradient\n        of the binomial or multinomial deviance loss function. Binary\n        classification is a special case where only a single regression tree is\n        induced.\n\n    sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning\n        method used for classification.\n        Creates a model that predicts the value of a target variable by\n        learning simple decision rules inferred from the data features.\n\n    References\n    ----------\n    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n           on-Line Learning and an Application to Boosting\", 1995.\n\n    .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import AdaBoostClassifier\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_samples=1000, n_features=4,\n    ...                            n_informative=2, n_redundant=0,\n    ...                            random_state=0, shuffle=False)\n    >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n    >>> clf.fit(X, y)\n    AdaBoostClassifier(n_estimators=100, random_state=0)\n    >>> clf.predict([[0, 0, 0, 0]])\n    array([1])\n    >>> clf.score(X, y)\n    0.983...\n    \"\"\"",
                "filename": "sklearn/ensemble/_weight_boosting.py",
                "start_index": 10956,
                "end_index": 17100,
                "start_line": 342,
                "end_line": 1261,
                "max_line": 1266,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "def check_fit_check_is_fitted(name, estimator_orig):\n    # Make sure that estimator doesn't pass check_is_fitted before calling fit\n    # and that passes check_is_fitted once it's fit.\n\n    rng = np.random.RandomState(42)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    if \"warm_start\" in estimator.get_params():\n        estimator.set_params(warm_start=False)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n    if is_regressor(estimator_orig):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(low=0, high=2, size=n_samples)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if not _safe_tags(estimator).get(\"stateless\", False):\n        # stateless estimators (such as FunctionTransformer) are always \"fit\"!\n        try:\n            check_is_fitted(estimator)\n            raise AssertionError(\n                f\"{estimator.__class__.__name__} passes check_is_fitted before being\"\n                \" fit!\"\n            )\n        except NotFittedError:\n            pass\n    estimator.fit(X, y)\n    try:\n        check_is_fitted(estimator)\n    except NotFittedError as e:\n        raise NotFittedError(\n            \"Estimator fails to pass `check_is_fitted` even though it has been fit.\"\n        ) from e\n\n\ndef check_n_features_in(name, estimator_orig):\n    # Make sure that n_features_in_ attribute doesn't exist until fit is\n    # called, and that its value is correct.\n\n    rng = np.random.RandomState(0)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    if \"warm_start\" in estimator.get_params():\n        estimator.set_params(warm_start=False)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n    if is_regressor(estimator_orig):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(low=0, high=2, size=n_samples)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    assert not hasattr(estimator, \"n_features_in_\")\n    estimator.fit(X, y)\n    assert hasattr(estimator, \"n_features_in_\")\n    assert estimator.n_features_in_ == X.shape[1]\n\n\ndef check_requires_y_none(name, estimator_orig):\n    # Make sure that an estimator with requires_y=True fails gracefully when\n    # given y=None\n\n    rng = np.random.RandomState(0)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n\n    expected_err_msgs = (\n        \"requires y to be passed, but the target y is None\",\n        \"Expected array-like (array or non-string sequence), got None\",\n        \"y should be a 1d array\",\n    )\n\n    try:\n        estimator.fit(X, None)\n    except ValueError as ve:\n        if not any(msg in str(ve) for msg in expected_err_msgs):\n            raise ve",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 138164,
                "end_index": 141063,
                "start_line": 3892,
                "end_line": 3980,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "class AdaBoostClassifier(ClassifierMixin, BaseWeightBoosting):",
                "filename": "sklearn/ensemble/_weight_boosting.py",
                "start_index": 10889,
                "end_index": 10951,
                "start_line": 341,
                "end_line": 341,
                "max_line": 1266,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "_parameter_constraints: dict = {\n        **BaseWeightBoosting._parameter_constraints,\n        \"loss\": [StrOptions({\"linear\", \"square\", \"exponential\"})],\n    }\n\n    def __init__(\n        self,\n        estimator=None,\n        *,\n        n_estimators=50,\n        learning_rate=1.0,\n        loss=\"linear\",\n        random_state=None,\n        base_estimator=\"deprecated\",\n    ):\n        super().__init__(\n            estimator=estimator,\n            n_estimators=n_estimators,\n            learning_rate=learning_rate,\n            random_state=random_state,\n            base_estimator=base_estimator,\n        )\n\n        self.loss = loss\n        self.random_state = random_state\n\n    def _validate_estimator(self):\n        \"\"\"Check the estimator and set the estimator_ attribute.\"\"\"\n        super()._validate_estimator(default=DecisionTreeRegressor(max_depth=3))",
                "filename": "sklearn/ensemble/_weight_boosting.py",
                "start_index": 38485,
                "end_index": 39339,
                "start_line": 66,
                "end_line": 1103,
                "max_line": 1266,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "def check_outliers_fit_predict(name, estimator_orig):\n    # Check fit_predict for outlier detectors.\n\n    n_samples = 300\n    X, _ = make_blobs(n_samples=n_samples, random_state=0)\n    X = shuffle(X, random_state=7)\n    n_samples, n_features = X.shape\n    estimator = clone(estimator_orig)\n\n    set_random_state(estimator)\n\n    y_pred = estimator.fit_predict(X)\n    assert y_pred.shape == (n_samples,)\n    assert y_pred.dtype.kind == \"i\"\n    assert_array_equal(np.unique(y_pred), np.array([-1, 1]))\n\n    # check fit_predict = fit.predict when the estimator has both a predict and\n    # a fit_predict method. recall that it is already assumed here that the\n    # estimator has a fit_predict method\n    if hasattr(estimator, \"predict\"):\n        y_pred_2 = estimator.fit(X).predict(X)\n        assert_array_equal(y_pred, y_pred_2)\n\n    if hasattr(estimator, \"contamination\"):\n        # proportion of outliers equal to contamination parameter when not\n        # set to 'auto'\n        expected_outliers = 30\n        contamination = float(expected_outliers) / n_samples\n        estimator.set_params(contamination=contamination)\n        y_pred = estimator.fit_predict(X)\n\n        num_outliers = np.sum(y_pred != 1)\n        # num_outliers should be equal to expected_outliers unless\n        # there are ties in the decision_function values. this can\n        # only be tested for estimators with a decision_function\n        # method\n        if num_outliers != expected_outliers and hasattr(\n            estimator, \"decision_function\"\n        ):\n            decision = estimator.decision_function(X)\n            check_outlier_corruption(num_outliers, expected_outliers, decision)\n\n\ndef check_fit_non_negative(name, estimator_orig):\n    # Check that proper warning is raised for non-negative X\n    # when tag requires_positive_X is present\n    X = np.array([[-1.0, 1], [-1.0, 1]])\n    y = np.array([1, 2])\n    estimator = clone(estimator_orig)\n    with raises(ValueError):\n        estimator.fit(X, y)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 134030,
                "end_index": 136018,
                "start_line": 3781,
                "end_line": 4110,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/ensemble/_weight_boosting.py": [
                {
                    "chunk": {
                        "code": "\"\"\"Base class for AdaBoost estimators.\n\n    Warning: This class should not be used directly. Use derived classes\n    instead.\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"estimator\": [HasMethods([\"fit\", \"predict\"]), None],\n        \"n_estimators\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"learning_rate\": [Interval(Real, 0, None, closed=\"neither\")],\n        \"random_state\": [\"random_state\"],\n        \"base_estimator\": [\n            HasMethods([\"fit\", \"predict\"]),\n            StrOptions({\"deprecated\"}),\n            None,\n        ],\n    }\n\n    @abstractmethod\n    def __init__(\n        self,\n        estimator=None,\n        *,\n        n_estimators=50,\n        estimator_params=tuple(),\n        learning_rate=1.0,\n        random_state=None,\n        base_estimator=\"deprecated\",\n    ):\n        super().__init__(\n            estimator=estimator,\n            n_estimators=n_estimators,\n            estimator_params=estimator_params,\n            base_estimator=base_estimator,\n        )\n\n        self.learning_rate = learning_rate\n        self.random_state = random_state\n\n    def _check_X(self, X):\n        # Only called to validate X in non-fit methods, therefore reset=False\n        return self._validate_data(\n            X,\n            accept_sparse=[\"csr\", \"csc\"],\n            ensure_2d=True,\n            allow_nd=True,\n            dtype=None,\n            reset=False,\n        )",
                        "filename": "sklearn/ensemble/_weight_boosting.py",
                        "start_index": 1654,
                        "end_index": 3056,
                        "start_line": 60,
                        "end_line": 1199,
                        "max_line": 1266,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the '_parameter_constraints' dictionary where the 'base_estimator' parameter is defined. It needs to be updated to allow 'None' as a valid value."
                },
                {
                    "chunk": {
                        "code": "_parameter_constraints: dict = {\n        **BaseWeightBoosting._parameter_constraints,\n        \"algorithm\": [StrOptions({\"SAMME\", \"SAMME.R\"})],\n    }\n\n    def __init__(\n        self,\n        estimator=None,\n        *,\n        n_estimators=50,\n        learning_rate=1.0,\n        algorithm=\"SAMME.R\",\n        random_state=None,\n        base_estimator=\"deprecated\",\n    ):\n        super().__init__(\n            estimator=estimator,\n            n_estimators=n_estimators,\n            learning_rate=learning_rate,\n            random_state=random_state,\n            base_estimator=base_estimator,\n        )\n\n        self.algorithm = algorithm\n\n    def _validate_estimator(self):\n        \"\"\"Check the estimator and set the estimator_ attribute.\"\"\"\n        super()._validate_estimator(default=DecisionTreeClassifier(max_depth=1))\n\n        #  SAMME-R requires predict_proba-enabled base estimators\n        if self.algorithm == \"SAMME.R\":\n            if not hasattr(self.estimator_, \"predict_proba\"):\n                raise TypeError(\n                    \"AdaBoostClassifier with algorithm='SAMME.R' requires \"\n                    \"that the weak learner supports the calculation of class \"\n                    \"probabilities with a predict_proba method.\\n\"\n                    \"Please change the base estimator or set \"\n                    \"algorithm='SAMME' instead.\"\n                )\n        if not has_fit_parameter(self.estimator_, \"sample_weight\"):\n            raise ValueError(\n                f\"{self.estimator.__class__.__name__} doesn't support sample_weight.\"\n            )",
                        "filename": "sklearn/ensemble/_weight_boosting.py",
                        "start_index": 17106,
                        "end_index": 18678,
                        "start_line": 66,
                        "end_line": 1199,
                        "max_line": 1266,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the '__init__' method of 'AdaBoostClassifier' where 'base_estimator' is set to 'deprecated'. It may need to be reviewed to ensure that the deprecation is handled correctly when 'None' is passed."
                },
                {
                    "chunk": {
                        "code": "\"\"\"An AdaBoost classifier.\n\n    An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n    classifier on the original dataset and then fits additional copies of the\n    classifier on the same dataset but where the weights of incorrectly\n    classified instances are adjusted such that subsequent classifiers focus\n    more on difficult cases.\n\n    This class implements the algorithm known as AdaBoost-SAMME [2].\n\n    Read more in the :ref:`User Guide <adaboost>`.\n\n    .. versionadded:: 0.14\n\n    Parameters\n    ----------\n    estimator : object, default=None\n        The base estimator from which the boosted ensemble is built.\n        Support for sample weighting is required, as well as proper\n        ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n        the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n        initialized with `max_depth=1`.\n\n        .. versionadded:: 1.2\n           `base_estimator` was renamed to `estimator`.\n\n    n_estimators : int, default=50\n        The maximum number of estimators at which boosting is terminated.\n        In case of perfect fit, the learning procedure is stopped early.\n        Values must be in the range `[1, inf)`.\n\n    learning_rate : float, default=1.0\n        Weight applied to each classifier at each boosting iteration. A higher\n        learning rate increases the contribution of each classifier. There is\n        a trade-off between the `learning_rate` and `n_estimators` parameters.\n        Values must be in the range `(0.0, inf)`.\n\n    algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'\n        If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n        ``estimator`` must support calculation of class probabilities.\n        If 'SAMME' then use the SAMME discrete boosting algorithm.\n        The SAMME.R algorithm typically converges faster than SAMME,\n        achieving a lower test error with fewer boosting iterations.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given at each `estimator` at each\n        boosting iteration.\n        Thus, it is only used when `estimator` exposes a `random_state`.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    base_estimator : object, default=None\n        The base estimator from which the boosted ensemble is built.\n        Support for sample weighting is required, as well as proper\n        ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n        the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n        initialized with `max_depth=1`.\n\n        .. deprecated:: 1.2\n            `base_estimator` is deprecated and will be removed in 1.4.\n            Use `estimator` instead.\n\n    Attributes\n    ----------\n    estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n        .. versionadded:: 1.2\n           `base_estimator_` was renamed to `estimator_`.\n\n    base_estimator_ : estimator\n        The base estimator from which the ensemble is grown.\n\n        .. deprecated:: 1.2\n            `base_estimator_` is deprecated and will be removed in 1.4.\n            Use `estimator_` instead.\n\n    estimators_ : list of classifiers\n        The collection of fitted sub-estimators.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    n_classes_ : int\n        The number of classes.\n\n    estimator_weights_ : ndarray of floats\n        Weights for each estimator in the boosted ensemble.\n\n    estimator_errors_ : ndarray of floats\n        Classification error for each estimator in the boosted\n        ensemble.\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances if supported by the\n        ``estimator`` (when based on decision trees).\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    AdaBoostRegressor : An AdaBoost regressor that begins by fitting a\n        regressor on the original dataset and then fits additional copies of\n        the regressor on the same dataset but where the weights of instances\n        are adjusted according to the error of the current prediction.\n\n    GradientBoostingClassifier : GB builds an additive model in a forward\n        stage-wise fashion. Regression trees are fit on the negative gradient\n        of the binomial or multinomial deviance loss function. Binary\n        classification is a special case where only a single regression tree is\n        induced.\n\n    sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning\n        method used for classification.\n        Creates a model that predicts the value of a target variable by\n        learning simple decision rules inferred from the data features.\n\n    References\n    ----------\n    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n           on-Line Learning and an Application to Boosting\", 1995.\n\n    .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n\n    Examples\n    --------\n    >>> from sklearn.ensemble import AdaBoostClassifier\n    >>> from sklearn.datasets import make_classification\n    >>> X, y = make_classification(n_samples=1000, n_features=4,\n    ...                            n_informative=2, n_redundant=0,\n    ...                            random_state=0, shuffle=False)\n    >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n    >>> clf.fit(X, y)\n    AdaBoostClassifier(n_estimators=100, random_state=0)\n    >>> clf.predict([[0, 0, 0, 0]])\n    array([1])\n    >>> clf.score(X, y)\n    0.983...\n    \"\"\"",
                        "filename": "sklearn/ensemble/_weight_boosting.py",
                        "start_index": 10956,
                        "end_index": 17100,
                        "start_line": 342,
                        "end_line": 1261,
                        "max_line": 1266,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet provides the documentation for 'AdaBoostClassifier', which includes information about the 'base_estimator' parameter. It may need to be updated to reflect the changes made to handle 'None' as a valid value."
                }
            ]
        },
        "solution_task": "- Update the `_parameter_constraints` dictionary in the `BaseWeightBoosting` class to allow `None` as a valid value for `base_estimator`.\n- Ensure that the `AdaBoostClassifier` class constructor and `_validate_estimator` method handle the `base_estimator=None` case correctly, without raising an `InvalidParameterError`.\n- Deprecate the `base_estimator` parameter in favor of the `estimator` parameter in the `AdaBoostClassifier` class.\n- Modify the `AdaBoostClassifier` class to set `estimator_` attribute to a default `DecisionTreeClassifier(max_depth=1)` when `base_estimator` is `None`.\n- Remove any checks that raise `InvalidParameterError` when `base_estimator` is `None` in the `AdaBoostClassifier` class.\n- Update the `fit` method of the `AdaBoostClassifier` to handle `base_estimator=None` by setting the `estimator_` attribute to the default estimator if not already set.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE provides a detailed and actionable plan to address the bug described in the PROMPT. It suggests updating the '_parameter_constraints' dictionary to allow 'None' as a valid value for 'base_estimator', which directly addresses the issue of the deprecation not handling 'base_estimator=None' properly. Additionally, it advises ensuring that the 'AdaBoostClassifier' class constructor and '_validate_estimator' method handle the 'base_estimator=None' case correctly, which is crucial for preventing the 'InvalidParameterError'. The RESPONSE also correctly identifies the need to deprecate the 'base_estimator' parameter in favor of 'estimator', and to set a default estimator when 'base_estimator' is 'None'. Furthermore, it suggests removing checks that raise 'InvalidParameterError' for 'base_estimator=None' and updating the 'fit' method to handle this case. The RESPONSE also provides links to the relevant code snippets that need to be checked, which is highly useful for developers looking to fix the bug. The RESPONSE does not receive a perfect score because it does not explicitly mention updating the documentation to reflect the changes, which is a minor aspect of the overall solution.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711738092.788513,
        "result": "- Update the `_parameter_constraints` dictionary in the `BaseWeightBoosting` class to allow `None` as a valid value for `base_estimator`.\n- Ensure that the `AdaBoostClassifier` class constructor and `_validate_estimator` method handle the `base_estimator=None` case correctly, without raising an `InvalidParameterError`.\n- Deprecate the `base_estimator` parameter in favor of the `estimator` parameter in the `AdaBoostClassifier` class.\n- Modify the `AdaBoostClassifier` class to set `estimator_` attribute to a default `DecisionTreeClassifier(max_depth=1)` when `base_estimator` is `None`.\n- Remove any checks that raise `InvalidParameterError` when `base_estimator` is `None` in the `AdaBoostClassifier` class.\n- Update the `fit` method of the `AdaBoostClassifier` to handle `base_estimator=None` by setting the `estimator_` attribute to the default estimator if not already set.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn) \u2192 [ensemble](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn/ensemble)- [Lines 60 - 1199](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/ensemble/_weight_boosting.py#L60-L1199)\n\n This snippet contains the '_parameter_constraints' dictionary where the 'base_estimator' parameter is defined. It needs to be updated to allow 'None' as a valid value. \n\n\n   - [Lines 66 - 1199](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/ensemble/_weight_boosting.py#L66-L1199)\n\n This snippet shows the '__init__' method of 'AdaBoostClassifier' where 'base_estimator' is set to 'deprecated'. It may need to be reviewed to ensure that the deprecation is handled correctly when 'None' is passed. \n\n\n   - [Lines 342 - 1261](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/ensemble/_weight_boosting.py#L342-L1261)\n\n This snippet provides the documentation for 'AdaBoostClassifier', which includes information about the 'base_estimator' parameter. It may need to be updated to reflect the changes made to handle 'None' as a valid value."
    }
}