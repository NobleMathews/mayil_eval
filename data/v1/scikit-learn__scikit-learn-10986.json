{
    "id": "scikit-learn__scikit-learn-10986",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
    "task_id": 5175021216580044911,
    "title": "Warm start bug when fitting a LogisticRegression model on binary outcomes with `multi_class='multinomial'`.",
    "body": "<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: http://stackoverflow.com/questions/tagged/scikit-learn\r\n- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn\r\nFor more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nBug when fitting a LogisticRegression model on binary outcomes with multi_class='multinomial' when using warm start. Note that it is similar to the issue here https://github.com/scikit-learn/scikit-learn/issues/9889 i.e. only using a 1D `coef` object on binary outcomes even when using `multi_class='multinomial'` as opposed to a 2D `coef` object.\r\n\r\n#### Steps/Code to Reproduce\r\n<!--\r\nExample:\r\n```python\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nfrom sklearn.decomposition import LatentDirichletAllocation\r\n\r\ndocs = [\"Help I have a bug\" for i in range(1000)]\r\n\r\nvectorizer = CountVectorizer(input=docs, analyzer='word')\r\nlda_features = vectorizer.fit_transform(docs)\r\n\r\nlda_model = LatentDirichletAllocation(\r\n    n_topics=10,\r\n    learning_method='online',\r\n    evaluate_every=10,\r\n    n_jobs=4,\r\n)\r\nmodel = lda_model.fit(lda_features)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https://gist.github.com\r\n-->\r\n    from sklearn.linear_model import LogisticRegression\r\n    import sklearn.metrics\r\n    import numpy as np\r\n\r\n    # Set up a logistic regression object\r\n    lr = LogisticRegression(C=1000000, multi_class='multinomial',\r\n                        solver='sag', tol=0.0001, warm_start=True,\r\n                        verbose=0)\r\n\r\n    # Set independent variable values\r\n    Z = np.array([\r\n    [ 0.        ,  0.        ],\r\n    [ 1.33448632,  0.        ],\r\n    [ 1.48790105, -0.33289528],\r\n    [-0.47953866, -0.61499779],\r\n    [ 1.55548163,  1.14414766],\r\n    [-0.31476657, -1.29024053],\r\n    [-1.40220786, -0.26316645],\r\n    [ 2.227822  , -0.75403668],\r\n    [-0.78170885, -1.66963585],\r\n    [ 2.24057471, -0.74555021],\r\n    [-1.74809665,  2.25340192],\r\n    [-1.74958841,  2.2566389 ],\r\n    [ 2.25984734, -1.75106702],\r\n    [ 0.50598996, -0.77338402],\r\n    [ 1.21968303,  0.57530831],\r\n    [ 1.65370219, -0.36647173],\r\n    [ 0.66569897,  1.77740068],\r\n    [-0.37088553, -0.92379819],\r\n    [-1.17757946, -0.25393047],\r\n    [-1.624227  ,  0.71525192]])\r\n\r\n    # Set dependant variable values\r\n    Y = np.array([1, 0, 0, 1, 0, 0, 0, 0, \r\n              0, 0, 1, 1, 1, 0, 0, 1, \r\n              0, 0, 1, 1], dtype=np.int32)\r\n    \r\n    # First fit model normally\r\n    lr.fit(Z, Y)\r\n\r\n    p = lr.predict_proba(Z)\r\n    print(sklearn.metrics.log_loss(Y, p)) # ...\r\n\r\n    print(lr.intercept_)\r\n    print(lr.coef_)\r\n\r\n    # Now fit model after a warm start\r\n    lr.fit(Z, Y)\r\n\r\n    p = lr.predict_proba(Z)\r\n    print(sklearn.metrics.log_loss(Y, p)) # ...\r\n\r\n    print(lr.intercept_)\r\n    print(lr.coef_)\r\n\r\n\r\n\r\n#### Expected Results\r\nThe predictions should be the same as the model converged the first time it was run.\r\n\r\n#### Actual Results\r\nThe predictions are different. In fact the more times you re-run the fit the worse it gets. This is actually the only reason I was able to catch the bug. It is caused by the line here https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/logistic.py#L678.\r\n\r\n     w0[:, :coef.shape[1]] = coef\r\n\r\nAs `coef` is `(1, n_features)`, but `w0` is `(2, n_features)`, this causes the `coef` value to be broadcast into the `w0`. This some sort of singularity issue when training resulting in worse performance. Note that had it not done exactly this i.e. `w0` was simply initialised by some random values, this bug would be very hard to catch because of course each time the model would converge just not as fast as one would hope when warm starting.\r\n\r\n#### Further Information\r\nThe fix I believe is very easy, just need to swap the previous line to \r\n\r\n     if n_classes == 1:\r\n         w0[0, :coef.shape[1]] = -coef  # Be careful to get these the right way around\r\n         w0[1, :coef.shape[1]] = coef\r\n     else:\r\n         w0[:, :coef.shape[1]] = coef\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\n-->\r\nLinux-4.13.0-37-generic-x86_64-with-Ubuntu-16.04-xenial\r\nPython 3.5.2 (default, Nov 23 2017, 16:37:01)\r\nNumPy 1.14.2\r\nSciPy 1.0.0\r\nScikit-Learn 0.20.dev0 (built from latest master)\r\n\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.20.git",
    "requested_commit": "",
    "mayil_response": "- The issue is related to the incorrect shape of the `coef_` attribute when using `LogisticRegression` with `multi_class='multinomial'` and `warm_start=True` for binary outcomes.\n- The `coef_` attribute should be a 2D array even for binary classification when `multi_class='multinomial'`.\n- The fix should ensure that `coef_` is correctly broadcasted into `w0` during warm start.\n\nSolution:\n- Modify the `fit` method in the `LogisticRegression` class to handle the shape of `coef_` correctly during warm start.\n- Check if `multi_class` is set to `'multinomial'` and if the problem is binary (i.e., `n_classes == 2`).\n- If the conditions are met and `warm_start` is `True`, reshape `coef_` to be a 2D array before using it to initialize `w0`.\n\n```python\nclass LogisticRegression:\n    # ... (other parts of the class)\n\n    def fit(self, X, y):\n        # ... (initial part of the fit method)\n\n        # Check if it is a warm start and if coef_ needs to be reshaped\n        if self.warm_start and hasattr(self, \"coef_\") and self.multi_class == \"multinomial\":\n            n_classes = len(np.unique(y))\n            if n_classes == 2:\n                self.coef_ = self.coef_.reshape(1, -1)\n\n        # ... (rest of the fit method)\n```\n\n- Ensure that the `coef_` attribute is reshaped only when transitioning from a binary to a multinomial problem.\n- This change should be placed before any operations that involve `coef_` during the warm start process.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/linear_model)- [Lines 377 - 414](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_logistic.py#L377-L414)\n\n This snippet contains the exact line of code that is mentioned in the issue description as causing the bug, and it includes the proposed fix. \n\n\n   - [Lines 176 - 2220](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_logistic.py#L176-L2220)\n\n This snippet provides context on the 'warm_start' parameter and how it is used in LogisticRegression, which is relevant to the issue of warm starting with multinomial logistic regression. \n\n\n   - [Lines 338 - 2213](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_logistic.py#L338-L2213)\n\n This snippet shows the initialization of the coefficient array 'w0' for different types of logistic regression, which is relevant to the issue since the bug is related to the shape of 'w0' during warm start.",
    "ai_cost": {
        "search_embedding": 2.6e-07,
        "issue_classifier": 0.005174,
        "snippet_processor": 0.06475,
        "issue_star_creation": 0.04989,
        "issue_star_solver": 0.08403000000000001,
        "bouncer": 0.12060000000000001
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711741500.190345,
        "relevant_snippets": [
            {
                "code": "# warm start: this is not the first time fit was called\n        else:\n            # Check that the maximum number of iterations is not smaller\n            # than the number of iterations from the previous fit\n            if self.max_iter < self.n_iter_:\n                raise ValueError(\n                    \"max_iter=%d must be larger than or equal to \"\n                    \"n_iter_=%d when warm_start==True\" % (self.max_iter, self.n_iter_)\n                )\n\n            # Convert array attributes to lists\n            self.train_score_ = self.train_score_.tolist()\n            self.validation_score_ = self.validation_score_.tolist()\n\n            # Compute raw predictions\n            raw_predictions = self._raw_predict(X_binned_train, n_threads=n_threads)\n            if self.do_early_stopping_ and self._use_validation_data:\n                raw_predictions_val = self._raw_predict(\n                    X_binned_val, n_threads=n_threads\n                )\n            else:\n                raw_predictions_val = None\n\n            if self.do_early_stopping_ and self.scoring != \"loss\":\n                # Compute the subsample set\n                (\n                    X_binned_small_train,\n                    y_small_train,\n                    sample_weight_small_train,\n                ) = self._get_small_trainset(\n                    X_binned_train, y_train, sample_weight_train, self._random_seed\n                )\n\n            # Get the predictors from the previous fit\n            predictors = self._predictors\n\n            begin_at_stage = self.n_iter_",
                "filename": "sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py",
                "start_index": 23720,
                "end_index": 25283,
                "start_line": 593,
                "end_line": 629,
                "max_line": 2009,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "if skip_slow and solver == \"lightning\" and penalty == \"l1\":\n        print(\"skip_slowping l1 logistic regression with solver lightning.\")\n        return\n\n    print(\n        \"Solving %s logistic regression with penalty %s, solver %s.\"\n        % (\"binary\" if single_target else \"multinomial\", penalty, solver)\n    )\n\n    if solver == \"lightning\":\n        from lightning.classification import SAGAClassifier\n\n    if single_target or solver not in [\"sag\", \"saga\"]:\n        multi_class = \"ovr\"\n    else:\n        multi_class = \"multinomial\"\n    X = X.astype(dtype)\n    y = y.astype(dtype)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, random_state=42, stratify=y\n    )\n    n_samples = X_train.shape[0]\n    n_classes = np.unique(y_train).shape[0]\n    test_scores = [1]\n    train_scores = [1]\n    accuracies = [1 / n_classes]\n    times = [0]\n\n    if penalty == \"l2\":\n        alpha = 1.0 / (C * n_samples)\n        beta = 0\n        lightning_penalty = None\n    else:\n        alpha = 0.0\n        beta = 1.0 / (C * n_samples)\n        lightning_penalty = \"l1\"\n\n    for this_max_iter in range(1, max_iter + 1, 2):\n        print(\n            \"[%s, %s, %s] Max iter: %s\"\n            % (\n                \"binary\" if single_target else \"multinomial\",\n                penalty,\n                solver,\n                this_max_iter,\n            )\n        )\n        if solver == \"lightning\":\n            lr = SAGAClassifier(\n                loss=\"log\",\n                alpha=alpha,\n                beta=beta,\n                penalty=lightning_penalty,\n                tol=-1,\n                max_iter=this_max_iter,\n            )\n        else:\n            lr = LogisticRegression(\n                solver=solver,\n                multi_class=multi_class,\n                C=C,\n                penalty=penalty,\n                fit_intercept=False,\n                tol=0,\n                max_iter=this_max_iter,\n                random_state=42,\n            )\n\n        # Makes cpu cache even for all fit calls\n        X_train.max()\n        t0 = time.clock()\n\n        lr.fit(X_train, y_train)\n        train_time = time.clock() - t0\n\n        scores = []\n        for X, y in [(X_train, y_train), (X_test, y_test)]:\n            try:\n                y_pred = lr.predict_proba(X)\n            except NotImplementedError:\n                # Lightning predict_proba is not implemented for n_classes > 2\n                y_pred = _predict_proba(lr, X)\n            score = log_loss(y, y_pred, normalize=False) / n_samples\n            score += 0.5 * alpha * np.sum(lr.coef_**2) + beta * np.sum(\n                np.abs(lr.coef_)\n            )\n            scores.append(score)\n        train_score, test_score = tuple(scores)\n\n        y_pred = lr.predict(X_test)\n        accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\n        test_scores.append(test_score)\n        train_scores.append(train_score)\n        accuracies.append(accuracy)\n        times.append(train_time)",
                "filename": "benchmarks/bench_saga.py",
                "start_index": 870,
                "end_index": 3823,
                "start_line": 38,
                "end_line": 132,
                "max_line": 380,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"\n====================================================\nMulticlass sparse logistic regression on 20newgroups\n====================================================\n\nComparison of multinomial logistic L1 vs one-versus-rest L1 logistic regression\nto classify documents from the newgroups20 dataset. Multinomial logistic\nregression yields more accurate results and is faster to train on the larger\nscale dataset.\n\nHere we use the l1 sparsity that trims the weights of not informative\nfeatures to zero. This is good if the goal is to extract the strongly\ndiscriminative vocabulary of each class. If the goal is to get the best\npredictive accuracy, it is better to use the non sparsity-inducing l2 penalty\ninstead.\n\nA more traditional (and possibly better) way to predict on a sparse subset of\ninput features would be to use univariate feature selection followed by a\ntraditional (l2-penalised) logistic regression model.\n\n\"\"\"\n\n# Author: Arthur Mensch\n\nimport timeit\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import fetch_20newsgroups_vectorized\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\nt0 = timeit.default_timer()\n\n# We use SAGA solver\nsolver = \"saga\"\n\n# Turn down for faster run time\nn_samples = 5000\n\nX, y = fetch_20newsgroups_vectorized(subset=\"all\", return_X_y=True)\nX = X[:n_samples]\ny = y[:n_samples]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=42, stratify=y, test_size=0.1\n)\ntrain_samples, n_features = X_train.shape\nn_classes = np.unique(y).shape[0]\n\nprint(\n    \"Dataset 20newsgroup, train_samples=%i, n_features=%i, n_classes=%i\"\n    % (train_samples, n_features, n_classes)\n)\n\nmodels = {\n    \"ovr\": {\"name\": \"One versus Rest\", \"iters\": [1, 2, 3]},\n    \"multinomial\": {\"name\": \"Multinomial\", \"iters\": [1, 2, 5]},\n}",
                "filename": "examples/linear_model/plot_sparse_logistic_regression_20newsgroups.py",
                "start_index": 0,
                "end_index": 1997,
                "start_line": 1,
                "end_line": 63,
                "max_line": 125,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "if coef is not None:\n        # it must work both giving the bias term and not\n        if multi_class == \"ovr\":\n            if coef.size not in (n_features, w0.size):\n                raise ValueError(\n                    \"Initialization coef is of shape %d, expected shape %d or %d\"\n                    % (coef.size, n_features, w0.size)\n                )\n            w0[: coef.size] = coef\n        else:\n            # For binary problems coef.shape[0] should be 1, otherwise it\n            # should be classes.size.\n            n_classes = classes.size\n            if n_classes == 2:\n                n_classes = 1\n\n            if coef.shape[0] != n_classes or coef.shape[1] not in (\n                n_features,\n                n_features + 1,\n            ):\n                raise ValueError(\n                    \"Initialization coef is of shape (%d, %d), expected \"\n                    \"shape (%d, %d) or (%d, %d)\"\n                    % (\n                        coef.shape[0],\n                        coef.shape[1],\n                        classes.size,\n                        n_features,\n                        classes.size,\n                        n_features + 1,\n                    )\n                )\n\n            if n_classes == 1:\n                w0[0, : coef.shape[1]] = -coef\n                w0[1, : coef.shape[1]] = coef\n            else:\n                w0[:, : coef.shape[1]] = coef",
                "filename": "sklearn/linear_model/_logistic.py",
                "start_index": 14578,
                "end_index": 15975,
                "start_line": 377,
                "end_line": 414,
                "max_line": 2222,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"\nLogistic Regression\n\"\"\"\n\n# Author: Gael Varoquaux <gael.varoquaux@normalesup.org>\n#         Fabian Pedregosa <f@bianp.net>\n#         Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n#         Manoj Kumar <manojkumarsivaraj334@gmail.com>\n#         Lars Buitinck\n#         Simon Wu <s8wu@uwaterloo.ca>\n#         Arthur Mensch <arthur.mensch@m4x.org\n\nimport numbers\nimport warnings\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom joblib import effective_n_jobs\nfrom scipy import optimize\n\nfrom sklearn.metrics import get_scorer_names\n\nfrom .._loss.loss import HalfBinomialLoss, HalfMultinomialLoss\nfrom ..base import _fit_context\nfrom ..metrics import get_scorer\nfrom ..model_selection import check_cv\nfrom ..preprocessing import LabelBinarizer, LabelEncoder\nfrom ..svm._base import _fit_liblinear\nfrom ..utils import (\n    Bunch,\n    check_array,\n    check_consistent_length,\n    check_random_state,\n    compute_class_weight,\n)\nfrom ..utils._param_validation import Interval, StrOptions\nfrom ..utils.extmath import row_norms, softmax\nfrom ..utils.metadata_routing import (\n    MetadataRouter,\n    MethodMapping,\n    _routing_enabled,\n    process_routing,\n)\nfrom ..utils.multiclass import check_classification_targets\nfrom ..utils.optimize import _check_optimize_result, _newton_cg\nfrom ..utils.parallel import Parallel, delayed\nfrom ..utils.validation import (\n    _check_method_params,\n    _check_sample_weight,\n    check_is_fitted,\n)\nfrom ._base import BaseEstimator, LinearClassifierMixin, SparseCoefMixin\nfrom ._glm.glm import NewtonCholeskySolver\nfrom ._linear_loss import LinearModelLoss\nfrom ._sag import sag_solver\n\n_LOGISTIC_SOLVER_CONVERGENCE_MSG = (\n    \"Please also refer to the documentation for alternative solver options:\\n\"\n    \"    https://scikit-learn.org/stable/modules/linear_model.html\"\n    \"#logistic-regression\"\n)\n\n\ndef _check_solver(solver, penalty, dual):\n    # TODO(1.4): Remove \"none\" option\n    if solver not in [\"liblinear\", \"saga\"] and penalty not in (\"l2\", \"none\", None):\n        raise ValueError(\n            \"Solver %s supports only 'l2' or 'none' penalties, got %s penalty.\"\n            % (solver, penalty)\n        )\n    if solver != \"liblinear\" and dual:\n        raise ValueError(\n            \"Solver %s supports only dual=False, got dual=%s\" % (solver, dual)\n        )\n\n    if penalty == \"elasticnet\" and solver != \"saga\":\n        raise ValueError(\n            \"Only 'saga' solver supports elasticnet penalty, got solver={}.\".format(\n                solver\n            )\n        )\n\n    if solver == \"liblinear\" and penalty == \"none\":\n        raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n\n    return solver",
                "filename": "sklearn/linear_model/_logistic.py",
                "start_index": 0,
                "end_index": 2699,
                "start_line": 1,
                "end_line": 86,
                "max_line": 2222,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"Logistic Regression CV (aka logit, MaxEnt) classifier.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    This class implements logistic regression using liblinear, newton-cg, sag\n    of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\n    regularization with primal formulation. The liblinear solver supports both\n    L1 and L2 regularization, with a dual formulation only for the L2 penalty.\n    Elastic-Net penalty is only supported by the saga solver.\n\n    For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\n    is selected by the cross-validator\n    :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\n    using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\n    solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\n\n    Read more in the :ref:`User Guide <logistic_regression>`.\n\n    Parameters\n    ----------\n    Cs : int or list of floats, default=10\n        Each of the values in Cs describes the inverse of regularization\n        strength. If Cs is as an int, then a grid of Cs values are chosen\n        in a logarithmic scale between 1e-4 and 1e4.\n        Like in support vector machines, smaller values specify stronger\n        regularization.\n\n    fit_intercept : bool, default=True\n        Specifies if a constant (a.k.a. bias or intercept) should be\n        added to the decision function.\n\n    cv : int or cross-validation generator, default=None\n        The default cross-validation generator used is Stratified K-Folds.\n        If an integer is provided, then it is the number of folds used.\n        See the module :mod:`sklearn.model_selection` module for the\n        list of possible cross-validation objects.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    dual : bool, default=False\n        Dual or primal formulation. Dual formulation is only implemented for\n        l2 penalty with liblinear solver. Prefer dual=False when\n        n_samples > n_features.\n\n    penalty : {'l1', 'l2', 'elasticnet'}, default='l2'\n        Specify the norm of the penalty:\n\n        - `'l2'`: add a L2 penalty term (used by default);\n        - `'l1'`: add a L1 penalty term;\n        - `'elasticnet'`: both L1 and L2 penalty terms are added.\n\n        .. warning::\n           Some penalties may not work with some solvers. See the parameter\n           `solver` below, to know the compatibility between the penalty and\n           solver.\n\n    scoring : str or callable, default=None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``. For a list of scoring functions\n        that can be used, look at :mod:`sklearn.metrics`. The\n        default scoring option used is 'accuracy'.\n\n    solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'}, \\\n            default='lbfgs'\n\n        Algorithm to use in the optimization problem. Default is 'lbfgs'.\n        To choose a solver, you might want to consider the following aspects:\n\n            - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n              and 'saga' are faster for large ones;\n            - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n              'lbfgs' handle multinomial loss;\n            - 'liblinear' might be slower in :class:`LogisticRegressionCV`\n              because it does not handle warm-starting. 'liblinear' is\n              limited to one-versus-rest schemes.\n            - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n              especially with one-hot encoded categorical features with rare\n              categories. Note that it is limited to binary classification and the\n              one-versus-rest reduction for multiclass classification. Be aware that\n              the memory usage of this solver has a quadratic dependency on\n              `n_features` because it explicitly computes the Hessian matrix.\n\n        .. warning::\n           The choice of the algorithm depends on the penalty chosen.\n           Supported penalties by solver:\n\n           - 'lbfgs'           -   ['l2']\n           - 'liblinear'       -   ['l1', 'l2']\n           - 'newton-cg'       -   ['l2']\n           - 'newton-cholesky' -   ['l2']\n           - 'sag'             -   ['l2']\n           - 'saga'            -   ['elasticnet', 'l1', 'l2']\n\n        .. note::\n           'sag' and 'saga' fast convergence is only guaranteed on features\n           with approximately the same scale. You can preprocess the data with\n           a scaler from :mod:`sklearn.preprocessing`.\n\n        .. versionadded:: 0.17\n           Stochastic Average Gradient descent solver.\n        .. versionadded:: 0.19\n           SAGA solver.\n        .. versionadded:: 1.2\n           newton-cholesky solver.\n\n    tol : float, default=1e-4\n        Tolerance for stopping criteria.\n\n    max_iter : int, default=100\n        Maximum number of iterations of the optimization algorithm.\n\n    class_weight : dict or 'balanced', default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n        .. versionadded:: 0.17\n           class_weight == 'balanced'\n\n    n_jobs : int, default=None\n        Number of CPU cores used during the cross-validation loop.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : int, default=0\n        For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n        positive number for verbosity.\n\n    refit : bool, default=True\n        If set to True, the scores are averaged across all folds, and the\n        coefs and the C that corresponds to the best score is taken, and a\n        final refit is done using these parameters.\n        Otherwise the coefs, intercepts and C that correspond to the\n        best scores across folds are averaged.\n\n    intercept_scaling : float, default=1\n        Useful only when the solver 'liblinear' is used\n        and self.fit_intercept is set to True. In this case, x becomes\n        [x, self.intercept_scaling],\n        i.e. a \"synthetic\" feature with constant value equal to\n        intercept_scaling is appended to the instance vector.\n        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n\n        Note! the synthetic feature weight is subject to l1/l2 regularization\n        as all other features.\n        To lessen the effect of regularization on synthetic feature weight\n        (and therefore on the intercept) intercept_scaling has to be increased.\n\n    multi_class : {'auto, 'ovr', 'multinomial'}, default='auto'\n        If the option chosen is 'ovr', then a binary problem is fit for each\n        label. For 'multinomial' the loss minimised is the multinomial loss fit\n        across the entire probability distribution, *even when the data is\n        binary*. 'multinomial' is unavailable when solver='liblinear'.\n        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n        and otherwise selects 'multinomial'.\n\n        .. versionadded:: 0.18\n           Stochastic Average Gradient descent solver for 'multinomial' case.\n        .. versionchanged:: 0.22\n            Default changed from 'ovr' to 'auto' in 0.22.\n\n    random_state : int, RandomState instance, default=None\n        Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\n        Note that this only applies to the solver and not the cross-validation\n        generator. See :term:`Glossary <random_state>` for details.\n\n    l1_ratios : list of float, default=None\n        The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n        Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n        using ``penalty='l2'``, while 1 is equivalent to using\n        ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n        of L1 and L2.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes, )\n        A list of class labels known to the classifier.\n\n    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n        Coefficient of the features in the decision function.\n\n        `coef_` is of shape (1, n_features) when the given problem\n        is binary.\n\n    intercept_ : ndarray of shape (1,) or (n_classes,)\n        Intercept (a.k.a. bias) added to the decision function.\n\n        If `fit_intercept` is set to False, the intercept is set to zero.\n        `intercept_` is of shape(1,) when the problem is binary.\n\n    Cs_ : ndarray of shape (n_cs)\n        Array of C i.e. inverse of regularization parameter values used\n        for cross-validation.\n\n    l1_ratios_ : ndarray of shape (n_l1_ratios)\n        Array of l1_ratios used for cross-validation. If no l1_ratio is used\n        (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n\n    coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or \\",
                "filename": "sklearn/linear_model/_logistic.py",
                "start_index": 54232,
                "end_index": 63750,
                "start_line": 1435,
                "end_line": 1642,
                "max_line": 2222,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "default='lbfgs'\n\n        Algorithm to use in the optimization problem. Default is 'lbfgs'.\n        To choose a solver, you might want to consider the following aspects:\n\n            - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n              and 'saga' are faster for large ones;\n            - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n              'lbfgs' handle multinomial loss;\n            - 'liblinear' is limited to one-versus-rest schemes.\n            - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n              especially with one-hot encoded categorical features with rare\n              categories. Note that it is limited to binary classification and the\n              one-versus-rest reduction for multiclass classification. Be aware that\n              the memory usage of this solver has a quadratic dependency on\n              `n_features` because it explicitly computes the Hessian matrix.\n\n        .. warning::\n           The choice of the algorithm depends on the penalty chosen.\n           Supported penalties by solver:\n\n           - 'lbfgs'           -   ['l2', None]\n           - 'liblinear'       -   ['l1', 'l2']\n           - 'newton-cg'       -   ['l2', None]\n           - 'newton-cholesky' -   ['l2', None]\n           - 'sag'             -   ['l2', None]\n           - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n\n        .. note::\n           'sag' and 'saga' fast convergence is only guaranteed on features\n           with approximately the same scale. You can preprocess the data with\n           a scaler from :mod:`sklearn.preprocessing`.\n\n        .. seealso::\n           Refer to the User Guide for more information regarding\n           :class:`LogisticRegression` and more specifically the\n           :ref:`Table <Logistic_regression>`\n           summarizing solver/penalty supports.\n\n        .. versionadded:: 0.17\n           Stochastic Average Gradient descent solver.\n        .. versionadded:: 0.19\n           SAGA solver.\n        .. versionchanged:: 0.22\n            The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n        .. versionadded:: 1.2\n           newton-cholesky solver.\n\n    max_iter : int, default=100\n        Maximum number of iterations taken for the solvers to converge.\n\n    multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n        If the option chosen is 'ovr', then a binary problem is fit for each\n        label. For 'multinomial' the loss minimised is the multinomial loss fit\n        across the entire probability distribution, *even when the data is\n        binary*. 'multinomial' is unavailable when solver='liblinear'.\n        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n        and otherwise selects 'multinomial'.\n\n        .. versionadded:: 0.18\n           Stochastic Average Gradient descent solver for 'multinomial' case.\n        .. versionchanged:: 0.22\n            Default changed from 'ovr' to 'auto' in 0.22.\n\n    verbose : int, default=0\n        For the liblinear and lbfgs solvers set verbose to any positive\n        number for verbosity.\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n\n        .. versionadded:: 0.17\n           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n\n    n_jobs : int, default=None\n        Number of CPU cores used when parallelizing over classes if\n        multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n        set to 'liblinear' regardless of whether 'multi_class' is specified or\n        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors.\n        See :term:`Glossary <n_jobs>` for more details.\n\n    l1_ratio : float, default=None\n        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n        to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n        combination of L1 and L2.\n\n    Attributes\n    ----------\n\n    classes_ : ndarray of shape (n_classes, )\n        A list of class labels known to the classifier.\n\n    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n        Coefficient of the features in the decision function.\n\n        `coef_` is of shape (1, n_features) when the given problem is binary.\n        In particular, when `multi_class='multinomial'`, `coef_` corresponds\n        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n\n    intercept_ : ndarray of shape (1,) or (n_classes,)\n        Intercept (a.k.a. bias) added to the decision function.\n\n        If `fit_intercept` is set to False, the intercept is set to zero.\n        `intercept_` is of shape (1,) when the given problem is binary.\n        In particular, when `multi_class='multinomial'`, `intercept_`\n        corresponds to outcome 1 (True) and `-intercept_` corresponds to\n        outcome 0 (False).\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : ndarray of shape (n_classes,) or (1, )\n        Actual number of iterations for all classes. If binary or multinomial,\n        it returns only 1 element. For liblinear solver, only the maximum\n        number of iteration across all classes is given.\n\n        .. versionchanged:: 0.20\n\n            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n\n    See Also\n    --------\n    SGDClassifier : Incrementally trained logistic regression (when given\n        the parameter ``loss=\"log_loss\"``).\n    LogisticRegressionCV : Logistic regression with built-in cross validation.\n\n    Notes\n    -----\n    The underlying C implementation uses a random number generator to\n    select features when fitting the model. It is thus not uncommon,\n    to have slightly different results for the same input data. If\n    that happens, try with a smaller tol parameter.\n\n    Predict output may not match that of standalone liblinear in certain\n    cases. See :ref:`differences from liblinear <liblinear_differences>`\n    in the narrative documentation.\n\n    References\n    ----------\n\n    L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n        Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n        http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n\n    LIBLINEAR -- A Library for Large Linear Classification\n        https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n\n    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n        Minimizing Finite Sums with the Stochastic Average Gradient\n        https://hal.inria.fr/hal-00860051/document\n\n    SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n            :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n            for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n\n    Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n        methods for logistic regression and maximum entropy models.\n        Machine Learning 85(1-2):41-75.\n        https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = LogisticRegression(random_state=0).fit(X, y)\n    >>> clf.predict(X[:2, :])\n    array([0, 0])\n    >>> clf.predict_proba(X[:2, :])\n    array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n           [9.7...e-01, 2.8...e-02, ...e-08]])\n    >>> clf.score(X, y)\n    0.97...\n    \"\"\"",
                "filename": "sklearn/linear_model/_logistic.py",
                "start_index": 34310,
                "end_index": 42477,
                "start_line": 176,
                "end_line": 2220,
                "max_line": 2222,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "if multi_class == \"multinomial\":\n        if solver in [\"lbfgs\", \"newton-cg\"]:\n            # scipy.optimize.minimize and newton-cg accept only ravelled parameters,\n            # i.e. 1d-arrays. LinearModelLoss expects classes to be contiguous and\n            # reconstructs the 2d-array via w0.reshape((n_classes, -1), order=\"F\").\n            # As w0 is F-contiguous, ravel(order=\"F\") also avoids a copy.\n            w0 = w0.ravel(order=\"F\")\n            loss = LinearModelLoss(\n                base_loss=HalfMultinomialLoss(n_classes=classes.size),\n                fit_intercept=fit_intercept,\n            )\n        target = Y_multi\n        if solver in \"lbfgs\":\n            func = loss.loss_gradient\n        elif solver == \"newton-cg\":\n            func = loss.loss\n            grad = loss.gradient\n            hess = loss.gradient_hessian_product  # hess = [gradient, hessp]\n        warm_start_sag = {\"coef\": w0.T}\n    else:\n        target = y_bin\n        if solver == \"lbfgs\":\n            loss = LinearModelLoss(\n                base_loss=HalfBinomialLoss(), fit_intercept=fit_intercept\n            )\n            func = loss.loss_gradient\n        elif solver == \"newton-cg\":\n            loss = LinearModelLoss(\n                base_loss=HalfBinomialLoss(), fit_intercept=fit_intercept\n            )\n            func = loss.loss\n            grad = loss.gradient\n            hess = loss.gradient_hessian_product  # hess = [gradient, hessp]\n        elif solver == \"newton-cholesky\":\n            loss = LinearModelLoss(\n                base_loss=HalfBinomialLoss(), fit_intercept=fit_intercept\n            )\n        warm_start_sag = {\"coef\": np.expand_dims(w0, axis=1)}\n\n    coefs = list()\n    n_iter = np.zeros(len(Cs), dtype=np.int32)",
                "filename": "sklearn/linear_model/_logistic.py",
                "start_index": 15981,
                "end_index": 17714,
                "start_line": 416,
                "end_line": 456,
                "max_line": 2222,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"\n=====================================================\nMNIST classification using multinomial logistic + L1\n=====================================================\n\nHere we fit a multinomial logistic regression with L1 penalty on a subset of\nthe MNIST digits classification task. We use the SAGA algorithm for this\npurpose: this a solver that is fast when the number of samples is significantly\nlarger than the number of features and is able to finely optimize non-smooth\nobjective functions which is the case with the l1-penalty. Test accuracy\nreaches > 0.8, while weight vectors remains *sparse* and therefore more easily\n*interpretable*.\n\nNote that this accuracy of this l1-penalized linear model is significantly\nbelow what can be reached by an l2-penalized linear model or a non-linear\nmulti-layer perceptron model on this dataset.\n\n\"\"\"\n\n# Author: Arthur Mensch <arthur.mensch@m4x.org>\n# License: BSD 3 clause\n\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import check_random_state\n\n# Turn down for faster convergence\nt0 = time.time()\ntrain_samples = 5000\n\n# Load data from https://www.openml.org/d/554\nX, y = fetch_openml(\n    \"mnist_784\", version=1, return_X_y=True, as_frame=False, parser=\"pandas\"\n)\n\nrandom_state = check_random_state(0)\npermutation = random_state.permutation(X.shape[0])\nX = X[permutation]\ny = y[permutation]\nX = X.reshape((X.shape[0], -1))\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=train_samples, test_size=10000\n)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Turn up tolerance for faster convergence\nclf = LogisticRegression(C=50.0 / train_samples, penalty=\"l1\", solver=\"saga\", tol=0.1)\nclf.fit(X_train, y_train)\nsparsity = np.mean(clf.coef_ == 0) * 100\nscore = clf.score(X_test, y_test)\n# print('Best C % .4f' % clf.C_)\nprint(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\nprint(\"Test score with L1 penalty: %.4f\" % score)\n\ncoef = clf.coef_.copy()\nplt.figure(figsize=(10, 5))\nscale = np.abs(coef).max()\nfor i in range(10):\n    l1_plot = plt.subplot(2, 5, i + 1)\n    l1_plot.imshow(\n        coef[i].reshape(28, 28),\n        interpolation=\"nearest\",\n        cmap=plt.cm.RdBu,\n        vmin=-scale,\n        vmax=scale,\n    )\n    l1_plot.set_xticks(())\n    l1_plot.set_yticks(())\n    l1_plot.set_xlabel(\"Class %i\" % i)\nplt.suptitle(\"Classification vector for...\")\n\nrun_time = time.time() - t0\nprint(\"Example run in %.3f s\" % run_time)\nplt.show()",
                "filename": "examples/linear_model/plot_sparse_logistic_regression_mnist.py",
                "start_index": 0,
                "end_index": 2697,
                "start_line": 1,
                "end_line": 85,
                "max_line": 85,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "if multi_class == \"ovr\":\n        w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)\n        mask = y == pos_class\n        y_bin = np.ones(y.shape, dtype=X.dtype)\n        if solver in [\"lbfgs\", \"newton-cg\", \"newton-cholesky\"]:\n            # HalfBinomialLoss, used for those solvers, represents y in [0, 1] instead\n            # of in [-1, 1].\n            mask_classes = np.array([0, 1])\n            y_bin[~mask] = 0.0\n        else:\n            mask_classes = np.array([-1, 1])\n            y_bin[~mask] = -1.0\n\n        # for compute_class_weight\n        if class_weight == \"balanced\":\n            class_weight_ = compute_class_weight(\n                class_weight, classes=mask_classes, y=y_bin\n            )\n            sample_weight *= class_weight_[le.fit_transform(y_bin)]\n\n    else:\n        if solver in [\"sag\", \"saga\", \"lbfgs\", \"newton-cg\"]:\n            # SAG, lbfgs and newton-cg multinomial solvers need LabelEncoder,\n            # not LabelBinarizer, i.e. y as a 1d-array of integers.\n            # LabelEncoder also saves memory compared to LabelBinarizer, especially\n            # when n_classes is large.\n            le = LabelEncoder()\n            Y_multi = le.fit_transform(y).astype(X.dtype, copy=False)\n        else:\n            # For liblinear solver, apply LabelBinarizer, i.e. y is one-hot encoded.\n            lbin = LabelBinarizer()\n            Y_multi = lbin.fit_transform(y)\n            if Y_multi.shape[1] == 1:\n                Y_multi = np.hstack([1 - Y_multi, Y_multi])\n\n        w0 = np.zeros(\n            (classes.size, n_features + int(fit_intercept)), order=\"F\", dtype=X.dtype\n        )",
                "filename": "sklearn/linear_model/_logistic.py",
                "start_index": 12948,
                "end_index": 14572,
                "start_line": 338,
                "end_line": 2213,
                "max_line": 2222,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/linear_model/_logistic.py": [
                {
                    "chunk": {
                        "code": "if coef is not None:\n        # it must work both giving the bias term and not\n        if multi_class == \"ovr\":\n            if coef.size not in (n_features, w0.size):\n                raise ValueError(\n                    \"Initialization coef is of shape %d, expected shape %d or %d\"\n                    % (coef.size, n_features, w0.size)\n                )\n            w0[: coef.size] = coef\n        else:\n            # For binary problems coef.shape[0] should be 1, otherwise it\n            # should be classes.size.\n            n_classes = classes.size\n            if n_classes == 2:\n                n_classes = 1\n\n            if coef.shape[0] != n_classes or coef.shape[1] not in (\n                n_features,\n                n_features + 1,\n            ):\n                raise ValueError(\n                    \"Initialization coef is of shape (%d, %d), expected \"\n                    \"shape (%d, %d) or (%d, %d)\"\n                    % (\n                        coef.shape[0],\n                        coef.shape[1],\n                        classes.size,\n                        n_features,\n                        classes.size,\n                        n_features + 1,\n                    )\n                )\n\n            if n_classes == 1:\n                w0[0, : coef.shape[1]] = -coef\n                w0[1, : coef.shape[1]] = coef\n            else:\n                w0[:, : coef.shape[1]] = coef",
                        "filename": "sklearn/linear_model/_logistic.py",
                        "start_index": 14578,
                        "end_index": 15975,
                        "start_line": 377,
                        "end_line": 414,
                        "max_line": 2222,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the exact line of code that is mentioned in the issue description as causing the bug, and it includes the proposed fix."
                },
                {
                    "chunk": {
                        "code": "default='lbfgs'\n\n        Algorithm to use in the optimization problem. Default is 'lbfgs'.\n        To choose a solver, you might want to consider the following aspects:\n\n            - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n              and 'saga' are faster for large ones;\n            - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n              'lbfgs' handle multinomial loss;\n            - 'liblinear' is limited to one-versus-rest schemes.\n            - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n              especially with one-hot encoded categorical features with rare\n              categories. Note that it is limited to binary classification and the\n              one-versus-rest reduction for multiclass classification. Be aware that\n              the memory usage of this solver has a quadratic dependency on\n              `n_features` because it explicitly computes the Hessian matrix.\n\n        .. warning::\n           The choice of the algorithm depends on the penalty chosen.\n           Supported penalties by solver:\n\n           - 'lbfgs'           -   ['l2', None]\n           - 'liblinear'       -   ['l1', 'l2']\n           - 'newton-cg'       -   ['l2', None]\n           - 'newton-cholesky' -   ['l2', None]\n           - 'sag'             -   ['l2', None]\n           - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n\n        .. note::\n           'sag' and 'saga' fast convergence is only guaranteed on features\n           with approximately the same scale. You can preprocess the data with\n           a scaler from :mod:`sklearn.preprocessing`.\n\n        .. seealso::\n           Refer to the User Guide for more information regarding\n           :class:`LogisticRegression` and more specifically the\n           :ref:`Table <Logistic_regression>`\n           summarizing solver/penalty supports.\n\n        .. versionadded:: 0.17\n           Stochastic Average Gradient descent solver.\n        .. versionadded:: 0.19\n           SAGA solver.\n        .. versionchanged:: 0.22\n            The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n        .. versionadded:: 1.2\n           newton-cholesky solver.\n\n    max_iter : int, default=100\n        Maximum number of iterations taken for the solvers to converge.\n\n    multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n        If the option chosen is 'ovr', then a binary problem is fit for each\n        label. For 'multinomial' the loss minimised is the multinomial loss fit\n        across the entire probability distribution, *even when the data is\n        binary*. 'multinomial' is unavailable when solver='liblinear'.\n        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n        and otherwise selects 'multinomial'.\n\n        .. versionadded:: 0.18\n           Stochastic Average Gradient descent solver for 'multinomial' case.\n        .. versionchanged:: 0.22\n            Default changed from 'ovr' to 'auto' in 0.22.\n\n    verbose : int, default=0\n        For the liblinear and lbfgs solvers set verbose to any positive\n        number for verbosity.\n\n    warm_start : bool, default=False\n        When set to True, reuse the solution of the previous call to fit as\n        initialization, otherwise, just erase the previous solution.\n        Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n\n        .. versionadded:: 0.17\n           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n\n    n_jobs : int, default=None\n        Number of CPU cores used when parallelizing over classes if\n        multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n        set to 'liblinear' regardless of whether 'multi_class' is specified or\n        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n        context. ``-1`` means using all processors.\n        See :term:`Glossary <n_jobs>` for more details.\n\n    l1_ratio : float, default=None\n        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n        to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n        combination of L1 and L2.\n\n    Attributes\n    ----------\n\n    classes_ : ndarray of shape (n_classes, )\n        A list of class labels known to the classifier.\n\n    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n        Coefficient of the features in the decision function.\n\n        `coef_` is of shape (1, n_features) when the given problem is binary.\n        In particular, when `multi_class='multinomial'`, `coef_` corresponds\n        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n\n    intercept_ : ndarray of shape (1,) or (n_classes,)\n        Intercept (a.k.a. bias) added to the decision function.\n\n        If `fit_intercept` is set to False, the intercept is set to zero.\n        `intercept_` is of shape (1,) when the given problem is binary.\n        In particular, when `multi_class='multinomial'`, `intercept_`\n        corresponds to outcome 1 (True) and `-intercept_` corresponds to\n        outcome 0 (False).\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : ndarray of shape (n_classes,) or (1, )\n        Actual number of iterations for all classes. If binary or multinomial,\n        it returns only 1 element. For liblinear solver, only the maximum\n        number of iteration across all classes is given.\n\n        .. versionchanged:: 0.20\n\n            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n\n    See Also\n    --------\n    SGDClassifier : Incrementally trained logistic regression (when given\n        the parameter ``loss=\"log_loss\"``).\n    LogisticRegressionCV : Logistic regression with built-in cross validation.\n\n    Notes\n    -----\n    The underlying C implementation uses a random number generator to\n    select features when fitting the model. It is thus not uncommon,\n    to have slightly different results for the same input data. If\n    that happens, try with a smaller tol parameter.\n\n    Predict output may not match that of standalone liblinear in certain\n    cases. See :ref:`differences from liblinear <liblinear_differences>`\n    in the narrative documentation.\n\n    References\n    ----------\n\n    L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n        Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n        http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n\n    LIBLINEAR -- A Library for Large Linear Classification\n        https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n\n    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n        Minimizing Finite Sums with the Stochastic Average Gradient\n        https://hal.inria.fr/hal-00860051/document\n\n    SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n            :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n            for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n\n    Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n        methods for logistic regression and maximum entropy models.\n        Machine Learning 85(1-2):41-75.\n        https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = LogisticRegression(random_state=0).fit(X, y)\n    >>> clf.predict(X[:2, :])\n    array([0, 0])\n    >>> clf.predict_proba(X[:2, :])\n    array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n           [9.7...e-01, 2.8...e-02, ...e-08]])\n    >>> clf.score(X, y)\n    0.97...\n    \"\"\"",
                        "filename": "sklearn/linear_model/_logistic.py",
                        "start_index": 34310,
                        "end_index": 42477,
                        "start_line": 176,
                        "end_line": 2220,
                        "max_line": 2222,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet provides context on the 'warm_start' parameter and how it is used in LogisticRegression, which is relevant to the issue of warm starting with multinomial logistic regression."
                },
                {
                    "chunk": {
                        "code": "if multi_class == \"ovr\":\n        w0 = np.zeros(n_features + int(fit_intercept), dtype=X.dtype)\n        mask = y == pos_class\n        y_bin = np.ones(y.shape, dtype=X.dtype)\n        if solver in [\"lbfgs\", \"newton-cg\", \"newton-cholesky\"]:\n            # HalfBinomialLoss, used for those solvers, represents y in [0, 1] instead\n            # of in [-1, 1].\n            mask_classes = np.array([0, 1])\n            y_bin[~mask] = 0.0\n        else:\n            mask_classes = np.array([-1, 1])\n            y_bin[~mask] = -1.0\n\n        # for compute_class_weight\n        if class_weight == \"balanced\":\n            class_weight_ = compute_class_weight(\n                class_weight, classes=mask_classes, y=y_bin\n            )\n            sample_weight *= class_weight_[le.fit_transform(y_bin)]\n\n    else:\n        if solver in [\"sag\", \"saga\", \"lbfgs\", \"newton-cg\"]:\n            # SAG, lbfgs and newton-cg multinomial solvers need LabelEncoder,\n            # not LabelBinarizer, i.e. y as a 1d-array of integers.\n            # LabelEncoder also saves memory compared to LabelBinarizer, especially\n            # when n_classes is large.\n            le = LabelEncoder()\n            Y_multi = le.fit_transform(y).astype(X.dtype, copy=False)\n        else:\n            # For liblinear solver, apply LabelBinarizer, i.e. y is one-hot encoded.\n            lbin = LabelBinarizer()\n            Y_multi = lbin.fit_transform(y)\n            if Y_multi.shape[1] == 1:\n                Y_multi = np.hstack([1 - Y_multi, Y_multi])\n\n        w0 = np.zeros(\n            (classes.size, n_features + int(fit_intercept)), order=\"F\", dtype=X.dtype\n        )",
                        "filename": "sklearn/linear_model/_logistic.py",
                        "start_index": 12948,
                        "end_index": 14572,
                        "start_line": 338,
                        "end_line": 2213,
                        "max_line": 2222,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the initialization of the coefficient array 'w0' for different types of logistic regression, which is relevant to the issue since the bug is related to the shape of 'w0' during warm start."
                }
            ]
        },
        "solution_task": "- The issue is related to the incorrect shape of the `coef_` attribute when using `LogisticRegression` with `multi_class='multinomial'` and `warm_start=True` for binary outcomes.\n- The `coef_` attribute should be a 2D array even for binary classification when `multi_class='multinomial'`.\n- The fix should ensure that `coef_` is correctly broadcasted into `w0` during warm start.\n\nSolution:\n- Modify the `fit` method in the `LogisticRegression` class to handle the shape of `coef_` correctly during warm start.\n- Check if `multi_class` is set to `'multinomial'` and if the problem is binary (i.e., `n_classes == 2`).\n- If the conditions are met and `warm_start` is `True`, reshape `coef_` to be a 2D array before using it to initialize `w0`.\n\n```python\nclass LogisticRegression:\n    # ... (other parts of the class)\n\n    def fit(self, X, y):\n        # ... (initial part of the fit method)\n\n        # Check if it is a warm start and if coef_ needs to be reshaped\n        if self.warm_start and hasattr(self, \"coef_\") and self.multi_class == \"multinomial\":\n            n_classes = len(np.unique(y))\n            if n_classes == 2:\n                self.coef_ = self.coef_.reshape(1, -1)\n\n        # ... (rest of the fit method)\n```\n\n- Ensure that the `coef_` attribute is reshaped only when transitioning from a binary to a multinomial problem.\n- This change should be placed before any operations that involve `coef_` during the warm start process.",
        "bouncer_response": {
            "score": 8,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE accurately identifies the core issue of the bug, which is the incorrect shape of the `coef_` attribute during warm start in a binary classification with `multi_class='multinomial'`. The solution provided is actionable and directly addresses the problem by suggesting a modification to the `fit` method of the `LogisticRegression` class to handle the shape of `coef_` correctly. The RESPONSE also includes a code snippet that demonstrates where and how to implement the proposed fix, which is highly relevant to the bug described in the PROMPT. The RESPONSE does not address all aspects of the bug, such as the specific line of code causing the issue, but it provides a clear direction for a fix, which is why it scores an 8.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711741701.1798558,
        "result": "- The issue is related to the incorrect shape of the `coef_` attribute when using `LogisticRegression` with `multi_class='multinomial'` and `warm_start=True` for binary outcomes.\n- The `coef_` attribute should be a 2D array even for binary classification when `multi_class='multinomial'`.\n- The fix should ensure that `coef_` is correctly broadcasted into `w0` during warm start.\n\nSolution:\n- Modify the `fit` method in the `LogisticRegression` class to handle the shape of `coef_` correctly during warm start.\n- Check if `multi_class` is set to `'multinomial'` and if the problem is binary (i.e., `n_classes == 2`).\n- If the conditions are met and `warm_start` is `True`, reshape `coef_` to be a 2D array before using it to initialize `w0`.\n\n```python\nclass LogisticRegression:\n    # ... (other parts of the class)\n\n    def fit(self, X, y):\n        # ... (initial part of the fit method)\n\n        # Check if it is a warm start and if coef_ needs to be reshaped\n        if self.warm_start and hasattr(self, \"coef_\") and self.multi_class == \"multinomial\":\n            n_classes = len(np.unique(y))\n            if n_classes == 2:\n                self.coef_ = self.coef_.reshape(1, -1)\n\n        # ... (rest of the fit method)\n```\n\n- Ensure that the `coef_` attribute is reshaped only when transitioning from a binary to a multinomial problem.\n- This change should be placed before any operations that involve `coef_` during the warm start process.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/linear_model)- [Lines 377 - 414](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_logistic.py#L377-L414)\n\n This snippet contains the exact line of code that is mentioned in the issue description as causing the bug, and it includes the proposed fix. \n\n\n   - [Lines 176 - 2220](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_logistic.py#L176-L2220)\n\n This snippet provides context on the 'warm_start' parameter and how it is used in LogisticRegression, which is relevant to the issue of warm starting with multinomial logistic regression. \n\n\n   - [Lines 338 - 2213](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_logistic.py#L338-L2213)\n\n This snippet shows the initialization of the coefficient array 'w0' for different types of logistic regression, which is relevant to the issue since the bug is related to the shape of 'w0' during warm start."
    }
}