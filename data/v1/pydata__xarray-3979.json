{
    "id": "pydata__xarray-3979",
    "repo_name": "testbed/pydata__xarray__0.12",
    "task_id": 6566835244217292658,
    "title": "xr.full_like (often) fails when other is chunked and fill_value is non-scalar",
    "body": "I've been running into some issues when using `xr.full_like`, when my `other.data` is a chunked dask array, and the `fill_value` is a numpy array.\r\n\r\nNow, I just checked, ``full_like`` mentions only scalar in the signature. However, this is a very convenient way to get all the coordinates and dimensions attached to an array like this, so it feels like desirable functionality. And as I mention below, both numpy and dask function similary, taking much more than just scalars.\r\nhttps://xarray.pydata.org/en/stable/generated/xarray.full_like.html\r\n\r\n#### MCVE Code Sample\r\n```python\r\nx = [1, 2, 3, 4]\r\ny = [1, 2, 3]\r\nda1 = xr.DataArray(dask.array.ones((3, 4), chunks=(1, 4)), {\"y\": y, \"x\": x}, (\"y\", \"x\"))\r\nda2 = xr.full_like(da1, np.ones((3, 4)))\r\nprint(da2.values)\r\n```\r\n\r\nThis results in an error:\r\n`ValueError: could not broadcast input array from shape (1,3) into shape (1,4)`\r\n\r\n#### Expected Output\r\nExpected is a DataArray with the dimensions and coords of `other`, and the numpy array of `fill_value` as its data.\r\n\r\n#### Problem Description\r\nThe issue lies here: https://github.com/pydata/xarray/blob/2c77eb531b6689f9f1d2adbde0d8bf852f1f7362/xarray/core/common.py#L1420-L1436\r\n\r\nCalling `dask.array.full` with the given number of chunks results in it trying to to apply the `fill_value` for every individual chunk.\r\n\r\nAs one would expect, if I set `fill_value` to the size of a single chunk it doesn't error:\r\n```python\r\nda2 = xr.full_like(da1, np.ones((1, 4)))\r\nprint(da2.values)\r\n```\r\n\r\nIt does fail on a similarly chunked dask array (since it's applying it for every chunk):\r\n```python\r\nda2 = xr.full_like(da1, dask.array.ones((3, 4)))\r\nprint(da2.values)\r\n```\r\n\r\nThe most obvious solution would be to force it down the `np.full_like` route, since all the values already exist in memory anyway. So maybe another type check does the trick. However, `full()` accepts quite a variety of arguments for the fill value (scalars, numpy arrays, lists, tuples, ranges). The dask docs mention only a scalar in the signature for ``dask.array.full``:\r\nhttps://docs.dask.org/en/latest/array-api.html#dask.array.full\r\nAs does numpy.full:\r\nhttps://docs.scipy.org/doc/numpy/reference/generated/numpy.full.html\r\n\r\nHowever, in all cases, they still broadcast automatically...\r\n```python\r\na = np.full((2, 2), [1, 2]\r\n>>> array([[1, 2],\r\n       [1, 2]])\r\n```\r\n\r\nSo kind of undefined behavior of a blocked `full`?\r\n\r\n#### Versions\r\n\r\n<details><summary>Output of `xr.show_versions()`</summary>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 21:48:41) [MSC v.1916 64 bit (AMD64)]\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 158 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en\r\nLOCALE: None.None\r\nlibhdf5: 1.10.5\r\nlibnetcdf: 4.7.3\r\n\r\nxarray: 0.15.1\r\npandas: 0.25.3\r\nnumpy: 1.17.5\r\nscipy: 1.3.1\r\nnetCDF4: 1.5.3\r\npydap: None\r\nh5netcdf: None\r\nh5py: 2.10.0\r\nNio: None\r\nzarr: 2.4.0\r\ncftime: 1.0.4.2\r\nnc_time_axis: None\r\nPseudoNetCDF: None\r\nrasterio: 1.1.2\r\ncfgrib: None\r\niris: None\r\nbottleneck: 1.3.2\r\ndask: 2.9.2\r\ndistributed: 2.10.0\r\nmatplotlib: 3.1.2\r\ncartopy: None\r\nseaborn: 0.10.0\r\nnumbagg: None\r\nsetuptools: 46.1.3.post20200325\r\npip: 20.0.2\r\nconda: None\r\npytest: 5.3.4\r\nIPython: 7.13.0\r\nsphinx: 2.3.1\r\n</details>\r\n\nxr.full_like (often) fails when other is chunked and fill_value is non-scalar\nI've been running into some issues when using `xr.full_like`, when my `other.data` is a chunked dask array, and the `fill_value` is a numpy array.\r\n\r\nNow, I just checked, ``full_like`` mentions only scalar in the signature. However, this is a very convenient way to get all the coordinates and dimensions attached to an array like this, so it feels like desirable functionality. And as I mention below, both numpy and dask function similary, taking much more than just scalars.\r\nhttps://xarray.pydata.org/en/stable/generated/xarray.full_like.html\r\n\r\n#### MCVE Code Sample\r\n```python\r\nx = [1, 2, 3, 4]\r\ny = [1, 2, 3]\r\nda1 = xr.DataArray(dask.array.ones((3, 4), chunks=(1, 4)), {\"y\": y, \"x\": x}, (\"y\", \"x\"))\r\nda2 = xr.full_like(da1, np.ones((3, 4)))\r\nprint(da2.values)\r\n```\r\n\r\nThis results in an error:\r\n`ValueError: could not broadcast input array from shape (1,3) into shape (1,4)`\r\n\r\n#### Expected Output\r\nExpected is a DataArray with the dimensions and coords of `other`, and the numpy array of `fill_value` as its data.\r\n\r\n#### Problem Description\r\nThe issue lies here: https://github.com/pydata/xarray/blob/2c77eb531b6689f9f1d2adbde0d8bf852f1f7362/xarray/core/common.py#L1420-L1436\r\n\r\nCalling `dask.array.full` with the given number of chunks results in it trying to to apply the `fill_value` for every individual chunk.\r\n\r\nAs one would expect, if I set `fill_value` to the size of a single chunk it doesn't error:\r\n```python\r\nda2 = xr.full_like(da1, np.ones((1, 4)))\r\nprint(da2.values)\r\n```\r\n\r\nIt does fail on a similarly chunked dask array (since it's applying it for every chunk):\r\n```python\r\nda2 = xr.full_like(da1, dask.array.ones((3, 4)))\r\nprint(da2.values)\r\n```\r\n\r\nThe most obvious solution would be to force it down the `np.full_like` route, since all the values already exist in memory anyway. So maybe another type check does the trick. However, `full()` accepts quite a variety of arguments for the fill value (scalars, numpy arrays, lists, tuples, ranges). The dask docs mention only a scalar in the signature for ``dask.array.full``:\r\nhttps://docs.dask.org/en/latest/array-api.html#dask.array.full\r\nAs does numpy.full:\r\nhttps://docs.scipy.org/doc/numpy/reference/generated/numpy.full.html\r\n\r\nHowever, in all cases, they still broadcast automatically...\r\n```python\r\na = np.full((2, 2), [1, 2]\r\n>>> array([[1, 2],\r\n       [1, 2]])\r\n```\r\n\r\nSo kind of undefined behavior of a blocked `full`?\r\n\r\n#### Versions\r\n\r\n<details><summary>Output of `xr.show_versions()`</summary>\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 21:48:41) [MSC v.1916 64 bit (AMD64)]\r\npython-bits: 64\r\nOS: Windows\r\nOS-release: 10\r\nmachine: AMD64\r\nprocessor: Intel64 Family 6 Model 158 Stepping 9, GenuineIntel\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en\r\nLOCALE: None.None\r\nlibhdf5: 1.10.5\r\nlibnetcdf: 4.7.3\r\n\r\nxarray: 0.15.1\r\npandas: 0.25.3\r\nnumpy: 1.17.5\r\nscipy: 1.3.1\r\nnetCDF4: 1.5.3\r\npydap: None\r\nh5netcdf: None\r\nh5py: 2.10.0\r\nNio: None\r\nzarr: 2.4.0\r\ncftime: 1.0.4.2\r\nnc_time_axis: None\r\nPseudoNetCDF: None\r\nrasterio: 1.1.2\r\ncfgrib: None\r\niris: None\r\nbottleneck: 1.3.2\r\ndask: 2.9.2\r\ndistributed: 2.10.0\r\nmatplotlib: 3.1.2\r\ncartopy: None\r\nseaborn: 0.10.0\r\nnumbagg: None\r\nsetuptools: 46.1.3.post20200325\r\npip: 20.0.2\r\nconda: None\r\npytest: 5.3.4\r\nIPython: 7.13.0\r\nsphinx: 2.3.1\r\n</details>\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/pydata__xarray__0.12.git",
    "requested_commit": "",
    "mayil_response": "Doesn't seem like we have fully grasped all the specifics of this ticket \ud83d\ude15.                   But we've outlined some ideas that might help you get started:\n\n- The issue is caused by passing a non-scalar `fill_value` (a numpy array) to `xr.full_like`, which expects a scalar or a dict-like object when `other` is a Dataset.\n- The `xr.full_like` function is designed to create a new DataArray or Dataset with the same shape as the input, filled with the provided `fill_value`.\n- The error message indicates that broadcasting of the non-scalar `fill_value` is failing because the shapes are incompatible.\n\nTo resolve the issue:\n- Change the `fill_value` argument in the `xr.full_like` function call to be a scalar value instead of a numpy array.\n- The correct usage of `xr.full_like` should be with a scalar `fill_value` that can be broadcast across the entire shape of `da1`.\n\nHere is the corrected code snippet:\n```python\nda1 = xr.DataArray(dask.array.ones((3, 4), chunks=(1, 4)), {\"y\": y, \"x\": x}, (\"y\", \"x\"))\nda2 = xr.full_like(da1, 1)  # Use a scalar value for fill_value\n```\n\nThis will create `da2` as a DataArray with the same shape and chunks as `da1`, filled with ones.\n\n### Code snippets to check\n\n-  \u2192 [xarray](https://github.com/testbed/pydata__xarray__0.12/tree//xarray) \u2192 [core](https://github.com/testbed/pydata__xarray__0.12/tree//xarray/core)- [Lines 39 - 1685](https://github.com/testbed/pydata__xarray__0.12/blob//xarray/core/common.py#L39-L1685)\n\n This snippet contains the implementation of `full_like` where the issue with non-scalar `fill_value` and chunked arrays is likely to be located. \n\n\n   - [Lines 1434 - 1995](https://github.com/testbed/pydata__xarray__0.12/blob//xarray/core/common.py#L1434-L1995)\n\n This snippet includes the function signatures for `full_like` which may need to be reviewed to ensure they align with the expected behavior when handling non-scalar `fill_value`. \n\n\n   - [Lines 214 - 1995](https://github.com/testbed/pydata__xarray__0.12/blob//xarray/core/common.py#L214-L1995)\n\n This snippet contains the inner function `_full_like_variable` which is called by `full_like` and may need to be modified to handle non-scalar `fill_value` correctly with chunked arrays.",
    "ai_cost": {
        "search_embedding": 2.6e-07,
        "issue_classifier": 0.0007295,
        "snippet_processor": 0.07126,
        "issue_star_creation": 0.05117,
        "issue_star_solver": 0.07,
        "bouncer": 0.04018000000000001
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711759604.857899,
        "relevant_snippets": [
            {
                "code": "taarray import DataArray\n    from xarray.core.dataset import Dataset\n    from xarray.core.variable import Variable\n\n    if not is_scalar(fill_value) and not (\n        isinstance(other, Dataset) and isinstance(fill_value, dict)\n    ):\n        raise ValueError(\n            f\"fill_value must be scalar or, for datasets, a dict-like. Received {fill_value} instead.\"\n        )\n\n    if isinstance(other, Dataset):\n        if not isinstance(fill_value, dict):\n            fill_value = {k: fill_value for k in other.data_vars.keys()}\n\n        dtype_: Mapping[Any, DTypeLikeSave]\n        if not isinstance(dtype, Mapping):\n            dtype_ = {k: dtype for k in other.data_vars.keys()}\n        else:\n            dtype_ = dtype\n\n        data_vars = {\n            k: _full_like_variable(\n                v.variable,\n                fill_value.get(k, dtypes.NA),\n                dtype_.get(k, None),\n                chunks,\n                chunked_array_type,\n                from_array_kwargs,\n            )\n            for k, v in other.data_vars.items()\n        }\n        return Dataset(data_vars, coords=other.coords, attrs=other.attrs)\n    elif isinstance(other, DataArray):\n        if isinstance(dtype, Mapping):\n            raise ValueError(\"'dtype' cannot be dict-like when passing a DataArray\")\n        return DataArray(\n            _full_like_variable(\n                other.variable,\n                fill_value,\n                dtype,\n                chunks,\n                chunked_array_type,\n                from_array_kwargs,\n            ),\n            dims=other.dims,\n            coords=other.coords,\n            attrs=other.attrs,\n            name=other.name,\n        )\n    elif isinstance(other, Variable):\n        if isinstance(dtype, Mapping):\n            raise ValueError(\"'dtype' cannot be dict-like when passing a Variable\")\n        return _full_like_variable(\n            other, fill_value, dtype, chunks, chunked_array_type, from_array_kwargs\n        )\n    else:\n        raise TypeError(\"Expected DataArray, Dataset, or Variable\")\n\n\ndef _full_like_variab",
                "filename": "xarray/core/common.py",
                "start_index": 56815,
                "end_index": 58885,
                "start_line": 39,
                "end_line": 1685,
                "max_line": 2054,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": ""
            },
            {
                "code": "ke(\n    other: DataArray,\n    fill_value: Any,\n    dtype: DTypeLikeSave | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> DataArray:\n    ...\n\n\n@overload\ndef full_like(\n    other: Dataset,\n    fill_value: Any,\n    dtype: DTypeMaybeMapping | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Dataset:\n    ...\n\n\n@overload\ndef full_like(\n    other: Variable,\n    fill_value: Any,\n    dtype: DTypeLikeSave | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Variable:\n    ...\n\n\n@overload\ndef full_like(\n    other: Dataset | DataArray,\n    fill_value: Any,\n    dtype: DTypeMaybeMapping | None = None,\n    *,\n    chunks: T_Chunks = {},\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Dataset | DataArray:\n    ...\n\n\n@overload\ndef full_like(\n    other: Dataset | DataArray | Variable,\n    fill_value: Any,\n    dtype: DTypeMaybeMapping | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Dataset | DataArray | Variable:\n    ...\n\n\ndef full_like(\n    ot",
                "filename": "xarray/core/common.py",
                "start_index": 51091,
                "end_index": 52487,
                "start_line": 1434,
                "end_line": 1995,
                "max_line": 2054,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": ""
            },
            {
                "code": "w object with the same shape and type as a given object.\n\n    Returned object will be chunked if if the given object is chunked, or if chunks or chunked_array_type are specified.\n\n    Parameters\n    ----------\n    other : DataArray, Dataset or Variable\n        The reference object in input\n    fill_value : scalar or dict-like\n        Value to fill the new object with before returning it. If\n        other is a Dataset, may also be a dict-like mapping data\n        variables to fill values.\n    dtype : dtype or dict-like of dtype, optional\n        dtype of the new array. If a dict-like, maps dtypes to\n        variables. If omitted, it defaults to other.dtype.\n    chunks : int, \"auto\", tuple of int or mapping of Hashable to int, optional\n        Chunk sizes along each dimension, e.g., ``5``, ``\"auto\"``, ``(5, 5)`` or\n        ``{\"x\": 5, \"y\": 5}``.\n    chunked_array_type: str, optional\n        Which chunked array type to coerce the underlying data array to.\n        Defaults to 'dask' if installed, else whatever is registered via the `ChunkManagerEnetryPoint` system.\n        Experimental API that should not be relied upon.\n    from_array_kwargs: dict, optional\n        Additional keyword arguments passed on to the `ChunkManagerEntrypoint.from_array` method used to create\n        chunked arrays, via whichever chunk manager is specified through the `chunked_array_type` kwarg.\n        For example, with dask as the default chunked array type, this method would pass additional kwargs\n        to :py:func:`dask.array.from_array`. Experimental API that should not be relied upon.\n\n    Returns\n    -------\n    out : same as object\n        New object with the same shape and type as other, with the data\n        filled with fill_value. Coords will be copied from other.\n        If other is based on dask, the new one will be as well, and will be\n        split in the same chunks.\n\n    Examples\n    --------\n    >>> x = xr.DataArray(\n    ...     np.arange(6).reshape(2, 3),\n    ...     dims=[\"lat\", \"lon\"],\n    ...     coords={\"lat\": [1, 2], \"lon\": [0, 1, 2]},\n    ... )\n    >>> x\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    Coordinates:\n      * lat      (lat) int64 1 2\n      * lon      (lon) int64 0 1 2\n\n    >>> xr.full_like(x, 1)\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[1, 1, 1],\n           [1, 1, 1]])\n    Coordinates:\n      * lat      (lat) int64 1 2\n      * lon      (lon) int64 0 1 2\n\n    >>> xr.full_like(x, 0.5)\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[0, 0, 0],\n           [0, 0, 0]])\n    Coordinates:\n      * lat      (lat) int64 1 2\n      * lon      (lon) int64 0 1 2\n\n    >>> xr.full_like(x, 0.5, dtype=np.double)\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[0.5, 0.5, 0.5],\n           [0.5, 0.5, 0.5]])\n    Coordinates:\n      * lat      (lat) int64 1 2\n      * lon      (lon) int64 0 1 2\n\n    >>> xr.full_like(x, np.nan, dtype=np.double)\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[nan, nan, nan],\n           [nan, nan, nan]])\n    Coordinates:\n      * lat      (lat) int64 1 2\n      * lon      (lon) int64 0 1 2\n\n    >>> ds = xr.Dataset(\n    ...     {\"a\": (\"x\", [3, 5, 2]), \"b\": (\"x\", [9, 1, 0])}, coords={\"x\": [2, 4, 6]}\n    ... )\n    >>> ds\n    <xarray.Dataset>\n    Dimensions:  (x: 3)\n    Coordinates:\n      * x        (x) int64 2 4 6\n    Data variables:\n        a        (x) int64 3 5 2\n        b        (x) int64 9 1 0\n    >>> xr.full_like(ds, fill_value={\"a\": 1, \"b\": 2})\n    <xarray.Dataset>\n    Dimensions:  (x: 3)\n    Coordinates:\n      * x        (x) int64 2 4 6\n    Data variables:\n        a        (x) int64 1 1 1\n        b        (x) int64 2 2 2\n    >>> xr.full_like(ds, fill_value={\"a\": 1, \"b\": 2}, dtype={\"a\": bool, \"b\": float})\n    <xarray.Dataset>\n    Dimensions:  (x: 3)\n    Coordinates:\n      * x        (x) int64 2 4 6\n    Data variables:\n        a        (x) bool True True True\n        b        (x) float64 2.0 2.0 2.0\n\n    See Also\n    --------\n    zeros_like\n    ones_like\n\n    \"\"\"\n    from xarray.core.da",
                "filename": "xarray/core/common.py",
                "start_index": 52781,
                "end_index": 56815,
                "start_line": 1508,
                "end_line": 1626,
                "max_line": 2054,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": ""
            },
            {
                "code": "le(\n    other: Variable,\n    fill_value: Any,\n    dtype: DTypeLike | None = None,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Variable:\n    \"\"\"Inner function of full_like, where other must be a variable\"\"\"\n    from xarray.core.variable import Variable\n\n    if fill_value is dtypes.NA:\n        fill_value = dtypes.get_fill_value(dtype if dtype is not None else other.dtype)\n\n    if (\n        is_chunked_array(other.data)\n        or chunked_array_type is not None\n        or chunks is not None\n    ):\n        if chunked_array_type is None:\n            chunkmanager = get_chunked_array_type(other.data)\n        else:\n            chunkmanager = guess_chunkmanager(chunked_array_type)\n\n        if dtype is None:\n            dtype = other.dtype\n\n        if from_array_kwargs is None:\n            from_array_kwargs = {}\n\n        data = chunkmanager.array_api.full(\n            other.shape,\n            fill_value,\n            dtype=dtype,\n            chunks=chunks if chunks else other.data.chunks,\n            **from_array_kwargs,\n        )\n    else:\n        data = np.full_like(other.data, fill_value, dtype=dtype)\n\n    return Variable(dims=other.dims, data=data, attrs=other.attrs)\n\n\n@overload\ndef zeros_like(\n    other: DataArray,\n    dtype: DTypeLikeSave | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> DataArray:\n    ...\n\n\n@overload\ndef zeros_like(\n    other: Dataset,\n    dtype: DTypeMaybeMapping | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Dataset:\n    ...\n\n\n@overload\ndef zeros_like(\n    other: Variable,\n    dtype: DTypeLikeSave | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Variable:\n    ...\n\n\n@overload\ndef zeros_like(\n    other: Dataset | DataArray,\n    dtype: DTypeMaybeMapping | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Dataset | DataArray:\n    ...\n\n\n@overload\ndef zeros_like(\n    other: Dataset | DataArray | Variable,\n    dtype: DTypeMaybeMapping | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Dataset | DataArray | Variable:\n    ...\n\n\ndef zeros_like(\n    o",
                "filename": "xarray/core/common.py",
                "start_index": 58885,
                "end_index": 61470,
                "start_line": 214,
                "end_line": 1995,
                "max_line": 2054,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": ""
            },
            {
                "code": "her: Dataset | DataArray | Variable,\n    dtype: DTypeMaybeMapping | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Dataset | DataArray | Variable:\n    \"\"\"Return a new object of ones with the same shape and\n    type as a given dataarray or dataset.\n\n    Parameters\n    ----------\n    other : DataArray, Dataset, or Variable\n        The reference object. The output will have the same dimensions and coordinates as this object.\n    dtype : dtype, optional\n        dtype of the new array. If omitted, it defaults to other.dtype.\n    chunks : int, \"auto\", tuple of int or mapping of Hashable to int, optional\n        Chunk sizes along each dimension, e.g., ``5``, ``\"auto\"``, ``(5, 5)`` or\n        ``{\"x\": 5, \"y\": 5}``.\n    chunked_array_type: str, optional\n        Which chunked array type to coerce the underlying data array to.\n        Defaults to 'dask' if installed, else whatever is registered via the `ChunkManagerEnetryPoint` system.\n        Experimental API that should not be relied upon.\n    from_array_kwargs: dict, optional\n        Additional keyword arguments passed on to the `ChunkManagerEntrypoint.from_array` method used to create\n        chunked arrays, via whichever chunk manager is specified through the `chunked_array_type` kwarg.\n        For example, with dask as the default chunked array type, this method would pass additional kwargs\n        to :py:func:`dask.array.from_array`. Experimental API that should not be relied upon.\n\n    Returns\n    -------\n    out : same as object\n        New object of ones with the same shape and type as other.\n\n    Examples\n    --------\n    >>> x = xr.DataArray(\n    ...     np.arange(6).reshape(2, 3),\n    ...     dims=[\"lat\", \"lon\"],\n    ...     coords={\"lat\": [1, 2], \"lon\": [0, 1, 2]},\n    ... )\n    >>> x\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    Coordinates:\n      * lat      (lat) int64 1 2\n      * lon      (lon) int64 0 1 2\n\n    >>> xr.ones_like(x)\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[1, 1, 1],\n           [1, 1, 1]])\n    Coordinates:\n      * lat      (lat) int64 1 2\n      * lon      (lon) int64 0 1 2\n\n    See Also\n    --------\n    zeros_like\n    full_like\n\n    \"\"\"\n    return full_like(\n        other,\n        1,\n        dtype,\n        chunks=chunks,\n        chunked_array_type=chunked_array_type,\n        from_array_kwargs=from_array_kwargs,\n    )\n\n\ndef get_chunksizes(",
                "filename": "xarray/core/common.py",
                "start_index": 65503,
                "end_index": 68008,
                "start_line": 1487,
                "end_line": 2004,
                "max_line": 2054,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": ""
            },
            {
                "code": "ther: Dataset | DataArray | Variable,\n    dtype: DTypeMaybeMapping | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Dataset | DataArray | Variable:\n    \"\"\"Return a new object of zeros with the same shape and\n    type as a given dataarray or dataset.\n\n    Parameters\n    ----------\n    other : DataArray, Dataset or Variable\n        The reference object. The output will have the same dimensions and coordinates as this object.\n    dtype : dtype, optional\n        dtype of the new array. If omitted, it defaults to other.dtype.\n    chunks : int, \"auto\", tuple of int or mapping of Hashable to int, optional\n        Chunk sizes along each dimension, e.g., ``5``, ``\"auto\"``, ``(5, 5)`` or\n        ``{\"x\": 5, \"y\": 5}``.\n    chunked_array_type: str, optional\n        Which chunked array type to coerce the underlying data array to.\n        Defaults to 'dask' if installed, else whatever is registered via the `ChunkManagerEnetryPoint` system.\n        Experimental API that should not be relied upon.\n    from_array_kwargs: dict, optional\n        Additional keyword arguments passed on to the `ChunkManagerEntrypoint.from_array` method used to create\n        chunked arrays, via whichever chunk manager is specified through the `chunked_array_type` kwarg.\n        For example, with dask as the default chunked array type, this method would pass additional kwargs\n        to :py:func:`dask.array.from_array`. Experimental API that should not be relied upon.\n\n    Returns\n    -------\n    out : DataArray, Dataset or Variable\n        New object of zeros with the same shape and type as other.\n\n    Examples\n    --------\n    >>> x = xr.DataArray(\n    ...     np.arange(6).reshape(2, 3),\n    ...     dims=[\"lat\", \"lon\"],\n    ...     coords={\"lat\": [1, 2], \"lon\": [0, 1, 2]},\n    ... )\n    >>> x\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[0, 1, 2],\n           [3, 4, 5]])\n    Coordinates:\n      * lat      (lat) int64 1 2\n      * lon      (lon) int64 0 1 2\n\n    >>> xr.zeros_like(x)\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[0, 0, 0],\n           [0, 0, 0]])\n    Coordinates:\n      * lat      (lat) int64 1 2\n      * lon      (lon) int64 0 1 2\n\n    >>> xr.zeros_like(x, dtype=float)\n    <xarray.DataArray (lat: 2, lon: 3)>\n    array([[0., 0., 0.],\n           [0., 0., 0.]])\n    Coordinates:\n      * lat      (lat) int64 1 2\n      * lon      (lon) int64 0 1 2\n\n    See Also\n    --------\n    ones_like\n    full_like\n\n    \"\"\"\n    return full_like(\n        other,\n        0,\n        dtype,\n        chunks=chunks,\n        chunked_array_type=chunked_array_type,\n        from_array_kwargs=from_array_kwargs,\n    )\n\n\n@overload\ndef ones_like(\n    other: DataArray,\n    dtype: DTypeLikeSave | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> DataArray:\n    ...\n\n\n@overload\ndef ones_li",
                "filename": "xarray/core/common.py",
                "start_index": 61470,
                "end_index": 64455,
                "start_line": 1487,
                "end_line": 1930,
                "max_line": 2054,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": ""
            },
            {
                "code": "name: \ud83d\udca1 Feature Request\ndescription: Suggest an idea for xarray\nlabels: [enhancement]\nbody:\n  - type: textarea\n    id: description\n    attributes:\n      label: Is your feature request related to a problem?\n      description: |\n        Please do a quick search of existing issues to make sure that this has not been asked before.\n        Please provide a clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n    validations:\n      required: true\n  - type: textarea\n    id: solution\n    attributes:\n      label: Describe the solution you'd like\n      description: |\n        A clear and concise description of what you want to happen.\n  - type: textarea\n    id: alternatives\n    attributes:\n      label: Describe alternatives you've considered\n      description: |\n        A clear and concise description of any alternative solutions or features you've considered.\n    validations:\n      required: false\n  - type: textarea\n    id: additional-context\n    attributes:\n      label: Additional context\n      description: |\n        Add any other context about the feature request here.\n    validations:\n      required: false",
                "filename": ".github/ISSUE_TEMPLATE/newfeature.yml",
                "start_index": 0,
                "end_index": 1154,
                "start_line": 1,
                "end_line": 35,
                "max_line": 35,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": ""
            },
            {
                "code": "import pandas as pd\n\nimport xarray as xr\n\nfrom . import parameterized, randn, requires_dask\n\n\ndef make_bench_data(shape, frac_nan, chunks):\n    vals = randn(shape, frac_nan)\n    coords = {\"time\": pd.date_range(\"2000-01-01\", freq=\"D\", periods=shape[0])}\n    da = xr.DataArray(vals, dims=(\"time\", \"x\", \"y\"), coords=coords)\n\n    if chunks is not None:\n        da = da.chunk(chunks)\n\n    return da\n\n\nclass DataArrayMissingInterpolateNA:\n    def setup(self, shape, chunks, limit):\n        if chunks is not None:\n            requires_dask()\n        self.da = make_bench_data(shape, 0.1, chunks)\n\n    @parameterized(\n        [\"shape\", \"chunks\", \"limit\"],\n        (\n            [(365, 75, 75)],\n            [None, {\"x\": 25, \"y\": 25}],\n            [None, 3],\n        ),\n    )\n    def time_interpolate_na(self, shape, chunks, limit):\n        actual = self.da.interpolate_na(dim=\"time\", method=\"linear\", limit=limit)\n\n        if chunks is not None:\n            actual = actual.compute()\n\n\nclass DataArrayMissingBottleneck:\n    def setup(self, shape, chunks, limit):\n        if chunks is not None:\n            requires_dask()\n        self.da = make_bench_data(shape, 0.1, chunks)\n\n    @parameterized(\n        [\"shape\", \"chunks\", \"limit\"],\n        (\n            [(365, 75, 75)],\n            [None, {\"x\": 25, \"y\": 25}],\n            [None, 3],\n        ),\n    )\n    def time_ffill(self, shape, chunks, limit):\n        actual = self.da.ffill(dim=\"time\", limit=limit)\n\n        if chunks is not None:\n            actual = actual.compute()\n\n    @parameterized(\n        [\"shape\", \"chunks\", \"limit\"],\n        (\n            [(365, 75, 75)],\n            [None, {\"x\": 25, \"y\": 25}],\n            [None, 3],\n        ),\n    )\n    def time_bfill(self, shape, chunks, limit):\n        actual = self.da.ffill(dim=\"time\", limit=limit)\n\n        if chunks is not None:\n            actual = actual.compute()",
                "filename": "asv_bench/benchmarks/dataarray_missing.py",
                "start_index": 0,
                "end_index": 1871,
                "start_line": 1,
                "end_line": 72,
                "max_line": 72,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": ""
            },
            {
                "code": "#    pint doesn't implement `np.full_like` in a way that's\n        #    currently compatible.\n        sparse_array_type = array_type(\"sparse\")\n        needs_full_reindex = any(\n            is_duck_dask_array(v.data)\n            or isinstance(v.data, sparse_array_type)\n            or not isinstance(v.data, np.ndarray)\n            for v in nonindexes\n        )\n\n        for d in dims:\n            if needs_full_reindex:\n                result = result._unstack_full_reindex(\n                    d, stacked_indexes[d], fill_value, sparse\n                )\n            else:\n                result = result._unstack_once(d, stacked_indexes[d], fill_value, sparse)\n        return result",
                "filename": "xarray/core/dataset.py",
                "start_index": 206205,
                "end_index": 206888,
                "start_line": 5484,
                "end_line": 9713,
                "max_line": 10260,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": ""
            },
            {
                "code": "_MINCOUNT_DOCSTRING = \"\"\"\nmin_count : int, default: None\n    The required number of valid values to perform the operation. If\n    fewer than min_count non-NA values are present the result will be\n    NA. Only used if skipna is set to True or defaults to True for the\n    array's dtype. New in version 0.10.8: Added with the default being\n    None. Changed in version 0.17.0: if specified on an integer array\n    and skipna=True, the result will be a float array.\"\"\"\n\n\ndef fillna(data, other, join=\"left\", dataset_join=\"left\"):\n    \"\"\"Fill missing values in this object with data from the other object.\n    Follows normal broadcasting and alignment rules.\n\n    Parameters\n    ----------\n    join : {\"outer\", \"inner\", \"left\", \"right\"}, optional\n        Method for joining the indexes of the passed objects along each\n        dimension\n        - \"outer\": use the union of object indexes\n        - \"inner\": use the intersection of object indexes\n        - \"left\": use indexes from the first object with each dimension\n        - \"right\": use indexes from the last object with each dimension\n        - \"exact\": raise `ValueError` instead of aligning when indexes to be\n          aligned are not equal\n    dataset_join : {\"outer\", \"inner\", \"left\", \"right\"}, optional\n        Method for joining variables of Dataset objects with mismatched\n        data variables.\n        - \"outer\": take variables from both Dataset objects\n        - \"inner\": take only overlapped variables\n        - \"left\": take only variables from the first object\n        - \"right\": take only variables from the last object\n    \"\"\"\n    from xarray.core.computation import apply_ufunc\n\n    return apply_ufunc(\n        duck_array_ops.fillna,\n        data,\n        other,\n        join=join,\n        dask=\"allowed\",\n        dataset_join=dataset_join,\n        dataset_fill_value=np.nan,\n        keep_attrs=True,\n    )\n\n\ndef where_method(self, cond, other=dtypes.NA):\n    \"\"\"Return elements from `self` or `other` depending on `cond`.\n\n    Parameters\n    ----------\n    cond : DataArray or Dataset with boolean dtype\n        Locations at which to preserve this objects values.\n    other : scalar, DataArray or Dataset, optional\n        Value to use for locations in this object where ``cond`` is False.\n        By default, inserts missing values.\n\n    Returns\n    -------\n    Same type as caller.\n    \"\"\"\n    from xarray.core.computation import apply_ufunc\n\n    # alignment for three arguments is complicated, so don't support it yet\n    join = \"inner\" if other is dtypes.NA else \"exact\"\n    return apply_ufunc(\n        duck_array_ops.where_method,\n        self,\n        cond,\n        other,\n        join=join,\n        dataset_join=join,\n        dask=\"allowed\",\n        keep_attrs=True,\n    )",
                "filename": "xarray/core/ops.py",
                "start_index": 2849,
                "end_index": 5597,
                "start_line": 113,
                "end_line": 276,
                "max_line": 335,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "xarray/core/common.py": [
                {
                    "chunk": {
                        "code": "taarray import DataArray\n    from xarray.core.dataset import Dataset\n    from xarray.core.variable import Variable\n\n    if not is_scalar(fill_value) and not (\n        isinstance(other, Dataset) and isinstance(fill_value, dict)\n    ):\n        raise ValueError(\n            f\"fill_value must be scalar or, for datasets, a dict-like. Received {fill_value} instead.\"\n        )\n\n    if isinstance(other, Dataset):\n        if not isinstance(fill_value, dict):\n            fill_value = {k: fill_value for k in other.data_vars.keys()}\n\n        dtype_: Mapping[Any, DTypeLikeSave]\n        if not isinstance(dtype, Mapping):\n            dtype_ = {k: dtype for k in other.data_vars.keys()}\n        else:\n            dtype_ = dtype\n\n        data_vars = {\n            k: _full_like_variable(\n                v.variable,\n                fill_value.get(k, dtypes.NA),\n                dtype_.get(k, None),\n                chunks,\n                chunked_array_type,\n                from_array_kwargs,\n            )\n            for k, v in other.data_vars.items()\n        }\n        return Dataset(data_vars, coords=other.coords, attrs=other.attrs)\n    elif isinstance(other, DataArray):\n        if isinstance(dtype, Mapping):\n            raise ValueError(\"'dtype' cannot be dict-like when passing a DataArray\")\n        return DataArray(\n            _full_like_variable(\n                other.variable,\n                fill_value,\n                dtype,\n                chunks,\n                chunked_array_type,\n                from_array_kwargs,\n            ),\n            dims=other.dims,\n            coords=other.coords,\n            attrs=other.attrs,\n            name=other.name,\n        )\n    elif isinstance(other, Variable):\n        if isinstance(dtype, Mapping):\n            raise ValueError(\"'dtype' cannot be dict-like when passing a Variable\")\n        return _full_like_variable(\n            other, fill_value, dtype, chunks, chunked_array_type, from_array_kwargs\n        )\n    else:\n        raise TypeError(\"Expected DataArray, Dataset, or Variable\")\n\n\ndef _full_like_variab",
                        "filename": "xarray/core/common.py",
                        "start_index": 56815,
                        "end_index": 58885,
                        "start_line": 39,
                        "end_line": 1685,
                        "max_line": 2054,
                        "git_instance": "github",
                        "repo_name": "testbed/pydata__xarray__0.12",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the implementation of `full_like` where the issue with non-scalar `fill_value` and chunked arrays is likely to be located."
                },
                {
                    "chunk": {
                        "code": "ke(\n    other: DataArray,\n    fill_value: Any,\n    dtype: DTypeLikeSave | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> DataArray:\n    ...\n\n\n@overload\ndef full_like(\n    other: Dataset,\n    fill_value: Any,\n    dtype: DTypeMaybeMapping | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Dataset:\n    ...\n\n\n@overload\ndef full_like(\n    other: Variable,\n    fill_value: Any,\n    dtype: DTypeLikeSave | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Variable:\n    ...\n\n\n@overload\ndef full_like(\n    other: Dataset | DataArray,\n    fill_value: Any,\n    dtype: DTypeMaybeMapping | None = None,\n    *,\n    chunks: T_Chunks = {},\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Dataset | DataArray:\n    ...\n\n\n@overload\ndef full_like(\n    other: Dataset | DataArray | Variable,\n    fill_value: Any,\n    dtype: DTypeMaybeMapping | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Dataset | DataArray | Variable:\n    ...\n\n\ndef full_like(\n    ot",
                        "filename": "xarray/core/common.py",
                        "start_index": 51091,
                        "end_index": 52487,
                        "start_line": 1434,
                        "end_line": 1995,
                        "max_line": 2054,
                        "git_instance": "github",
                        "repo_name": "testbed/pydata__xarray__0.12",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes the function signatures for `full_like` which may need to be reviewed to ensure they align with the expected behavior when handling non-scalar `fill_value`."
                },
                {
                    "chunk": {
                        "code": "le(\n    other: Variable,\n    fill_value: Any,\n    dtype: DTypeLike | None = None,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Variable:\n    \"\"\"Inner function of full_like, where other must be a variable\"\"\"\n    from xarray.core.variable import Variable\n\n    if fill_value is dtypes.NA:\n        fill_value = dtypes.get_fill_value(dtype if dtype is not None else other.dtype)\n\n    if (\n        is_chunked_array(other.data)\n        or chunked_array_type is not None\n        or chunks is not None\n    ):\n        if chunked_array_type is None:\n            chunkmanager = get_chunked_array_type(other.data)\n        else:\n            chunkmanager = guess_chunkmanager(chunked_array_type)\n\n        if dtype is None:\n            dtype = other.dtype\n\n        if from_array_kwargs is None:\n            from_array_kwargs = {}\n\n        data = chunkmanager.array_api.full(\n            other.shape,\n            fill_value,\n            dtype=dtype,\n            chunks=chunks if chunks else other.data.chunks,\n            **from_array_kwargs,\n        )\n    else:\n        data = np.full_like(other.data, fill_value, dtype=dtype)\n\n    return Variable(dims=other.dims, data=data, attrs=other.attrs)\n\n\n@overload\ndef zeros_like(\n    other: DataArray,\n    dtype: DTypeLikeSave | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> DataArray:\n    ...\n\n\n@overload\ndef zeros_like(\n    other: Dataset,\n    dtype: DTypeMaybeMapping | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Dataset:\n    ...\n\n\n@overload\ndef zeros_like(\n    other: Variable,\n    dtype: DTypeLikeSave | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Variable:\n    ...\n\n\n@overload\ndef zeros_like(\n    other: Dataset | DataArray,\n    dtype: DTypeMaybeMapping | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Dataset | DataArray:\n    ...\n\n\n@overload\ndef zeros_like(\n    other: Dataset | DataArray | Variable,\n    dtype: DTypeMaybeMapping | None = None,\n    *,\n    chunks: T_Chunks = None,\n    chunked_array_type: str | None = None,\n    from_array_kwargs: dict[str, Any] | None = None,\n) -> Dataset | DataArray | Variable:\n    ...\n\n\ndef zeros_like(\n    o",
                        "filename": "xarray/core/common.py",
                        "start_index": 58885,
                        "end_index": 61470,
                        "start_line": 214,
                        "end_line": 1995,
                        "max_line": 2054,
                        "git_instance": "github",
                        "repo_name": "testbed/pydata__xarray__0.12",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the inner function `_full_like_variable` which is called by `full_like` and may need to be modified to handle non-scalar `fill_value` correctly with chunked arrays."
                }
            ]
        },
        "solution_task": "- The issue is caused by passing a non-scalar `fill_value` (a numpy array) to `xr.full_like`, which expects a scalar or a dict-like object when `other` is a Dataset.\n- The `xr.full_like` function is designed to create a new DataArray or Dataset with the same shape as the input, filled with the provided `fill_value`.\n- The error message indicates that broadcasting of the non-scalar `fill_value` is failing because the shapes are incompatible.\n\nTo resolve the issue:\n- Change the `fill_value` argument in the `xr.full_like` function call to be a scalar value instead of a numpy array.\n- The correct usage of `xr.full_like` should be with a scalar `fill_value` that can be broadcast across the entire shape of `da1`.\n\nHere is the corrected code snippet:\n```python\nda1 = xr.DataArray(dask.array.ones((3, 4), chunks=(1, 4)), {\"y\": y, \"x\": x}, (\"y\", \"x\"))\nda2 = xr.full_like(da1, 1)  # Use a scalar value for fill_value\n```\n\nThis will create `da2` as a DataArray with the same shape and chunks as `da1`, filled with ones.",
        "bouncer_response": {
            "score": 2,
            "criteria": "The RESPONSE must provide actionable, relevant guidance to more parts of the PROMPT's described issue to score higher.",
            "supporting_evidence": "The RESPONSE suggests changing the `fill_value` to a scalar, which is not in line with the user's need to use a non-scalar `fill_value`. The user's expectation is to use a numpy array as the `fill_value`, which is a legitimate use case as per the user's description of numpy and dask's behavior. The RESPONSE fails to address the core issue of the bug, which is the inability to broadcast a non-scalar `fill_value` across a chunked dask array. Additionally, the RESPONSE includes links to code snippets that are irrelevant to the actual xarray repository and version mentioned in the PROMPT, which could lead to confusion and does not provide actionable insight.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711759846.820061,
        "result": "Doesn't seem like we have fully grasped all the specifics of this ticket \ud83d\ude15.                   But we've outlined some ideas that might help you get started:\n\n- The issue is caused by passing a non-scalar `fill_value` (a numpy array) to `xr.full_like`, which expects a scalar or a dict-like object when `other` is a Dataset.\n- The `xr.full_like` function is designed to create a new DataArray or Dataset with the same shape as the input, filled with the provided `fill_value`.\n- The error message indicates that broadcasting of the non-scalar `fill_value` is failing because the shapes are incompatible.\n\nTo resolve the issue:\n- Change the `fill_value` argument in the `xr.full_like` function call to be a scalar value instead of a numpy array.\n- The correct usage of `xr.full_like` should be with a scalar `fill_value` that can be broadcast across the entire shape of `da1`.\n\nHere is the corrected code snippet:\n```python\nda1 = xr.DataArray(dask.array.ones((3, 4), chunks=(1, 4)), {\"y\": y, \"x\": x}, (\"y\", \"x\"))\nda2 = xr.full_like(da1, 1)  # Use a scalar value for fill_value\n```\n\nThis will create `da2` as a DataArray with the same shape and chunks as `da1`, filled with ones.\n\n### Code snippets to check\n\n-  \u2192 [xarray](https://github.com/testbed/pydata__xarray__0.12/tree//xarray) \u2192 [core](https://github.com/testbed/pydata__xarray__0.12/tree//xarray/core)- [Lines 39 - 1685](https://github.com/testbed/pydata__xarray__0.12/blob//xarray/core/common.py#L39-L1685)\n\n This snippet contains the implementation of `full_like` where the issue with non-scalar `fill_value` and chunked arrays is likely to be located. \n\n\n   - [Lines 1434 - 1995](https://github.com/testbed/pydata__xarray__0.12/blob//xarray/core/common.py#L1434-L1995)\n\n This snippet includes the function signatures for `full_like` which may need to be reviewed to ensure they align with the expected behavior when handling non-scalar `fill_value`. \n\n\n   - [Lines 214 - 1995](https://github.com/testbed/pydata__xarray__0.12/blob//xarray/core/common.py#L214-L1995)\n\n This snippet contains the inner function `_full_like_variable` which is called by `full_like` and may need to be modified to handle non-scalar `fill_value` correctly with chunked arrays."
    }
}