{
    "id": "scikit-learn__scikit-learn-10687",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
    "task_id": 5897631879051740912,
    "title": "Shape of `coef_` wrong for linear_model.Lasso when using `fit_intercept=False` ",
    "body": "<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: http://stackoverflow.com/questions/tagged/scikit-learn\r\n- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn\r\nFor more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nShape of `coef_` wrong for linear_model.Lasso when using `fit_intercept=False` \r\n\r\n#### Steps/Code to Reproduce\r\nExample:\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import linear_model\r\n\r\nest_intercept = linear_model.Lasso(fit_intercept=True)\r\nest_intercept.fit(np.c_[np.ones(3)], np.ones(3))\r\nassert est_intercept.coef_.shape  == (1,)\r\n```\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import linear_model\r\n\r\nest_no_intercept = linear_model.Lasso(fit_intercept=False)\r\nest_no_intercept.fit(np.c_[np.ones(3)], np.ones(3))\r\nassert est_no_intercept.coef_.shape  == (1,)\r\n```\r\n\r\n#### Expected Results\r\nthe second snippet should not raise, but it does. The first snippet is ok. I pasted it as a reference\r\n\r\n#### Actual Results\r\n```python\r\nIn [2]: %paste\r\nimport numpy as np\r\nfrom sklearn import linear_model\r\nest_intercept = linear_model.Lasso(fit_intercept=True)\r\nest_intercept.fit(np.c_[np.ones(3)], np.ones(3))\r\nassert est_intercept.coef_.shape  == (1,)\r\n\r\n\r\n\r\nIn [3]: %paste\r\nimport numpy as np\r\nfrom sklearn import linear_model\r\n\r\nest_no_intercept = linear_model.Lasso(fit_intercept=False)\r\nest_no_intercept.fit(np.c_[np.ones(3)], np.ones(3))\r\nassert est_no_intercept.coef_.shape  == (1,)\r\n\r\n\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-3-5ffa9cfd4df7> in <module>()\r\n      4 est_no_intercept = linear_model.Lasso(fit_intercept=False)\r\n      5 est_no_intercept.fit(np.c_[np.ones(3)], np.ones(3))\r\n----> 6 assert est_no_intercept.coef_.shape  == (1,)\r\n\r\nAssertionError:\r\n```\r\n\r\n#### Versions\r\nLinux-3.2.0-4-amd64-x86_64-with-debian-7.11\r\n('Python', '2.7.3 (default, Mar 13 2014, 11:03:55) \\n[GCC 4.7.2]')\r\n('NumPy', '1.13.3')\r\n('SciPy', '0.19.1')\r\n('Scikit-Learn', '0.18.2')\r\n\r\n\r\n\r\n<!-- Thanks for contributing! -->\r\n\n[MRG] Shape of `coef_` wrong for linear_model.Lasso when using `fit_intercept=False`\n<!--\r\nThanks for contributing a pull request! Please ensure you have taken a look at\r\nthe contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests\r\n-->\r\n#### Reference Issue\r\nFixes #10571 \n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.20.git",
    "requested_commit": "",
    "mayil_response": "- Check if the `coef_` attribute is being reshaped correctly after fitting the `Lasso` model when `fit_intercept=False`.\n- Ensure that the `coef_` attribute has the expected shape `(n_features,)` or `(n_targets, n_features)` after fitting, depending on whether `y` is one-dimensional or multi-dimensional.\n- If the `coef_` shape is incorrect, modify the `fit` method of the `Lasso` class to return `coef_` with the correct shape when `fit_intercept=False`.\n- If the `fit` method is modifying the shape of `coef_` based on the `fit_intercept` parameter, ensure that it does not add an intercept term to `coef_` when `fit_intercept=False`.\n- If the issue persists, consider checking if the problem is related to the input data shape or the handling of the `fit_intercept` parameter within the `fit` method.\n- If the `fit` method is part of a larger pipeline or influenced by other classes or mixins, ensure that the `fit_intercept` parameter is being correctly passed and handled throughout the pipeline.\n- Since the issue mentions an `AssertionError`, review any assertions related to `coef_`'s shape and adjust them to account for the case when `fit_intercept=False`.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/linear_model)- [Lines 781 - 3083](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_coordinate_descent.py#L781-L3083)\n\n This snippet defines the Lasso class, including the shape of the coef_ attribute, which is central to the issue.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/linear_model)- [Lines 11 - 69](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_linear_loss.py#L11-L69)\n\n This snippet contains the initialization of the LinearLoss class which may be relevant to how the coef_ attribute is handled when fit_intercept is set to False.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/linear_model)- [Lines 476 - 757](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_base.py#L476-L757)\n\n This snippet is part of the SparseCoefMixin which could be involved in the representation of the coef_ attribute.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.000641,
        "snippet_processor": 0.08109000000000001,
        "issue_star_creation": 0.027979999999999998,
        "issue_star_solver": 0.06982000000000001,
        "bouncer": 0.03111
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711742308.39617,
        "relevant_snippets": [
            {
                "code": "of such arrays\n        If a list is passed it's expected to be one of n_targets such arrays.\n        The varying values of the coefficients along the path. It is not\n        present if the ``fit_path`` parameter is ``False``. If this is a list\n        of array-like, the length of the outer list is `n_targets`.\n\n    coef_ : array-like of shape (n_features,) or (n_targets, n_features)\n        Parameter vector (w in the formulation formula).\n\n    intercept_ : float or array-like of shape (n_targets,)\n        Independent term in decision function.\n\n    n_iter_ : array-like or int\n        The number of iterations taken by lars_path to find the\n        grid of alphas for each target.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    lars_path : Compute Least Angle Regression or Lasso\n        path using LARS algorithm.\n    lasso_path : Compute Lasso path with coordinate descent.\n    Lasso : Linear Model trained with L1 prior as\n        regularizer (aka the Lasso).\n    LassoCV : Lasso linear model with iterative fitting\n        along a regularization path.\n    LassoLarsCV: Cross-validated Lasso, using the LARS algorithm.\n    LassoLarsIC : Lasso model fit with Lars using BIC\n        or AIC for model selection.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> reg = linear_model.LassoLars(alpha=0.01)\n    >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])\n    LassoLars(alpha=0.01)\n    >>> print(reg.coef_)\n    [ 0.         -0.955...]\n    \"\"\"",
                "filename": "sklearn/linear_model/_least_angle.py",
                "start_index": 46979,
                "end_index": 48821,
                "start_line": 948,
                "end_line": 2367,
                "max_line": 2381,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"\n==============================\nLasso on dense and sparse data\n==============================\n\nWe show that linear_model.Lasso provides the same results for dense and sparse\ndata and that in the case of sparse data the speed is improved.\n\n\"\"\"\n\nfrom time import time\n\nfrom scipy import linalg, sparse\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import Lasso\n\n# %%\n# Comparing the two Lasso implementations on Dense data\n# -----------------------------------------------------\n#\n# We create a linear regression problem that is suitable for the Lasso,\n# that is to say, with more features than samples. We then store the data\n# matrix in both dense (the usual) and sparse format, and train a Lasso on\n# each. We compute the runtime of both and check that they learned the\n# same model by computing the Euclidean norm of the difference between the\n# coefficients they learned. Because the data is dense, we expect better\n# runtime with a dense data format.\n\nX, y = make_regression(n_samples=200, n_features=5000, random_state=0)\n# create a copy of X in sparse format\nX_sp = sparse.coo_matrix(X)\n\nalpha = 1\nsparse_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=1000)\ndense_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=1000)\n\nt0 = time()\nsparse_lasso.fit(X_sp, y)\nprint(f\"Sparse Lasso done in {(time() - t0):.3f}s\")\n\nt0 = time()\ndense_lasso.fit(X, y)\nprint(f\"Dense Lasso done in {(time() - t0):.3f}s\")\n\n# compare the regression coefficients\ncoeff_diff = linalg.norm(sparse_lasso.coef_ - dense_lasso.coef_)\nprint(f\"Distance between coefficients : {coeff_diff:.2e}\")\n\n#\n# %%\n# Comparing the two Lasso implementations on Sparse data\n# ------------------------------------------------------\n#\n# We make the previous problem sparse by replacing all small values with 0\n# and run the same comparisons as above. Because the data is now sparse, we\n# expect the implementation that uses the sparse data format to be faster.\n\n# make a copy of the previous data\nXs = X.copy()\n# make Xs sparse by replacing the values lower than 2.5 with 0s\nXs[Xs < 2.5] = 0.0\n# create a copy of Xs in sparse format\nXs_sp = sparse.coo_matrix(Xs)\nXs_sp = Xs_sp.tocsc()\n\n# compute the proportion of non-zero coefficient in the data matrix\nprint(f\"Matrix density : {(Xs_sp.nnz / float(X.size) * 100):.3f}%\")\n\nalpha = 0.1\nsparse_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=10000)\ndense_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=10000)\n\nt0 = time()\nsparse_lasso.fit(Xs_sp, y)\nprint(f\"Sparse Lasso done in {(time() - t0):.3f}s\")\n\nt0 = time()\ndense_lasso.fit(Xs, y)\nprint(f\"Dense Lasso done in  {(time() - t0):.3f}s\")\n\n# compare the regression coefficients\ncoeff_diff = linalg.norm(sparse_lasso.coef_ - dense_lasso.coef_)\nprint(f\"Distance between coefficients : {coeff_diff:.2e}\")\n\n# %%",
                "filename": "examples/linear_model/plot_lasso_dense_vs_sparse_data.py",
                "start_index": 0,
                "end_index": 2825,
                "start_line": 1,
                "end_line": 86,
                "max_line": 86,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "(n_targets, n_features)\n        Readonly property derived from ``coef_``.\n\n    intercept_ : float or ndarray of shape (n_targets,)\n        Independent term in decision function.\n\n    n_iter_ : int or list of int\n        Number of iterations run by the coordinate descent solver to reach\n        the specified tolerance.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    lars_path : Regularization path using LARS.\n    lasso_path : Regularization path using Lasso.\n    LassoLars : Lasso Path along the regularization parameter using LARS algorithm.\n    LassoCV : Lasso alpha parameter by cross-validation.\n    LassoLarsCV : Lasso least angle parameter algorithm by cross-validation.\n    sklearn.decomposition.sparse_encode : Sparse coding array estimator.\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    Regularization improves the conditioning of the problem and\n    reduces the variance of the estimates. Larger values specify stronger\n    regularization. Alpha corresponds to `1 / (2C)` in other linear\n    models such as :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n    assumed to be specific to the targets. Hence they must correspond in\n    number.\n\n    The precise stopping criteria based on `tol` are the following: First, check that\n    that maximum coordinate update, i.e. :math:`\\\\max_j |w_j^{new} - w_j^{old}|`\n    is smaller than `tol` times the maximum absolute coefficient, :math:`\\\\max_j |w_j|`.\n    If so, then additionally check whether the dual gap is smaller than `tol` times\n    :math:`||y||_2^2 / n_{\\\\text{samples}}`.\n\n    The target can be a 2-dimensional array, resulting in the optimization of the\n    following objective::\n\n        (1 / (2 * n_samples)) * ||Y - XW||^2_F + alpha * ||W||_11\n\n    where :math:`||W||_{1,1}` is the sum of the magnitude of the matrix coefficients.\n    It should not be confused with :class:`~sklearn.linear_model.MultiTaskLasso` which\n    instead penalizes the :math:`L_{2,1}` norm of the coefficients, yielding row-wise\n    sparsity in the coefficients.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.Lasso(alpha=0.1)\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    Lasso(alpha=0.1)\n    >>> print(clf.coef_)\n    [0.85 0.  ]\n    >>> print(clf.intercept_)\n    0.15...\n    \"\"\"",
                "filename": "sklearn/linear_model/_coordinate_descent.py",
                "start_index": 41257,
                "end_index": 44115,
                "start_line": 781,
                "end_line": 3083,
                "max_line": 3084,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"General class for loss functions with raw_prediction = X @ coef + intercept.\n\n    Note that raw_prediction is also known as linear predictor.\n\n    The loss is the sum of per sample losses and includes a term for L2\n    regularization::\n\n        loss = sum_i s_i loss(y_i, X_i @ coef + intercept)\n               + 1/2 * l2_reg_strength * ||coef||_2^2\n\n    with sample weights s_i=1 if sample_weight=None.\n\n    Gradient and hessian, for simplicity without intercept, are::\n\n        gradient = X.T @ loss.gradient + l2_reg_strength * coef\n        hessian = X.T @ diag(loss.hessian) @ X + l2_reg_strength * identity\n\n    Conventions:\n        if fit_intercept:\n            n_dof =  n_features + 1\n        else:\n            n_dof = n_features\n\n        if base_loss.is_multiclass:\n            coef.shape = (n_classes, n_dof) or ravelled (n_classes * n_dof,)\n        else:\n            coef.shape = (n_dof,)\n\n        The intercept term is at the end of the coef array:\n        if base_loss.is_multiclass:\n            if coef.shape (n_classes, n_dof):\n                intercept = coef[:, -1]\n            if coef.shape (n_classes * n_dof,)\n                intercept = coef[n_features::n_dof] = coef[(n_dof-1)::n_dof]\n            intercept.shape = (n_classes,)\n        else:\n            intercept = coef[-1]\n\n    Note: If coef has shape (n_classes * n_dof,), the 2d-array can be reconstructed as\n\n        coef.reshape((n_classes, -1), order=\"F\")\n\n    The option order=\"F\" makes coef[:, i] contiguous. This, in turn, makes the\n    coefficients without intercept, coef[:, :-1], contiguous and speeds up\n    matrix-vector computations.\n\n    Note: If the average loss per sample is wanted instead of the sum of the loss per\n    sample, one can simply use a rescaled sample_weight such that\n    sum(sample_weight) = 1.\n\n    Parameters\n    ----------\n    base_loss : instance of class BaseLoss from sklearn._loss.\n    fit_intercept : bool\n    \"\"\"\n\n    def __init__(self, base_loss, fit_intercept):\n        self.base_loss = base_loss\n        self.fit_intercept = fit_intercept",
                "filename": "sklearn/linear_model/_linear_loss.py",
                "start_index": 187,
                "end_index": 2247,
                "start_line": 11,
                "end_line": 69,
                "max_line": 658,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"\n======================================================================\nCommon pitfalls in the interpretation of coefficients of linear models\n======================================================================\n\nIn linear models, the target value is modeled as a linear combination of the\nfeatures (see the :ref:`linear_model` User Guide section for a description of a\nset of linear models available in scikit-learn). Coefficients in multiple linear\nmodels represent the relationship between the given feature, :math:`X_i` and the\ntarget, :math:`y`, assuming that all the other features remain constant\n(`conditional dependence\n<https://en.wikipedia.org/wiki/Conditional_dependence>`_). This is different\nfrom plotting :math:`X_i` versus :math:`y` and fitting a linear relationship: in\nthat case all possible values of the other features are taken into account in\nthe estimation (marginal dependence).\n\nThis example will provide some hints in interpreting coefficient in linear\nmodels, pointing at problems that arise when either the linear model is not\nappropriate to describe the dataset, or when features are correlated.\n\n.. note::\n\n    Keep in mind that the features :math:`X` and the outcome :math:`y` are in\n    general the result of a data generating process that is unknown to us.\n    Machine learning models are trained to approximate the unobserved\n    mathematical function that links :math:`X` to :math:`y` from sample data. As\n    a result, any interpretation made about a model may not necessarily\n    generalize to the true data generating process. This is especially true when\n    the model is of bad quality or when the sample data is not representative of\n    the population.\n\nWe will use data from the `\"Current Population Survey\"\n<https://www.openml.org/d/534>`_ from 1985 to predict wage as a function of\nvarious features such as experience, age, or education.\n\n.. contents::\n   :local:\n   :depth: 1\n\n\"\"\"\n\n# %%\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport seaborn as sns\n\n# %%\n# The dataset: wages\n# ------------------\n#\n# We fetch the data from `OpenML <http://openml.org/>`_.\n# Note that setting the parameter `as_frame` to True will retrieve the data\n# as a pandas dataframe.\nfrom sklearn.datasets import fetch_openml\n\nsurvey = fetch_openml(data_id=534, as_frame=True, parser=\"pandas\")\n\n# %%\n# Then, we identify features `X` and targets `y`: the column WAGE is our\n# target variable (i.e., the variable which we want to predict).\n\nX = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n# %%\n# Note that the dataset contains categorical and numerical variables.\n# We will need to take this into account when preprocessing the dataset\n# thereafter.\n\nX.head()\n\n# %%\n# Our target for prediction: the wage.\n# Wages are described as floating-point number in dollars per hour.\n\n# %%\ny = survey.target.values.ravel()\nsurvey.target.head()\n\n# %%\n# We split the sample into a train and a test dataset.",
                "filename": "examples/inspection/plot_linear_model_coefficient_interpretation.py",
                "start_index": 0,
                "end_index": 2982,
                "start_line": 1,
                "end_line": 83,
                "max_line": 757,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"\n==========================\nNon-negative least squares\n==========================\n\nIn this example, we fit a linear model with positive constraints on the\nregression coefficients and compare the estimated coefficients to a classic\nlinear regression.\n\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.metrics import r2_score\n\n# %%\n# Generate some random data\nnp.random.seed(42)\n\nn_samples, n_features = 200, 50\nX = np.random.randn(n_samples, n_features)\ntrue_coef = 3 * np.random.randn(n_features)\n# Threshold coefficients to render them non-negative\ntrue_coef[true_coef < 0] = 0\ny = np.dot(X, true_coef)\n\n# Add some noise\ny += 5 * np.random.normal(size=(n_samples,))\n\n# %%\n# Split the data in train set and test set\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n\n# %%\n# Fit the Non-Negative least squares.\nfrom sklearn.linear_model import LinearRegression\n\nreg_nnls = LinearRegression(positive=True)\ny_pred_nnls = reg_nnls.fit(X_train, y_train).predict(X_test)\nr2_score_nnls = r2_score(y_test, y_pred_nnls)\nprint(\"NNLS R2 score\", r2_score_nnls)\n\n# %%\n# Fit an OLS.\nreg_ols = LinearRegression()\ny_pred_ols = reg_ols.fit(X_train, y_train).predict(X_test)\nr2_score_ols = r2_score(y_test, y_pred_ols)\nprint(\"OLS R2 score\", r2_score_ols)\n\n\n# %%\n# Comparing the regression coefficients between OLS and NNLS, we can observe\n# they are highly correlated (the dashed line is the identity relation),\n# but the non-negative constraint shrinks some to 0.\n# The Non-Negative Least squares inherently yield sparse results.\n\nfig, ax = plt.subplots()\nax.plot(reg_ols.coef_, reg_nnls.coef_, linewidth=0, marker=\".\")\n\nlow_x, high_x = ax.get_xlim()\nlow_y, high_y = ax.get_ylim()\nlow = max(low_x, low_y)\nhigh = min(high_x, high_y)\nax.plot([low, high], [low, high], ls=\"--\", c=\".3\", alpha=0.5)\nax.set_xlabel(\"OLS regression coefficients\", fontweight=\"bold\")\nax.set_ylabel(\"NNLS regression coefficients\", fontweight=\"bold\")",
                "filename": "examples/linear_model/plot_nnls.py",
                "start_index": 0,
                "end_index": 2006,
                "start_line": 1,
                "end_line": 69,
                "max_line": 69,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "gbdt_no_cst = HistGradientBoostingRegressor().fit(X, y)\ngbdt_cst = HistGradientBoostingRegressor(monotonic_cst=[1, 0]).fit(X, y)\n\n# plot_partial_dependence has been removed in version 1.2. From 1.2, use\n# PartialDependenceDisplay instead.\n# disp = plot_partial_dependence(\ndisp = PartialDependenceDisplay.from_estimator(\n    gbdt_no_cst,\n    X,\n    features=[0],\n    feature_names=[\"feature 0\"],\n    line_kw={\"linewidth\": 4, \"label\": \"unconstrained\", \"color\": \"tab:blue\"},\n)\n# plot_partial_dependence(\nPartialDependenceDisplay.from_estimator(\n    gbdt_cst,\n    X,\n    features=[0],\n    line_kw={\"linewidth\": 4, \"label\": \"constrained\", \"color\": \"tab:orange\"},\n    ax=disp.axes_,\n)\ndisp.axes_[0, 0].plot(\n    X[:, 0], y, \"o\", alpha=0.5, zorder=-1, label=\"samples\", color=\"tab:green\"\n)\ndisp.axes_[0, 0].set_ylim(-3, 3)\ndisp.axes_[0, 0].set_xlim(-1, 1)\nplt.legend()\nplt.show()\n\n##############################################################################\n# Sample-weight support for Lasso and ElasticNet\n# ----------------------------------------------\n# The two linear regressors :class:`~sklearn.linear_model.Lasso` and\n# :class:`~sklearn.linear_model.ElasticNet` now support sample weights.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import Lasso\nimport numpy as np\n\nn_samples, n_features = 1000, 20\nrng = np.random.RandomState(0)\nX, y = make_regression(n_samples, n_features, random_state=rng)\nsample_weight = rng.rand(n_samples)\nX_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(\n    X, y, sample_weight, random_state=rng\n)\nreg = Lasso()\nreg.fit(X_train, y_train, sample_weight=sw_train)\nprint(reg.score(X_test, y_test, sw_test))",
                "filename": "examples/release_highlights/plot_release_highlights_0_23_0.py",
                "start_index": 5973,
                "end_index": 7708,
                "start_line": 140,
                "end_line": 189,
                "max_line": 189,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "if coef is not None:\n        # it must work both giving the bias term and not\n        if multi_class == \"ovr\":\n            if coef.size not in (n_features, w0.size):\n                raise ValueError(\n                    \"Initialization coef is of shape %d, expected shape %d or %d\"\n                    % (coef.size, n_features, w0.size)\n                )\n            w0[: coef.size] = coef\n        else:\n            # For binary problems coef.shape[0] should be 1, otherwise it\n            # should be classes.size.\n            n_classes = classes.size\n            if n_classes == 2:\n                n_classes = 1\n\n            if coef.shape[0] != n_classes or coef.shape[1] not in (\n                n_features,\n                n_features + 1,\n            ):\n                raise ValueError(\n                    \"Initialization coef is of shape (%d, %d), expected \"\n                    \"shape (%d, %d) or (%d, %d)\"\n                    % (\n                        coef.shape[0],\n                        coef.shape[1],\n                        classes.size,\n                        n_features,\n                        classes.size,\n                        n_features + 1,\n                    )\n                )\n\n            if n_classes == 1:\n                w0[0, : coef.shape[1]] = -coef\n                w0[1, : coef.shape[1]] = coef\n            else:\n                w0[:, : coef.shape[1]] = coef",
                "filename": "sklearn/linear_model/_logistic.py",
                "start_index": 14578,
                "end_index": 15975,
                "start_line": 377,
                "end_line": 414,
                "max_line": 2222,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"Cross-validated Lasso, using the LARS algorithm.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    The optimization objective for Lasso is::\n\n    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\n    Read more in the :ref:`User Guide <least_angle_regression>`.\n\n    Parameters\n    ----------\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    verbose : bool or int, default=False\n        Sets the verbosity amount.\n\n    max_iter : int, default=500\n        Maximum number of iterations to perform.\n\n    normalize : bool, default=False\n        This parameter is ignored when ``fit_intercept`` is set to False.\n        If True, the regressors X will be normalized before regression by\n        subtracting the mean and dividing by the l2-norm.\n        If you wish to standardize, please use\n        :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n        on an estimator with ``normalize=False``.\n\n        .. versionchanged:: 1.2\n           default changed from True to False in 1.2.\n\n        .. deprecated:: 1.2\n            ``normalize`` was deprecated in version 1.2 and will be removed in 1.4.\n\n    precompute : bool or 'auto' , default='auto'\n        Whether to use a precomputed Gram matrix to speed up\n        calculations. If set to ``'auto'`` let us decide. The Gram matrix\n        cannot be passed as argument since we will use only subsets of X.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross-validation,\n        - integer, to specify the number of folds.\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    max_n_alphas : int, default=1000\n        The maximum number of points on the path used to compute the\n        residuals in the cross-validation.\n\n    n_jobs : int or None, default=None\n        Number of CPUs to use during the cross validation.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    eps : float, default=np.finfo(float).eps\n        The machine-precision regularization in the computation of the\n        Cholesky diagonal factors. Increase this for very ill-conditioned\n        systems. Unlike the ``tol`` parameter in some iterative\n        optimization-based algorithms, this parameter does not control\n        the tolerance of the optimization.\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    positive : bool, default=False\n        Restrict coefficients to be >= 0. Be aware that you might want to\n        remove fit_intercept which is set True by default.\n        Under the positive restriction the model coefficients do not converge\n        to the ordinary-least-squares solution for small values of alpha.\n        Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n        0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n        algorithm are typically in congruence with the solution of the\n        coordinate descent Lasso estimator.\n        As a consequence using LassoLarsCV only makes sense for problems where\n        a sparse solution is expected and/or reached.\n\n    Attributes\n    ----------\n    coef_ : array-like of shape (n_features,)\n        parameter vector (w in the formulation formula)\n\n    intercept_ : float\n        independent term in decision function.\n\n    coef_path_ : array-like of shape (n_features, n_alphas)\n        the varying values of the coefficients along the path\n\n    alpha_ : float\n        the estimated regularization parameter alpha\n\n    alphas_ : array-like of shape (n_alphas,)\n        the different values of alpha along the path\n\n    cv_alphas_ : array-like of shape (n_cv_alphas,)\n        all the values of alpha along the path for the different folds\n\n    mse_path_ : array-like of shape (n_folds, n_cv_alphas)\n        the mean square error on left-out for each fold along the path\n        (alpha values given by ``cv_alphas``)\n\n    n_iter_ : array-like or int\n        the number of iterations run by Lars with the optimal alpha.\n\n    active_ : list of int\n        Indices of active variables at the end of the path.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    lars_path : Compute Least Angle Regression or Lasso\n        path using LARS algorithm.\n    lasso_path : Compute Lasso path with coordinate descent.\n    Lasso : Linear Model trained with L1 prior as\n        regularizer (aka the Lasso).\n    LassoCV : Lasso linear model with iterative fitting\n        along a regularization path.\n    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n    LassoLarsIC : Lasso model fit with Lars using BIC\n        or AIC for model selection.\n    sklearn.decomposition.sparse_encode : Sparse coding.\n\n    Notes\n    -----\n    The object solves the same problem as the\n    :class:`~sklearn.linear_model.LassoCV` object. However, unlike the\n    :class:`~sklearn.linear_model.LassoCV`, it find the relevant alphas values\n    by itself. In general, because of this property, it will be more stable.\n    However, it is more fragile to heavily multicollinear datasets.\n\n    It is more efficient than the :class:`~sklearn.linear_model.LassoCV` if\n    only a small number of features are selected compared to the total number,\n    for instance if there are very few samples compared to the number of\n    features.\n\n    In `fit`, once the best parameter `alpha` is found through\n    cross-validation, the model is fit again using the entire training set.\n\n    Examples\n    --------\n    >>> from sklearn.linear_model import LassoLarsCV\n    >>> from sklearn.datasets import make_regression\n    >>> X, y = make_regression(noise=4.0, random_state=0)\n    >>> reg = LassoLarsCV(cv=5).fit(X, y)\n    >>> reg.score(X, y)\n    0.9993...\n    >>> reg.alpha_\n    0.3972...\n    >>> reg.predict(X[:1,])\n    array([-78.4831...])\n    \"\"\"",
                "filename": "sklearn/linear_model/_least_angle.py",
                "start_index": 65298,
                "end_index": 72184,
                "start_line": 1833,
                "end_line": 2367,
                "max_line": 2381,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "class SparseCoefMixin:\n    \"\"\"Mixin for converting coef_ to and from CSR format.\n\n    L1-regularizing estimators should inherit this.\n    \"\"\"\n\n    def densify(self):\n        \"\"\"\n        Convert coefficient matrix to dense array format.\n\n        Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n        default format of ``coef_`` and is required for fitting, so calling\n        this method is only required on models that have previously been\n        sparsified; otherwise, it is a no-op.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        msg = \"Estimator, %(name)s, must be fitted before densifying.\"\n        check_is_fitted(self, msg=msg)\n        if sp.issparse(self.coef_):\n            self.coef_ = self.coef_.toarray()\n        return self\n\n    def sparsify(self):\n        \"\"\"\n        Convert coefficient matrix to sparse format.\n\n        Converts the ``coef_`` member to a scipy.sparse matrix, which for\n        L1-regularized models can be much more memory- and storage-efficient\n        than the usual numpy.ndarray representation.\n\n        The ``intercept_`` member is not converted.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.\n        \"\"\"\n        msg = \"Estimator, %(name)s, must be fitted before sparsifying.\"\n        check_is_fitted(self, msg=msg)\n        self.coef_ = sp.csr_matrix(self.coef_)\n        return self",
                "filename": "sklearn/linear_model/_base.py",
                "start_index": 15778,
                "end_index": 17717,
                "start_line": 476,
                "end_line": 757,
                "max_line": 920,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/linear_model/_coordinate_descent.py": [
                {
                    "chunk": {
                        "code": "(n_targets, n_features)\n        Readonly property derived from ``coef_``.\n\n    intercept_ : float or ndarray of shape (n_targets,)\n        Independent term in decision function.\n\n    n_iter_ : int or list of int\n        Number of iterations run by the coordinate descent solver to reach\n        the specified tolerance.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    lars_path : Regularization path using LARS.\n    lasso_path : Regularization path using Lasso.\n    LassoLars : Lasso Path along the regularization parameter using LARS algorithm.\n    LassoCV : Lasso alpha parameter by cross-validation.\n    LassoLarsCV : Lasso least angle parameter algorithm by cross-validation.\n    sklearn.decomposition.sparse_encode : Sparse coding array estimator.\n\n    Notes\n    -----\n    The algorithm used to fit the model is coordinate descent.\n\n    To avoid unnecessary memory duplication the X argument of the fit method\n    should be directly passed as a Fortran-contiguous numpy array.\n\n    Regularization improves the conditioning of the problem and\n    reduces the variance of the estimates. Larger values specify stronger\n    regularization. Alpha corresponds to `1 / (2C)` in other linear\n    models such as :class:`~sklearn.linear_model.LogisticRegression` or\n    :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n    assumed to be specific to the targets. Hence they must correspond in\n    number.\n\n    The precise stopping criteria based on `tol` are the following: First, check that\n    that maximum coordinate update, i.e. :math:`\\\\max_j |w_j^{new} - w_j^{old}|`\n    is smaller than `tol` times the maximum absolute coefficient, :math:`\\\\max_j |w_j|`.\n    If so, then additionally check whether the dual gap is smaller than `tol` times\n    :math:`||y||_2^2 / n_{\\\\text{samples}}`.\n\n    The target can be a 2-dimensional array, resulting in the optimization of the\n    following objective::\n\n        (1 / (2 * n_samples)) * ||Y - XW||^2_F + alpha * ||W||_11\n\n    where :math:`||W||_{1,1}` is the sum of the magnitude of the matrix coefficients.\n    It should not be confused with :class:`~sklearn.linear_model.MultiTaskLasso` which\n    instead penalizes the :math:`L_{2,1}` norm of the coefficients, yielding row-wise\n    sparsity in the coefficients.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.Lasso(alpha=0.1)\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    Lasso(alpha=0.1)\n    >>> print(clf.coef_)\n    [0.85 0.  ]\n    >>> print(clf.intercept_)\n    0.15...\n    \"\"\"",
                        "filename": "sklearn/linear_model/_coordinate_descent.py",
                        "start_index": 41257,
                        "end_index": 44115,
                        "start_line": 781,
                        "end_line": 3083,
                        "max_line": 3084,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet defines the Lasso class, including the shape of the coef_ attribute, which is central to the issue."
                }
            ],
            "sklearn/linear_model/_linear_loss.py": [
                {
                    "chunk": {
                        "code": "\"\"\"General class for loss functions with raw_prediction = X @ coef + intercept.\n\n    Note that raw_prediction is also known as linear predictor.\n\n    The loss is the sum of per sample losses and includes a term for L2\n    regularization::\n\n        loss = sum_i s_i loss(y_i, X_i @ coef + intercept)\n               + 1/2 * l2_reg_strength * ||coef||_2^2\n\n    with sample weights s_i=1 if sample_weight=None.\n\n    Gradient and hessian, for simplicity without intercept, are::\n\n        gradient = X.T @ loss.gradient + l2_reg_strength * coef\n        hessian = X.T @ diag(loss.hessian) @ X + l2_reg_strength * identity\n\n    Conventions:\n        if fit_intercept:\n            n_dof =  n_features + 1\n        else:\n            n_dof = n_features\n\n        if base_loss.is_multiclass:\n            coef.shape = (n_classes, n_dof) or ravelled (n_classes * n_dof,)\n        else:\n            coef.shape = (n_dof,)\n\n        The intercept term is at the end of the coef array:\n        if base_loss.is_multiclass:\n            if coef.shape (n_classes, n_dof):\n                intercept = coef[:, -1]\n            if coef.shape (n_classes * n_dof,)\n                intercept = coef[n_features::n_dof] = coef[(n_dof-1)::n_dof]\n            intercept.shape = (n_classes,)\n        else:\n            intercept = coef[-1]\n\n    Note: If coef has shape (n_classes * n_dof,), the 2d-array can be reconstructed as\n\n        coef.reshape((n_classes, -1), order=\"F\")\n\n    The option order=\"F\" makes coef[:, i] contiguous. This, in turn, makes the\n    coefficients without intercept, coef[:, :-1], contiguous and speeds up\n    matrix-vector computations.\n\n    Note: If the average loss per sample is wanted instead of the sum of the loss per\n    sample, one can simply use a rescaled sample_weight such that\n    sum(sample_weight) = 1.\n\n    Parameters\n    ----------\n    base_loss : instance of class BaseLoss from sklearn._loss.\n    fit_intercept : bool\n    \"\"\"\n\n    def __init__(self, base_loss, fit_intercept):\n        self.base_loss = base_loss\n        self.fit_intercept = fit_intercept",
                        "filename": "sklearn/linear_model/_linear_loss.py",
                        "start_index": 187,
                        "end_index": 2247,
                        "start_line": 11,
                        "end_line": 69,
                        "max_line": 658,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the initialization of the LinearLoss class which may be relevant to how the coef_ attribute is handled when fit_intercept is set to False."
                }
            ],
            "sklearn/linear_model/_base.py": [
                {
                    "chunk": {
                        "code": "class SparseCoefMixin:\n    \"\"\"Mixin for converting coef_ to and from CSR format.\n\n    L1-regularizing estimators should inherit this.\n    \"\"\"\n\n    def densify(self):\n        \"\"\"\n        Convert coefficient matrix to dense array format.\n\n        Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n        default format of ``coef_`` and is required for fitting, so calling\n        this method is only required on models that have previously been\n        sparsified; otherwise, it is a no-op.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n        \"\"\"\n        msg = \"Estimator, %(name)s, must be fitted before densifying.\"\n        check_is_fitted(self, msg=msg)\n        if sp.issparse(self.coef_):\n            self.coef_ = self.coef_.toarray()\n        return self\n\n    def sparsify(self):\n        \"\"\"\n        Convert coefficient matrix to sparse format.\n\n        Converts the ``coef_`` member to a scipy.sparse matrix, which for\n        L1-regularized models can be much more memory- and storage-efficient\n        than the usual numpy.ndarray representation.\n\n        The ``intercept_`` member is not converted.\n\n        Returns\n        -------\n        self\n            Fitted estimator.\n\n        Notes\n        -----\n        For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n        this may actually *increase* memory usage, so use this method with\n        care. A rule of thumb is that the number of zero elements, which can\n        be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n        to provide significant benefits.\n\n        After calling this method, further fitting with the partial_fit\n        method (if any) will not work until you call densify.\n        \"\"\"\n        msg = \"Estimator, %(name)s, must be fitted before sparsifying.\"\n        check_is_fitted(self, msg=msg)\n        self.coef_ = sp.csr_matrix(self.coef_)\n        return self",
                        "filename": "sklearn/linear_model/_base.py",
                        "start_index": 15778,
                        "end_index": 17717,
                        "start_line": 476,
                        "end_line": 757,
                        "max_line": 920,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet is part of the SparseCoefMixin which could be involved in the representation of the coef_ attribute."
                }
            ]
        },
        "solution_task": "- Check if the `coef_` attribute is being reshaped correctly after fitting the `Lasso` model when `fit_intercept=False`.\n- Ensure that the `coef_` attribute has the expected shape `(n_features,)` or `(n_targets, n_features)` after fitting, depending on whether `y` is one-dimensional or multi-dimensional.\n- If the `coef_` shape is incorrect, modify the `fit` method of the `Lasso` class to return `coef_` with the correct shape when `fit_intercept=False`.\n- If the `fit` method is modifying the shape of `coef_` based on the `fit_intercept` parameter, ensure that it does not add an intercept term to `coef_` when `fit_intercept=False`.\n- If the issue persists, consider checking if the problem is related to the input data shape or the handling of the `fit_intercept` parameter within the `fit` method.\n- If the `fit` method is part of a larger pipeline or influenced by other classes or mixins, ensure that the `fit_intercept` parameter is being correctly passed and handled throughout the pipeline.\n- Since the issue mentions an `AssertionError`, review any assertions related to `coef_`'s shape and adjust them to account for the case when `fit_intercept=False`.",
        "bouncer_response": {
            "score": 7,
            "criteria": "RESPONSES that are highly relevant, offering clear and comprehensive guidance or solutions for the described bug, should score between 7 and 9.",
            "supporting_evidence": "The RESPONSE provides a series of actionable steps to investigate and potentially resolve the issue with the `coef_` shape in the `Lasso` model when `fit_intercept=False`. It suggests checking the reshaping of `coef_`, ensuring the expected shape is correct, modifying the `fit` method if necessary, and considering the influence of the input data shape and the handling of the `fit_intercept` parameter. Additionally, it advises checking the entire pipeline for correct parameter passing and handling. The RESPONSE also includes references to specific code snippets that could be relevant to the issue, which is helpful for someone looking to fix the bug. However, the RESPONSE does not provide a definitive solution or identify the exact cause of the bug, which is why it does not score a 10.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711742504.9814851,
        "result": "- Check if the `coef_` attribute is being reshaped correctly after fitting the `Lasso` model when `fit_intercept=False`.\n- Ensure that the `coef_` attribute has the expected shape `(n_features,)` or `(n_targets, n_features)` after fitting, depending on whether `y` is one-dimensional or multi-dimensional.\n- If the `coef_` shape is incorrect, modify the `fit` method of the `Lasso` class to return `coef_` with the correct shape when `fit_intercept=False`.\n- If the `fit` method is modifying the shape of `coef_` based on the `fit_intercept` parameter, ensure that it does not add an intercept term to `coef_` when `fit_intercept=False`.\n- If the issue persists, consider checking if the problem is related to the input data shape or the handling of the `fit_intercept` parameter within the `fit` method.\n- If the `fit` method is part of a larger pipeline or influenced by other classes or mixins, ensure that the `fit_intercept` parameter is being correctly passed and handled throughout the pipeline.\n- Since the issue mentions an `AssertionError`, review any assertions related to `coef_`'s shape and adjust them to account for the case when `fit_intercept=False`.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/linear_model)- [Lines 781 - 3083](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_coordinate_descent.py#L781-L3083)\n\n This snippet defines the Lasso class, including the shape of the coef_ attribute, which is central to the issue.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/linear_model)- [Lines 11 - 69](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_linear_loss.py#L11-L69)\n\n This snippet contains the initialization of the LinearLoss class which may be relevant to how the coef_ attribute is handled when fit_intercept is set to False.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/linear_model)- [Lines 476 - 757](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/linear_model/_base.py#L476-L757)\n\n This snippet is part of the SparseCoefMixin which could be involved in the representation of the coef_ attribute."
    }
}