{
    "id": "scikit-learn__scikit-learn-13087",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
    "task_id": 8100449976501654792,
    "title": "Feature request: support for arbitrary bin spacing in calibration.calibration_curve",
    "body": "#### Description\r\nI was using [`sklearn.calibration.calibration_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.calibration_curve.html), and it currently accepts an `n_bins` parameter to specify the number of bins to evenly partition the probability space between 0 and 1.\r\n\r\nHowever, I am using this in combination with a gradient boosting model in which the probabilities are very uncalibrated, and most of the predictions are close to 0. When I use the calibrated classifier, the result is very noisy because there are many data points in some bins and few, if any, in others (see example below).\r\n\r\nIn the code below, I made a work-around to do what I want and show a plot of my output (in semilog space because of the skewed distribution). I haven't contributed to a large open-source project before, but if there's agreement this would be a useful feature, I would be happy to try to draft up a PR.\r\n\r\n#### My work-around\r\n```python\r\nimport numpy as np\r\n\r\ndef my_calibration_curve(y_true, y_prob, my_bins):\r\n    prob_true = []\r\n    prob_pred = []\r\n    for i in range(len(my_bins) - 1):\r\n        idx_use = np.logical_and(y_prob < my_bins[i+1], y_prob >= my_bins[i])\r\n        prob_true.append(y_true[idx_use].mean())\r\n        prob_pred.append(y_pred[idx_use].mean())\r\n    return prob_true, prob_pred\r\n\r\n# example bins:\r\n# my_bins = np.concatenate([[0], np.logspace(-3, 0, 10)])\r\n```\r\n\r\n#### Results comparison\r\nNotice the large disparity in results between the different bins chosen. For this reason, I think the user should be able to choose the bin edges, as in numpy's or matplotlib's [histogram](https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html) functions.\r\n\r\n![image](https://user-images.githubusercontent.com/7298871/52183657-d1e18c80-27be-11e9-9c84-011c043e0978.png)\r\n\r\n\r\n#### Versions\r\n\r\n```\r\nDarwin-18.0.0-x86_64-i386-64bit\r\nPython 3.6.4 |Anaconda custom (x86_64)| (default, Jan 16 2018, 12:04:33) \r\n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\r\nNumPy 1.15.1\r\nSciPy 1.1.0\r\nScikit-Learn 0.19.1\r\n```\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.21.git",
    "requested_commit": "",
    "mayil_response": "To implement the feature request for supporting arbitrary bin spacing in `sklearn.calibration.calibration_curve`, you can follow these steps:\n\n- Modify the `calibration_curve` function to accept an additional parameter `bins` which can be either an integer or an array-like structure specifying the bin edges.\n- If `bins` is an integer, the current behavior is retained, using either the 'uniform' or 'quantile' strategy.\n- If `bins` is an array, use these edges directly to determine the bins.\n- Update the documentation to reflect the new parameter.\n\nHere's a high-level overview of the changes to the `calibration_curve` function:\n\n```python\nfrom sklearn.utils import column_or_1d, check_consistent_length\nfrom sklearn.utils.validation import _check_pos_label_consistency\nimport numpy as np\n\ndef calibration_curve(y_true, y_prob, *, pos_label=None, n_bins=5, strategy='uniform', bins=None):\n    y_true = column_or_1d(y_true)\n    y_prob = column_or_1d(y_prob)\n    check_consistent_length(y_true, y_prob)\n    pos_label = _check_pos_label_consistency(pos_label, y_true)\n\n    if y_prob.min() < 0 or y_prob.max() > 1:\n        raise ValueError(\"y_prob has values outside [0, 1].\")\n\n    labels = np.unique(y_true)\n    if len(labels) > 2:\n        raise ValueError(f\"Only binary classification is supported. Provided labels {labels}.\")\n    y_true = y_true == pos_label\n\n    if bins is None:\n        if strategy == \"quantile\":  # Determine bin edges by distribution of data\n            quantiles = np.linspace(0, 1, n_bins + 1)\n            bins = np.percentile(y_prob, quantiles * 100)\n        elif strategy == \"uniform\":\n            bins = np.linspace(0.0, 1.0, n_bins + 1)\n        else:\n            raise ValueError(\"Invalid entry to 'strategy' input. Strategy must be either 'quantile' or 'uniform'.\")\n    else:\n        bins = np.sort(np.unique(bins))  # Ensure bins are unique and sorted\n\n    binids = np.digitize(y_prob, bins) - 1  # Find bin index for each y_prob\n\n    bin_sums = np.bincount(binids, weights=y_prob, minlength=len(bins))\n    bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))\n    bin_total = np.bincount(binids, minlength=len(bins))\n\n    nonzero = bin_total != 0\n    prob_true = bin_true[nonzero] / bin_total[nonzero]\n    prob_pred = bin_sums[nonzero] / bin_total[nonzero]\n\n    return prob_true, prob_pred\n```\n\nNote that the `bins` parameter should be handled carefully to ensure that it is compatible with the existing API and does not introduce breaking changes. The above code assumes that if `bins` is provided, it overrides the `n_bins` and `strategy` parameters.\n\nRemember to update the documentation and add tests to cover the new functionality. Testing should include cases where `bins` is an integer, an array of bin edges, and validation of input errors.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn)- [Lines 961 - 1348](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/calibration.py#L961-L1348)\n\n This snippet contains the definition of the `calibration_curve` function, which is the subject of the feature request. \n\n\n   - [Lines 969 - 1046](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/calibration.py#L969-L1046)\n\n This snippet provides the documentation and implementation details of the `calibration_curve` function, which is directly related to the feature request. \n\n\n   - [Lines 1048 - 1069](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/calibration.py#L1048-L1069)\n\n This snippet shows the current implementation of binning strategy within the `calibration_curve` function, which would need to be modified to support arbitrary bin spacing.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0004185,
        "snippet_processor": 0.08203,
        "issue_star_creation": 0.032729999999999995,
        "issue_star_solver": 0.07929000000000001,
        "bouncer": 0.030320000000000003
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711741138.279336,
        "relevant_snippets": [
            {
                "code": "n_bins = 20\nfor row_idx, label, df in zip(range(2), [\"train\", \"test\"], [df_train, df_test]):\n    df[\"Frequency\"].hist(bins=np.linspace(-1, 30, n_bins), ax=axes[row_idx, 0])\n\n    axes[row_idx, 0].set_title(\"Data\")\n    axes[row_idx, 0].set_yscale(\"log\")\n    axes[row_idx, 0].set_xlabel(\"y (observed Frequency)\")\n    axes[row_idx, 0].set_ylim([1e1, 5e5])\n    axes[row_idx, 0].set_ylabel(label + \" samples\")\n\n    for idx, model in enumerate([ridge_glm, poisson_glm, poisson_gbrt]):\n        y_pred = model.predict(df)\n\n        pd.Series(y_pred).hist(\n            bins=np.linspace(-1, 4, n_bins), ax=axes[row_idx, idx + 1]\n        )\n        axes[row_idx, idx + 1].set(\n            title=model[-1].__class__.__name__,\n            yscale=\"log\",\n            xlabel=\"y_pred (predicted expected Frequency)\",\n        )\nplt.tight_layout()\n\n# %%\n# The experimental data presents a long tail distribution for ``y``. In all\n# models, we predict the expected frequency of a random variable, so we will\n# have necessarily fewer extreme values than for the observed realizations of\n# that random variable. This explains that the mode of the histograms of model\n# predictions doesn't necessarily correspond to the smallest value.\n# Additionally, the normal distribution used in ``Ridge`` has a constant\n# variance, while for the Poisson distribution used in ``PoissonRegressor`` and\n# ``HistGradientBoostingRegressor``, the variance is proportional to the\n# predicted expected value.\n#\n# Thus, among the considered estimators, ``PoissonRegressor`` and\n# ``HistGradientBoostingRegressor`` are a-priori better suited for modeling the\n# long tail distribution of the non-negative data as compared to the ``Ridge``\n# model which makes a wrong assumption on the distribution of the target\n# variable.\n#\n# The ``HistGradientBoostingRegressor`` estimator has the most flexibility and\n# is able to predict higher expected values.\n#\n# Note that we could have used the least squares loss for the\n# ``HistGradientBoostingRegressor`` model. This would wrongly assume a normal\n# distributed response variable as does the `Ridge` model, and possibly\n# also lead to slightly negative predictions. However the gradient boosted\n# trees would still perform relatively well and in particular better than\n# ``PoissonRegressor`` thanks to the flexibility of the trees combined with the\n# large number of training samples.\n#\n# Evaluation of the calibration of predictions\n# --------------------------------------------\n#\n# To ensure that estimators yield reasonable predictions for different\n# policyholder types, we can bin test samples according to ``y_pred`` returned\n# by each model. Then for each bin, we compare the mean predicted ``y_pred``,\n# with the mean observed target:\n\nfrom sklearn.utils import gen_even_slices",
                "filename": "examples/linear_model/plot_poisson_regression_non_normal_loss.py",
                "start_index": 11648,
                "end_index": 14430,
                "start_line": 331,
                "end_line": 390,
                "max_line": 594,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n==============================\nProbability Calibration curves\n==============================\n\nWhen performing classification one often wants to predict not only the class\nlabel, but also the associated probability. This probability gives some\nkind of confidence on the prediction. This example demonstrates how to\nvisualize how well calibrated the predicted probabilities are using calibration\ncurves, also known as reliability diagrams. Calibration of an uncalibrated\nclassifier will also be demonstrated.\n\n\"\"\"\n\n# Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD 3 clause.\n# %%\n# Dataset\n# -------\n#\n# We will use a synthetic binary classification dataset with 100,000 samples\n# and 20 features. Of the 20 features, only 2 are informative, 10 are\n# redundant (random combinations of the informative features) and the\n# remaining 8 are uninformative (random numbers). Of the 100,000 samples, 1,000\n# will be used for model fitting and the rest for testing.\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(\n    n_samples=100_000, n_features=20, n_informative=2, n_redundant=10, random_state=42\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.99, random_state=42\n)\n\n# %%\n# Calibration curves\n# ------------------\n#\n# Gaussian Naive Bayes\n# ^^^^^^^^^^^^^^^^^^^^\n#\n# First, we will compare:\n#\n# * :class:`~sklearn.linear_model.LogisticRegression` (used as baseline\n#   since very often, properly regularized logistic regression is well\n#   calibrated by default thanks to the use of the log-loss)\n# * Uncalibrated :class:`~sklearn.naive_bayes.GaussianNB`\n# * :class:`~sklearn.naive_bayes.GaussianNB` with isotonic and sigmoid\n#   calibration (see :ref:`User Guide <calibration>`)\n#\n# Calibration curves for all 4 conditions are plotted below, with the average\n# predicted probability for each bin on the x-axis and the fraction of positive\n# classes in each bin on the y-axis.\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\nfrom sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\n\nlr = LogisticRegression(C=1.0)\ngnb = GaussianNB()\ngnb_isotonic = CalibratedClassifierCV(gnb, cv=2, method=\"isotonic\")\ngnb_sigmoid = CalibratedClassifierCV(gnb, cv=2, method=\"sigmoid\")\n\nclf_list = [\n    (lr, \"Logistic\"),\n    (gnb, \"Naive Bayes\"),\n    (gnb_isotonic, \"Naive Bayes + Isotonic\"),\n    (gnb_sigmoid, \"Naive Bayes + Sigmoid\"),\n]\n\n# %%\nfig = plt.figure(figsize=(10, 10))\ngs = GridSpec(4, 2)\ncolors = plt.get_cmap(\"Dark2\")\n\nax_calibration_curve = fig.add_subplot(gs[:2, :2])\ncalibration_displays = {}",
                "filename": "examples/calibration/plot_calibration_curve.py",
                "start_index": 0,
                "end_index": 2838,
                "start_line": 1,
                "end_line": 241,
                "max_line": 336,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "name: Feature request\ndescription: Suggest a new algorithm, enhancement to an existing algorithm, etc.\nlabels: ['New Feature', 'Needs Triage']\n\nbody:\n- type: markdown\n  attributes:\n    value: >\n      #### If you want to propose a new algorithm, please refer first to the [scikit-learn inclusion criterion](https://scikit-learn.org/stable/faq.html#what-are-the-inclusion-criteria-for-new-algorithms).\n- type: textarea\n  attributes:\n    label: Describe the workflow you want to enable\n  validations:\n    required: true\n- type: textarea\n  attributes:\n    label: Describe your proposed solution\n  validations:\n    required: true\n- type: textarea\n  attributes:\n    label: Describe alternatives you've considered, if relevant\n- type: textarea\n  attributes:\n    label: Additional context",
                "filename": ".github/ISSUE_TEMPLATE/feature_request.yml",
                "start_index": 0,
                "end_index": 780,
                "start_line": 1,
                "end_line": 25,
                "max_line": 25,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n======================================\nProbability calibration of classifiers\n======================================\n\nWhen performing classification you often want to predict not only\nthe class label, but also the associated probability. This probability\ngives you some kind of confidence on the prediction. However, not all\nclassifiers provide well-calibrated probabilities, some being over-confident\nwhile others being under-confident. Thus, a separate calibration of predicted\nprobabilities is often desirable as a postprocessing. This example illustrates\ntwo different methods for this calibration and evaluates the quality of the\nreturned probabilities using Brier's score\n(see https://en.wikipedia.org/wiki/Brier_score).\n\nCompared are the estimated probability using a Gaussian naive Bayes classifier\nwithout calibration, with a sigmoid calibration, and with a non-parametric\nisotonic calibration. One can observe that only the non-parametric model is\nable to provide a probability calibration that returns probabilities close\nto the expected 0.5 for most of the samples belonging to the middle\ncluster with heterogeneous labels. This results in a significantly improved\nBrier score.\n\n\"\"\"\n# Authors:\n# Mathieu Blondel <mathieu@mblondel.org>\n# Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n# Balazs Kegl <balazs.kegl@gmail.com>\n# Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD Style.\n\n# %%\n# Generate synthetic dataset\n# --------------------------\nimport numpy as np\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import train_test_split\n\nn_samples = 50000\nn_bins = 3  # use 3 bins for calibration_curve as we have 3 clusters here\n\n# Generate 3 blobs with 2 classes where the second blob contains\n# half positive samples and half negative samples. Probability in this\n# blob is therefore 0.5.\ncenters = [(-5, -5), (0, 0), (5, 5)]\nX, y = make_blobs(n_samples=n_samples, centers=centers, shuffle=False, random_state=42)\n\ny[: n_samples // 2] = 0\ny[n_samples // 2 :] = 1\nsample_weight = np.random.RandomState(42).rand(y.shape[0])\n\n# split train, test for calibration\nX_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(\n    X, y, sample_weight, test_size=0.9, random_state=42\n)\n\n# %%\n# Gaussian Naive-Bayes\n# --------------------\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import brier_score_loss\nfrom sklearn.naive_bayes import GaussianNB\n\n# With no calibration\nclf = GaussianNB()\nclf.fit(X_train, y_train)  # GaussianNB itself does not support sample-weights\nprob_pos_clf = clf.predict_proba(X_test)[:, 1]\n\n# With isotonic calibration\nclf_isotonic = CalibratedClassifierCV(clf, cv=2, method=\"isotonic\")\nclf_isotonic.fit(X_train, y_train, sample_weight=sw_train)\nprob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1]\n\n# With sigmoid calibration\nclf_sigmoid = CalibratedClassifierCV(clf, cv=2, method=\"sigmoid\")\nclf_sigmoid.fit(X_train, y_train, sample_weight=sw_train)",
                "filename": "examples/calibration/plot_calibration.py",
                "start_index": 0,
                "end_index": 2989,
                "start_line": 1,
                "end_line": 77,
                "max_line": 144,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "def _mean_frequency_by_risk_group(y_true, y_pred, sample_weight=None, n_bins=100):\n    \"\"\"Compare predictions and observations for bins ordered by y_pred.\n\n    We order the samples by ``y_pred`` and split it in bins.\n    In each bin the observed mean is compared with the predicted mean.\n\n    Parameters\n    ----------\n    y_true: array-like of shape (n_samples,)\n        Ground truth (correct) target values.\n    y_pred: array-like of shape (n_samples,)\n        Estimated target values.\n    sample_weight : array-like of shape (n_samples,)\n        Sample weights.\n    n_bins: int\n        Number of bins to use.\n\n    Returns\n    -------\n    bin_centers: ndarray of shape (n_bins,)\n        bin centers\n    y_true_bin: ndarray of shape (n_bins,)\n        average y_pred for each bin\n    y_pred_bin: ndarray of shape (n_bins,)\n        average y_pred for each bin\n    \"\"\"\n    idx_sort = np.argsort(y_pred)\n    bin_centers = np.arange(0, 1, 1 / n_bins) + 0.5 / n_bins\n    y_pred_bin = np.zeros(n_bins)\n    y_true_bin = np.zeros(n_bins)\n\n    for n, sl in enumerate(gen_even_slices(len(y_true), n_bins)):\n        weights = sample_weight[idx_sort][sl]\n        y_pred_bin[n] = np.average(y_pred[idx_sort][sl], weights=weights)\n        y_true_bin[n] = np.average(y_true[idx_sort][sl], weights=weights)\n    return bin_centers, y_true_bin, y_pred_bin\n\n\nprint(f\"Actual number of claims: {df_test['ClaimNb'].sum()}\")\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\nplt.subplots_adjust(wspace=0.3)\n\nfor axi, model in zip(ax.ravel(), [ridge_glm, poisson_glm, poisson_gbrt, dummy]):\n    y_pred = model.predict(df_test)\n    y_true = df_test[\"Frequency\"].values\n    exposure = df_test[\"Exposure\"].values\n    q, y_true_seg, y_pred_seg = _mean_frequency_by_risk_group(\n        y_true, y_pred, sample_weight=exposure, n_bins=10\n    )\n\n    # Name of the model after the estimator used in the last step of the\n    # pipeline.\n    print(f\"Predicted number of claims by {model[-1]}: {np.sum(y_pred * exposure):.1f}\")\n\n    axi.plot(q, y_pred_seg, marker=\"x\", linestyle=\"--\", label=\"predictions\")\n    axi.plot(q, y_true_seg, marker=\"o\", linestyle=\"--\", label=\"observations\")\n    axi.set_xlim(0, 1.0)\n    axi.set_ylim(0, 0.5)\n    axi.set(\n        title=model[-1],\n        xlabel=\"Fraction of samples sorted by y_pred\",\n        ylabel=\"Mean Frequency (y_pred)\",\n    )\n    axi.legend()\nplt.tight_layout()\n\n# %%\n# The dummy regression model predicts a constant frequency. This model does not\n# attribute the same tied rank to all samples but is none-the-less globally\n# well calibrated (to estimate the mean frequency of the entire population).\n#\n# The ``Ridge`` regression model can predict very low expected frequencies that\n# do not match the data. It can therefore severely under-estimate the risk for\n# some policyholders.\n#\n# ``PoissonRegressor`` and ``HistGradientBoostingRegressor`` show better\n# consistency between predicted and observed targets, especially for low\n# predicted target values.\n#",
                "filename": "examples/linear_model/plot_poisson_regression_non_normal_loss.py",
                "start_index": 14433,
                "end_index": 17414,
                "start_line": 393,
                "end_line": 592,
                "max_line": 594,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "for jj in range(n_features):\n            column = X[:, jj]\n            col_min, col_max = column.min(), column.max()\n\n            if col_min == col_max:\n                warnings.warn(\n                    \"Feature %d is constant and will be replaced with 0.\" % jj\n                )\n                n_bins[jj] = 1\n                bin_edges[jj] = np.array([-np.inf, np.inf])\n                continue\n\n            if self.strategy == \"uniform\":\n                bin_edges[jj] = np.linspace(col_min, col_max, n_bins[jj] + 1)\n\n            elif self.strategy == \"quantile\":\n                quantiles = np.linspace(0, 100, n_bins[jj] + 1)\n                if sample_weight is None:\n                    bin_edges[jj] = np.asarray(np.percentile(column, quantiles))\n                else:\n                    bin_edges[jj] = np.asarray(\n                        [\n                            _weighted_percentile(column, sample_weight, q)\n                            for q in quantiles\n                        ],\n                        dtype=np.float64,\n                    )\n            elif self.strategy == \"kmeans\":\n                from ..cluster import KMeans  # fixes import loops\n\n                # Deterministic initialization with uniform spacing\n                uniform_edges = np.linspace(col_min, col_max, n_bins[jj] + 1)\n                init = (uniform_edges[1:] + uniform_edges[:-1])[:, None] * 0.5\n\n                # 1D k-means procedure\n                km = KMeans(n_clusters=n_bins[jj], init=init, n_init=1)\n                centers = km.fit(\n                    column[:, None], sample_weight=sample_weight\n                ).cluster_centers_[:, 0]\n                # Must sort, centers may be unsorted even with sorted init\n                centers.sort()\n                bin_edges[jj] = (centers[1:] + centers[:-1]) * 0.5\n                bin_edges[jj] = np.r_[col_min, bin_edges[jj], col_max]\n\n            # Remove bins whose width are too small (i.e., <= 1e-8)\n            if self.strategy in (\"quantile\", \"kmeans\"):\n                mask = np.ediff1d(bin_edges[jj], to_begin=np.inf) > 1e-8\n                bin_edges[jj] = bin_edges[jj][mask]\n                if len(bin_edges[jj]) - 1 != n_bins[jj]:\n                    warnings.warn(\n                        \"Bins whose width are too small (i.e., <= \"\n                        \"1e-8) in feature %d are removed. Consider \"\n                        \"decreasing the number of bins.\" % jj\n                    )\n                    n_bins[jj] = len(bin_edges[jj]) - 1\n\n        self.bin_edges_ = bin_edges\n        self.n_bins_ = n_bins",
                "filename": "sklearn/preprocessing/_discretization.py",
                "start_index": 9311,
                "end_index": 11891,
                "start_line": 264,
                "end_line": 321,
                "max_line": 463,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "if strategy == \"quantile\":  # Determine bin edges by distribution of data\n        quantiles = np.linspace(0, 1, n_bins + 1)\n        bins = np.percentile(y_prob, quantiles * 100)\n    elif strategy == \"uniform\":\n        bins = np.linspace(0.0, 1.0, n_bins + 1)\n    else:\n        raise ValueError(\n            \"Invalid entry to 'strategy' input. Strategy \"\n            \"must be either 'quantile' or 'uniform'.\"\n        )\n\n    binids = np.searchsorted(bins[1:-1], y_prob)\n\n    bin_sums = np.bincount(binids, weights=y_prob, minlength=len(bins))\n    bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))\n    bin_total = np.bincount(binids, minlength=len(bins))\n\n    nonzero = bin_total != 0\n    prob_true = bin_true[nonzero] / bin_total[nonzero]\n    prob_pred = bin_sums[nonzero] / bin_total[nonzero]\n\n    return prob_true, prob_pred",
                "filename": "sklearn/calibration.py",
                "start_index": 36081,
                "end_index": 36923,
                "start_line": 1048,
                "end_line": 1069,
                "max_line": 1447,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "def calibration_curve(\n    y_true,\n    y_prob,\n    *,\n    pos_label=None,\n    n_bins=5,\n    strategy=\"uniform\",\n):",
                "filename": "sklearn/calibration.py",
                "start_index": 33260,
                "end_index": 33374,
                "start_line": 961,
                "end_line": 1348,
                "max_line": 1447,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "class CalibrationDisplay(_BinaryClassifierCurveDisplayMixin):",
                "filename": "sklearn/calibration.py",
                "start_index": 36926,
                "end_index": 36987,
                "start_line": 1072,
                "end_line": 1072,
                "max_line": 1447,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"Compute true and predicted probabilities for a calibration curve.\n\n    The method assumes the inputs come from a binary classifier, and\n    discretize the [0, 1] interval into bins.\n\n    Calibration curves may also be referred to as reliability diagrams.\n\n    Read more in the :ref:`User Guide <calibration>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,)\n        True targets.\n\n    y_prob : array-like of shape (n_samples,)\n        Probabilities of the positive class.\n\n    pos_label : int, float, bool or str, default=None\n        The label of the positive class.\n\n        .. versionadded:: 1.1\n\n    n_bins : int, default=5\n        Number of bins to discretize the [0, 1] interval. A bigger number\n        requires more data. Bins with no samples (i.e. without\n        corresponding values in `y_prob`) will not be returned, thus the\n        returned arrays may have less than `n_bins` values.\n\n    strategy : {'uniform', 'quantile'}, default='uniform'\n        Strategy used to define the widths of the bins.\n\n        uniform\n            The bins have identical widths.\n        quantile\n            The bins have the same number of samples and depend on `y_prob`.\n\n    Returns\n    -------\n    prob_true : ndarray of shape (n_bins,) or smaller\n        The proportion of samples whose class is the positive class, in each\n        bin (fraction of positives).\n\n    prob_pred : ndarray of shape (n_bins,) or smaller\n        The mean predicted probability in each bin.\n\n    References\n    ----------\n    Alexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good\n    Probabilities With Supervised Learning, in Proceedings of the 22nd\n    International Conference on Machine Learning (ICML).\n    See section 4 (Qualitative Analysis of Predictions).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.calibration import calibration_curve\n    >>> y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])\n    >>> y_pred = np.array([0.1, 0.2, 0.3, 0.4, 0.65, 0.7, 0.8, 0.9,  1.])\n    >>> prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=3)\n    >>> prob_true\n    array([0. , 0.5, 1. ])\n    >>> prob_pred\n    array([0.2  , 0.525, 0.85 ])\n    \"\"\"\n    y_true = column_or_1d(y_true)\n    y_prob = column_or_1d(y_prob)\n    check_consistent_length(y_true, y_prob)\n    pos_label = _check_pos_label_consistency(pos_label, y_true)\n\n    if y_prob.min() < 0 or y_prob.max() > 1:\n        raise ValueError(\"y_prob has values outside [0, 1].\")\n\n    labels = np.unique(y_true)\n    if len(labels) > 2:\n        raise ValueError(\n            f\"Only binary classification is supported. Provided labels {labels}.\"\n        )\n    y_true = y_true == pos_label",
                "filename": "sklearn/calibration.py",
                "start_index": 33379,
                "end_index": 36075,
                "start_line": 969,
                "end_line": 1046,
                "max_line": 1447,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            }
        ],
        "classification": "feature request",
        "snippet_processor_task": {
            "sklearn/calibration.py": [
                {
                    "chunk": {
                        "code": "def calibration_curve(\n    y_true,\n    y_prob,\n    *,\n    pos_label=None,\n    n_bins=5,\n    strategy=\"uniform\",\n):",
                        "filename": "sklearn/calibration.py",
                        "start_index": 33260,
                        "end_index": 33374,
                        "start_line": 961,
                        "end_line": 1348,
                        "max_line": 1447,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the definition of the `calibration_curve` function, which is the subject of the feature request."
                },
                {
                    "chunk": {
                        "code": "\"\"\"Compute true and predicted probabilities for a calibration curve.\n\n    The method assumes the inputs come from a binary classifier, and\n    discretize the [0, 1] interval into bins.\n\n    Calibration curves may also be referred to as reliability diagrams.\n\n    Read more in the :ref:`User Guide <calibration>`.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,)\n        True targets.\n\n    y_prob : array-like of shape (n_samples,)\n        Probabilities of the positive class.\n\n    pos_label : int, float, bool or str, default=None\n        The label of the positive class.\n\n        .. versionadded:: 1.1\n\n    n_bins : int, default=5\n        Number of bins to discretize the [0, 1] interval. A bigger number\n        requires more data. Bins with no samples (i.e. without\n        corresponding values in `y_prob`) will not be returned, thus the\n        returned arrays may have less than `n_bins` values.\n\n    strategy : {'uniform', 'quantile'}, default='uniform'\n        Strategy used to define the widths of the bins.\n\n        uniform\n            The bins have identical widths.\n        quantile\n            The bins have the same number of samples and depend on `y_prob`.\n\n    Returns\n    -------\n    prob_true : ndarray of shape (n_bins,) or smaller\n        The proportion of samples whose class is the positive class, in each\n        bin (fraction of positives).\n\n    prob_pred : ndarray of shape (n_bins,) or smaller\n        The mean predicted probability in each bin.\n\n    References\n    ----------\n    Alexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good\n    Probabilities With Supervised Learning, in Proceedings of the 22nd\n    International Conference on Machine Learning (ICML).\n    See section 4 (Qualitative Analysis of Predictions).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.calibration import calibration_curve\n    >>> y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])\n    >>> y_pred = np.array([0.1, 0.2, 0.3, 0.4, 0.65, 0.7, 0.8, 0.9,  1.])\n    >>> prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=3)\n    >>> prob_true\n    array([0. , 0.5, 1. ])\n    >>> prob_pred\n    array([0.2  , 0.525, 0.85 ])\n    \"\"\"\n    y_true = column_or_1d(y_true)\n    y_prob = column_or_1d(y_prob)\n    check_consistent_length(y_true, y_prob)\n    pos_label = _check_pos_label_consistency(pos_label, y_true)\n\n    if y_prob.min() < 0 or y_prob.max() > 1:\n        raise ValueError(\"y_prob has values outside [0, 1].\")\n\n    labels = np.unique(y_true)\n    if len(labels) > 2:\n        raise ValueError(\n            f\"Only binary classification is supported. Provided labels {labels}.\"\n        )\n    y_true = y_true == pos_label",
                        "filename": "sklearn/calibration.py",
                        "start_index": 33379,
                        "end_index": 36075,
                        "start_line": 969,
                        "end_line": 1046,
                        "max_line": 1447,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet provides the documentation and implementation details of the `calibration_curve` function, which is directly related to the feature request."
                },
                {
                    "chunk": {
                        "code": "if strategy == \"quantile\":  # Determine bin edges by distribution of data\n        quantiles = np.linspace(0, 1, n_bins + 1)\n        bins = np.percentile(y_prob, quantiles * 100)\n    elif strategy == \"uniform\":\n        bins = np.linspace(0.0, 1.0, n_bins + 1)\n    else:\n        raise ValueError(\n            \"Invalid entry to 'strategy' input. Strategy \"\n            \"must be either 'quantile' or 'uniform'.\"\n        )\n\n    binids = np.searchsorted(bins[1:-1], y_prob)\n\n    bin_sums = np.bincount(binids, weights=y_prob, minlength=len(bins))\n    bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))\n    bin_total = np.bincount(binids, minlength=len(bins))\n\n    nonzero = bin_total != 0\n    prob_true = bin_true[nonzero] / bin_total[nonzero]\n    prob_pred = bin_sums[nonzero] / bin_total[nonzero]\n\n    return prob_true, prob_pred",
                        "filename": "sklearn/calibration.py",
                        "start_index": 36081,
                        "end_index": 36923,
                        "start_line": 1048,
                        "end_line": 1069,
                        "max_line": 1447,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the current implementation of binning strategy within the `calibration_curve` function, which would need to be modified to support arbitrary bin spacing."
                }
            ]
        },
        "solution_task": "To implement the feature request for supporting arbitrary bin spacing in `sklearn.calibration.calibration_curve`, you can follow these steps:\n\n- Modify the `calibration_curve` function to accept an additional parameter `bins` which can be either an integer or an array-like structure specifying the bin edges.\n- If `bins` is an integer, the current behavior is retained, using either the 'uniform' or 'quantile' strategy.\n- If `bins` is an array, use these edges directly to determine the bins.\n- Update the documentation to reflect the new parameter.\n\nHere's a high-level overview of the changes to the `calibration_curve` function:\n\n```python\nfrom sklearn.utils import column_or_1d, check_consistent_length\nfrom sklearn.utils.validation import _check_pos_label_consistency\nimport numpy as np\n\ndef calibration_curve(y_true, y_prob, *, pos_label=None, n_bins=5, strategy='uniform', bins=None):\n    y_true = column_or_1d(y_true)\n    y_prob = column_or_1d(y_prob)\n    check_consistent_length(y_true, y_prob)\n    pos_label = _check_pos_label_consistency(pos_label, y_true)\n\n    if y_prob.min() < 0 or y_prob.max() > 1:\n        raise ValueError(\"y_prob has values outside [0, 1].\")\n\n    labels = np.unique(y_true)\n    if len(labels) > 2:\n        raise ValueError(f\"Only binary classification is supported. Provided labels {labels}.\")\n    y_true = y_true == pos_label\n\n    if bins is None:\n        if strategy == \"quantile\":  # Determine bin edges by distribution of data\n            quantiles = np.linspace(0, 1, n_bins + 1)\n            bins = np.percentile(y_prob, quantiles * 100)\n        elif strategy == \"uniform\":\n            bins = np.linspace(0.0, 1.0, n_bins + 1)\n        else:\n            raise ValueError(\"Invalid entry to 'strategy' input. Strategy must be either 'quantile' or 'uniform'.\")\n    else:\n        bins = np.sort(np.unique(bins))  # Ensure bins are unique and sorted\n\n    binids = np.digitize(y_prob, bins) - 1  # Find bin index for each y_prob\n\n    bin_sums = np.bincount(binids, weights=y_prob, minlength=len(bins))\n    bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))\n    bin_total = np.bincount(binids, minlength=len(bins))\n\n    nonzero = bin_total != 0\n    prob_true = bin_true[nonzero] / bin_total[nonzero]\n    prob_pred = bin_sums[nonzero] / bin_total[nonzero]\n\n    return prob_true, prob_pred\n```\n\nNote that the `bins` parameter should be handled carefully to ensure that it is compatible with the existing API and does not introduce breaking changes. The above code assumes that if `bins` is provided, it overrides the `n_bins` and `strategy` parameters.\n\nRemember to update the documentation and add tests to cover the new functionality. Testing should include cases where `bins` is an integer, an array of bin edges, and validation of input errors.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant suggestions that can directly lead to the successful implementation or improvement of the requested feature.",
            "supporting_evidence": "The RESPONSE provides a detailed and actionable plan for implementing the feature request to support arbitrary bin spacing in the `calibration_curve` function. It suggests modifying the function to accept an additional parameter for bin edges, outlines the logic for handling different types of `bins` input, and emphasizes the need to update documentation and add tests. The RESPONSE also includes a high-level overview of the changes with a sample code snippet, which is directly relevant to the feature request. Furthermore, it provides links to the relevant code snippets in the existing `sklearn` repository, which would be useful for someone looking to implement this feature. The RESPONSE is comprehensive and addresses all aspects of the feature request, facilitating its successful implementation.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711741352.538134,
        "result": "To implement the feature request for supporting arbitrary bin spacing in `sklearn.calibration.calibration_curve`, you can follow these steps:\n\n- Modify the `calibration_curve` function to accept an additional parameter `bins` which can be either an integer or an array-like structure specifying the bin edges.\n- If `bins` is an integer, the current behavior is retained, using either the 'uniform' or 'quantile' strategy.\n- If `bins` is an array, use these edges directly to determine the bins.\n- Update the documentation to reflect the new parameter.\n\nHere's a high-level overview of the changes to the `calibration_curve` function:\n\n```python\nfrom sklearn.utils import column_or_1d, check_consistent_length\nfrom sklearn.utils.validation import _check_pos_label_consistency\nimport numpy as np\n\ndef calibration_curve(y_true, y_prob, *, pos_label=None, n_bins=5, strategy='uniform', bins=None):\n    y_true = column_or_1d(y_true)\n    y_prob = column_or_1d(y_prob)\n    check_consistent_length(y_true, y_prob)\n    pos_label = _check_pos_label_consistency(pos_label, y_true)\n\n    if y_prob.min() < 0 or y_prob.max() > 1:\n        raise ValueError(\"y_prob has values outside [0, 1].\")\n\n    labels = np.unique(y_true)\n    if len(labels) > 2:\n        raise ValueError(f\"Only binary classification is supported. Provided labels {labels}.\")\n    y_true = y_true == pos_label\n\n    if bins is None:\n        if strategy == \"quantile\":  # Determine bin edges by distribution of data\n            quantiles = np.linspace(0, 1, n_bins + 1)\n            bins = np.percentile(y_prob, quantiles * 100)\n        elif strategy == \"uniform\":\n            bins = np.linspace(0.0, 1.0, n_bins + 1)\n        else:\n            raise ValueError(\"Invalid entry to 'strategy' input. Strategy must be either 'quantile' or 'uniform'.\")\n    else:\n        bins = np.sort(np.unique(bins))  # Ensure bins are unique and sorted\n\n    binids = np.digitize(y_prob, bins) - 1  # Find bin index for each y_prob\n\n    bin_sums = np.bincount(binids, weights=y_prob, minlength=len(bins))\n    bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))\n    bin_total = np.bincount(binids, minlength=len(bins))\n\n    nonzero = bin_total != 0\n    prob_true = bin_true[nonzero] / bin_total[nonzero]\n    prob_pred = bin_sums[nonzero] / bin_total[nonzero]\n\n    return prob_true, prob_pred\n```\n\nNote that the `bins` parameter should be handled carefully to ensure that it is compatible with the existing API and does not introduce breaking changes. The above code assumes that if `bins` is provided, it overrides the `n_bins` and `strategy` parameters.\n\nRemember to update the documentation and add tests to cover the new functionality. Testing should include cases where `bins` is an integer, an array of bin edges, and validation of input errors.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn)- [Lines 961 - 1348](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/calibration.py#L961-L1348)\n\n This snippet contains the definition of the `calibration_curve` function, which is the subject of the feature request. \n\n\n   - [Lines 969 - 1046](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/calibration.py#L969-L1046)\n\n This snippet provides the documentation and implementation details of the `calibration_curve` function, which is directly related to the feature request. \n\n\n   - [Lines 1048 - 1069](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/calibration.py#L1048-L1069)\n\n This snippet shows the current implementation of binning strategy within the `calibration_curve` function, which would need to be modified to support arbitrary bin spacing."
    }
}