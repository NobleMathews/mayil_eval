{
    "id": "scikit-learn__scikit-learn-14706",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
    "task_id": 5136252200408667047,
    "title": "kernel_approximation.Nystroem does not support precomputed kernel",
    "body": "The documentation says that precomputed kernels are supported in Nystroem, but in reality it does not seem to be the case: https://scikit-learn.org/stable/modules/kernel_approximation.html\r\n\r\n> By default Nystroem uses the rbf kernel, but it can use any kernel function or a precomputed kernel matrix.\r\n\r\nExample code:\r\n```python\r\nfrom sklearn.kernel_approximation import Nystroem\r\nnys = Nystroem(kernel='precomputed')\r\nnys.fit_transform(K)\r\n```\r\nLeads to `KeyError: 'precomputed'`\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.22.git",
    "requested_commit": "",
    "mayil_response": "- Check if `Nystroem` class has a `fit` method that handles the `kernel='precomputed'` case.\n- If not, modify the `fit` method of the `Nystroem` class to handle `kernel='precomputed'`.\n- Ensure that when `kernel='precomputed'`, the `fit` method does not attempt to compute the kernel again but instead uses the provided precomputed kernel matrix.\n- Update the `_get_kernel_params` method to correctly handle the `'precomputed'` kernel case without raising a `ValueError`.\n- Add a check in the `fit_transform` method to ensure that when `kernel='precomputed'`, the input `X` is a square matrix (i.e., `X.shape[0] == X.shape[1]`), as it should be a precomputed kernel matrix.\n- If the input is not a square matrix, raise an appropriate error message indicating that a square matrix is expected for a precomputed kernel.\n- Add unit tests to cover the new functionality and ensure that passing a precomputed kernel matrix to `Nystroem` with `kernel='precomputed'` does not raise a `KeyError` and works as expected.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn)- [Lines 864 - 1089](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/kernel_approximation.py#L864-L1089)\n\n This snippet contains the class definition and documentation for Nystroem, which is relevant to the issue as it mentions the kernel parameter and its default value. It is necessary to check if 'precomputed' is actually supported as per the documentation. \n\n\n   - [Lines 1104 - 1134](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/kernel_approximation.py#L1104-L1134)\n\n This snippet contains the _get_kernel_params method which checks if the kernel is callable or 'precomputed' and raises a ValueError if gamma, coef0, or degree are passed. It is relevant because it directly handles the 'precomputed' kernel case and may contain the logic that leads to the KeyError.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0009725,
        "snippet_processor": 0.08512000000000002,
        "issue_star_creation": 0.049199999999999994,
        "issue_star_solver": 0.054560000000000004,
        "bouncer": 0.02766
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711739000.9760451,
        "relevant_snippets": [
            {
                "code": "\"\"\"Approximate a kernel map using a subset of the training data.\n\n    Constructs an approximate feature map for an arbitrary kernel\n    using a subset of the data as basis.\n\n    Read more in the :ref:`User Guide <nystroem_kernel_approx>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    kernel : str or callable, default='rbf'\n        Kernel map to be approximated. A callable should accept two arguments\n        and the keyword arguments passed to this object as `kernel_params`, and\n        should return a floating point number.\n\n    gamma : float, default=None\n        Gamma parameter for the RBF, laplacian, polynomial, exponential chi2\n        and sigmoid kernels. Interpretation of the default value is left to\n        the kernel; see the documentation for sklearn.metrics.pairwise.\n        Ignored by other kernels.\n\n    coef0 : float, default=None\n        Zero coefficient for polynomial and sigmoid kernels.\n        Ignored by other kernels.\n\n    degree : float, default=None\n        Degree of the polynomial kernel. Ignored by other kernels.\n\n    kernel_params : dict, default=None\n        Additional parameters (keyword arguments) for kernel function passed\n        as callable object.\n\n    n_components : int, default=100\n        Number of features to construct.\n        How many data points will be used to construct the mapping.\n\n    random_state : int, RandomState instance or None, default=None\n        Pseudo-random number generator to control the uniform sampling without\n        replacement of `n_components` of the training data to construct the\n        basis kernel.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. This works by breaking\n        down the kernel matrix into `n_jobs` even slices and computing them in\n        parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Subset of training points used to construct the feature map.\n\n    component_indices_ : ndarray of shape (n_components)\n        Indices of ``components_`` in the training set.\n\n    normalization_ : ndarray of shape (n_components, n_components)\n        Normalization matrix needed for embedding.\n        Square root of the kernel matrix on ``components_``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\n    PolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch.\n    RBFSampler : Approximate a RBF kernel feature map using random Fourier\n        features.\n    SkewedChi2Sampler : Approximate feature map for \"skewed chi-squared\" kernel.\n    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n\n    References\n    ----------\n    * Williams, C.K.I. and Seeger, M.\n      \"Using the Nystroem method to speed up kernel machines\",\n      Advances in neural information processing systems 2001\n\n    * T. Yang, Y. Li, M. Mahdavi, R. Jin and Z. Zhou\n      \"Nystroem Method vs Random Fourier Features: A Theoretical and Empirical\n      Comparison\",\n      Advances in Neural Information Processing Systems 2012\n\n    Examples\n    --------\n    >>> from sklearn import datasets, svm\n    >>> from sklearn.kernel_approximation import Nystroem\n    >>> X, y = datasets.load_digits(n_class=9, return_X_y=True)\n    >>> data = X / 16.\n    >>> clf = svm.LinearSVC(dual=\"auto\")\n    >>> feature_map_nystroem = Nystroem(gamma=.2,\n    ...                                 random_state=1,\n    ...                                 n_components=300)\n    >>> data_transformed = feature_map_nystroem.fit_transform(data)\n    >>> clf.fit(data_transformed, y)\n    LinearSVC(dual='auto')\n    >>> clf.score(data_transformed, y)\n    0.9987...\n    \"\"\"",
                "filename": "sklearn/kernel_approximation.py",
                "start_index": 31336,
                "end_index": 35702,
                "start_line": 864,
                "end_line": 1089,
                "max_line": 1134,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "def _get_kernel_params(self):\n        params = self.kernel_params\n        if params is None:\n            params = {}\n        if not callable(self.kernel) and self.kernel != \"precomputed\":\n            for param in KERNEL_PARAMS[self.kernel]:\n                if getattr(self, param) is not None:\n                    params[param] = getattr(self, param)\n        else:\n            if (\n                self.gamma is not None\n                or self.coef0 is not None\n                or self.degree is not None\n            ):\n                raise ValueError(\n                    \"Don't pass gamma, coef0 or degree to \"\n                    \"Nystroem if using a callable \"\n                    \"or precomputed kernel\"\n                )\n\n        return params\n\n    def _more_tags(self):\n        return {\n            \"_xfail_checks\": {\n                \"check_transformer_preserve_dtypes\": (\n                    \"dtypes are preserved but not at a close enough precision\"\n                )\n            },\n            \"preserves_dtype\": [np.float64, np.float32],\n        }",
                "filename": "sklearn/kernel_approximation.py",
                "start_index": 39643,
                "end_index": 40703,
                "start_line": 1104,
                "end_line": 1134,
                "max_line": 1134,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"\n==================================================\nExplicit feature map approximation for RBF kernels\n==================================================\n\nAn example illustrating the approximation of the feature map\nof an RBF kernel.\n\n.. currentmodule:: sklearn.kernel_approximation\n\nIt shows how to use :class:`RBFSampler` and :class:`Nystroem` to\napproximate the feature map of an RBF kernel for classification with an SVM on\nthe digits dataset. Results using a linear SVM in the original space, a linear\nSVM using the approximate mappings and using a kernelized SVM are compared.\nTimings and accuracy for varying amounts of Monte Carlo samplings (in the case\nof :class:`RBFSampler`, which uses random Fourier features) and different sized\nsubsets of the training set (for :class:`Nystroem`) for the approximate mapping\nare shown.\n\nPlease note that the dataset here is not large enough to show the benefits\nof kernel approximation, as the exact SVM is still reasonably fast.\n\nSampling more dimensions clearly leads to better classification results, but\ncomes at a greater cost. This means there is a tradeoff between runtime and\naccuracy, given by the parameter n_components. Note that solving the Linear\nSVM and also the approximate kernel SVM could be greatly accelerated by using\nstochastic gradient descent via :class:`~sklearn.linear_model.SGDClassifier`.\nThis is not easily possible for the case of the kernelized SVM.\n\n\"\"\"\n\n# %%\n# Python package and dataset imports, load dataset\n# ---------------------------------------------------\n\n\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n#         Andreas Mueller <amueller@ais.uni-bonn.de>\n# License: BSD 3 clause\n\n# Standard scientific Python imports\nfrom time import time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Import datasets, classifiers and performance metrics\nfrom sklearn import datasets, pipeline, svm\nfrom sklearn.decomposition import PCA\nfrom sklearn.kernel_approximation import Nystroem, RBFSampler\n\n# The digits dataset\ndigits = datasets.load_digits(n_class=9)\n\n\n# %%\n# Timing and accuracy plots\n# --------------------------------------------------\n# To apply an classifier on this data, we need to flatten the image, to\n# turn the data in a (samples, feature) matrix:\nn_samples = len(digits.data)\ndata = digits.data / 16.0\ndata -= data.mean(axis=0)\n\n# We learn the digits on the first half of the digits\ndata_train, targets_train = (data[: n_samples // 2], digits.target[: n_samples // 2])\n\n\n# Now predict the value of the digit on the second half:\ndata_test, targets_test = (data[n_samples // 2 :], digits.target[n_samples // 2 :])\n# data_test = scaler.transform(data_test)\n\n# Create a classifier: a support vector classifier\nkernel_svm = svm.SVC(gamma=0.2)\nlinear_svm = svm.LinearSVC(dual=\"auto\")\n\n# create pipeline from kernel approximation\n# and linear svm\nfeature_map_fourier = RBFSampler(gamma=0.2, random_state=1)\nfeature_map_nystroem = Nystroem(gamma=0.2, random_state=1)",
                "filename": "examples/miscellaneous/plot_kernel_approximation.py",
                "start_index": 0,
                "end_index": 2987,
                "start_line": 1,
                "end_line": 80,
                "max_line": 254,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"\nThe :mod:`sklearn.kernel_approximation` module implements several\napproximate kernel feature maps based on Fourier transforms and Count Sketches.\n\"\"\"\n\n# Author: Andreas Mueller <amueller@ais.uni-bonn.de>\n#         Daniel Lopez-Sanchez (TensorSketch) <lope@usal.es>\n\n# License: BSD 3 clause\n\nimport warnings\nfrom numbers import Integral, Real\n\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy.linalg import svd\n\ntry:\n    from scipy.fft import fft, ifft\nexcept ImportError:  # scipy < 1.4\n    from scipy.fftpack import fft, ifft\n\nfrom .base import (\n    BaseEstimator,\n    ClassNamePrefixFeaturesOutMixin,\n    TransformerMixin,\n    _fit_context,\n)\nfrom .metrics.pairwise import KERNEL_PARAMS, PAIRWISE_KERNEL_FUNCTIONS, pairwise_kernels\nfrom .utils import check_random_state, deprecated\nfrom .utils._param_validation import Interval, StrOptions\nfrom .utils.extmath import safe_sparse_dot\nfrom .utils.validation import (\n    _check_feature_names_in,\n    check_is_fitted,\n    check_non_negative,\n)",
                "filename": "sklearn/kernel_approximation.py",
                "start_index": 0,
                "end_index": 1004,
                "start_line": 1,
                "end_line": 1131,
                "max_line": 1134,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "fourier_approx_svm = pipeline.Pipeline(\n    [(\"feature_map\", feature_map_fourier), (\"svm\", svm.LinearSVC(dual=\"auto\"))]\n)\n\nnystroem_approx_svm = pipeline.Pipeline(\n    [(\"feature_map\", feature_map_nystroem), (\"svm\", svm.LinearSVC(dual=\"auto\"))]\n)\n\n# fit and predict using linear and kernel svm:\n\nkernel_svm_time = time()\nkernel_svm.fit(data_train, targets_train)\nkernel_svm_score = kernel_svm.score(data_test, targets_test)\nkernel_svm_time = time() - kernel_svm_time\n\nlinear_svm_time = time()\nlinear_svm.fit(data_train, targets_train)\nlinear_svm_score = linear_svm.score(data_test, targets_test)\nlinear_svm_time = time() - linear_svm_time\n\nsample_sizes = 30 * np.arange(1, 10)\nfourier_scores = []\nnystroem_scores = []\nfourier_times = []\nnystroem_times = []\n\nfor D in sample_sizes:\n    fourier_approx_svm.set_params(feature_map__n_components=D)\n    nystroem_approx_svm.set_params(feature_map__n_components=D)\n    start = time()\n    nystroem_approx_svm.fit(data_train, targets_train)\n    nystroem_times.append(time() - start)\n\n    start = time()\n    fourier_approx_svm.fit(data_train, targets_train)\n    fourier_times.append(time() - start)\n\n    fourier_score = fourier_approx_svm.score(data_test, targets_test)\n    nystroem_score = nystroem_approx_svm.score(data_test, targets_test)\n    nystroem_scores.append(nystroem_score)\n    fourier_scores.append(fourier_score)\n\n# plot the results:\nplt.figure(figsize=(16, 4))\naccuracy = plt.subplot(121)\n# second y axis for timings\ntimescale = plt.subplot(122)\n\naccuracy.plot(sample_sizes, nystroem_scores, label=\"Nystroem approx. kernel\")\ntimescale.plot(sample_sizes, nystroem_times, \"--\", label=\"Nystroem approx. kernel\")\n\naccuracy.plot(sample_sizes, fourier_scores, label=\"Fourier approx. kernel\")\ntimescale.plot(sample_sizes, fourier_times, \"--\", label=\"Fourier approx. kernel\")\n\n# horizontal lines for exact rbf and linear kernels:\naccuracy.plot(\n    [sample_sizes[0], sample_sizes[-1]],\n    [linear_svm_score, linear_svm_score],\n    label=\"linear svm\",\n)\ntimescale.plot(\n    [sample_sizes[0], sample_sizes[-1]],\n    [linear_svm_time, linear_svm_time],\n    \"--\",\n    label=\"linear svm\",\n)\n\naccuracy.plot(\n    [sample_sizes[0], sample_sizes[-1]],\n    [kernel_svm_score, kernel_svm_score],\n    label=\"rbf svm\",\n)\ntimescale.plot(\n    [sample_sizes[0], sample_sizes[-1]],\n    [kernel_svm_time, kernel_svm_time],\n    \"--\",\n    label=\"rbf svm\",\n)\n\n# vertical line for dataset dimensionality = 64\naccuracy.plot([64, 64], [0.7, 1], label=\"n_features\")\n\n# legends and labels\naccuracy.set_title(\"Classification accuracy\")\ntimescale.set_title(\"Training times\")\naccuracy.set_xlim(sample_sizes[0], sample_sizes[-1])\naccuracy.set_xticks(())\naccuracy.set_ylim(np.min(fourier_scores), 1)\ntimescale.set_xlabel(\"Sampling steps = transformed feature dimension\")\naccuracy.set_ylabel(\"Classification accuracy\")\ntimescale.set_ylabel(\"Training time in seconds\")\naccuracy.legend(loc=\"best\")\ntimescale.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n\n\n# %%",
                "filename": "examples/miscellaneous/plot_kernel_approximation.py",
                "start_index": 2988,
                "end_index": 5964,
                "start_line": 81,
                "end_line": 178,
                "max_line": 254,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "# Modeling non-linear feature interactions with kernels\n# -----------------------------------------------------\n#\n# The previous analysis highlighted the need to model the interactions between\n# `\"workingday\"` and `\"hours\"`. Another example of a such a non-linear\n# interaction that we would like to model could be the impact of the rain that\n# might not be the same during the working days and the week-ends and holidays\n# for instance.\n#\n# To model all such interactions, we could either use a polynomial expansion on\n# all marginal features at once, after their spline-based expansion. However,\n# this would create a quadratic number of features which can cause overfitting\n# and computational tractability issues.\n#\n# Alternatively, we can use the Nystr\u00f6m method to compute an approximate\n# polynomial kernel expansion. Let us try the latter:\nfrom sklearn.kernel_approximation import Nystroem\n\ncyclic_spline_poly_pipeline = make_pipeline(\n    cyclic_spline_transformer,\n    Nystroem(kernel=\"poly\", degree=2, n_components=300, random_state=0),\n    RidgeCV(alphas=alphas),\n)\nevaluate(cyclic_spline_poly_pipeline, X, y, cv=ts_cv)\n\n# %%\n#\n# We observe that this model can almost rival the performance of the gradient\n# boosted trees with an average error around 5% of the maximum demand.\n#\n# Note that while the final step of this pipeline is a linear regression model,\n# the intermediate steps such as the spline feature extraction and the Nystr\u00f6m\n# kernel approximation are highly non-linear. As a result the compound pipeline\n# is much more expressive than a simple linear regression model with raw features.\n#\n# For the sake of completeness, we also evaluate the combination of one-hot\n# encoding and kernel approximation:\n\none_hot_poly_pipeline = make_pipeline(\n    ColumnTransformer(\n        transformers=[\n            (\"categorical\", one_hot_encoder, categorical_columns),\n            (\"one_hot_time\", one_hot_encoder, [\"hour\", \"weekday\", \"month\"]),\n        ],\n        remainder=\"passthrough\",\n    ),\n    Nystroem(kernel=\"poly\", degree=2, n_components=300, random_state=0),\n    RidgeCV(alphas=alphas),\n)\nevaluate(one_hot_poly_pipeline, X, y, cv=ts_cv)\n\n\n# %%\n# While one-hot encoded features were competitive with spline-based features\n# when using linear models, this is no longer the case when using a low-rank\n# approximation of a non-linear kernel: this can be explained by the fact that\n# spline features are smoother and allow the kernel approximation to find a\n# more expressive decision function.\n#\n# Let us now have a qualitative look at the predictions of the kernel models\n# and of the gradient boosted trees that should be able to better model\n# non-linear interactions between features:\ngbrt_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\ngbrt_predictions = gbrt_pipeline.predict(X.iloc[test_0])\n\none_hot_poly_pipeline.fit(X.iloc[train_0], y.iloc[train_0])\none_hot_poly_predictions = one_hot_poly_pipeline.predict(X.iloc[test_0])",
                "filename": "examples/applications/plot_cyclical_feature_engineering.py",
                "start_index": 23804,
                "end_index": 26753,
                "start_line": 649,
                "end_line": 715,
                "max_line": 844,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"\n======================================================\nScalable learning with polynomial kernel approximation\n======================================================\n\n.. currentmodule:: sklearn.kernel_approximation\n\nThis example illustrates the use of :class:`PolynomialCountSketch` to\nefficiently generate polynomial kernel feature-space approximations.\nThis is used to train linear classifiers that approximate the accuracy\nof kernelized ones.\n\nWe use the Covtype dataset [2], trying to reproduce the experiments on the\noriginal paper of Tensor Sketch [1], i.e. the algorithm implemented by\n:class:`PolynomialCountSketch`.\n\nFirst, we compute the accuracy of a linear classifier on the original\nfeatures. Then, we train linear classifiers on different numbers of\nfeatures (`n_components`) generated by :class:`PolynomialCountSketch`,\napproximating the accuracy of a kernelized classifier in a scalable manner.\n\n\"\"\"\n\n# Author: Daniel Lopez-Sanchez <lope@usal.es>\n# License: BSD 3 clause\n\n# %%\n# Preparing the data\n# ------------------\n#\n# Load the Covtype dataset, which contains 581,012 samples\n# with 54 features each, distributed among 6 classes. The goal of this dataset\n# is to predict forest cover type from cartographic variables only\n# (no remotely sensed data). After loading, we transform it into a binary\n# classification problem to match the version of the dataset in the\n# LIBSVM webpage [2], which was the one used in [1].\n\nfrom sklearn.datasets import fetch_covtype\n\nX, y = fetch_covtype(return_X_y=True)\n\ny[y != 2] = 0\ny[y == 2] = 1  # We will try to separate class 2 from the other 6 classes.\n\n# %%\n# Partitioning the data\n# ---------------------\n#\n# Here we select 5,000 samples for training and 10,000 for testing.\n# To actually reproduce the results in the original Tensor Sketch paper,\n# select 100,000 for training.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=5_000, test_size=10_000, random_state=42\n)\n\n# %%\n# Feature normalization\n# ---------------------\n#\n# Now scale features to the range [0, 1] to match the format of the dataset in\n# the LIBSVM webpage, and then normalize to unit length as done in the\n# original Tensor Sketch paper [1].\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler, Normalizer\n\nmm = make_pipeline(MinMaxScaler(), Normalizer())\nX_train = mm.fit_transform(X_train)\nX_test = mm.transform(X_test)\n\n# %%\n# Establishing a baseline model\n# -----------------------------\n#\n# As a baseline, train a linear SVM on the original features and print the\n# accuracy. We also measure and store accuracies and training times to\n# plot them later.\n\nimport time\n\nfrom sklearn.svm import LinearSVC\n\nresults = {}\n\nlsvm = LinearSVC(dual=\"auto\")\nstart = time.time()\nlsvm.fit(X_train, y_train)\nlsvm_time = time.time() - start\nlsvm_score = 100 * lsvm.score(X_test, y_test)\n\nresults[\"LSVM\"] = {\"time\": lsvm_time, \"score\": lsvm_score}",
                "filename": "examples/kernel_approximation/plot_scalable_poly_kernels.py",
                "start_index": 0,
                "end_index": 2988,
                "start_line": 1,
                "end_line": 94,
                "max_line": 251,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "def _dense_predict(self, X):\n        X = self._compute_kernel(X)\n        if X.ndim == 1:\n            X = check_array(X, order=\"C\", accept_large_sparse=False)\n\n        kernel = self.kernel\n        if callable(self.kernel):\n            kernel = \"precomputed\"\n            if X.shape[1] != self.shape_fit_[0]:\n                raise ValueError(\n                    \"X.shape[1] = %d should be equal to %d, \"\n                    \"the number of samples at training time\"\n                    % (X.shape[1], self.shape_fit_[0])\n                )\n\n        svm_type = LIBSVM_IMPL.index(self._impl)\n\n        return libsvm.predict(\n            X,\n            self.support_,\n            self.support_vectors_,\n            self._n_support,\n            self._dual_coef_,\n            self._intercept_,\n            self._probA,\n            self._probB,\n            svm_type=svm_type,\n            kernel=kernel,\n            degree=self.degree,\n            coef0=self.coef0,\n            gamma=self._gamma,\n            cache_size=self.cache_size,\n        )\n\n    def _sparse_predict(self, X):\n        # Precondition: X is a csr_matrix of dtype np.float64.\n        kernel = self.kernel\n        if callable(kernel):\n            kernel = \"precomputed\"\n\n        kernel_type = self._sparse_kernels.index(kernel)\n\n        C = 0.0  # C is not useful here\n\n        return libsvm_sparse.libsvm_sparse_predict(\n            X.data,\n            X.indices,\n            X.indptr,\n            self.support_vectors_.data,\n            self.support_vectors_.indices,\n            self.support_vectors_.indptr,\n            self._dual_coef_.data,\n            self._intercept_,\n            LIBSVM_IMPL.index(self._impl),\n            kernel_type,\n            self.degree,\n            self._gamma,\n            self.coef0,\n            self.tol,\n            C,\n            # TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\n            getattr(self, \"_class_weight\", np.empty(0)),\n            self.nu,\n            self.epsilon,\n            self.shrinking,\n            self.probability,\n            self._n_support,\n            self._probA,\n            self._probB,\n        )\n\n    def _compute_kernel(self, X):\n        \"\"\"Return the data transformed by a callable kernel\"\"\"\n        if callable(self.kernel):\n            # in the case of precomputed kernel given as a function, we\n            # have to compute explicitly the kernel matrix\n            kernel = self.kernel(X, self.__Xfit)\n            if sp.issparse(kernel):\n                kernel = kernel.toarray()\n            X = np.asarray(kernel, dtype=np.float64, order=\"C\")\n        return X",
                "filename": "sklearn/svm/_base.py",
                "start_index": 15287,
                "end_index": 17890,
                "start_line": 435,
                "end_line": 645,
                "max_line": 1254,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"\n========================================================================\nBenchmark for explicit feature map approximation of polynomial kernels\n========================================================================\n\nAn example illustrating the approximation of the feature map\nof an Homogeneous Polynomial kernel.\n\n.. currentmodule:: sklearn.kernel_approximation\n\nIt shows how to use :class:`PolynomialCountSketch` and :class:`Nystroem` to\napproximate the feature map of a polynomial kernel for\nclassification with an SVM on the digits dataset. Results using a linear\nSVM in the original space, a linear SVM using the approximate mappings\nand a kernelized SVM are compared.\n\nThe first plot shows the classification accuracy of Nystroem [2] and\nPolynomialCountSketch [1] as the output dimension (n_components) grows.\nIt also shows the accuracy of a linear SVM and a polynomial kernel SVM\non the same data.\n\nThe second plot explores the scalability of PolynomialCountSketch\nand Nystroem. For a sufficiently large output dimension,\nPolynomialCountSketch should be faster as it is O(n(d+klog k))\nwhile Nystroem is O(n(dk+k^2)). In addition, Nystroem requires\na time-consuming training phase, while training is almost immediate\nfor PolynomialCountSketch, whose training phase boils down to\ninitializing some random variables (because is data-independent).\n\n[1] Pham, N., & Pagh, R. (2013, August). Fast and scalable polynomial\nkernels via explicit feature maps. In Proceedings of the 19th ACM SIGKDD\ninternational conference on Knowledge discovery and data mining (pp. 239-247)\n(http://chbrown.github.io/kdd-2013-usb/kdd/p239.pdf)\n\n[2] Charikar, M., Chen, K., & Farach-Colton, M. (2002, July). Finding frequent\nitems in data streams. In International Colloquium on Automata, Languages, and\nProgramming (pp. 693-703). Springer, Berlin, Heidelberg.\n(http://www.vldb.org/pvldb/1/1454225.pdf)\n\n\"\"\"\n# Author: Daniel Lopez-Sanchez <lope@usal.es>\n# License: BSD 3 clause\n\n# Load data manipulation functions\n# Will use this for timing results\nfrom time import time\n\n# Some common libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.kernel_approximation import Nystroem, PolynomialCountSketch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\n\n# Import SVM classifiers and feature map approximation algorithms\nfrom sklearn.svm import SVC, LinearSVC\n\n# Split data in train and test sets\nX, y = load_digits()[\"data\"], load_digits()[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n\n# Set the range of n_components for our experiments\nout_dims = range(20, 400, 20)\n\n# Evaluate Linear SVM\nlsvm = LinearSVC().fit(X_train, y_train)\nlsvm_score = 100 * lsvm.score(X_test, y_test)\n\n# Evaluate kernelized SVM\nksvm = SVC(kernel=\"poly\", degree=2, gamma=1.0).fit(X_train, y_train)\nksvm_score = 100 * ksvm.score(X_test, y_test)\n\n# Evaluate PolynomialCountSketch + LinearSVM",
                "filename": "benchmarks/bench_plot_polynomial_kernel_approximation.py",
                "start_index": 0,
                "end_index": 2991,
                "start_line": 1,
                "end_line": 75,
                "max_line": 175,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"\n==========================================================================\nFitting an Elastic Net with a precomputed Gram Matrix and Weighted Samples\n==========================================================================\n\nThe following example shows how to precompute the gram matrix\nwhile using weighted samples with an :class:`~sklearn.linear_model.ElasticNet`.\n\nIf weighted samples are used, the design matrix must be centered and then\nrescaled by the square root of the weight vector before the gram matrix\nis computed.\n\n.. note::\n  `sample_weight` vector is also rescaled to sum to `n_samples`, see the\n   documentation for the `sample_weight` parameter to\n   :meth:`~sklearn.linear_model.ElasticNet.fit`.\n\n\"\"\"\n\n# %%\n# Let's start by loading the dataset and creating some sample weights.\nimport numpy as np\n\nfrom sklearn.datasets import make_regression\n\nrng = np.random.RandomState(0)\n\nn_samples = int(1e5)\nX, y = make_regression(n_samples=n_samples, noise=0.5, random_state=rng)\n\nsample_weight = rng.lognormal(size=n_samples)\n# normalize the sample weights\nnormalized_weights = sample_weight * (n_samples / (sample_weight.sum()))\n\n# %%\n# To fit the elastic net using the `precompute` option together with the sample\n# weights, we must first center the design matrix,  and rescale it by the\n# normalized weights prior to computing the gram matrix.\nX_offset = np.average(X, axis=0, weights=normalized_weights)\nX_centered = X - np.average(X, axis=0, weights=normalized_weights)\nX_scaled = X_centered * np.sqrt(normalized_weights)[:, np.newaxis]\ngram = np.dot(X_scaled.T, X_scaled)\n\n# %%\n# We can now proceed with fitting. We must passed the centered design matrix to\n# `fit` otherwise the elastic net estimator will detect that it is uncentered\n# and discard the gram matrix we passed. However, if we pass the scaled design\n# matrix, the preprocessing code will incorrectly rescale it a second time.\nfrom sklearn.linear_model import ElasticNet\n\nlm = ElasticNet(alpha=0.01, precompute=gram)\nlm.fit(X_centered, y, sample_weight=normalized_weights)",
                "filename": "examples/linear_model/plot_elastic_net_precomputed_gram_matrix_with_weighted_samples.py",
                "start_index": 0,
                "end_index": 2056,
                "start_line": 1,
                "end_line": 52,
                "max_line": 52,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/kernel_approximation.py": [
                {
                    "chunk": {
                        "code": "\"\"\"Approximate a kernel map using a subset of the training data.\n\n    Constructs an approximate feature map for an arbitrary kernel\n    using a subset of the data as basis.\n\n    Read more in the :ref:`User Guide <nystroem_kernel_approx>`.\n\n    .. versionadded:: 0.13\n\n    Parameters\n    ----------\n    kernel : str or callable, default='rbf'\n        Kernel map to be approximated. A callable should accept two arguments\n        and the keyword arguments passed to this object as `kernel_params`, and\n        should return a floating point number.\n\n    gamma : float, default=None\n        Gamma parameter for the RBF, laplacian, polynomial, exponential chi2\n        and sigmoid kernels. Interpretation of the default value is left to\n        the kernel; see the documentation for sklearn.metrics.pairwise.\n        Ignored by other kernels.\n\n    coef0 : float, default=None\n        Zero coefficient for polynomial and sigmoid kernels.\n        Ignored by other kernels.\n\n    degree : float, default=None\n        Degree of the polynomial kernel. Ignored by other kernels.\n\n    kernel_params : dict, default=None\n        Additional parameters (keyword arguments) for kernel function passed\n        as callable object.\n\n    n_components : int, default=100\n        Number of features to construct.\n        How many data points will be used to construct the mapping.\n\n    random_state : int, RandomState instance or None, default=None\n        Pseudo-random number generator to control the uniform sampling without\n        replacement of `n_components` of the training data to construct the\n        basis kernel.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    n_jobs : int, default=None\n        The number of jobs to use for the computation. This works by breaking\n        down the kernel matrix into `n_jobs` even slices and computing them in\n        parallel.\n\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionadded:: 0.24\n\n    Attributes\n    ----------\n    components_ : ndarray of shape (n_components, n_features)\n        Subset of training points used to construct the feature map.\n\n    component_indices_ : ndarray of shape (n_components)\n        Indices of ``components_`` in the training set.\n\n    normalization_ : ndarray of shape (n_components, n_components)\n        Normalization matrix needed for embedding.\n        Square root of the kernel matrix on ``components_``.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\n    PolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch.\n    RBFSampler : Approximate a RBF kernel feature map using random Fourier\n        features.\n    SkewedChi2Sampler : Approximate feature map for \"skewed chi-squared\" kernel.\n    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n\n    References\n    ----------\n    * Williams, C.K.I. and Seeger, M.\n      \"Using the Nystroem method to speed up kernel machines\",\n      Advances in neural information processing systems 2001\n\n    * T. Yang, Y. Li, M. Mahdavi, R. Jin and Z. Zhou\n      \"Nystroem Method vs Random Fourier Features: A Theoretical and Empirical\n      Comparison\",\n      Advances in Neural Information Processing Systems 2012\n\n    Examples\n    --------\n    >>> from sklearn import datasets, svm\n    >>> from sklearn.kernel_approximation import Nystroem\n    >>> X, y = datasets.load_digits(n_class=9, return_X_y=True)\n    >>> data = X / 16.\n    >>> clf = svm.LinearSVC(dual=\"auto\")\n    >>> feature_map_nystroem = Nystroem(gamma=.2,\n    ...                                 random_state=1,\n    ...                                 n_components=300)\n    >>> data_transformed = feature_map_nystroem.fit_transform(data)\n    >>> clf.fit(data_transformed, y)\n    LinearSVC(dual='auto')\n    >>> clf.score(data_transformed, y)\n    0.9987...\n    \"\"\"",
                        "filename": "sklearn/kernel_approximation.py",
                        "start_index": 31336,
                        "end_index": 35702,
                        "start_line": 864,
                        "end_line": 1089,
                        "max_line": 1134,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the class definition and documentation for Nystroem, which is relevant to the issue as it mentions the kernel parameter and its default value. It is necessary to check if 'precomputed' is actually supported as per the documentation."
                },
                {
                    "chunk": {
                        "code": "def _get_kernel_params(self):\n        params = self.kernel_params\n        if params is None:\n            params = {}\n        if not callable(self.kernel) and self.kernel != \"precomputed\":\n            for param in KERNEL_PARAMS[self.kernel]:\n                if getattr(self, param) is not None:\n                    params[param] = getattr(self, param)\n        else:\n            if (\n                self.gamma is not None\n                or self.coef0 is not None\n                or self.degree is not None\n            ):\n                raise ValueError(\n                    \"Don't pass gamma, coef0 or degree to \"\n                    \"Nystroem if using a callable \"\n                    \"or precomputed kernel\"\n                )\n\n        return params\n\n    def _more_tags(self):\n        return {\n            \"_xfail_checks\": {\n                \"check_transformer_preserve_dtypes\": (\n                    \"dtypes are preserved but not at a close enough precision\"\n                )\n            },\n            \"preserves_dtype\": [np.float64, np.float32],\n        }",
                        "filename": "sklearn/kernel_approximation.py",
                        "start_index": 39643,
                        "end_index": 40703,
                        "start_line": 1104,
                        "end_line": 1134,
                        "max_line": 1134,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the _get_kernel_params method which checks if the kernel is callable or 'precomputed' and raises a ValueError if gamma, coef0, or degree are passed. It is relevant because it directly handles the 'precomputed' kernel case and may contain the logic that leads to the KeyError."
                }
            ]
        },
        "solution_task": "- Check if `Nystroem` class has a `fit` method that handles the `kernel='precomputed'` case.\n- If not, modify the `fit` method of the `Nystroem` class to handle `kernel='precomputed'`.\n- Ensure that when `kernel='precomputed'`, the `fit` method does not attempt to compute the kernel again but instead uses the provided precomputed kernel matrix.\n- Update the `_get_kernel_params` method to correctly handle the `'precomputed'` kernel case without raising a `ValueError`.\n- Add a check in the `fit_transform` method to ensure that when `kernel='precomputed'`, the input `X` is a square matrix (i.e., `X.shape[0] == X.shape[1]`), as it should be a precomputed kernel matrix.\n- If the input is not a square matrix, raise an appropriate error message indicating that a square matrix is expected for a precomputed kernel.\n- Add unit tests to cover the new functionality and ensure that passing a precomputed kernel matrix to `Nystroem` with `kernel='precomputed'` does not raise a `KeyError` and works as expected.",
        "bouncer_response": {
            "score": 7,
            "criteria": "RESPONSES that are highly relevant, offering clear and comprehensive guidance or solutions for the described bug, should score between 7 and 9.",
            "supporting_evidence": "The RESPONSE provides a series of actionable steps that are directly related to the issue of the Nystroem class not supporting precomputed kernels, despite the documentation stating otherwise. It suggests checking and modifying the `fit` method to handle the 'precomputed' case, updating the `_get_kernel_params` method, adding a check for the input matrix shape in `fit_transform`, and adding unit tests. These steps are all relevant to addressing the bug. The RESPONSE also references specific code snippets to check, which is useful for locating the potential source of the bug. However, the RESPONSE does not provide actual code changes or patches, which would be necessary to fully resolve the issue. Therefore, it does not score a 10 but is still highly valuable.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711739131.612774,
        "result": "- Check if `Nystroem` class has a `fit` method that handles the `kernel='precomputed'` case.\n- If not, modify the `fit` method of the `Nystroem` class to handle `kernel='precomputed'`.\n- Ensure that when `kernel='precomputed'`, the `fit` method does not attempt to compute the kernel again but instead uses the provided precomputed kernel matrix.\n- Update the `_get_kernel_params` method to correctly handle the `'precomputed'` kernel case without raising a `ValueError`.\n- Add a check in the `fit_transform` method to ensure that when `kernel='precomputed'`, the input `X` is a square matrix (i.e., `X.shape[0] == X.shape[1]`), as it should be a precomputed kernel matrix.\n- If the input is not a square matrix, raise an appropriate error message indicating that a square matrix is expected for a precomputed kernel.\n- Add unit tests to cover the new functionality and ensure that passing a precomputed kernel matrix to `Nystroem` with `kernel='precomputed'` does not raise a `KeyError` and works as expected.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn)- [Lines 864 - 1089](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/kernel_approximation.py#L864-L1089)\n\n This snippet contains the class definition and documentation for Nystroem, which is relevant to the issue as it mentions the kernel parameter and its default value. It is necessary to check if 'precomputed' is actually supported as per the documentation. \n\n\n   - [Lines 1104 - 1134](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/kernel_approximation.py#L1104-L1134)\n\n This snippet contains the _get_kernel_params method which checks if the kernel is callable or 'precomputed' and raises a ValueError if gamma, coef0, or degree are passed. It is relevant because it directly handles the 'precomputed' kernel case and may contain the logic that leads to the KeyError."
    }
}