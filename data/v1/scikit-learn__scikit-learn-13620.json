{
    "id": "scikit-learn__scikit-learn-13620",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
    "task_id": 6788844410201938280,
    "title": "Bug in Gradient Boosting: Feature Importances do not sum to 1",
    "body": "#### Description\r\n\r\nI found conditions when Feature Importance values do not add up to 1 in ensemble tree methods, like Gradient Boosting Trees or AdaBoost Trees.  \r\n\r\nThis error occurs once the ensemble reaches a large number of estimators.  The exact conditions depend variously.  For example, the error shows up sooner with a smaller amount of training samples.  Or, if the depth of the tree is large.  \r\n\r\nWhen this error appears, the predicted value seems to have converged.  But it\u2019s unclear if the error is causing the predicted value not to change with more estimators.  In fact, the feature importance sum goes lower and lower with more estimators thereafter.  \r\n\r\nConsequently, it's questionable if the tree ensemble code is functioning as expected.  \r\n\r\nHere's sample code to reproduce this:\r\n\r\n``` python\r\nimport numpy as np\r\nfrom sklearn import datasets\r\nfrom sklearn.ensemble import GradientBoostingRegressor\r\n\r\nboston = datasets.load_boston()\r\nX, Y = (boston.data, boston.target)\r\n\r\nn_estimators = 720\r\n# Note: From 712 onwards, the feature importance sum is less than 1\r\n\r\nparams = {'n_estimators': n_estimators, 'max_depth': 6, 'learning_rate': 0.1}\r\nclf = GradientBoostingRegressor(**params)\r\nclf.fit(X, Y)\r\n\r\nfeature_importance_sum = np.sum(clf.feature_importances_)\r\nprint(\"At n_estimators = %i, feature importance sum = %f\" % (n_estimators , feature_importance_sum))\r\n```\r\n\r\n_Output:_\r\n\r\n```\r\nAt n_estimators = 720, feature importance sum = 0.987500\r\n```\r\n\r\nIn fact, if we examine the tree at each staged prediction, we'll see that the feature importance goes to 0 after we hit a certain number of estimators.  (For the code above, it's 712.)\r\n\r\nHere's code to describe what I mean:\r\n\r\n``` python\r\nfor i, tree in enumerate(clf.estimators_):\r\n    feature_importance_sum = np.sum(tree[0].feature_importances_)\r\n    print(\"At n_estimators = %i, feature importance sum = %f\" % (i , feature_importance_sum))\r\n```\r\n\r\n_Output:_\r\n\r\n```\r\n...\r\nAt n_estimators = 707, feature importance sum = 1.000000\r\nAt n_estimators = 708, feature importance sum = 1.000000\r\nAt n_estimators = 709, feature importance sum = 1.000000\r\nAt n_estimators = 710, feature importance sum = 1.000000\r\nAt n_estimators = 711, feature importance sum = 0.000000\r\nAt n_estimators = 712, feature importance sum = 0.000000\r\nAt n_estimators = 713, feature importance sum = 0.000000\r\nAt n_estimators = 714, feature importance sum = 0.000000\r\nAt n_estimators = 715, feature importance sum = 0.000000\r\nAt n_estimators = 716, feature importance sum = 0.000000\r\nAt n_estimators = 717, feature importance sum = 0.000000\r\nAt n_estimators = 718, feature importance sum = 0.000000\r\n...\r\n```\r\n\r\nI wonder if we\u2019re hitting some floating point calculation error. \r\n\r\nBTW, I've posted this issue on the mailing list [Link](https://mail.python.org/pipermail/scikit-learn/2016-September/000508.html).  There aren't a lot of discussion, but others seem to think there's a bug here too.\r\n\r\nHope we can get this fixed or clarified.\r\n\r\nThank you!\r\n-Doug\r\n#### Versions\r\n\r\nWindows-7;'Python', '2.7.9 ;'NumPy', '1.9.2';'SciPy', '0.15.1';'Scikit-Learn', '0.16.1' \r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.21.git",
    "requested_commit": "",
    "mayil_response": "Based on the provided information, the issue seems to be related to the calculation of feature importances in ensemble tree methods like GradientBoostingRegressor and AdaBoost. The sum of feature importances is expected to be 1, but it is reported to decrease with more estimators, even reaching 0 in some cases. This could be a floating-point precision error or an issue with the implementation of the feature importance calculation.\n\nTo address the issue, consider the following steps:\n\n- Ensure that the calculation of feature importances is normalized correctly across all trees in the ensemble. The sum of feature importances for each tree should be 1, and the overall feature importances should be averaged across all trees.\n- Check for any floating-point precision issues that may arise when summing small numbers. This can be particularly problematic if the number of estimators is large.\n- Verify that the feature importances are being accumulated correctly across all estimators in the ensemble. This involves checking the `feature_importances_` property and how it is computed from the individual trees' importances.\n- If the issue persists, consider updating to a more recent version of the libraries (NumPy, SciPy, Scikit-Learn) as the issue might have been addressed in later versions.\n- As a last resort, if the problem is identified as a bug in the Scikit-Learn implementation, report the issue to the Scikit-Learn GitHub repository with a reproducible example.\n\nHere is a pseudocode snippet that demonstrates how feature importances should be normalized and averaged across all trees in an ensemble:\n\n```python\nclass GradientBoostingRegressor:\n    # ...\n\n    @property\n    def feature_importances_(self):\n        all_importances = [tree.feature_importances_ for tree in self.estimators_]\n        # Normalize feature importances for each tree to sum to 1\n        all_importances_normalized = [imp / np.sum(imp) for imp in all_importances]\n        # Average feature importances across all trees\n        avg_importances = np.mean(all_importances_normalized, axis=0)\n        return avg_importances\n```\n\nEnsure that the `feature_importances_` for each individual tree (`tree.feature_importances_`) are calculated correctly and sum to 1 before averaging them across all trees in the ensemble.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [ensemble](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/ensemble)- [Lines 835 - 1077](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/ensemble/_gb.py#L835-L1077)\n\n This snippet is from the actual implementation file of the Gradient Boosting algorithm in scikit-learn, which is directly related to the issue reported. It contains the parameters and attributes of the GradientBoostingClassifier, which may need to be reviewed or modified to address the feature importance sum issue.\n-  \u2192 [examples](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//examples) \u2192 [ensemble](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//examples/ensemble)- [Lines 1 - 91](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//examples/ensemble/plot_gradient_boosting_regularization.py#L1-L91)\n\n This snippet contains an example of using Gradient Boosting with regularization in scikit-learn. It is relevant because it demonstrates the use of the GradientBoostingClassifier, which is the subject of the reported issue. The example may need to be reviewed to ensure it aligns with expected behavior after any potential fixes.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0008225,
        "snippet_processor": 0.051160000000000004,
        "issue_star_creation": 0.035809999999999995,
        "issue_star_solver": 0.07194,
        "bouncer": 0.02488
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711740446.581038,
        "relevant_snippets": [
            {
                "code": "\"\"\"\n=============================\nDiscrete versus Real AdaBoost\n=============================\n\nThis notebook is based on Figure 10.2 from Hastie et al 2009 [1]_ and\nillustrates the difference in performance between the discrete SAMME [2]_\nboosting algorithm and real SAMME.R boosting algorithm. Both algorithms are\nevaluated on a binary classification task where the target Y is a non-linear\nfunction of 10 input features.\n\nDiscrete SAMME AdaBoost adapts based on errors in predicted class labels\nwhereas real SAMME.R uses the predicted class probabilities.\n\n.. [1] T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical\n    Learning Ed. 2\", Springer, 2009.\n\n.. [2] J Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\",\n    Statistics and Its Interface, 2009.\n\n\"\"\"\n\n# %%\n# Preparing the data and baseline models\n# --------------------------------------\n# We start by generating the binary classification dataset\n# used in Hastie et al. 2009, Example 10.2.\n\n# Authors: Peter Prettenhofer <peter.prettenhofer@gmail.com>,\n#          Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\nfrom sklearn import datasets\n\nX, y = datasets.make_hastie_10_2(n_samples=12_000, random_state=1)\n\n# %%\n# Now, we set the hyperparameters for our AdaBoost classifiers.\n# Be aware, a learning rate of 1.0 may not be optimal for both SAMME and SAMME.R\n\nn_estimators = 400\nlearning_rate = 1.0\n\n# %%\n# We split the data into a training and a test set.\n# Then, we train our baseline classifiers, a `DecisionTreeClassifier` with `depth=9`\n# and a \"stump\" `DecisionTreeClassifier` with `depth=1` and compute the test error.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=2_000, shuffle=False\n)\n\ndt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)\ndt_stump.fit(X_train, y_train)\ndt_stump_err = 1.0 - dt_stump.score(X_test, y_test)\n\ndt = DecisionTreeClassifier(max_depth=9, min_samples_leaf=1)\ndt.fit(X_train, y_train)\ndt_err = 1.0 - dt.score(X_test, y_test)\n\n# %%\n# Adaboost with discrete SAMME and real SAMME.R\n# ---------------------------------------------\n# We now define the discrete and real AdaBoost classifiers\n# and fit them to the training set.\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada_discrete = AdaBoostClassifier(\n    estimator=dt_stump,\n    learning_rate=learning_rate,\n    n_estimators=n_estimators,\n    algorithm=\"SAMME\",\n)\nada_discrete.fit(X_train, y_train)\n\n# %%\n\nada_real = AdaBoostClassifier(\n    estimator=dt_stump,\n    learning_rate=learning_rate,\n    n_estimators=n_estimators,\n    algorithm=\"SAMME.R\",\n)\nada_real.fit(X_train, y_train)\n\n# %%\n# Now, let's compute the test error of the discrete and\n# real AdaBoost classifiers for each new stump in `n_estimators`\n# added to the ensemble.\n\nimport numpy as np\n\nfrom sklearn.metrics import zero_one_loss\n\nada_discrete_err = np.zeros((n_estimators,))",
                "filename": "examples/ensemble/plot_adaboost_hastie_10_2.py",
                "start_index": 0,
                "end_index": 2974,
                "start_line": 1,
                "end_line": 100,
                "max_line": 172,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "boosting_errors = pd.DataFrame(\n    {\n        \"Number of trees\": range(1, n_estimators + 1),\n        \"AdaBoost\": [\n            misclassification_error(y_test, y_pred)\n            for y_pred in adaboost_clf.staged_predict(X_test)\n        ],\n    }\n).set_index(\"Number of trees\")\nax = boosting_errors.plot()\nax.set_ylabel(\"Misclassification error on test set\")\nax.set_title(\"Convergence of AdaBoost algorithm\")\n\nplt.plot(\n    [boosting_errors.index.min(), boosting_errors.index.max()],\n    [weak_learners_misclassification_error, weak_learners_misclassification_error],\n    color=\"tab:orange\",\n    linestyle=\"dashed\",\n)\nplt.plot(\n    [boosting_errors.index.min(), boosting_errors.index.max()],\n    [\n        dummy_classifiers_misclassification_error,\n        dummy_classifiers_misclassification_error,\n    ],\n    color=\"c\",\n    linestyle=\"dotted\",\n)\nplt.legend([\"AdaBoost\", \"DecisionTreeClassifier\", \"DummyClassifier\"], loc=1)\nplt.show()\n\n# %%\n# The plot shows the missclassification error on the test set after each\n# boosting iteration. We see that the error of the boosted trees converges to an\n# error of around 0.3 after 50 iterations, indicating a significantly higher\n# accuracy compared to a single tree, as illustrated by the dashed line in the\n# plot.\n#\n# The misclassification error jitters because the `SAMME` algorithm uses the\n# discrete outputs of the weak learners to train the boosted model.\n#\n# The convergence of :class:`~sklearn.ensemble.AdaBoostClassifier` is mainly\n# influenced by the learning rate (i.e `learning_rate`), the number of weak\n# learners used (`n_estimators`), and the expressivity of the weak learners\n# (e.g. `max_leaf_nodes`).\n\n# %%\n# Errors and weights of the Weak Learners\n# ***************************************\n# As previously mentioned, AdaBoost is a forward stagewise additive model. We\n# now focus on understanding the relationship between the attributed weights of\n# the weak learners and their statistical performance.\n#\n# We use the fitted :class:`~sklearn.ensemble.AdaBoostClassifier`'s attributes\n# `estimator_errors_` and `estimator_weights_` to investigate this link.\nweak_learners_info = pd.DataFrame(\n    {\n        \"Number of trees\": range(1, n_estimators + 1),\n        \"Errors\": adaboost_clf.estimator_errors_,\n        \"Weights\": adaboost_clf.estimator_weights_,\n    }\n).set_index(\"Number of trees\")\n\naxs = weak_learners_info.plot(\n    subplots=True, layout=(1, 2), figsize=(10, 4), legend=False, color=\"tab:blue\"\n)\naxs[0, 0].set_ylabel(\"Train error\")\naxs[0, 0].set_title(\"Weak learner's training error\")\naxs[0, 1].set_ylabel(\"Weight\")\naxs[0, 1].set_title(\"Weak learner's weight\")\nfig = axs[0, 0].get_figure()\nfig.suptitle(\"Weak learner's errors and weights for the AdaBoostClassifier\")\nfig.tight_layout()\n\n# %%\n# On the left plot, we show the weighted error of each weak learner on the\n# reweighted training set at each boosting iteration. On the right plot, we show\n# the weights associated with each weak learner later used to make the",
                "filename": "examples/ensemble/plot_adaboost_multiclass.py",
                "start_index": 5807,
                "end_index": 8801,
                "start_line": 148,
                "end_line": 225,
                "max_line": 253,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n================================\nGradient Boosting regularization\n================================\n\nIllustration of the effect of different regularization strategies\nfor Gradient Boosting. The example is taken from Hastie et al 2009 [1]_.\n\nThe loss function used is binomial deviance. Regularization via\nshrinkage (``learning_rate < 1.0``) improves performance considerably.\nIn combination with shrinkage, stochastic gradient boosting\n(``subsample < 1.0``) can produce more accurate models by reducing the\nvariance via bagging.\nSubsampling without shrinkage usually does poorly.\nAnother strategy to reduce the variance is by subsampling the features\nanalogous to the random splits in Random Forests\n(via the ``max_features`` parameter).\n\n.. [1] T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical\n    Learning Ed. 2\", Springer, 2009.\n\n\"\"\"\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import datasets, ensemble\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\n\nX, y = datasets.make_hastie_10_2(n_samples=4000, random_state=1)\n\n# map labels from {-1, 1} to {0, 1}\nlabels, y = np.unique(y, return_inverse=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=0)\n\noriginal_params = {\n    \"n_estimators\": 400,\n    \"max_leaf_nodes\": 4,\n    \"max_depth\": None,\n    \"random_state\": 2,\n    \"min_samples_split\": 5,\n}\n\nplt.figure()\n\nfor label, color, setting in [\n    (\"No shrinkage\", \"orange\", {\"learning_rate\": 1.0, \"subsample\": 1.0}),\n    (\"learning_rate=0.2\", \"turquoise\", {\"learning_rate\": 0.2, \"subsample\": 1.0}),\n    (\"subsample=0.5\", \"blue\", {\"learning_rate\": 1.0, \"subsample\": 0.5}),\n    (\n        \"learning_rate=0.2, subsample=0.5\",\n        \"gray\",\n        {\"learning_rate\": 0.2, \"subsample\": 0.5},\n    ),\n    (\n        \"learning_rate=0.2, max_features=2\",\n        \"magenta\",\n        {\"learning_rate\": 0.2, \"max_features\": 2},\n    ),\n]:\n    params = dict(original_params)\n    params.update(setting)\n\n    clf = ensemble.GradientBoostingClassifier(**params)\n    clf.fit(X_train, y_train)\n\n    # compute test set deviance\n    test_deviance = np.zeros((params[\"n_estimators\"],), dtype=np.float64)\n\n    for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):\n        test_deviance[i] = 2 * log_loss(y_test, y_proba[:, 1])\n\n    plt.plot(\n        (np.arange(test_deviance.shape[0]) + 1)[::5],\n        test_deviance[::5],\n        \"-\",\n        color=color,\n        label=label,\n    )\n\nplt.legend(loc=\"upper right\")\nplt.xlabel(\"Boosting Iterations\")\nplt.ylabel(\"Test Set Deviance\")\n\nplt.show()",
                "filename": "examples/ensemble/plot_gradient_boosting_regularization.py",
                "start_index": 0,
                "end_index": 2695,
                "start_line": 1,
                "end_line": 91,
                "max_line": 91,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n==========================================\nFeature importances with a forest of trees\n==========================================\n\nThis example shows the use of a forest of trees to evaluate the importance of\nfeatures on an artificial classification task. The blue bars are the feature\nimportances of the forest, along with their inter-trees variability represented\nby the error bars.\n\nAs expected, the plot suggests that 3 features are informative, while the\nremaining are not.\n\n\"\"\"\n\nimport matplotlib.pyplot as plt\n\n# %%\n# Data generation and model fitting\n# ---------------------------------\n# We generate a synthetic dataset with only 3 informative features. We will\n# explicitly not shuffle the dataset to ensure that the informative features\n# will correspond to the three first columns of X. In addition, we will split\n# our dataset into training and testing subsets.\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(\n    n_samples=1000,\n    n_features=10,\n    n_informative=3,\n    n_redundant=0,\n    n_repeated=0,\n    n_classes=2,\n    random_state=0,\n    shuffle=False,\n)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n# %%\n# A random forest classifier will be fitted to compute the feature importances.\nfrom sklearn.ensemble import RandomForestClassifier\n\nfeature_names = [f\"feature {i}\" for i in range(X.shape[1])]\nforest = RandomForestClassifier(random_state=0)\nforest.fit(X_train, y_train)\n\n# %%\n# Feature importance based on mean decrease in impurity\n# -----------------------------------------------------\n# Feature importances are provided by the fitted attribute\n# `feature_importances_` and they are computed as the mean and standard\n# deviation of accumulation of the impurity decrease within each tree.\n#\n# .. warning::\n#     Impurity-based feature importances can be misleading for **high\n#     cardinality** features (many unique values). See\n#     :ref:`permutation_importance` as an alternative below.\nimport time\n\nimport numpy as np\n\nstart_time = time.time()\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\nelapsed_time = time.time() - start_time\n\nprint(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n\n# %%\n# Let's plot the impurity-based importance.\nimport pandas as pd\n\nforest_importances = pd.Series(importances, index=feature_names)\n\nfig, ax = plt.subplots()\nforest_importances.plot.bar(yerr=std, ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()\n\n# %%\n# We observe that, as expected, the three first features are found important.\n#\n# Feature importance based on feature permutation\n# -----------------------------------------------\n# Permutation feature importance overcomes limitations of the impurity-based",
                "filename": "examples/ensemble/plot_forest_importances.py",
                "start_index": 0,
                "end_index": 2927,
                "start_line": 1,
                "end_line": 87,
                "max_line": 117,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n======================================\nDecision Tree Regression with AdaBoost\n======================================\n\nA decision tree is boosted using the AdaBoost.R2 [1]_ algorithm on a 1D\nsinusoidal dataset with a small amount of Gaussian noise.\n299 boosts (300 decision trees) is compared with a single decision tree\nregressor. As the number of boosts is increased the regressor can fit more\ndetail.\n\n.. [1] `H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n        <https://citeseerx.ist.psu.edu/doc_view/pid/8d49e2dedb817f2c3330e74b63c5fc86d2399ce3>`_\n\n\"\"\"\n\n# %%\n# Preparing the data\n# ------------------\n# First, we prepare dummy data with a sinusoidal relationship and some gaussian noise.\n\n# Author: Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\n\nrng = np.random.RandomState(1)\nX = np.linspace(0, 6, 100)[:, np.newaxis]\ny = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n\n# %%\n# Training and prediction with DecisionTree and AdaBoost Regressors\n# -----------------------------------------------------------------\n# Now, we define the classifiers and fit them to the data.\n# Then we predict on that same data to see how well they could fit it.\n# The first regressor is a `DecisionTreeRegressor` with `max_depth=4`.\n# The second regressor is an `AdaBoostRegressor` with a `DecisionTreeRegressor`\n# of `max_depth=4` as base learner and will be built with `n_estimators=300`\n# of those base learners.\n\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nregr_1 = DecisionTreeRegressor(max_depth=4)\n\nregr_2 = AdaBoostRegressor(\n    DecisionTreeRegressor(max_depth=4), n_estimators=300, random_state=rng\n)\n\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\ny_1 = regr_1.predict(X)\ny_2 = regr_2.predict(X)\n\n# %%\n# Plotting the results\n# --------------------\n# Finally, we plot how well our two regressors,\n# single decision tree regressor and AdaBoost regressor, could fit the data.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncolors = sns.color_palette(\"colorblind\")\n\nplt.figure()\nplt.scatter(X, y, color=colors[0], label=\"training samples\")\nplt.plot(X, y_1, color=colors[1], label=\"n_estimators=1\", linewidth=2)\nplt.plot(X, y_2, color=colors[2], label=\"n_estimators=300\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Boosted Decision Tree Regression\")\nplt.legend()\nplt.show()",
                "filename": "examples/ensemble/plot_adaboost_regression.py",
                "start_index": 0,
                "end_index": 2420,
                "start_line": 1,
                "end_line": 76,
                "max_line": 76,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "num_proc = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n\ncat_proc = make_pipeline(\n    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n    OneHotEncoder(handle_unknown=\"ignore\"),\n)\n\npreprocessor = make_column_transformer(\n    (num_proc, (\"feat1\", \"feat3\")), (cat_proc, (\"feat0\", \"feat2\"))\n)\n\nclf = make_pipeline(preprocessor, LogisticRegression())\nclf\n\n##############################################################################\n# Scalability and stability improvements to KMeans\n# ------------------------------------------------\n# The :class:`~sklearn.cluster.KMeans` estimator was entirely re-worked, and it\n# is now significantly faster and more stable. In addition, the Elkan algorithm\n# is now compatible with sparse matrices. The estimator uses OpenMP based\n# parallelism instead of relying on joblib, so the `n_jobs` parameter has no\n# effect anymore. For more details on how to control the number of threads,\n# please refer to our :ref:`parallelism` notes.\nimport scipy\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import completeness_score\n\nrng = np.random.RandomState(0)\nX, y = make_blobs(random_state=rng)\nX = scipy.sparse.csr_matrix(X)\nX_train, X_test, _, y_test = train_test_split(X, y, random_state=rng)\nkmeans = KMeans(n_init=\"auto\").fit(X_train)\nprint(completeness_score(kmeans.predict(X_test), y_test))\n\n##############################################################################\n# Improvements to the histogram-based Gradient Boosting estimators\n# ----------------------------------------------------------------\n# Various improvements were made to\n# :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and\n# :class:`~sklearn.ensemble.HistGradientBoostingRegressor`. On top of the\n# Poisson loss mentioned above, these estimators now support :ref:`sample\n# weights <sw_hgbdt>`. Also, an automatic early-stopping criterion was added:\n# early-stopping is enabled by default when the number of samples exceeds 10k.\n# Finally, users can now define :ref:`monotonic constraints\n# <monotonic_cst_gbdt>` to constrain the predictions based on the variations of\n# specific features. In the following example, we construct a target that is\n# generally positively correlated with the first feature, with some noise.\n# Applying monotoinc constraints allows the prediction to capture the global\n# effect of the first feature, instead of fitting the noise.\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# from sklearn.inspection import plot_partial_dependence\nfrom sklearn.inspection import PartialDependenceDisplay\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nn_samples = 500\nrng = np.random.RandomState(0)\nX = rng.randn(n_samples, 2)\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\ny = 5 * X[:, 0] + np.sin(10 * np.pi * X[:, 0]) - noise",
                "filename": "examples/release_highlights/plot_release_highlights_0_23_0.py",
                "start_index": 2976,
                "end_index": 5971,
                "start_line": 74,
                "end_line": 138,
                "max_line": 189,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"Gradient Boosting for classification.\n\n    This algorithm builds an additive model in a forward stage-wise fashion; it\n    allows for the optimization of arbitrary differentiable loss functions. In\n    each stage ``n_classes_`` regression trees are fit on the negative gradient\n    of the loss function, e.g. binary or multiclass log loss. Binary\n    classification is a special case where only a single regression tree is\n    induced.\n\n    :class:`sklearn.ensemble.HistGradientBoostingClassifier` is a much faster\n    variant of this algorithm for intermediate datasets (`n_samples >= 10_000`).\n\n    Read more in the :ref:`User Guide <gradient_boosting>`.\n\n    Parameters\n    ----------\n    loss : {'log_loss', 'exponential'}, default='log_loss'\n        The loss function to be optimized. 'log_loss' refers to binomial and\n        multinomial deviance, the same as used in logistic regression.\n        It is a good choice for classification with probabilistic outputs.\n        For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.\n\n    learning_rate : float, default=0.1\n        Learning rate shrinks the contribution of each tree by `learning_rate`.\n        There is a trade-off between learning_rate and n_estimators.\n        Values must be in the range `[0.0, inf)`.\n\n    n_estimators : int, default=100\n        The number of boosting stages to perform. Gradient boosting\n        is fairly robust to over-fitting so a large number usually\n        results in better performance.\n        Values must be in the range `[1, inf)`.\n\n    subsample : float, default=1.0\n        The fraction of samples to be used for fitting the individual base\n        learners. If smaller than 1.0 this results in Stochastic Gradient\n        Boosting. `subsample` interacts with the parameter `n_estimators`.\n        Choosing `subsample < 1.0` leads to a reduction of variance\n        and an increase in bias.\n        Values must be in the range `(0.0, 1.0]`.\n\n    criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'\n        The function to measure the quality of a split. Supported criteria are\n        'friedman_mse' for the mean squared error with improvement score by\n        Friedman, 'squared_error' for mean squared error. The default value of\n        'friedman_mse' is generally the best as it can provide a better\n        approximation in some cases.\n\n        .. versionadded:: 0.18\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, values must be in the range `[2, inf)`.\n        - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`\n          will be `ceil(min_samples_split * n_samples)`.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, values must be in the range `[1, inf)`.\n        - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`\n          will be `ceil(min_samples_leaf * n_samples)`.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n        Values must be in the range `[0.0, 0.5]`.\n\n    max_depth : int or None, default=3\n        Maximum depth of the individual regression estimators. The maximum\n        depth limits the number of nodes in the tree. Tune this parameter\n        for best performance; the best value depends on the interaction\n        of the input variables. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n        If int, values must be in the range `[1, inf)`.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n        Values must be in the range `[0.0, inf)`.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    init : estimator or 'zero', default=None\n        An estimator object that is used to compute the initial predictions.\n        ``init`` has to provide :term:`fit` and :term:`predict_proba`. If\n        'zero', the initial raw predictions are set to zero. By default, a\n        ``DummyEstimator`` predicting the classes priors is used.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given to each Tree estimator at each\n        boosting iteration.\n        In addition, it controls the random permutation of the features at\n        each split (see Notes for more details).\n        It also controls the random splitting of the training data to obtain a\n        validation set if `n_iter_no_change` is not None.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    max_features : {'sqrt', 'log2'}, int or float, default=None\n        The number of features to consider when looking for the best split:\n\n        - If int, values must be in the range `[1, inf)`.\n        - If float, values must be in the range `(0.0, 1.0]` and the features\n          considered at each split will be `max(1, int(max_features * n_features_in_))`.\n        - If 'sqrt', then `max_features=sqrt(n_features)`.\n        - If 'log2', then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Choosing `max_features < n_features` leads to a reduction of variance\n        and an increase in bias.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    verbose : int, default=0\n        Enable verbose output. If 1 then it prints progress and performance\n        once in a while (the more trees the lower the frequency). If greater\n        than 1 then it prints progress and performance for every tree.\n        Values must be in the range `[0, inf)`.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        Values must be in the range `[2, inf)`.\n        If `None`, then unlimited number of leaf nodes.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just erase the\n        previous solution. See :term:`the Glossary <warm_start>`.\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Values must be in the range `(0.0, 1.0)`.\n        Only used if ``n_iter_no_change`` is set to an integer.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=None\n        ``n_iter_no_change`` is used to decide if early stopping will be used\n        to terminate training when validation score is not improving. By\n        default it is set to None to disable early stopping. If set to a\n        number, it will set aside ``validation_fraction`` size of the training\n        data as validation and terminate training when validation score is not\n        improving in all of the previous ``n_iter_no_change`` numbers of\n        iterations. The split is stratified.\n        Values must be in the range `[1, inf)`.\n\n        .. versionadded:: 0.20\n\n    tol : float, default=1e-4\n        Tolerance for the early stopping. When the loss is not improving\n        by at least tol for ``n_iter_no_change`` iterations (if set to a\n        number), the training stops.\n        Values must be in the range `[0.0, inf)`.\n\n        .. versionadded:: 0.20\n\n    ccp_alpha : non-negative float, default=0.0\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n        Values must be in the range `[0.0, inf)`.\n        See :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    n_estimators_ : int\n        The number of estimators as selected by early stopping (if\n        ``n_iter_no_change`` is specified). Otherwise it is set to\n        ``n_estimators``.\n\n        .. versionadded:: 0.20\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    oob_improvement_ : ndarray of shape (n_estimators,)\n        The improvement in loss on the out-of-bag samples\n        relative to the previous iteration.\n        ``oob_improvement_[0]`` is the improvement in\n        loss of the first stage over the ``init`` estimator.\n        Only available if ``subsample < 1.0``.\n\n    oob_scores_ : ndarray of shape (n_estimators,)\n        The full history of the loss values on the out-of-bag\n        samples. Only available if `subsample < 1.0`.\n\n        .. versionadded:: 1.3\n\n    oob_score_ : float\n        The last value of the loss on the out-of-bag samples. It is\n        the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.\n\n        .. versionadded:: 1.3\n\n    train_score_ : ndarray of shape (n_estimators,)\n        The i-th score ``train_score_[i]`` is the loss of the\n        model at iteration ``i`` on the in-bag sample.\n        If ``subsample == 1`` this is the loss on the training data.\n\n    init_ : estimator\n        The estimator that provides the initial predictions.\n        Set via the ``init`` argument or ``loss.init_estimator``.\n\n    estimators_ : ndarray of DecisionTreeRegressor of \\",
                "filename": "sklearn/ensemble/_gb.py",
                "start_index": 31581,
                "end_index": 42812,
                "start_line": 835,
                "end_line": 1077,
                "max_line": 1828,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n======================================\nGradient Boosting Out-of-Bag estimates\n======================================\nOut-of-bag (OOB) estimates can be a useful heuristic to estimate\nthe \"optimal\" number of boosting iterations.\nOOB estimates are almost identical to cross-validation estimates but\nthey can be computed on-the-fly without the need for repeated model\nfitting.\nOOB estimates are only available for Stochastic Gradient Boosting\n(i.e. ``subsample < 1.0``), the estimates are derived from the improvement\nin loss based on the examples not included in the bootstrap sample\n(the so-called out-of-bag examples).\nThe OOB estimator is a pessimistic estimator of the true\ntest loss, but remains a fairly good approximation for a small number of trees.\nThe figure shows the cumulative sum of the negative OOB improvements\nas a function of the boosting iteration. As you can see, it tracks the test\nloss for the first hundred iterations but then diverges in a\npessimistic way.\nThe figure also shows the performance of 3-fold cross validation which\nusually gives a better estimate of the test loss\nbut is computationally more demanding.\n\"\"\"\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.special import expit\n\nfrom sklearn import ensemble\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import KFold, train_test_split\n\n# Generate data (adapted from G. Ridgeway's gbm example)\nn_samples = 1000\nrandom_state = np.random.RandomState(13)\nx1 = random_state.uniform(size=n_samples)\nx2 = random_state.uniform(size=n_samples)\nx3 = random_state.randint(0, 4, size=n_samples)\n\np = expit(np.sin(3 * x1) - 4 * x2 + x3)\ny = random_state.binomial(1, p, size=n_samples)\n\nX = np.c_[x1, x2, x3]\n\nX = X.astype(np.float32)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=9)\n\n# Fit classifier with out-of-bag estimates\nparams = {\n    \"n_estimators\": 1200,\n    \"max_depth\": 3,\n    \"subsample\": 0.5,\n    \"learning_rate\": 0.01,\n    \"min_samples_leaf\": 1,\n    \"random_state\": 3,\n}\nclf = ensemble.GradientBoostingClassifier(**params)\n\nclf.fit(X_train, y_train)\nacc = clf.score(X_test, y_test)\nprint(\"Accuracy: {:.4f}\".format(acc))\n\nn_estimators = params[\"n_estimators\"]\nx = np.arange(n_estimators) + 1\n\n\ndef heldout_score(clf, X_test, y_test):\n    \"\"\"compute deviance scores on ``X_test`` and ``y_test``.\"\"\"\n    score = np.zeros((n_estimators,), dtype=np.float64)\n    for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):\n        score[i] = 2 * log_loss(y_test, y_proba[:, 1])\n    return score",
                "filename": "examples/ensemble/plot_gradient_boosting_oob.py",
                "start_index": 0,
                "end_index": 2640,
                "start_line": 1,
                "end_line": 76,
                "max_line": 144,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "\"\"\"\n=====================\nMonotonic Constraints\n=====================\n\nThis example illustrates the effect of monotonic constraints on a gradient\nboosting estimator.\n\nWe build an artificial dataset where the target value is in general\npositively correlated with the first feature (with some random and\nnon-random variations), and in general negatively correlated with the second\nfeature.\n\nBy imposing a monotonic increase or a monotonic decrease constraint, respectively,\non the features during the learning process, the estimator is able to properly follow\nthe general trend instead of being subject to the variations.\n\nThis example was inspired by the `XGBoost documentation\n<https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html>`_.\n\n\"\"\"\n# %%\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.inspection import PartialDependenceDisplay\n\nrng = np.random.RandomState(0)\n\nn_samples = 1000\nf_0 = rng.rand(n_samples)\nf_1 = rng.rand(n_samples)\nX = np.c_[f_0, f_1]\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n\n# y is positively correlated with f_0, and negatively correlated with f_1\ny = 5 * f_0 + np.sin(10 * np.pi * f_0) - 5 * f_1 - np.cos(10 * np.pi * f_1) + noise\n\n\n# %%\n# Fit a first model on this dataset without any constraints.\ngbdt_no_cst = HistGradientBoostingRegressor()\ngbdt_no_cst.fit(X, y)\n\n# %%\n# Fit a second model on this dataset with monotonic increase (1)\n# and a monotonic decrease (-1) constraints, respectively.\ngbdt_with_monotonic_cst = HistGradientBoostingRegressor(monotonic_cst=[1, -1])\ngbdt_with_monotonic_cst.fit(X, y)\n\n\n# %%\n# Let's display the partial dependence of the predictions on the two features.\nfig, ax = plt.subplots()\ndisp = PartialDependenceDisplay.from_estimator(\n    gbdt_no_cst,\n    X,\n    features=[0, 1],\n    feature_names=(\n        \"First feature\",\n        \"Second feature\",\n    ),\n    line_kw={\"linewidth\": 4, \"label\": \"unconstrained\", \"color\": \"tab:blue\"},\n    ax=ax,\n)\nPartialDependenceDisplay.from_estimator(\n    gbdt_with_monotonic_cst,\n    X,\n    features=[0, 1],\n    line_kw={\"linewidth\": 4, \"label\": \"constrained\", \"color\": \"tab:orange\"},\n    ax=disp.axes_,\n)\n\nfor f_idx in (0, 1):\n    disp.axes_[0, f_idx].plot(\n        X[:, f_idx], y, \"o\", alpha=0.3, zorder=-1, color=\"tab:green\"\n    )\n    disp.axes_[0, f_idx].set_ylim(-6, 6)\n\nplt.legend()\nfig.suptitle(\"Monotonic constraints effect on partial dependences\")\nplt.show()\n\n# %%\n# We can see that the predictions of the unconstrained model capture the\n# oscillations of the data while the constrained model follows the general\n# trend and ignores the local variations.\n\n# %%\n# .. _monotonic_cst_features_names:\n#\n# Using feature names to specify monotonic constraints\n# ----------------------------------------------------\n#\n# Note that if the training data has feature names, it's possible to specify the\n# monotonic constraints by passing a dictionary:\nimport pandas as pd",
                "filename": "examples/ensemble/plot_monotonic_constraints.py",
                "start_index": 0,
                "end_index": 2972,
                "start_line": 1,
                "end_line": 98,
                "max_line": 108,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            },
            {
                "code": "for i, y_pred in enumerate(ada_discrete.staged_predict(X_test)):\n    ada_discrete_err[i] = zero_one_loss(y_pred, y_test)\n\nada_discrete_err_train = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(ada_discrete.staged_predict(X_train)):\n    ada_discrete_err_train[i] = zero_one_loss(y_pred, y_train)\n\nada_real_err = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(ada_real.staged_predict(X_test)):\n    ada_real_err[i] = zero_one_loss(y_pred, y_test)\n\nada_real_err_train = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(ada_real.staged_predict(X_train)):\n    ada_real_err_train[i] = zero_one_loss(y_pred, y_train)\n\n# %%\n# Plotting the results\n# --------------------\n# Finally, we plot the train and test errors of our baselines\n# and of the discrete and real AdaBoost classifiers\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nax.plot([1, n_estimators], [dt_stump_err] * 2, \"k-\", label=\"Decision Stump Error\")\nax.plot([1, n_estimators], [dt_err] * 2, \"k--\", label=\"Decision Tree Error\")\n\ncolors = sns.color_palette(\"colorblind\")\n\nax.plot(\n    np.arange(n_estimators) + 1,\n    ada_discrete_err,\n    label=\"Discrete AdaBoost Test Error\",\n    color=colors[0],\n)\nax.plot(\n    np.arange(n_estimators) + 1,\n    ada_discrete_err_train,\n    label=\"Discrete AdaBoost Train Error\",\n    color=colors[1],\n)\nax.plot(\n    np.arange(n_estimators) + 1,\n    ada_real_err,\n    label=\"Real AdaBoost Test Error\",\n    color=colors[2],\n)\nax.plot(\n    np.arange(n_estimators) + 1,\n    ada_real_err_train,\n    label=\"Real AdaBoost Train Error\",\n    color=colors[4],\n)\n\nax.set_ylim((0.0, 0.5))\nax.set_xlabel(\"Number of weak learners\")\nax.set_ylabel(\"error rate\")\n\nleg = ax.legend(loc=\"upper right\", fancybox=True)\nleg.get_frame().set_alpha(0.7)\n\nplt.show()\n# %%\n#\n# Concluding remarks\n# ------------------\n#\n# We observe that the error rate for both train and test sets of real AdaBoost\n# is lower than that of discrete AdaBoost.",
                "filename": "examples/ensemble/plot_adaboost_hastie_10_2.py",
                "start_index": 2975,
                "end_index": 4946,
                "start_line": 101,
                "end_line": 172,
                "max_line": 172,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/ensemble/_gb.py": [
                {
                    "chunk": {
                        "code": "\"\"\"Gradient Boosting for classification.\n\n    This algorithm builds an additive model in a forward stage-wise fashion; it\n    allows for the optimization of arbitrary differentiable loss functions. In\n    each stage ``n_classes_`` regression trees are fit on the negative gradient\n    of the loss function, e.g. binary or multiclass log loss. Binary\n    classification is a special case where only a single regression tree is\n    induced.\n\n    :class:`sklearn.ensemble.HistGradientBoostingClassifier` is a much faster\n    variant of this algorithm for intermediate datasets (`n_samples >= 10_000`).\n\n    Read more in the :ref:`User Guide <gradient_boosting>`.\n\n    Parameters\n    ----------\n    loss : {'log_loss', 'exponential'}, default='log_loss'\n        The loss function to be optimized. 'log_loss' refers to binomial and\n        multinomial deviance, the same as used in logistic regression.\n        It is a good choice for classification with probabilistic outputs.\n        For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.\n\n    learning_rate : float, default=0.1\n        Learning rate shrinks the contribution of each tree by `learning_rate`.\n        There is a trade-off between learning_rate and n_estimators.\n        Values must be in the range `[0.0, inf)`.\n\n    n_estimators : int, default=100\n        The number of boosting stages to perform. Gradient boosting\n        is fairly robust to over-fitting so a large number usually\n        results in better performance.\n        Values must be in the range `[1, inf)`.\n\n    subsample : float, default=1.0\n        The fraction of samples to be used for fitting the individual base\n        learners. If smaller than 1.0 this results in Stochastic Gradient\n        Boosting. `subsample` interacts with the parameter `n_estimators`.\n        Choosing `subsample < 1.0` leads to a reduction of variance\n        and an increase in bias.\n        Values must be in the range `(0.0, 1.0]`.\n\n    criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'\n        The function to measure the quality of a split. Supported criteria are\n        'friedman_mse' for the mean squared error with improvement score by\n        Friedman, 'squared_error' for mean squared error. The default value of\n        'friedman_mse' is generally the best as it can provide a better\n        approximation in some cases.\n\n        .. versionadded:: 0.18\n\n    min_samples_split : int or float, default=2\n        The minimum number of samples required to split an internal node:\n\n        - If int, values must be in the range `[2, inf)`.\n        - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`\n          will be `ceil(min_samples_split * n_samples)`.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_samples_leaf : int or float, default=1\n        The minimum number of samples required to be at a leaf node.\n        A split point at any depth will only be considered if it leaves at\n        least ``min_samples_leaf`` training samples in each of the left and\n        right branches.  This may have the effect of smoothing the model,\n        especially in regression.\n\n        - If int, values must be in the range `[1, inf)`.\n        - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`\n          will be `ceil(min_samples_leaf * n_samples)`.\n\n        .. versionchanged:: 0.18\n           Added float values for fractions.\n\n    min_weight_fraction_leaf : float, default=0.0\n        The minimum weighted fraction of the sum total of weights (of all\n        the input samples) required to be at a leaf node. Samples have\n        equal weight when sample_weight is not provided.\n        Values must be in the range `[0.0, 0.5]`.\n\n    max_depth : int or None, default=3\n        Maximum depth of the individual regression estimators. The maximum\n        depth limits the number of nodes in the tree. Tune this parameter\n        for best performance; the best value depends on the interaction\n        of the input variables. If None, then nodes are expanded until\n        all leaves are pure or until all leaves contain less than\n        min_samples_split samples.\n        If int, values must be in the range `[1, inf)`.\n\n    min_impurity_decrease : float, default=0.0\n        A node will be split if this split induces a decrease of the impurity\n        greater than or equal to this value.\n        Values must be in the range `[0.0, inf)`.\n\n        The weighted impurity decrease equation is the following::\n\n            N_t / N * (impurity - N_t_R / N_t * right_impurity\n                                - N_t_L / N_t * left_impurity)\n\n        where ``N`` is the total number of samples, ``N_t`` is the number of\n        samples at the current node, ``N_t_L`` is the number of samples in the\n        left child, and ``N_t_R`` is the number of samples in the right child.\n\n        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n        if ``sample_weight`` is passed.\n\n        .. versionadded:: 0.19\n\n    init : estimator or 'zero', default=None\n        An estimator object that is used to compute the initial predictions.\n        ``init`` has to provide :term:`fit` and :term:`predict_proba`. If\n        'zero', the initial raw predictions are set to zero. By default, a\n        ``DummyEstimator`` predicting the classes priors is used.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the random seed given to each Tree estimator at each\n        boosting iteration.\n        In addition, it controls the random permutation of the features at\n        each split (see Notes for more details).\n        It also controls the random splitting of the training data to obtain a\n        validation set if `n_iter_no_change` is not None.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    max_features : {'sqrt', 'log2'}, int or float, default=None\n        The number of features to consider when looking for the best split:\n\n        - If int, values must be in the range `[1, inf)`.\n        - If float, values must be in the range `(0.0, 1.0]` and the features\n          considered at each split will be `max(1, int(max_features * n_features_in_))`.\n        - If 'sqrt', then `max_features=sqrt(n_features)`.\n        - If 'log2', then `max_features=log2(n_features)`.\n        - If None, then `max_features=n_features`.\n\n        Choosing `max_features < n_features` leads to a reduction of variance\n        and an increase in bias.\n\n        Note: the search for a split does not stop until at least one\n        valid partition of the node samples is found, even if it requires to\n        effectively inspect more than ``max_features`` features.\n\n    verbose : int, default=0\n        Enable verbose output. If 1 then it prints progress and performance\n        once in a while (the more trees the lower the frequency). If greater\n        than 1 then it prints progress and performance for every tree.\n        Values must be in the range `[0, inf)`.\n\n    max_leaf_nodes : int, default=None\n        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n        Best nodes are defined as relative reduction in impurity.\n        Values must be in the range `[2, inf)`.\n        If `None`, then unlimited number of leaf nodes.\n\n    warm_start : bool, default=False\n        When set to ``True``, reuse the solution of the previous call to fit\n        and add more estimators to the ensemble, otherwise, just erase the\n        previous solution. See :term:`the Glossary <warm_start>`.\n\n    validation_fraction : float, default=0.1\n        The proportion of training data to set aside as validation set for\n        early stopping. Values must be in the range `(0.0, 1.0)`.\n        Only used if ``n_iter_no_change`` is set to an integer.\n\n        .. versionadded:: 0.20\n\n    n_iter_no_change : int, default=None\n        ``n_iter_no_change`` is used to decide if early stopping will be used\n        to terminate training when validation score is not improving. By\n        default it is set to None to disable early stopping. If set to a\n        number, it will set aside ``validation_fraction`` size of the training\n        data as validation and terminate training when validation score is not\n        improving in all of the previous ``n_iter_no_change`` numbers of\n        iterations. The split is stratified.\n        Values must be in the range `[1, inf)`.\n\n        .. versionadded:: 0.20\n\n    tol : float, default=1e-4\n        Tolerance for the early stopping. When the loss is not improving\n        by at least tol for ``n_iter_no_change`` iterations (if set to a\n        number), the training stops.\n        Values must be in the range `[0.0, inf)`.\n\n        .. versionadded:: 0.20\n\n    ccp_alpha : non-negative float, default=0.0\n        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n        subtree with the largest cost complexity that is smaller than\n        ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n        Values must be in the range `[0.0, inf)`.\n        See :ref:`minimal_cost_complexity_pruning` for details.\n\n        .. versionadded:: 0.22\n\n    Attributes\n    ----------\n    n_estimators_ : int\n        The number of estimators as selected by early stopping (if\n        ``n_iter_no_change`` is specified). Otherwise it is set to\n        ``n_estimators``.\n\n        .. versionadded:: 0.20\n\n    feature_importances_ : ndarray of shape (n_features,)\n        The impurity-based feature importances.\n        The higher, the more important the feature.\n        The importance of a feature is computed as the (normalized)\n        total reduction of the criterion brought by that feature.  It is also\n        known as the Gini importance.\n\n        Warning: impurity-based feature importances can be misleading for\n        high cardinality features (many unique values). See\n        :func:`sklearn.inspection.permutation_importance` as an alternative.\n\n    oob_improvement_ : ndarray of shape (n_estimators,)\n        The improvement in loss on the out-of-bag samples\n        relative to the previous iteration.\n        ``oob_improvement_[0]`` is the improvement in\n        loss of the first stage over the ``init`` estimator.\n        Only available if ``subsample < 1.0``.\n\n    oob_scores_ : ndarray of shape (n_estimators,)\n        The full history of the loss values on the out-of-bag\n        samples. Only available if `subsample < 1.0`.\n\n        .. versionadded:: 1.3\n\n    oob_score_ : float\n        The last value of the loss on the out-of-bag samples. It is\n        the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.\n\n        .. versionadded:: 1.3\n\n    train_score_ : ndarray of shape (n_estimators,)\n        The i-th score ``train_score_[i]`` is the loss of the\n        model at iteration ``i`` on the in-bag sample.\n        If ``subsample == 1`` this is the loss on the training data.\n\n    init_ : estimator\n        The estimator that provides the initial predictions.\n        Set via the ``init`` argument or ``loss.init_estimator``.\n\n    estimators_ : ndarray of DecisionTreeRegressor of \\",
                        "filename": "sklearn/ensemble/_gb.py",
                        "start_index": 31581,
                        "end_index": 42812,
                        "start_line": 835,
                        "end_line": 1077,
                        "max_line": 1828,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet is from the actual implementation file of the Gradient Boosting algorithm in scikit-learn, which is directly related to the issue reported. It contains the parameters and attributes of the GradientBoostingClassifier, which may need to be reviewed or modified to address the feature importance sum issue."
                }
            ],
            "examples/ensemble/plot_gradient_boosting_regularization.py": [
                {
                    "chunk": {
                        "code": "\"\"\"\n================================\nGradient Boosting regularization\n================================\n\nIllustration of the effect of different regularization strategies\nfor Gradient Boosting. The example is taken from Hastie et al 2009 [1]_.\n\nThe loss function used is binomial deviance. Regularization via\nshrinkage (``learning_rate < 1.0``) improves performance considerably.\nIn combination with shrinkage, stochastic gradient boosting\n(``subsample < 1.0``) can produce more accurate models by reducing the\nvariance via bagging.\nSubsampling without shrinkage usually does poorly.\nAnother strategy to reduce the variance is by subsampling the features\nanalogous to the random splits in Random Forests\n(via the ``max_features`` parameter).\n\n.. [1] T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical\n    Learning Ed. 2\", Springer, 2009.\n\n\"\"\"\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import datasets, ensemble\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\n\nX, y = datasets.make_hastie_10_2(n_samples=4000, random_state=1)\n\n# map labels from {-1, 1} to {0, 1}\nlabels, y = np.unique(y, return_inverse=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=0)\n\noriginal_params = {\n    \"n_estimators\": 400,\n    \"max_leaf_nodes\": 4,\n    \"max_depth\": None,\n    \"random_state\": 2,\n    \"min_samples_split\": 5,\n}\n\nplt.figure()\n\nfor label, color, setting in [\n    (\"No shrinkage\", \"orange\", {\"learning_rate\": 1.0, \"subsample\": 1.0}),\n    (\"learning_rate=0.2\", \"turquoise\", {\"learning_rate\": 0.2, \"subsample\": 1.0}),\n    (\"subsample=0.5\", \"blue\", {\"learning_rate\": 1.0, \"subsample\": 0.5}),\n    (\n        \"learning_rate=0.2, subsample=0.5\",\n        \"gray\",\n        {\"learning_rate\": 0.2, \"subsample\": 0.5},\n    ),\n    (\n        \"learning_rate=0.2, max_features=2\",\n        \"magenta\",\n        {\"learning_rate\": 0.2, \"max_features\": 2},\n    ),\n]:\n    params = dict(original_params)\n    params.update(setting)\n\n    clf = ensemble.GradientBoostingClassifier(**params)\n    clf.fit(X_train, y_train)\n\n    # compute test set deviance\n    test_deviance = np.zeros((params[\"n_estimators\"],), dtype=np.float64)\n\n    for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):\n        test_deviance[i] = 2 * log_loss(y_test, y_proba[:, 1])\n\n    plt.plot(\n        (np.arange(test_deviance.shape[0]) + 1)[::5],\n        test_deviance[::5],\n        \"-\",\n        color=color,\n        label=label,\n    )\n\nplt.legend(loc=\"upper right\")\nplt.xlabel(\"Boosting Iterations\")\nplt.ylabel(\"Test Set Deviance\")\n\nplt.show()",
                        "filename": "examples/ensemble/plot_gradient_boosting_regularization.py",
                        "start_index": 0,
                        "end_index": 2695,
                        "start_line": 1,
                        "end_line": 91,
                        "max_line": 91,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains an example of using Gradient Boosting with regularization in scikit-learn. It is relevant because it demonstrates the use of the GradientBoostingClassifier, which is the subject of the reported issue. The example may need to be reviewed to ensure it aligns with expected behavior after any potential fixes."
                }
            ]
        },
        "solution_task": "Based on the provided information, the issue seems to be related to the calculation of feature importances in ensemble tree methods like GradientBoostingRegressor and AdaBoost. The sum of feature importances is expected to be 1, but it is reported to decrease with more estimators, even reaching 0 in some cases. This could be a floating-point precision error or an issue with the implementation of the feature importance calculation.\n\nTo address the issue, consider the following steps:\n\n- Ensure that the calculation of feature importances is normalized correctly across all trees in the ensemble. The sum of feature importances for each tree should be 1, and the overall feature importances should be averaged across all trees.\n- Check for any floating-point precision issues that may arise when summing small numbers. This can be particularly problematic if the number of estimators is large.\n- Verify that the feature importances are being accumulated correctly across all estimators in the ensemble. This involves checking the `feature_importances_` property and how it is computed from the individual trees' importances.\n- If the issue persists, consider updating to a more recent version of the libraries (NumPy, SciPy, Scikit-Learn) as the issue might have been addressed in later versions.\n- As a last resort, if the problem is identified as a bug in the Scikit-Learn implementation, report the issue to the Scikit-Learn GitHub repository with a reproducible example.\n\nHere is a pseudocode snippet that demonstrates how feature importances should be normalized and averaged across all trees in an ensemble:\n\n```python\nclass GradientBoostingRegressor:\n    # ...\n\n    @property\n    def feature_importances_(self):\n        all_importances = [tree.feature_importances_ for tree in self.estimators_]\n        # Normalize feature importances for each tree to sum to 1\n        all_importances_normalized = [imp / np.sum(imp) for imp in all_importances]\n        # Average feature importances across all trees\n        avg_importances = np.mean(all_importances_normalized, axis=0)\n        return avg_importances\n```\n\nEnsure that the `feature_importances_` for each individual tree (`tree.feature_importances_`) are calculated correctly and sum to 1 before averaging them across all trees in the ensemble.",
        "bouncer_response": {
            "score": 7,
            "criteria": "The RESPONSE offers clear and comprehensive guidance or solutions for the described bug.",
            "supporting_evidence": "The RESPONSE provides a detailed analysis of the potential causes of the bug, such as floating-point precision errors or issues with the implementation of feature importance calculation. It suggests actionable steps to address the issue, including normalization and averaging of feature importances, checking for floating-point precision issues, verifying the accumulation of feature importances, and updating libraries. The RESPONSE also includes a pseudocode snippet to demonstrate how feature importances should be normalized and averaged, which is directly relevant to the problem. Additionally, it suggests reporting the issue to the Scikit-Learn GitHub repository if it is identified as a bug, which is a valid course of action. The links to code snippets in the Scikit-Learn repository are helpful for further investigation. However, the RESPONSE does not address the fact that the user is using an outdated version of Scikit-Learn, which could be a significant factor in the issue.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711740646.153248,
        "result": "Based on the provided information, the issue seems to be related to the calculation of feature importances in ensemble tree methods like GradientBoostingRegressor and AdaBoost. The sum of feature importances is expected to be 1, but it is reported to decrease with more estimators, even reaching 0 in some cases. This could be a floating-point precision error or an issue with the implementation of the feature importance calculation.\n\nTo address the issue, consider the following steps:\n\n- Ensure that the calculation of feature importances is normalized correctly across all trees in the ensemble. The sum of feature importances for each tree should be 1, and the overall feature importances should be averaged across all trees.\n- Check for any floating-point precision issues that may arise when summing small numbers. This can be particularly problematic if the number of estimators is large.\n- Verify that the feature importances are being accumulated correctly across all estimators in the ensemble. This involves checking the `feature_importances_` property and how it is computed from the individual trees' importances.\n- If the issue persists, consider updating to a more recent version of the libraries (NumPy, SciPy, Scikit-Learn) as the issue might have been addressed in later versions.\n- As a last resort, if the problem is identified as a bug in the Scikit-Learn implementation, report the issue to the Scikit-Learn GitHub repository with a reproducible example.\n\nHere is a pseudocode snippet that demonstrates how feature importances should be normalized and averaged across all trees in an ensemble:\n\n```python\nclass GradientBoostingRegressor:\n    # ...\n\n    @property\n    def feature_importances_(self):\n        all_importances = [tree.feature_importances_ for tree in self.estimators_]\n        # Normalize feature importances for each tree to sum to 1\n        all_importances_normalized = [imp / np.sum(imp) for imp in all_importances]\n        # Average feature importances across all trees\n        avg_importances = np.mean(all_importances_normalized, axis=0)\n        return avg_importances\n```\n\nEnsure that the `feature_importances_` for each individual tree (`tree.feature_importances_`) are calculated correctly and sum to 1 before averaging them across all trees in the ensemble.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [ensemble](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/ensemble)- [Lines 835 - 1077](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/ensemble/_gb.py#L835-L1077)\n\n This snippet is from the actual implementation file of the Gradient Boosting algorithm in scikit-learn, which is directly related to the issue reported. It contains the parameters and attributes of the GradientBoostingClassifier, which may need to be reviewed or modified to address the feature importance sum issue.\n-  \u2192 [examples](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//examples) \u2192 [ensemble](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//examples/ensemble)- [Lines 1 - 91](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//examples/ensemble/plot_gradient_boosting_regularization.py#L1-L91)\n\n This snippet contains an example of using Gradient Boosting with regularization in scikit-learn. It is relevant because it demonstrates the use of the GradientBoostingClassifier, which is the subject of the reported issue. The example may need to be reviewed to ensure it aligns with expected behavior after any potential fixes."
    }
}