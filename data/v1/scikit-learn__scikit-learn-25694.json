{
    "id": "scikit-learn__scikit-learn-25694",
    "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
    "task_id": 5894620215514086844,
    "title": "MLPRegressor.partial_fit produces an error when early_stopping is True",
    "body": "### Describe the bug\n\nWIth `sklearn = 1.2.1`, when using `early_stopping = True`, `fit` works fine, but partial fit produces the following error:\r\n\r\nI think this is related to this change: https://github.com/scikit-learn/scikit-learn/pull/24683.\n\n### Steps/Code to Reproduce\n\n```\r\nimport numpy.random\r\nfrom sklearn.neural_network import MLPRegressor\r\n\r\nif __name__ == '__main__':\r\n    x = numpy.random.normal(size=(200, 4))\r\n    y = x[:, 0] * 2 + x[:, 1] * 3 + x[:, 3] + numpy.random.normal(size=(200,))\r\n\r\n    algo = MLPRegressor(early_stopping=True).fit(x, y)\r\n    algo.partial_fit(x, y)\r\n```\n\n### Expected Results\n\nIf early stopping is not supported for partial fit, it should handle this gracefully. If this is a bug - it should be fixed.\n\n### Actual Results\n\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/ilyastolyarov/Repos/new/clpu/examples/usecases/script.py\", line 12, in <module>\r\n    algo.partial_fit(x, y)\r\n  File \"/Users/ilyastolyarov/Repos/new/clpu/.venv/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1640, in partial_fit\r\n    return self._fit(X, y, incremental=True)\r\n  File \"/Users/ilyastolyarov/Repos/new/clpu/.venv/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 471, in _fit\r\n    self._fit_stochastic(\r\n  File \"/Users/ilyastolyarov/Repos/new/clpu/.venv/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 653, in _fit_stochastic\r\n    self._update_no_improvement_count(early_stopping, X_val, y_val)\r\n  File \"/Users/ilyastolyarov/Repos/new/clpu/.venv/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 721, in _update_no_improvement_count\r\n    if self.loss_curve_[-1] > self.best_loss_ - self.tol:\r\nTypeError: unsupported operand type(s) for -: 'NoneType' and 'float'\r\n```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.8.12 (default, May 16 2022, 17:58:21)  [Clang 13.0.0 (clang-1300.0.29.30)]\r\nexecutable: /Users/ilyastolyarov/Repos/new/clpu/.venv/bin/python\r\n   machine: macOS-11.6-x86_64-i386-64bit\r\nPython dependencies:\r\n      sklearn: 1.2.1\r\n          pip: 22.3.1\r\n   setuptools: 64.0.2\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: 0.29.33\r\n       pandas: 1.5.3\r\n   matplotlib: 3.7.0\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\nBuilt with OpenMP: True\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /Users/ilyastolyarov/Repos/new/clpu/.venv/lib/python3.8/site-packages/numpy/.dylibs/libopenblas64_.0.dylib\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Nehalem\r\n    num_threads: 8\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libomp\r\n       filepath: /Users/ilyastolyarov/Repos/new/clpu/.venv/lib/python3.8/site-packages/sklearn/.dylibs/libomp.dylib\r\n        version: None\r\n    num_threads: 8\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: /Users/ilyastolyarov/Repos/new/clpu/.venv/lib/python3.8/site-packages/scipy/.dylibs/libopenblas.0.dylib\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Nehalem\r\n    num_threads: 8\r\n```\n```\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__1.3.git",
    "requested_commit": "",
    "mayil_response": "- Add a check in the `MLPRegressor`'s `partial_fit` method to ensure that `early_stopping` is not used with `partial_fit`, as it is not supported. Raise a `ValueError` if `early_stopping` is set to `True` when calling `partial_fit`.\n\n```python\nclass MLPRegressor(BaseEstimator, RegressorMixin):\n    # ...\n\n    def partial_fit(self, X, y):\n        if self.early_stopping:\n            raise ValueError(\"early_stopping cannot be used with partial_fit\")\n        # existing code for partial_fit continues here\n        ...\n```\n\n- Ensure that `self.loss_curve_` is initialized before it is used in the `_update_no_improvement_count` method to avoid `NoneType` subtraction errors.\n\n```python\nclass MLPRegressor(BaseEstimator, RegressorMixin):\n    # ...\n\n    def _update_no_improvement_count(self, early_stopping, X_val, y_val):\n        if self.loss_curve_ is None:\n            self.loss_curve_ = []\n        # existing code for _update_no_improvement_count continues here\n        if self.loss_curve_ and self.loss_curve_[-1] > self.best_loss_ - self.tol:\n            # ...\n```\n\n- Alternatively, if `self.loss_curve_` is supposed to be populated elsewhere before `_update_no_improvement_count` is called, ensure that the code that is responsible for populating `self.loss_curve_` is called before `_update_no_improvement_count`.\n\n```python\nclass MLPRegressor(BaseEstimator, RegressorMixin):\n    # ...\n\n    def _fit_stochastic(self, ...):\n        # Ensure self.loss_curve_ is initialized or populated before calling _update_no_improvement_count\n        if self.loss_curve_ is None:\n            self.loss_curve_ = []\n        # existing code for _fit_stochastic continues here\n        ...\n```\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn) \u2192 [neural_network](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn/neural_network)- [Lines 1258 - 1258](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/neural_network/_multilayer_perceptron.py#L1258-L1258)\n\n This snippet defines the MLPRegressor class, which is directly related to the issue reported. The error occurs when calling partial_fit on an instance of MLPRegressor with early_stopping enabled.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn/utils)- [Lines 181 - 2142](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/utils/estimator_checks.py#L181-L2142)\n\n This snippet contains tests for the partial_fit method of estimators, which is relevant to the issue since the bug occurs during a call to partial_fit. \n\n\n   - [Lines 4160 - 4207](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/utils/estimator_checks.py#L4160-L4207)\n\n This snippet includes a check for early_stopping in the context of partial_fit, which is relevant to the issue as the error is related to early_stopping being enabled during partial_fit.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0003495,
        "snippet_processor": 0.04503,
        "issue_star_creation": 0.0381,
        "issue_star_solver": 0.06981000000000001,
        "bouncer": 0.02748
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711737893.1576161,
        "relevant_snippets": [
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_estimators_partial_fit_n_features(name, estimator_orig):\n    # check if number of features changes between calls to partial_fit.\n    if not hasattr(estimator_orig, \"partial_fit\"):\n        return\n    estimator = clone(estimator_orig)\n    X, y = make_blobs(n_samples=50, random_state=1)\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    y = _enforce_estimator_tags_y(estimator_orig, y)\n\n    try:\n        if is_classifier(estimator):\n            classes = np.unique(y)\n            estimator.partial_fit(X, y, classes=classes)\n        else:\n            estimator.partial_fit(X, y)\n    except NotImplementedError:\n        return\n\n    with raises(\n        ValueError,\n        err_msg=(\n            f\"The estimator {name} does not raise an error when the \"\n            \"number of features changes between calls to partial_fit.\"\n        ),\n    ):\n        estimator.partial_fit(X[:, :-1], y)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 72345,
                "end_index": 73285,
                "start_line": 181,
                "end_line": 2142,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "invalid_names = [\n        (names[::-1], \"Feature names must be in the same order as they were in fit.\"),\n        (\n            [f\"another_prefix_{i}\" for i in range(n_features)],\n            (\n                \"Feature names unseen at fit time:\\n- another_prefix_0\\n-\"\n                \" another_prefix_1\\n\"\n            ),\n        ),\n        (\n            names[:3],\n            f\"Feature names seen at fit time, yet now missing:\\n- {min(names[3:])}\\n\",\n        ),\n    ]\n    params = {\n        key: value\n        for key, value in estimator.get_params().items()\n        if \"early_stopping\" in key\n    }\n    early_stopping_enabled = any(value is True for value in params.values())\n\n    for invalid_name, additional_message in invalid_names:\n        X_bad = pd.DataFrame(X, columns=invalid_name, copy=False)\n\n        expected_msg = re.escape(\n            \"The feature names should match those that were passed during fit.\\n\"\n            f\"{additional_message}\"\n        )\n        for name, method in check_methods:\n            with raises(\n                ValueError, match=expected_msg, err_msg=f\"{name} did not raise\"\n            ):\n                method(X_bad)\n\n        # partial_fit checks on second call\n        # Do not call partial fit if early_stopping is on\n        if not hasattr(estimator, \"partial_fit\") or early_stopping_enabled:\n            continue\n\n        estimator = clone(estimator_orig)\n        if is_classifier(estimator):\n            classes = np.unique(y)\n            estimator.partial_fit(X, y, classes=classes)\n        else:\n            estimator.partial_fit(X, y)\n\n        with raises(ValueError, match=expected_msg):\n            estimator.partial_fit(X_bad, y)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 146663,
                "end_index": 148346,
                "start_line": 4160,
                "end_line": 4207,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "def _partial_fit(\n        self,\n        X,\n        y,\n        alpha,\n        C,\n        loss,\n        learning_rate,\n        max_iter,\n        classes,\n        sample_weight,\n        coef_init,\n        intercept_init,\n    ):\n        first_call = not hasattr(self, \"classes_\")\n        X, y = self._validate_data(\n            X,\n            y,\n            accept_sparse=\"csr\",\n            dtype=[np.float64, np.float32],\n            order=\"C\",\n            accept_large_sparse=False,\n            reset=first_call,\n        )\n\n        n_samples, n_features = X.shape\n\n        _check_partial_fit_first_call(self, classes)\n\n        n_classes = self.classes_.shape[0]\n\n        # Allocate datastructures from input arguments\n        self._expanded_class_weight = compute_class_weight(\n            self.class_weight, classes=self.classes_, y=y\n        )\n        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n\n        if getattr(self, \"coef_\", None) is None or coef_init is not None:\n            self._allocate_parameter_mem(\n                n_classes=n_classes,\n                n_features=n_features,\n                input_dtype=X.dtype,\n                coef_init=coef_init,\n                intercept_init=intercept_init,\n            )\n        elif n_features != self.coef_.shape[-1]:\n            raise ValueError(\n                \"Number of features %d does not match previous data %d.\"\n                % (n_features, self.coef_.shape[-1])\n            )\n\n        self.loss_function_ = self._get_loss_function(loss)\n        if not hasattr(self, \"t_\"):\n            self.t_ = 1.0\n\n        # delegate to concrete training procedure\n        if n_classes > 2:\n            self._fit_multiclass(\n                X,\n                y,\n                alpha=alpha,\n                C=C,\n                learning_rate=learning_rate,\n                sample_weight=sample_weight,\n                max_iter=max_iter,\n            )\n        elif n_classes == 2:\n            self._fit_binary(\n                X,\n                y,\n                alpha=alpha,\n                C=C,\n                learning_rate=learning_rate,\n                sample_weight=sample_weight,\n                max_iter=max_iter,\n            )\n        else:\n            raise ValueError(\n                \"The number of classes has to be greater than one; got %d class\"\n                % n_classes\n            )\n\n        return self",
                "filename": "sklearn/linear_model/_stochastic_gradient.py",
                "start_index": 18190,
                "end_index": 20593,
                "start_line": 571,
                "end_line": 2507,
                "max_line": 2574,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "@_available_if_estimator_has(\"partial_fit\")\n    @_fit_context(\n        # MultiOutput*.estimator is not validated yet\n        prefer_skip_nested_validation=False\n    )",
                "filename": "sklearn/multioutput.py",
                "start_index": 2976,
                "end_index": 3142,
                "start_line": 107,
                "end_line": 1164,
                "max_line": 1168,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "# :class:`~sklearn.ensemble.HistGradientBoostingRegressor`, we create two different\n# preprocessors, specific for each model.\n#\n# Preprocessor for the neural network model\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#\n# We will use a :class:`~sklearn.preprocessing.QuantileTransformer` to scale the\n# numerical features and encode the categorical features with a\n# :class:`~sklearn.preprocessing.OneHotEncoder`.\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, QuantileTransformer\n\nmlp_preprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", QuantileTransformer(n_quantiles=100), numerical_features),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n    ]\n)\nmlp_preprocessor\n\n# %%\n# Preprocessor for the gradient boosting model\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#\n# For the gradient boosting model, we leave the numerical features as-is and only\n# encode the categorical features using a\n# :class:`~sklearn.preprocessing.OrdinalEncoder`.\nfrom sklearn.preprocessing import OrdinalEncoder\n\nhgbdt_preprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", OrdinalEncoder(), categorical_features),\n        (\"num\", \"passthrough\", numerical_features),\n    ],\n    sparse_threshold=1,\n    verbose_feature_names_out=False,\n).set_output(transform=\"pandas\")\nhgbdt_preprocessor\n\n# %%\n# 1-way partial dependence with different models\n# ----------------------------------------------\n#\n# In this section, we will compute 1-way partial dependence with two different\n# machine-learning models: (i) a multi-layer perceptron and (ii) a\n# gradient-boosting model. With these two models, we illustrate how to compute and\n# interpret both partial dependence plot (PDP) for both numerical and categorical\n# features and individual conditional expectation (ICE).\n#\n# Multi-layer perceptron\n# ~~~~~~~~~~~~~~~~~~~~~~\n#\n# Let's fit a :class:`~sklearn.neural_network.MLPRegressor` and compute\n# single-variable partial dependence plots.\nfrom time import time\n\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import make_pipeline\n\nprint(\"Training MLPRegressor...\")\ntic = time()\nmlp_model = make_pipeline(\n    mlp_preprocessor,\n    MLPRegressor(\n        hidden_layer_sizes=(30, 15),\n        learning_rate_init=0.01,\n        early_stopping=True,\n        random_state=0,\n    ),\n)\nmlp_model.fit(X_train, y_train)\nprint(f\"done in {time() - tic:.3f}s\")\nprint(f\"Test R2 score: {mlp_model.score(X_test, y_test):.2f}\")\n\n# %%\n# We configured a pipeline using the preprocessor that we created specifically for the\n# neural network and tuned the neural network size and learning rate to get a reasonable\n# compromise between training time and predictive performance on a test set.\n#\n# Importantly, this tabular dataset has very different dynamic ranges for its\n# features. Neural networks tend to be very sensitive to features with varying",
                "filename": "examples/inspection/plot_partial_dependence.py",
                "start_index": 5963,
                "end_index": 8896,
                "start_line": 151,
                "end_line": 231,
                "max_line": 564,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "if self.n_iter_no_change is not None:\n            stratify = y if is_classifier(self) else None\n            X, X_val, y, y_val, sample_weight, sample_weight_val = train_test_split(\n                X,\n                y,\n                sample_weight,\n                random_state=self.random_state,\n                test_size=self.validation_fraction,\n                stratify=stratify,\n            )\n            if is_classifier(self):\n                if self._n_classes != np.unique(y).shape[0]:\n                    # We choose to error here. The problem is that the init\n                    # estimator would be trained on y, which has some missing\n                    # classes now, so its predictions would not have the\n                    # correct shape.\n                    raise ValueError(\n                        \"The training data after the early stopping split \"\n                        \"is missing some classes. Try using another random \"\n                        \"seed.\"\n                    )\n        else:\n            X_val = y_val = sample_weight_val = None",
                "filename": "sklearn/ensemble/_gb.py",
                "start_index": 15757,
                "end_index": 16828,
                "start_line": 434,
                "end_line": 456,
                "max_line": 1828,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "\"\"\"\n===============================================================\nPartial Dependence and Individual Conditional Expectation Plots\n===============================================================\n\nPartial dependence plots show the dependence between the target function [2]_\nand a set of features of interest, marginalizing over the values of all other\nfeatures (the complement features). Due to the limits of human perception, the\nsize of the set of features of interest must be small (usually, one or two)\nthus they are usually chosen among the most important features.\n\nSimilarly, an individual conditional expectation (ICE) plot [3]_\nshows the dependence between the target function and a feature of interest.\nHowever, unlike partial dependence plots, which show the average effect of the\nfeatures of interest, ICE plots visualize the dependence of the prediction on a\nfeature for each :term:`sample` separately, with one line per sample.\nOnly one feature of interest is supported for ICE plots.\n\nThis example shows how to obtain partial dependence and ICE plots from a\n:class:`~sklearn.neural_network.MLPRegressor` and a\n:class:`~sklearn.ensemble.HistGradientBoostingRegressor` trained on the\nbike sharing dataset. The example is inspired by [1]_.\n\n.. [1] `Molnar, Christoph. \"Interpretable machine learning.\n       A Guide for Making Black Box Models Explainable\",\n       2019. <https://christophm.github.io/interpretable-ml-book/>`_\n\n.. [2] For classification you can think of it as the regression score before\n       the link function.\n\n.. [3] :arxiv:`Goldstein, A., Kapelner, A., Bleich, J., and Pitkin, E. (2015).\n       \"Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of\n       Individual Conditional Expectation\". Journal of Computational and\n       Graphical Statistics, 24(1): 44-65 <1309.6392>`\n\"\"\"\n\n# %%\n# Bike sharing dataset preprocessing\n# ----------------------------------\n#\n# We will use the bike sharing dataset. The goal is to predict the number of bike\n# rentals using weather and season data as well as the datetime information.\nfrom sklearn.datasets import fetch_openml\n\nbikes = fetch_openml(\"Bike_Sharing_Demand\", version=2, as_frame=True, parser=\"pandas\")\n# Make an explicit copy to avoid \"SettingWithCopyWarning\" from pandas\nX, y = bikes.data.copy(), bikes.target\n\n# We use only a subset of the data to speed up the example.\nX = X.iloc[::5, :]\ny = y[::5]\n\n# %%\n# The feature `\"weather\"` has a particularity: the category `\"heavy_rain\"` is a rare\n# category.\nX[\"weather\"].value_counts()\n\n# %%\n# Because of this rare category, we collapse it into `\"rain\"`.\nX[\"weather\"].replace(to_replace=\"heavy_rain\", value=\"rain\", inplace=True)\n\n# %%\n# We now have a closer look at the `\"year\"` feature:\nX[\"year\"].value_counts()\n\n# %%\n# We see that we have data from two years. We use the first year to train the\n# model and the second year to test the model.\nmask_training = X[\"year\"] == 0.0\nX = X.drop(columns=[\"year\"])",
                "filename": "examples/inspection/plot_partial_dependence.py",
                "start_index": 0,
                "end_index": 2962,
                "start_line": 1,
                "end_line": 70,
                "max_line": 564,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "class MLPRegressor(RegressorMixin, BaseMultilayerPerceptron):",
                "filename": "sklearn/neural_network/_multilayer_perceptron.py",
                "start_index": 46129,
                "end_index": 46190,
                "start_line": 1258,
                "end_line": 1258,
                "max_line": 1646,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "if self.do_early_stopping_:\n                # populate train_score and validation_score with the\n                # predictions of the initial model (before the first tree)\n\n                if self.scoring == \"loss\":\n                    # we're going to compute scoring w.r.t the loss. As losses\n                    # take raw predictions as input (unlike the scorers), we\n                    # can optimize a bit and avoid repeating computing the\n                    # predictions of the previous trees. We'll re-use\n                    # raw_predictions (as it's needed for training anyway) for\n                    # evaluating the training loss, and create\n                    # raw_predictions_val for storing the raw predictions of\n                    # the validation data.\n\n                    if self._use_validation_data:\n                        raw_predictions_val = np.zeros(\n                            shape=(X_binned_val.shape[0], self.n_trees_per_iteration_),\n                            dtype=self._baseline_prediction.dtype,\n                            order=\"F\",\n                        )\n\n                        raw_predictions_val += self._baseline_prediction\n\n                    self._check_early_stopping_loss(\n                        raw_predictions=raw_predictions,\n                        y_train=y_train,\n                        sample_weight_train=sample_weight_train,\n                        raw_predictions_val=raw_predictions_val,\n                        y_val=y_val,\n                        sample_weight_val=sample_weight_val,\n                        n_threads=n_threads,\n                    )\n                else:\n                    self._scorer = check_scoring(self, self.scoring)\n                    # _scorer is a callable with signature (est, X, y) and\n                    # calls est.predict() or est.predict_proba() depending on\n                    # its nature.\n                    # Unfortunately, each call to _scorer() will compute\n                    # the predictions of all the trees. So we use a subset of\n                    # the training set to compute train scores.\n\n                    # Compute the subsample set\n                    (\n                        X_binned_small_train,\n                        y_small_train,\n                        sample_weight_small_train,\n                    ) = self._get_small_trainset(\n                        X_binned_train, y_train, sample_weight_train, self._random_seed\n                    )\n\n                    self._check_early_stopping_scorer(\n                        X_binned_small_train,\n                        y_small_train,\n                        sample_weight_small_train,\n                        X_binned_val,\n                        y_val,\n                        sample_weight_val,\n                    )\n            begin_at_stage = 0",
                "filename": "sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py",
                "start_index": 20867,
                "end_index": 23710,
                "start_line": 533,
                "end_line": 591,
                "max_line": 2009,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "def check_fit_check_is_fitted(name, estimator_orig):\n    # Make sure that estimator doesn't pass check_is_fitted before calling fit\n    # and that passes check_is_fitted once it's fit.\n\n    rng = np.random.RandomState(42)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    if \"warm_start\" in estimator.get_params():\n        estimator.set_params(warm_start=False)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n    if is_regressor(estimator_orig):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(low=0, high=2, size=n_samples)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if not _safe_tags(estimator).get(\"stateless\", False):\n        # stateless estimators (such as FunctionTransformer) are always \"fit\"!\n        try:\n            check_is_fitted(estimator)\n            raise AssertionError(\n                f\"{estimator.__class__.__name__} passes check_is_fitted before being\"\n                \" fit!\"\n            )\n        except NotFittedError:\n            pass\n    estimator.fit(X, y)\n    try:\n        check_is_fitted(estimator)\n    except NotFittedError as e:\n        raise NotFittedError(\n            \"Estimator fails to pass `check_is_fitted` even though it has been fit.\"\n        ) from e\n\n\ndef check_n_features_in(name, estimator_orig):\n    # Make sure that n_features_in_ attribute doesn't exist until fit is\n    # called, and that its value is correct.\n\n    rng = np.random.RandomState(0)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    if \"warm_start\" in estimator.get_params():\n        estimator.set_params(warm_start=False)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n    if is_regressor(estimator_orig):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(low=0, high=2, size=n_samples)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    assert not hasattr(estimator, \"n_features_in_\")\n    estimator.fit(X, y)\n    assert hasattr(estimator, \"n_features_in_\")\n    assert estimator.n_features_in_ == X.shape[1]\n\n\ndef check_requires_y_none(name, estimator_orig):\n    # Make sure that an estimator with requires_y=True fails gracefully when\n    # given y=None\n\n    rng = np.random.RandomState(0)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n\n    expected_err_msgs = (\n        \"requires y to be passed, but the target y is None\",\n        \"Expected array-like (array or non-string sequence), got None\",\n        \"y should be a 1d array\",\n    )\n\n    try:\n        estimator.fit(X, None)\n    except ValueError as ve:\n        if not any(msg in str(ve) for msg in expected_err_msgs):\n            raise ve",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 138164,
                "end_index": 141063,
                "start_line": 3892,
                "end_line": 3980,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/neural_network/_multilayer_perceptron.py": [
                {
                    "chunk": {
                        "code": "class MLPRegressor(RegressorMixin, BaseMultilayerPerceptron):",
                        "filename": "sklearn/neural_network/_multilayer_perceptron.py",
                        "start_index": 46129,
                        "end_index": 46190,
                        "start_line": 1258,
                        "end_line": 1258,
                        "max_line": 1646,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet defines the MLPRegressor class, which is directly related to the issue reported. The error occurs when calling partial_fit on an instance of MLPRegressor with early_stopping enabled."
                }
            ],
            "sklearn/utils/estimator_checks.py": [
                {
                    "chunk": {
                        "code": "@ignore_warnings(category=FutureWarning)\ndef check_estimators_partial_fit_n_features(name, estimator_orig):\n    # check if number of features changes between calls to partial_fit.\n    if not hasattr(estimator_orig, \"partial_fit\"):\n        return\n    estimator = clone(estimator_orig)\n    X, y = make_blobs(n_samples=50, random_state=1)\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    y = _enforce_estimator_tags_y(estimator_orig, y)\n\n    try:\n        if is_classifier(estimator):\n            classes = np.unique(y)\n            estimator.partial_fit(X, y, classes=classes)\n        else:\n            estimator.partial_fit(X, y)\n    except NotImplementedError:\n        return\n\n    with raises(\n        ValueError,\n        err_msg=(\n            f\"The estimator {name} does not raise an error when the \"\n            \"number of features changes between calls to partial_fit.\"\n        ),\n    ):\n        estimator.partial_fit(X[:, :-1], y)",
                        "filename": "sklearn/utils/estimator_checks.py",
                        "start_index": 72345,
                        "end_index": 73285,
                        "start_line": 181,
                        "end_line": 2142,
                        "max_line": 4639,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains tests for the partial_fit method of estimators, which is relevant to the issue since the bug occurs during a call to partial_fit."
                },
                {
                    "chunk": {
                        "code": "invalid_names = [\n        (names[::-1], \"Feature names must be in the same order as they were in fit.\"),\n        (\n            [f\"another_prefix_{i}\" for i in range(n_features)],\n            (\n                \"Feature names unseen at fit time:\\n- another_prefix_0\\n-\"\n                \" another_prefix_1\\n\"\n            ),\n        ),\n        (\n            names[:3],\n            f\"Feature names seen at fit time, yet now missing:\\n- {min(names[3:])}\\n\",\n        ),\n    ]\n    params = {\n        key: value\n        for key, value in estimator.get_params().items()\n        if \"early_stopping\" in key\n    }\n    early_stopping_enabled = any(value is True for value in params.values())\n\n    for invalid_name, additional_message in invalid_names:\n        X_bad = pd.DataFrame(X, columns=invalid_name, copy=False)\n\n        expected_msg = re.escape(\n            \"The feature names should match those that were passed during fit.\\n\"\n            f\"{additional_message}\"\n        )\n        for name, method in check_methods:\n            with raises(\n                ValueError, match=expected_msg, err_msg=f\"{name} did not raise\"\n            ):\n                method(X_bad)\n\n        # partial_fit checks on second call\n        # Do not call partial fit if early_stopping is on\n        if not hasattr(estimator, \"partial_fit\") or early_stopping_enabled:\n            continue\n\n        estimator = clone(estimator_orig)\n        if is_classifier(estimator):\n            classes = np.unique(y)\n            estimator.partial_fit(X, y, classes=classes)\n        else:\n            estimator.partial_fit(X, y)\n\n        with raises(ValueError, match=expected_msg):\n            estimator.partial_fit(X_bad, y)",
                        "filename": "sklearn/utils/estimator_checks.py",
                        "start_index": 146663,
                        "end_index": 148346,
                        "start_line": 4160,
                        "end_line": 4207,
                        "max_line": 4639,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes a check for early_stopping in the context of partial_fit, which is relevant to the issue as the error is related to early_stopping being enabled during partial_fit."
                }
            ]
        },
        "solution_task": "- Add a check in the `MLPRegressor`'s `partial_fit` method to ensure that `early_stopping` is not used with `partial_fit`, as it is not supported. Raise a `ValueError` if `early_stopping` is set to `True` when calling `partial_fit`.\n\n```python\nclass MLPRegressor(BaseEstimator, RegressorMixin):\n    # ...\n\n    def partial_fit(self, X, y):\n        if self.early_stopping:\n            raise ValueError(\"early_stopping cannot be used with partial_fit\")\n        # existing code for partial_fit continues here\n        ...\n```\n\n- Ensure that `self.loss_curve_` is initialized before it is used in the `_update_no_improvement_count` method to avoid `NoneType` subtraction errors.\n\n```python\nclass MLPRegressor(BaseEstimator, RegressorMixin):\n    # ...\n\n    def _update_no_improvement_count(self, early_stopping, X_val, y_val):\n        if self.loss_curve_ is None:\n            self.loss_curve_ = []\n        # existing code for _update_no_improvement_count continues here\n        if self.loss_curve_ and self.loss_curve_[-1] > self.best_loss_ - self.tol:\n            # ...\n```\n\n- Alternatively, if `self.loss_curve_` is supposed to be populated elsewhere before `_update_no_improvement_count` is called, ensure that the code that is responsible for populating `self.loss_curve_` is called before `_update_no_improvement_count`.\n\n```python\nclass MLPRegressor(BaseEstimator, RegressorMixin):\n    # ...\n\n    def _fit_stochastic(self, ...):\n        # Ensure self.loss_curve_ is initialized or populated before calling _update_no_improvement_count\n        if self.loss_curve_ is None:\n            self.loss_curve_ = []\n        # existing code for _fit_stochastic continues here\n        ...\n```",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE provides a clear and actionable solution to the problem described in the PROMPT. It suggests adding a check in the `MLPRegressor`'s `partial_fit` method to raise a `ValueError` if `early_stopping` is set to `True`, which is a direct response to the issue of `partial_fit` not supporting `early_stopping`. Additionally, it offers a solution to initialize `self.loss_curve_` to avoid `NoneType` subtraction errors, which addresses the TypeError encountered. The RESPONSE also includes an alternative solution to ensure that `self.loss_curve_` is populated before `_update_no_improvement_count` is called, which is relevant to the traceback provided. Furthermore, the RESPONSE provides links to the relevant code snippets in the sklearn repository, which is helpful for developers to locate and address the issue. The RESPONSE is comprehensive and provides multiple avenues to resolve the bug, making it highly valuable.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711738147.8538089,
        "result": "- Add a check in the `MLPRegressor`'s `partial_fit` method to ensure that `early_stopping` is not used with `partial_fit`, as it is not supported. Raise a `ValueError` if `early_stopping` is set to `True` when calling `partial_fit`.\n\n```python\nclass MLPRegressor(BaseEstimator, RegressorMixin):\n    # ...\n\n    def partial_fit(self, X, y):\n        if self.early_stopping:\n            raise ValueError(\"early_stopping cannot be used with partial_fit\")\n        # existing code for partial_fit continues here\n        ...\n```\n\n- Ensure that `self.loss_curve_` is initialized before it is used in the `_update_no_improvement_count` method to avoid `NoneType` subtraction errors.\n\n```python\nclass MLPRegressor(BaseEstimator, RegressorMixin):\n    # ...\n\n    def _update_no_improvement_count(self, early_stopping, X_val, y_val):\n        if self.loss_curve_ is None:\n            self.loss_curve_ = []\n        # existing code for _update_no_improvement_count continues here\n        if self.loss_curve_ and self.loss_curve_[-1] > self.best_loss_ - self.tol:\n            # ...\n```\n\n- Alternatively, if `self.loss_curve_` is supposed to be populated elsewhere before `_update_no_improvement_count` is called, ensure that the code that is responsible for populating `self.loss_curve_` is called before `_update_no_improvement_count`.\n\n```python\nclass MLPRegressor(BaseEstimator, RegressorMixin):\n    # ...\n\n    def _fit_stochastic(self, ...):\n        # Ensure self.loss_curve_ is initialized or populated before calling _update_no_improvement_count\n        if self.loss_curve_ is None:\n            self.loss_curve_ = []\n        # existing code for _fit_stochastic continues here\n        ...\n```\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn) \u2192 [neural_network](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn/neural_network)- [Lines 1258 - 1258](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/neural_network/_multilayer_perceptron.py#L1258-L1258)\n\n This snippet defines the MLPRegressor class, which is directly related to the issue reported. The error occurs when calling partial_fit on an instance of MLPRegressor with early_stopping enabled.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn/utils)- [Lines 181 - 2142](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/utils/estimator_checks.py#L181-L2142)\n\n This snippet contains tests for the partial_fit method of estimators, which is relevant to the issue since the bug occurs during a call to partial_fit. \n\n\n   - [Lines 4160 - 4207](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/utils/estimator_checks.py#L4160-L4207)\n\n This snippet includes a check for early_stopping in the context of partial_fit, which is relevant to the issue as the error is related to early_stopping being enabled during partial_fit."
    }
}