{
    "id": "sphinx-doc__sphinx-10757",
    "repo_name": "testbed/sphinx-doc__sphinx__5.2",
    "task_id": 4643513422784359383,
    "title": "linkcheck should check URLs of raw directives",
    "body": "**Is your feature request related to a problem? Please describe.**\r\nWhen using a [`raw` directive](https://docutils.sourceforge.io/docs/ref/rst/directives.html#raw-data-pass-through) with the `url` option, the `linkcheck` builder does not check this URL.\r\n\r\n**Describe the solution you'd like**\r\nI would expect the URLs of `raw` directives to be checked by `linkcheck`.\r\n\r\n\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/sphinx-doc__sphinx__5.2.git",
    "requested_commit": "",
    "mayil_response": "To implement the feature where `linkcheck` should check URLs of raw directives, you need to modify the `HyperlinkCollector` class to collect hyperlinks from `raw` directives and then ensure that these are checked by the `HyperlinkAvailabilityChecker`. Here's a step-by-step guide:\n\n- Modify the `HyperlinkCollector` class to collect URLs from `raw` directives with the `url` option.\n- Ensure that the `HyperlinkAvailabilityChecker` class processes these URLs.\n\nHere's the relevant code snippet to modify:\n\n```python\nclass HyperlinkCollector(SphinxPostTransform):\n    # ... existing code ...\n\n    def run(self, **kwargs: Any) -> None:\n        # ... existing code ...\n\n        # raw nodes\n        for rawnode in self.document.findall(nodes.raw):\n            if 'url' in rawnode.attributes:\n                uri = rawnode['url']\n                if uri and '://' in uri:\n                    add_uri(uri, rawnode)\n```\n\nIn the `HyperlinkCollector` class, we're adding a loop that goes through all `raw` nodes in the document. If a `raw` node has a `url` attribute, we check if it contains a URL (indicated by '://'). If it does, we add it to the list of URIs to be checked.\n\nThe `HyperlinkAvailabilityChecker` class already processes the collected URIs, so no further changes are needed there.\n\nMake sure to test the changes to ensure that URLs in `raw` directives are being checked correctly by the `linkcheck` builder.\n\n### Code snippets to check\n\n-  \u2192 [sphinx](https://github.com/testbed/sphinx-doc__sphinx__5.2/tree//sphinx) \u2192 [builders](https://github.com/testbed/sphinx-doc__sphinx__5.2/tree//sphinx/builders)- [Lines 493 - 559](https://github.com/testbed/sphinx-doc__sphinx__5.2/blob//sphinx/builders/linkcheck.py#L493-L559)\n\n This snippet contains the HyperlinkCollector class which is responsible for collecting hyperlinks from the document. It specifically includes a section for handling 'raw' nodes, which is directly related to the issue of checking URLs within 'raw' directives.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.000605,
        "snippet_processor": 0.07325000000000001,
        "issue_star_creation": 0.03653,
        "issue_star_solver": 0.06783,
        "bouncer": 0.03701
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711701193.3716888,
        "relevant_snippets": [
            {
                "code": "def allowed_redirect(url: str, new_url: str) -> bool:\n            return any(\n                from_url.match(url) and to_url.match(new_url)\n                for from_url, to_url\n                in self.config.linkcheck_allowed_redirects.items()\n            )\n\n        def check(docname: str) -> tuple[str, str, int]:\n            # check for various conditions without bothering the network\n\n            for doc_matcher in self.documents_exclude:\n                if doc_matcher.match(docname):\n                    info = (\n                        f'{docname} matched {doc_matcher.pattern} from '\n                        'linkcheck_exclude_documents'\n                    )\n                    return 'ignored', info, 0\n\n            if len(uri) == 0 or uri.startswith(('#', 'mailto:', 'tel:')):\n                return 'unchecked', '', 0\n            elif not uri.startswith(('http:', 'https:')):\n                if uri_re.match(uri):\n                    # non supported URI schemes (ex. ftp)\n                    return 'unchecked', '', 0\n                else:\n                    srcdir = path.dirname(self.env.doc2path(docname))\n                    if path.exists(path.join(srcdir, uri)):\n                        return 'working', '', 0\n                    else:\n                        return 'broken', '', 0\n\n            # need to actually check the URI\n            for _ in range(self.config.linkcheck_retries):\n                status, info, code = check_uri()\n                if status != \"broken\":\n                    break\n\n            return (status, info, code)\n\n        while True:\n            check_request = self.wqueue.get()\n            next_check, hyperlink = check_request\n            if hyperlink is None:\n                break\n\n            uri, docname, lineno = hyperlink\n\n            if uri is None:\n                break\n            netloc = urlparse(uri).netloc\n            try:\n                # Refresh rate limit.\n                # When there are many links in the queue, workers are all stuck waiting\n                # for responses, but the builder keeps queuing. Links in the queue may\n                # have been queued before rate limits were discovered.\n                next_check = self.rate_limits[netloc].next_check\n            except KeyError:\n                pass\n            if next_check > time.time():\n                # Sleep before putting message back in the queue to avoid\n                # waking up other threads.\n                time.sleep(QUEUE_POLL_SECS)\n                self.wqueue.put(CheckRequest(next_check, hyperlink), False)\n                self.wqueue.task_done()\n                continue\n            status, info, code = check(docname)\n            if status == 'rate-limited':\n                logger.info(darkgray('-rate limited-   ') + uri + darkgray(' | sleeping...'))\n            else:\n                self.rqueue.put(CheckResult(uri, docname, lineno, status, info, code))\n            self.wqueue.task_done()",
                "filename": "sphinx/builders/linkcheck.py",
                "start_index": 14844,
                "end_index": 17804,
                "start_line": 382,
                "end_line": 452,
                "max_line": 592,
                "git_instance": "github",
                "repo_name": "testbed/sphinx-doc__sphinx__5.2",
                "sha": ""
            },
            {
                "code": "class CheckExternalLinksBuilder(DummyBuilder):",
                "filename": "sphinx/builders/linkcheck.py",
                "start_index": 3040,
                "end_index": 3086,
                "start_line": 108,
                "end_line": 108,
                "max_line": 592,
                "git_instance": "github",
                "repo_name": "testbed/sphinx-doc__sphinx__5.2",
                "sha": ""
            },
            {
                "code": "\"\"\"\n    Checks for broken external links.\n    \"\"\"\n    name = 'linkcheck'\n    epilog = __('Look for any errors in the above output or in '\n                '%(outdir)s/output.txt')\n\n    def init(self) -> None:\n        self.broken_hyperlinks = 0\n        self.hyperlinks: dict[str, Hyperlink] = {}\n        # set a timeout for non-responding servers\n        socket.setdefaulttimeout(5.0)",
                "filename": "sphinx/builders/linkcheck.py",
                "start_index": 3091,
                "end_index": 3473,
                "start_line": 1,
                "end_line": 120,
                "max_line": 592,
                "git_instance": "github",
                "repo_name": "testbed/sphinx-doc__sphinx__5.2",
                "sha": ""
            },
            {
                "code": "\"\"\"The CheckExternalLinksBuilder class.\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport re\nimport socket\nimport time\nfrom copy import deepcopy\nfrom datetime import datetime, timezone\nfrom email.utils import parsedate_to_datetime\nfrom html.parser import HTMLParser\nfrom os import path\nfrom queue import PriorityQueue, Queue\nfrom threading import Thread\nfrom typing import Any, Generator, NamedTuple, Tuple, Union, cast\nfrom urllib.parse import unquote, urlparse, urlunparse\n\nfrom docutils import nodes\nfrom requests import Response\nfrom requests.exceptions import ConnectionError, HTTPError, TooManyRedirects\n\nfrom sphinx.application import Sphinx\nfrom sphinx.builders.dummy import DummyBuilder\nfrom sphinx.config import Config\nfrom sphinx.environment import BuildEnvironment\nfrom sphinx.locale import __\nfrom sphinx.transforms.post_transforms import SphinxPostTransform\nfrom sphinx.util import encode_uri, logging, requests\nfrom sphinx.util.console import darkgray, darkgreen, purple, red, turquoise  # type: ignore\nfrom sphinx.util.nodes import get_node_line\n\nlogger = logging.getLogger(__name__)\n\nuri_re = re.compile('([a-z]+:)?//')  # matches to foo:// and // (a protocol relative URL)\n\n\nclass Hyperlink(NamedTuple):\n    uri: str\n    docname: str\n    lineno: int | None\n\n\nclass CheckRequest(NamedTuple):\n    next_check: float\n    hyperlink: Hyperlink | None\n\n\nclass CheckResult(NamedTuple):\n    uri: str\n    docname: str\n    lineno: int\n    status: str\n    message: str\n    code: int\n\n\nclass RateLimit(NamedTuple):\n    delay: float\n    next_check: float\n\n\n# Tuple is old styled CheckRequest\nCheckRequestType = Union[CheckRequest, Tuple[float, str, str, int]]\n\nDEFAULT_REQUEST_HEADERS = {\n    'Accept': 'text/html,application/xhtml+xml;q=0.9,*/*;q=0.8',\n}\nCHECK_IMMEDIATELY = 0\nQUEUE_POLL_SECS = 1\nDEFAULT_DELAY = 60.0\n\n\nclass AnchorCheckParser(HTMLParser):\n    \"\"\"Specialized HTML parser that looks for a specific anchor.\"\"\"\n\n    def __init__(self, search_anchor: str) -> None:\n        super().__init__()\n\n        self.search_anchor = search_anchor\n        self.found = False\n\n    def handle_starttag(self, tag: Any, attrs: Any) -> None:\n        for key, value in attrs:\n            if key in ('id', 'name') and value == self.search_anchor:\n                self.found = True\n                break",
                "filename": "sphinx/builders/linkcheck.py",
                "start_index": 0,
                "end_index": 2310,
                "start_line": 1,
                "end_line": 430,
                "max_line": 592,
                "git_instance": "github",
                "repo_name": "testbed/sphinx-doc__sphinx__5.2",
                "sha": ""
            },
            {
                "code": "try:\n                if anchor and self.config.linkcheck_anchors:\n                    # Read the whole document and see if #anchor exists\n                    with requests.get(req_url, stream=True, config=self.config, auth=auth_info,\n                                      **kwargs) as response:\n                        response.raise_for_status()\n                        found = check_anchor(response, unquote(anchor))\n\n                    if not found:\n                        raise Exception(__(\"Anchor '%s' not found\") % anchor)\n                else:\n                    try:\n                        # try a HEAD request first, which should be easier on\n                        # the server and the network\n                        with requests.head(req_url, allow_redirects=True, config=self.config,\n                                           auth=auth_info, **kwargs) as response:\n                            response.raise_for_status()\n                    # Servers drop the connection on HEAD requests, causing\n                    # ConnectionError.\n                    except (ConnectionError, HTTPError, TooManyRedirects) as err:\n                        if isinstance(err, HTTPError) and err.response.status_code == 429:\n                            raise\n                        # retry with GET request if that fails, some servers\n                        # don't like HEAD requests.\n                        with requests.get(req_url, stream=True, config=self.config,\n                                          auth=auth_info, **kwargs) as response:\n                            response.raise_for_status()\n            except HTTPError as err:\n                if err.response.status_code == 401:\n                    # We'll take \"Unauthorized\" as working.\n                    return 'working', ' - unauthorized', 0\n                elif err.response.status_code == 429:\n                    next_check = self.limit_rate(err.response)\n                    if next_check is not None:\n                        self.wqueue.put(CheckRequest(next_check, hyperlink), False)\n                        return 'rate-limited', '', 0\n                    return 'broken', str(err), 0\n                elif err.response.status_code == 503:\n                    # We'll take \"Service Unavailable\" as ignored.\n                    return 'ignored', str(err), 0\n                else:\n                    return 'broken', str(err), 0\n            except Exception as err:\n                return 'broken', str(err), 0\n            else:\n                netloc = urlparse(req_url).netloc\n                try:\n                    del self.rate_limits[netloc]\n                except KeyError:\n                    pass",
                "filename": "sphinx/builders/linkcheck.py",
                "start_index": 11520,
                "end_index": 14211,
                "start_line": 156,
                "end_line": 468,
                "max_line": 592,
                "git_instance": "github",
                "repo_name": "testbed/sphinx-doc__sphinx__5.2",
                "sha": ""
            },
            {
                "code": "def setup(app: Sphinx) -> dict[str, Any]:\n    app.add_builder(CheckExternalLinksBuilder)\n    app.add_post_transform(HyperlinkCollector)\n\n    app.add_config_value('linkcheck_ignore', [], False)\n    app.add_config_value('linkcheck_exclude_documents', [], False)\n    app.add_config_value('linkcheck_allowed_redirects', {}, False)\n    app.add_config_value('linkcheck_auth', [], False)\n    app.add_config_value('linkcheck_request_headers', {}, False)\n    app.add_config_value('linkcheck_retries', 1, False)\n    app.add_config_value('linkcheck_timeout', None, False, [int, float])\n    app.add_config_value('linkcheck_workers', 5, False)\n    app.add_config_value('linkcheck_anchors', True, False)\n    # Anchors starting with ! are ignored since they are\n    # commonly used for dynamic pages\n    app.add_config_value('linkcheck_anchors_ignore', [\"^!\"], False)\n    app.add_config_value('linkcheck_rate_limit_timeout', 300.0, False)\n\n    app.add_event('linkcheck-process-uri')\n\n    app.connect('config-inited', compile_linkcheck_allowed_redirects, priority=800)\n\n    # FIXME: Disable URL rewrite handler for github.com temporarily.\n    # ref: https://github.com/sphinx-doc/sphinx/issues/9435\n    # app.connect('linkcheck-process-uri', rewrite_github_anchor)\n\n    return {\n        'version': 'builtin',\n        'parallel_read_safe': True,\n        'parallel_write_safe': True,\n    }",
                "filename": "sphinx/builders/linkcheck.py",
                "start_index": 21954,
                "end_index": 23325,
                "start_line": 562,
                "end_line": 592,
                "max_line": 592,
                "git_instance": "github",
                "repo_name": "testbed/sphinx-doc__sphinx__5.2",
                "sha": ""
            },
            {
                "code": "def process_result(self, result: CheckResult) -> None:\n        filename = self.env.doc2path(result.docname, False)\n\n        linkstat = {\"filename\": filename, \"lineno\": result.lineno,\n                    \"status\": result.status, \"code\": result.code, \"uri\": result.uri,\n                    \"info\": result.message}\n        self.write_linkstat(linkstat)\n\n        if result.status == 'unchecked':\n            return\n        if result.status == 'working' and result.message == 'old':\n            return\n        if result.lineno:\n            logger.info('(%16s: line %4d) ', result.docname, result.lineno, nonl=True)\n        if result.status == 'ignored':\n            if result.message:\n                logger.info(darkgray('-ignored- ') + result.uri + ': ' + result.message)\n            else:\n                logger.info(darkgray('-ignored- ') + result.uri)\n        elif result.status == 'local':\n            logger.info(darkgray('-local-   ') + result.uri)\n            self.write_entry('local', result.docname, filename, result.lineno, result.uri)\n        elif result.status == 'working':\n            logger.info(darkgreen('ok        ') + result.uri + result.message)\n        elif result.status == 'broken':\n            if self.app.quiet or self.app.warningiserror:\n                logger.warning(__('broken link: %s (%s)'), result.uri, result.message,\n                               location=(result.docname, result.lineno))\n            else:\n                logger.info(red('broken    ') + result.uri + red(' - ' + result.message))\n            self.write_entry('broken', result.docname, filename, result.lineno,\n                             result.uri + ': ' + result.message)\n            self.broken_hyperlinks += 1\n        elif result.status == 'redirected':\n            try:\n                text, color = {\n                    301: ('permanently', purple),\n                    302: ('with Found', purple),\n                    303: ('with See Other', purple),\n                    307: ('temporarily', turquoise),\n                    308: ('permanently', purple),\n                }[result.code]\n            except KeyError:\n                text, color = ('with unknown code', purple)\n            linkstat['text'] = text\n            if self.config.linkcheck_allowed_redirects:\n                logger.warning('redirect  ' + result.uri + ' - ' + text + ' to ' +\n                               result.message, location=(result.docname, result.lineno))\n            else:\n                logger.info(color('redirect  ') + result.uri +\n                            color(' - ' + text + ' to ' + result.message))\n            self.write_entry('redirected ' + text, result.docname, filename,\n                             result.lineno, result.uri + ' to ' + result.message)\n        else:\n            raise ValueError(\"Unknown status %s.\" % result.status)",
                "filename": "sphinx/builders/linkcheck.py",
                "start_index": 3479,
                "end_index": 6320,
                "start_line": 122,
                "end_line": 176,
                "max_line": 592,
                "git_instance": "github",
                "repo_name": "testbed/sphinx-doc__sphinx__5.2",
                "sha": ""
            },
            {
                "code": "class HyperlinkCollector(SphinxPostTransform):\n    builders = ('linkcheck',)\n    default_priority = 800\n\n    def run(self, **kwargs: Any) -> None:\n        builder = cast(CheckExternalLinksBuilder, self.app.builder)\n        hyperlinks = builder.hyperlinks\n\n        def add_uri(uri: str, node: nodes.Element) -> None:\n            newuri = self.app.emit_firstresult('linkcheck-process-uri', uri)\n            if newuri:\n                uri = newuri\n\n            try:\n                lineno = get_node_line(node)\n            except ValueError:\n                lineno = None\n            uri_info = Hyperlink(uri, self.env.docname, lineno)\n            if uri not in hyperlinks:\n                hyperlinks[uri] = uri_info\n\n        # reference nodes\n        for refnode in self.document.findall(nodes.reference):\n            if 'refuri' not in refnode:\n                continue\n            uri = refnode['refuri']\n            add_uri(uri, refnode)\n\n        # image nodes\n        for imgnode in self.document.findall(nodes.image):\n            uri = imgnode['candidates'].get('?')\n            if uri and '://' in uri:\n                add_uri(uri, imgnode)\n\n        # raw nodes\n        for rawnode in self.document.findall(nodes.raw):\n            uri = rawnode.get('source')\n            if uri and '://' in uri:\n                add_uri(uri, rawnode)\n\n\ndef rewrite_github_anchor(app: Sphinx, uri: str) -> str | None:\n    \"\"\"Rewrite anchor name of the hyperlink to github.com\n\n    The hyperlink anchors in github.com are dynamically generated.  This rewrites\n    them before checking and makes them comparable.\n    \"\"\"\n    parsed = urlparse(uri)\n    if parsed.hostname == \"github.com\" and parsed.fragment:\n        prefixed = parsed.fragment.startswith('user-content-')\n        if not prefixed:\n            fragment = f'user-content-{parsed.fragment}'\n            return urlunparse(parsed._replace(fragment=fragment))\n    return None\n\n\ndef compile_linkcheck_allowed_redirects(app: Sphinx, config: Config) -> None:\n    \"\"\"Compile patterns in linkcheck_allowed_redirects to the regexp objects.\"\"\"\n    for url, pattern in list(app.config.linkcheck_allowed_redirects.items()):\n        try:\n            app.config.linkcheck_allowed_redirects[re.compile(url)] = re.compile(pattern)\n        except re.error as exc:\n            logger.warning(__('Failed to compile regex in linkcheck_allowed_redirects: %r %s'),\n                           exc.pattern, exc.msg)\n        finally:\n            # Remove the original regexp-string\n            app.config.linkcheck_allowed_redirects.pop(url)",
                "filename": "sphinx/builders/linkcheck.py",
                "start_index": 19389,
                "end_index": 21951,
                "start_line": 493,
                "end_line": 559,
                "max_line": 592,
                "git_instance": "github",
                "repo_name": "testbed/sphinx-doc__sphinx__5.2",
                "sha": ""
            },
            {
                "code": "\"\"\"Extension to save typing and prevent hard-coding of base URLs in reST files.\n\nThis adds a new config value called ``extlinks`` that is created like this::\n\n   extlinks = {'exmpl': ('https://example.invalid/%s.html', caption), ...}\n\nNow you can use e.g. :exmpl:`foo` in your documents.  This will create a\nlink to ``https://example.invalid/foo.html``.  The link caption depends on\nthe *caption* value given:\n\n- If it is ``None``, the caption will be the full URL.\n- If it is a string, it must contain ``%s`` exactly once.  In this case the\n  caption will be *caption* with the role content substituted for ``%s``.\n\nYou can also give an explicit caption, e.g. :exmpl:`Foo <foo>`.\n\nBoth, the url string and the caption string must escape ``%`` as ``%%``.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport re\nfrom typing import Any\n\nfrom docutils import nodes, utils\nfrom docutils.nodes import Node, system_message\nfrom docutils.parsers.rst.states import Inliner\n\nimport sphinx\nfrom sphinx.application import Sphinx\nfrom sphinx.locale import __\nfrom sphinx.transforms.post_transforms import SphinxPostTransform\nfrom sphinx.util import logging, rst\nfrom sphinx.util.nodes import split_explicit_title\nfrom sphinx.util.typing import RoleFunction\n\nlogger = logging.getLogger(__name__)\n\n\nclass ExternalLinksChecker(SphinxPostTransform):\n    \"\"\"\n    For each external link, check if it can be replaced by an extlink.\n\n    We treat each ``reference`` node without ``internal`` attribute as an external link.\n    \"\"\"\n\n    default_priority = 500\n\n    def run(self, **kwargs: Any) -> None:\n        if not self.config.extlinks_detect_hardcoded_links:\n            return\n\n        for refnode in self.document.findall(nodes.reference):\n            self.check_uri(refnode)\n\n    def check_uri(self, refnode: nodes.reference) -> None:\n        \"\"\"\n        If the URI in ``refnode`` has a replacement in ``extlinks``,\n        emit a warning with a replacement suggestion.\n        \"\"\"\n        if 'internal' in refnode or 'refuri' not in refnode:\n            return\n\n        uri = refnode['refuri']\n        title = refnode.astext()\n\n        for alias, (base_uri, _caption) in self.app.config.extlinks.items():\n            uri_pattern = re.compile(re.escape(base_uri).replace('%s', '(?P<value>.+)'))\n\n            match = uri_pattern.match(uri)\n            if (\n                match and\n                match.groupdict().get('value') and\n                '/' not in match.groupdict()['value']\n            ):\n                # build a replacement suggestion\n                msg = __('hardcoded link %r could be replaced by an extlink '\n                         '(try using %r instead)')\n                value = match.groupdict().get('value')\n                if uri != title:\n                    replacement = f\":{alias}:`{rst.escape(title)} <{value}>`\"\n                else:\n                    replacement = f\":{alias}:`{value}`\"\n                logger.warning(msg, uri, replacement, location=refnode)",
                "filename": "sphinx/ext/extlinks.py",
                "start_index": 0,
                "end_index": 2976,
                "start_line": 1,
                "end_line": 84,
                "max_line": 120,
                "git_instance": "github",
                "repo_name": "testbed/sphinx-doc__sphinx__5.2",
                "sha": ""
            },
            {
                "code": "class HyperlinkAvailabilityChecker:\n    def __init__(self, env: BuildEnvironment, config: Config) -> None:\n        self.config = config\n        self.env = env\n        self.rate_limits: dict[str, RateLimit] = {}\n        self.rqueue: Queue[CheckResult] = Queue()\n        self.workers: list[Thread] = []\n        self.wqueue: PriorityQueue[CheckRequest] = PriorityQueue()\n\n        self.to_ignore = [re.compile(x) for x in self.config.linkcheck_ignore]\n\n    def invoke_threads(self) -> None:\n        for _i in range(self.config.linkcheck_workers):\n            thread = HyperlinkAvailabilityCheckWorker(self.env, self.config,\n                                                      self.rqueue, self.wqueue,\n                                                      self.rate_limits)\n            thread.start()\n            self.workers.append(thread)\n\n    def shutdown_threads(self) -> None:\n        self.wqueue.join()\n        for _worker in self.workers:\n            self.wqueue.put(CheckRequest(CHECK_IMMEDIATELY, None), False)\n\n    def check(self, hyperlinks: dict[str, Hyperlink]) -> Generator[CheckResult, None, None]:\n        self.invoke_threads()\n\n        total_links = 0\n        for hyperlink in hyperlinks.values():\n            if self.is_ignored_uri(hyperlink.uri):\n                yield CheckResult(hyperlink.uri, hyperlink.docname, hyperlink.lineno,\n                                  'ignored', '', 0)\n            else:\n                self.wqueue.put(CheckRequest(CHECK_IMMEDIATELY, hyperlink), False)\n                total_links += 1\n\n        done = 0\n        while done < total_links:\n            yield self.rqueue.get()\n            done += 1\n\n        self.shutdown_threads()\n\n    def is_ignored_uri(self, uri: str) -> bool:\n        return any(pat.match(uri) for pat in self.to_ignore)",
                "filename": "sphinx/builders/linkcheck.py",
                "start_index": 7221,
                "end_index": 9009,
                "start_line": 201,
                "end_line": 245,
                "max_line": 592,
                "git_instance": "github",
                "repo_name": "testbed/sphinx-doc__sphinx__5.2",
                "sha": ""
            }
        ],
        "classification": "feature request",
        "snippet_processor_task": {
            "sphinx/builders/linkcheck.py": [
                {
                    "chunk": {
                        "code": "class HyperlinkCollector(SphinxPostTransform):\n    builders = ('linkcheck',)\n    default_priority = 800\n\n    def run(self, **kwargs: Any) -> None:\n        builder = cast(CheckExternalLinksBuilder, self.app.builder)\n        hyperlinks = builder.hyperlinks\n\n        def add_uri(uri: str, node: nodes.Element) -> None:\n            newuri = self.app.emit_firstresult('linkcheck-process-uri', uri)\n            if newuri:\n                uri = newuri\n\n            try:\n                lineno = get_node_line(node)\n            except ValueError:\n                lineno = None\n            uri_info = Hyperlink(uri, self.env.docname, lineno)\n            if uri not in hyperlinks:\n                hyperlinks[uri] = uri_info\n\n        # reference nodes\n        for refnode in self.document.findall(nodes.reference):\n            if 'refuri' not in refnode:\n                continue\n            uri = refnode['refuri']\n            add_uri(uri, refnode)\n\n        # image nodes\n        for imgnode in self.document.findall(nodes.image):\n            uri = imgnode['candidates'].get('?')\n            if uri and '://' in uri:\n                add_uri(uri, imgnode)\n\n        # raw nodes\n        for rawnode in self.document.findall(nodes.raw):\n            uri = rawnode.get('source')\n            if uri and '://' in uri:\n                add_uri(uri, rawnode)\n\n\ndef rewrite_github_anchor(app: Sphinx, uri: str) -> str | None:\n    \"\"\"Rewrite anchor name of the hyperlink to github.com\n\n    The hyperlink anchors in github.com are dynamically generated.  This rewrites\n    them before checking and makes them comparable.\n    \"\"\"\n    parsed = urlparse(uri)\n    if parsed.hostname == \"github.com\" and parsed.fragment:\n        prefixed = parsed.fragment.startswith('user-content-')\n        if not prefixed:\n            fragment = f'user-content-{parsed.fragment}'\n            return urlunparse(parsed._replace(fragment=fragment))\n    return None\n\n\ndef compile_linkcheck_allowed_redirects(app: Sphinx, config: Config) -> None:\n    \"\"\"Compile patterns in linkcheck_allowed_redirects to the regexp objects.\"\"\"\n    for url, pattern in list(app.config.linkcheck_allowed_redirects.items()):\n        try:\n            app.config.linkcheck_allowed_redirects[re.compile(url)] = re.compile(pattern)\n        except re.error as exc:\n            logger.warning(__('Failed to compile regex in linkcheck_allowed_redirects: %r %s'),\n                           exc.pattern, exc.msg)\n        finally:\n            # Remove the original regexp-string\n            app.config.linkcheck_allowed_redirects.pop(url)",
                        "filename": "sphinx/builders/linkcheck.py",
                        "start_index": 19389,
                        "end_index": 21951,
                        "start_line": 493,
                        "end_line": 559,
                        "max_line": 592,
                        "git_instance": "github",
                        "repo_name": "testbed/sphinx-doc__sphinx__5.2",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the HyperlinkCollector class which is responsible for collecting hyperlinks from the document. It specifically includes a section for handling 'raw' nodes, which is directly related to the issue of checking URLs within 'raw' directives."
                }
            ]
        },
        "solution_task": "To implement the feature where `linkcheck` should check URLs of raw directives, you need to modify the `HyperlinkCollector` class to collect hyperlinks from `raw` directives and then ensure that these are checked by the `HyperlinkAvailabilityChecker`. Here's a step-by-step guide:\n\n- Modify the `HyperlinkCollector` class to collect URLs from `raw` directives with the `url` option.\n- Ensure that the `HyperlinkAvailabilityChecker` class processes these URLs.\n\nHere's the relevant code snippet to modify:\n\n```python\nclass HyperlinkCollector(SphinxPostTransform):\n    # ... existing code ...\n\n    def run(self, **kwargs: Any) -> None:\n        # ... existing code ...\n\n        # raw nodes\n        for rawnode in self.document.findall(nodes.raw):\n            if 'url' in rawnode.attributes:\n                uri = rawnode['url']\n                if uri and '://' in uri:\n                    add_uri(uri, rawnode)\n```\n\nIn the `HyperlinkCollector` class, we're adding a loop that goes through all `raw` nodes in the document. If a `raw` node has a `url` attribute, we check if it contains a URL (indicated by '://'). If it does, we add it to the list of URIs to be checked.\n\nThe `HyperlinkAvailabilityChecker` class already processes the collected URIs, so no further changes are needed there.\n\nMake sure to test the changes to ensure that URLs in `raw` directives are being checked correctly by the `linkcheck` builder.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant suggestions that can directly lead to the successful implementation or improvement of the requested feature.",
            "supporting_evidence": "The RESPONSE provides a clear and actionable step-by-step guide to implement the feature request of checking URLs within `raw` directives using the `linkcheck` builder. It specifically addresses the modification of the `HyperlinkCollector` class to include URLs from `raw` directives and confirms that no additional changes are needed for the `HyperlinkAvailabilityChecker` class. The RESPONSE also includes a relevant code snippet that demonstrates how to identify and add URLs from `raw` nodes to the list of URIs to be checked. Furthermore, it provides a link to the relevant section of the codebase where these changes should be made, which is highly useful for developers looking to implement this feature. The RESPONSE only lacks a minor detail about testing methodologies or examples, which would have made it a perfect 10.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711701253.147478,
        "result": "To implement the feature where `linkcheck` should check URLs of raw directives, you need to modify the `HyperlinkCollector` class to collect hyperlinks from `raw` directives and then ensure that these are checked by the `HyperlinkAvailabilityChecker`. Here's a step-by-step guide:\n\n- Modify the `HyperlinkCollector` class to collect URLs from `raw` directives with the `url` option.\n- Ensure that the `HyperlinkAvailabilityChecker` class processes these URLs.\n\nHere's the relevant code snippet to modify:\n\n```python\nclass HyperlinkCollector(SphinxPostTransform):\n    # ... existing code ...\n\n    def run(self, **kwargs: Any) -> None:\n        # ... existing code ...\n\n        # raw nodes\n        for rawnode in self.document.findall(nodes.raw):\n            if 'url' in rawnode.attributes:\n                uri = rawnode['url']\n                if uri and '://' in uri:\n                    add_uri(uri, rawnode)\n```\n\nIn the `HyperlinkCollector` class, we're adding a loop that goes through all `raw` nodes in the document. If a `raw` node has a `url` attribute, we check if it contains a URL (indicated by '://'). If it does, we add it to the list of URIs to be checked.\n\nThe `HyperlinkAvailabilityChecker` class already processes the collected URIs, so no further changes are needed there.\n\nMake sure to test the changes to ensure that URLs in `raw` directives are being checked correctly by the `linkcheck` builder.\n\n### Code snippets to check\n\n-  \u2192 [sphinx](https://github.com/testbed/sphinx-doc__sphinx__5.2/tree//sphinx) \u2192 [builders](https://github.com/testbed/sphinx-doc__sphinx__5.2/tree//sphinx/builders)- [Lines 493 - 559](https://github.com/testbed/sphinx-doc__sphinx__5.2/blob//sphinx/builders/linkcheck.py#L493-L559)\n\n This snippet contains the HyperlinkCollector class which is responsible for collecting hyperlinks from the document. It specifically includes a section for handling 'raw' nodes, which is directly related to the issue of checking URLs within 'raw' directives."
    }
}