{
    "id": "scikit-learn__scikit-learn-10803",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
    "task_id": 6829025700082238054,
    "title": "weighted KDE",
    "body": "Not sure this is the correct place, but I would very much appreciate the ability to \npass a weight for each sample in kde density estimation. \n\nThere exits a adapted version of scipy.stats.gaussian_kde : \nhttp://stackoverflow.com/questions/27623919/weighted-gaussian-kernel-density-estimation-in-python\n\nweighted KDE\nNot sure this is the correct place, but I would very much appreciate the ability to \npass a weight for each sample in kde density estimation. \n\nThere exits a adapted version of scipy.stats.gaussian_kde : \nhttp://stackoverflow.com/questions/27623919/weighted-gaussian-kernel-density-estimation-in-python\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.20.git",
    "requested_commit": "",
    "mayil_response": "To implement weighted KDE in the `KernelDensity` class within `sklearn.neighbors._kde.py`, you can follow these steps:\n\n- Modify the `KernelDensity` class to accept sample weights.\n- Update the `fit` method to handle the weights.\n- Adjust the kernel density estimation calculation to incorporate the weights.\n\nHere's a high-level overview of the changes needed:\n\n1. Modify the `KernelDensity` class to accept `sample_weight` as an optional parameter:\n\n```python\nclass KernelDensity(BaseEstimator):\n    def __init__(self, bandwidth=1.0, algorithm='auto', kernel='gaussian',\n                 metric='euclidean', atol=0, rtol=0, breadth_first=True,\n                 leaf_size=40, metric_params=None, sample_weight=None):\n        self.sample_weight = sample_weight\n        # ... rest of the __init__ code ...\n```\n\n2. Update the `fit` method to handle the `sample_weight` parameter:\n\n```python\nclass KernelDensity(BaseEstimator):\n    # ... rest of the class code ...\n\n    def fit(self, X, y=None, sample_weight=None):\n        # Store the weights if provided\n        if sample_weight is not None:\n            self.sample_weight = _check_sample_weight(sample_weight, X)\n        # ... rest of the fit code ...\n```\n\n3. Adjust the kernel density estimation calculation to incorporate the weights. This will likely involve modifying the `_kde.py` file where the actual KDE computation is performed. Since the code for this part is not provided, I can't give a precise implementation, but the general idea is to use the weights to scale the contribution of each point to the density estimate.\n\nHere's a pseudo-code snippet to illustrate how you might adjust the density estimation:\n\n```python\nclass KernelDensity(BaseEstimator):\n    # ... rest of the class code ...\n\n    def _compute_density(self, X):\n        # This is a pseudo-code function to illustrate the concept\n        # You will need to find where the density is actually computed and modify it\n        weighted_density = np.zeros_like(X)\n        for i, x in enumerate(X):\n            weight = self.sample_weight[i] if self.sample_weight is not None else 1\n            # Compute the density contribution of point x, scaled by its weight\n            density_contribution = weight * self._kernel_function(x)\n            weighted_density += density_contribution\n        return weighted_density / np.sum(self.sample_weight)\n```\n\n4. Ensure that all methods that rely on the density estimation are updated to handle the weighted density correctly.\n\n5. Add tests to verify that the weighted KDE works as expected.\n\nFor further research, you can look into the following:\n\n- The `scipy.stats.gaussian_kde` function, which already supports weighted KDE, to understand how it handles weights.\n- The `KernelDensity` implementation in scikit-learn to identify where the density is computed and how to incorporate weights into that computation.\n- The `_check_sample_weight` utility function from `sklearn.utils.validation` to properly validate and use the sample weights.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [neighbors](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/neighbors)- [Lines 1 - 34](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/neighbors/_kde.py#L1-L34)\n\n This snippet is from the file where the Kernel Density Estimation (KDE) is implemented in scikit-learn. Since the issue is about adding the ability to pass a weight for each sample in KDE, this snippet is directly relevant to the issue. \n\n\n   - [Lines 35 - 35](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/neighbors/_kde.py#L35-L35)\n\n This snippet defines the KernelDensity class which is likely where the changes need to be made to support weighted samples in KDE. It is necessary to look at this class definition to understand how to integrate the new feature.\n-  \u2192 [examples](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//examples) \u2192 [neighbors](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//examples/neighbors)- [Lines 1 - 64](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//examples/neighbors/plot_digits_kde_sampling.py#L1-L64)\n\n This snippet shows an example of using KernelDensity in practice. It may be useful to see how weights could be integrated into actual usage scenarios and to update the example accordingly once the feature is implemented.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.000641,
        "snippet_processor": 0.08109000000000001,
        "issue_star_creation": 0.027979999999999998,
        "issue_star_solver": 0.06982000000000001,
        "bouncer": 0.03111
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711742308.313788,
        "relevant_snippets": [
            {
                "code": "\"\"\"\nKernel Density Estimation\n-------------------------\n\"\"\"\n# Author: Jake Vanderplas <jakevdp@cs.washington.edu>\nimport itertools\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy.special import gammainc\n\nfrom ..base import BaseEstimator, _fit_context\nfrom ..neighbors._base import VALID_METRICS\nfrom ..utils import check_random_state\nfrom ..utils._param_validation import Interval, StrOptions\nfrom ..utils.extmath import row_norms\nfrom ..utils.validation import _check_sample_weight, check_is_fitted\nfrom ._ball_tree import BallTree\nfrom ._kd_tree import KDTree\n\nVALID_KERNELS = [\n    \"gaussian\",\n    \"tophat\",\n    \"epanechnikov\",\n    \"exponential\",\n    \"linear\",\n    \"cosine\",\n]\n\nTREE_DICT = {\"ball_tree\": BallTree, \"kd_tree\": KDTree}\n\n\n# TODO: implement a brute force version for testing purposes\n# TODO: create a density estimation base class?",
                "filename": "sklearn/neighbors/_kde.py",
                "start_index": 0,
                "end_index": 864,
                "start_line": 1,
                "end_line": 34,
                "max_line": 365,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"\n=====================\nSVM: Weighted samples\n=====================\n\nPlot decision function of a weighted dataset, where the size of points\nis proportional to its weight.\n\nThe sample weighting rescales the C parameter, which means that the classifier\nputs more emphasis on getting these points right. The effect might often be\nsubtle.\nTo emphasize the effect here, we particularly weight outliers, making the\ndeformation of the decision boundary very visible.\n\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import svm\n\n\ndef plot_decision_function(classifier, sample_weight, axis, title):\n    # plot the decision function\n    xx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))\n\n    Z = classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    axis.contourf(xx, yy, Z, alpha=0.75, cmap=plt.cm.bone)\n    axis.scatter(\n        X[:, 0],\n        X[:, 1],\n        c=y,\n        s=100 * sample_weight,\n        alpha=0.9,\n        cmap=plt.cm.bone,\n        edgecolors=\"black\",\n    )\n\n    axis.axis(\"off\")\n    axis.set_title(title)\n\n\n# we create 20 points\nnp.random.seed(0)\nX = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]\ny = [1] * 10 + [-1] * 10\nsample_weight_last_ten = abs(np.random.randn(len(X)))\nsample_weight_constant = np.ones(len(X))\n# and bigger weights to some outliers\nsample_weight_last_ten[15:] *= 5\nsample_weight_last_ten[9] *= 15\n\n# Fit the models.\n\n# This model does not take into account sample weights.\nclf_no_weights = svm.SVC(gamma=1)\nclf_no_weights.fit(X, y)\n\n# This other model takes into account some dedicated sample weights.\nclf_weights = svm.SVC(gamma=1)\nclf_weights.fit(X, y, sample_weight=sample_weight_last_ten)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nplot_decision_function(\n    clf_no_weights, sample_weight_constant, axes[0], \"Constant weights\"\n)\nplot_decision_function(clf_weights, sample_weight_last_ten, axes[1], \"Modified weights\")\n\nplt.show()",
                "filename": "examples/svm/plot_weighted_samples.py",
                "start_index": 0,
                "end_index": 2047,
                "start_line": 1,
                "end_line": 72,
                "max_line": 72,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"\n=====================\nSGD: Weighted samples\n=====================\n\nPlot decision function of a weighted dataset, where the size of points\nis proportional to its weight.\n\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import linear_model\n\n# we create 20 points\nnp.random.seed(0)\nX = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]\ny = [1] * 10 + [-1] * 10\nsample_weight = 100 * np.abs(np.random.randn(20))\n# and assign a bigger weight to the last 10 samples\nsample_weight[:10] *= 10\n\n# plot the weighted data points\nxx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))\nfig, ax = plt.subplots()\nax.scatter(\n    X[:, 0],\n    X[:, 1],\n    c=y,\n    s=sample_weight,\n    alpha=0.9,\n    cmap=plt.cm.bone,\n    edgecolor=\"black\",\n)\n\n# fit the unweighted model\nclf = linear_model.SGDClassifier(alpha=0.01, max_iter=100)\nclf.fit(X, y)\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nno_weights = ax.contour(xx, yy, Z, levels=[0], linestyles=[\"solid\"])\n\n# fit the weighted model\nclf = linear_model.SGDClassifier(alpha=0.01, max_iter=100)\nclf.fit(X, y, sample_weight=sample_weight)\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nsamples_weights = ax.contour(xx, yy, Z, levels=[0], linestyles=[\"dashed\"])\n\nno_weights_handles, _ = no_weights.legend_elements()\nweights_handles, _ = samples_weights.legend_elements()\nax.legend(\n    [no_weights_handles[0], weights_handles[0]],\n    [\"no weights\", \"with weights\"],\n    loc=\"lower left\",\n)\n\nax.set(xticks=(), yticks=())\nplt.show()",
                "filename": "examples/linear_model/plot_sgd_weighted_samples.py",
                "start_index": 0,
                "end_index": 1587,
                "start_line": 1,
                "end_line": 60,
                "max_line": 60,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "class KernelDensity(BaseEstimator):",
                "filename": "sklearn/neighbors/_kde.py",
                "start_index": 865,
                "end_index": 900,
                "start_line": 35,
                "end_line": 35,
                "max_line": 365,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_sample_weights_pandas_series(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type pandas.Series in the 'fit' function.\n    estimator = clone(estimator_orig)\n    try:\n        import pandas as pd\n\n        X = np.array(\n            [\n                [1, 1],\n                [1, 2],\n                [1, 3],\n                [1, 4],\n                [2, 1],\n                [2, 2],\n                [2, 3],\n                [2, 4],\n                [3, 1],\n                [3, 2],\n                [3, 3],\n                [3, 4],\n            ]\n        )\n        X = pd.DataFrame(_enforce_estimator_tags_X(estimator_orig, X), copy=False)\n        y = pd.Series([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2])\n        weights = pd.Series([1] * 12)\n        if _safe_tags(estimator, key=\"multioutput_only\"):\n            y = pd.DataFrame(y, copy=False)\n        try:\n            estimator.fit(X, y, sample_weight=weights)\n        except ValueError:\n            raise ValueError(\n                \"Estimator {0} raises error if \"\n                \"'sample_weight' parameter is of \"\n                \"type pandas.Series\".format(name)\n            )\n    except ImportError:\n        raise SkipTest(\n            \"pandas is not installed: not testing for \"\n            \"input of type pandas.Series to class weight.\"\n        )\n\n\n@ignore_warnings(category=(FutureWarning))\ndef check_sample_weights_not_an_array(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type _NotAnArray in the 'fit' function.\n    estimator = clone(estimator_orig)\n    X = np.array(\n        [\n            [1, 1],\n            [1, 2],\n            [1, 3],\n            [1, 4],\n            [2, 1],\n            [2, 2],\n            [2, 3],\n            [2, 4],\n            [3, 1],\n            [3, 2],\n            [3, 3],\n            [3, 4],\n        ]\n    )\n    X = _NotAnArray(_enforce_estimator_tags_X(estimator_orig, X))\n    y = _NotAnArray([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2])\n    weights = _NotAnArray([1] * 12)\n    if _safe_tags(estimator, key=\"multioutput_only\"):\n        y = _NotAnArray(y.data.reshape(-1, 1))\n    estimator.fit(X, y, sample_weight=weights)\n\n\n@ignore_warnings(category=(FutureWarning))\ndef check_sample_weights_list(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type list in the 'fit' function.\n    estimator = clone(estimator_orig)\n    rnd = np.random.RandomState(0)\n    n_samples = 30\n    X = _enforce_estimator_tags_X(estimator_orig, rnd.uniform(size=(n_samples, 3)))\n    y = np.arange(n_samples) % 3\n    y = _enforce_estimator_tags_y(estimator, y)\n    sample_weight = [3] * n_samples\n    # Test that estimators don't raise any exception\n    estimator.fit(X, y, sample_weight=sample_weight)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 38580,
                "end_index": 41430,
                "start_line": 181,
                "end_line": 1181,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "gbdt_no_cst = HistGradientBoostingRegressor().fit(X, y)\ngbdt_cst = HistGradientBoostingRegressor(monotonic_cst=[1, 0]).fit(X, y)\n\n# plot_partial_dependence has been removed in version 1.2. From 1.2, use\n# PartialDependenceDisplay instead.\n# disp = plot_partial_dependence(\ndisp = PartialDependenceDisplay.from_estimator(\n    gbdt_no_cst,\n    X,\n    features=[0],\n    feature_names=[\"feature 0\"],\n    line_kw={\"linewidth\": 4, \"label\": \"unconstrained\", \"color\": \"tab:blue\"},\n)\n# plot_partial_dependence(\nPartialDependenceDisplay.from_estimator(\n    gbdt_cst,\n    X,\n    features=[0],\n    line_kw={\"linewidth\": 4, \"label\": \"constrained\", \"color\": \"tab:orange\"},\n    ax=disp.axes_,\n)\ndisp.axes_[0, 0].plot(\n    X[:, 0], y, \"o\", alpha=0.5, zorder=-1, label=\"samples\", color=\"tab:green\"\n)\ndisp.axes_[0, 0].set_ylim(-3, 3)\ndisp.axes_[0, 0].set_xlim(-1, 1)\nplt.legend()\nplt.show()\n\n##############################################################################\n# Sample-weight support for Lasso and ElasticNet\n# ----------------------------------------------\n# The two linear regressors :class:`~sklearn.linear_model.Lasso` and\n# :class:`~sklearn.linear_model.ElasticNet` now support sample weights.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import Lasso\nimport numpy as np\n\nn_samples, n_features = 1000, 20\nrng = np.random.RandomState(0)\nX, y = make_regression(n_samples, n_features, random_state=rng)\nsample_weight = rng.rand(n_samples)\nX_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(\n    X, y, sample_weight, random_state=rng\n)\nreg = Lasso()\nreg.fit(X_train, y_train, sample_weight=sw_train)\nprint(reg.score(X_test, y_test, sw_test))",
                "filename": "examples/release_highlights/plot_release_highlights_0_23_0.py",
                "start_index": 5973,
                "end_index": 7708,
                "start_line": 140,
                "end_line": 189,
                "max_line": 189,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "\"\"\"\n=========================\nKernel Density Estimation\n=========================\n\nThis example shows how kernel density estimation (KDE), a powerful\nnon-parametric density estimation technique, can be used to learn\na generative model for a dataset.  With this generative model in place,\nnew samples can be drawn.  These new samples reflect the underlying model\nof the data.\n\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KernelDensity\n\n# load the data\ndigits = load_digits()\n\n# project the 64-dimensional data to a lower dimension\npca = PCA(n_components=15, whiten=False)\ndata = pca.fit_transform(digits.data)\n\n# use grid search cross-validation to optimize the bandwidth\nparams = {\"bandwidth\": np.logspace(-1, 1, 20)}\ngrid = GridSearchCV(KernelDensity(), params)\ngrid.fit(data)\n\nprint(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\n\n# use the best estimator to compute the kernel density estimate\nkde = grid.best_estimator_\n\n# sample 44 new points from the data\nnew_data = kde.sample(44, random_state=0)\nnew_data = pca.inverse_transform(new_data)\n\n# turn data into a 4x11 grid\nnew_data = new_data.reshape((4, 11, -1))\nreal_data = digits.data[:44].reshape((4, 11, -1))\n\n# plot real digits and resampled digits\nfig, ax = plt.subplots(9, 11, subplot_kw=dict(xticks=[], yticks=[]))\nfor j in range(11):\n    ax[4, j].set_visible(False)\n    for i in range(4):\n        im = ax[i, j].imshow(\n            real_data[i, j].reshape((8, 8)), cmap=plt.cm.binary, interpolation=\"nearest\"\n        )\n        im.set_clim(0, 16)\n        im = ax[i + 5, j].imshow(\n            new_data[i, j].reshape((8, 8)), cmap=plt.cm.binary, interpolation=\"nearest\"\n        )\n        im.set_clim(0, 16)\n\nax[0, 5].set_title(\"Selection from the input data\")\nax[5, 5].set_title('\"New\" digits drawn from the kernel density model')\n\nplt.show()",
                "filename": "examples/neighbors/plot_digits_kde_sampling.py",
                "start_index": 0,
                "end_index": 1989,
                "start_line": 1,
                "end_line": 64,
                "max_line": 64,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "for k in range(n_outputs):\n        y_full = y[:, k]\n        if sparse.issparse(y_full):\n            # Ok to densify a single column at a time\n            y_full = y_full.toarray().flatten()\n        classes_full = np.unique(y_full)\n        classes_missing = None\n\n        if class_weight == \"balanced\" or n_outputs == 1:\n            class_weight_k = class_weight\n        else:\n            class_weight_k = class_weight[k]\n\n        if indices is not None:\n            # Get class weights for the subsample, covering all classes in\n            # case some labels that were present in the original data are\n            # missing from the sample.\n            y_subsample = y_full[indices]\n            classes_subsample = np.unique(y_subsample)\n\n            weight_k = np.take(\n                compute_class_weight(\n                    class_weight_k, classes=classes_subsample, y=y_subsample\n                ),\n                np.searchsorted(classes_subsample, classes_full),\n                mode=\"clip\",\n            )\n\n            classes_missing = set(classes_full) - set(classes_subsample)\n        else:\n            weight_k = compute_class_weight(\n                class_weight_k, classes=classes_full, y=y_full\n            )\n\n        weight_k = weight_k[np.searchsorted(classes_full, y_full)]\n\n        if classes_missing:\n            # Make missing classes' weight zero\n            weight_k[np.in1d(y_full, list(classes_missing))] = 0.0\n\n        expanded_class_weight.append(weight_k)\n\n    expanded_class_weight = np.prod(expanded_class_weight, axis=0, dtype=np.float64)\n\n    return expanded_class_weight",
                "filename": "sklearn/utils/class_weight.py",
                "start_index": 5888,
                "end_index": 7492,
                "start_line": 157,
                "end_line": 201,
                "max_line": 201,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "for axi in ax[:, 0]:\n    axi.set_ylabel(\"Normalized Density\")\n\nfor axi in ax[1, :]:\n    axi.set_xlabel(\"x\")\n\n# ----------------------------------------------------------------------\n# Plot all available kernels\nX_plot = np.linspace(-6, 6, 1000)[:, None]\nX_src = np.zeros((1, 1))\n\nfig, ax = plt.subplots(2, 3, sharex=True, sharey=True)\nfig.subplots_adjust(left=0.05, right=0.95, hspace=0.05, wspace=0.05)\n\n\ndef format_func(x, loc):\n    if x == 0:\n        return \"0\"\n    elif x == 1:\n        return \"h\"\n    elif x == -1:\n        return \"-h\"\n    else:\n        return \"%ih\" % x\n\n\nfor i, kernel in enumerate(\n    [\"gaussian\", \"tophat\", \"epanechnikov\", \"exponential\", \"linear\", \"cosine\"]\n):\n    axi = ax.ravel()[i]\n    log_dens = KernelDensity(kernel=kernel).fit(X_src).score_samples(X_plot)\n    axi.fill(X_plot[:, 0], np.exp(log_dens), \"-k\", fc=\"#AAAAFF\")\n    axi.text(-2.6, 0.95, kernel)\n\n    axi.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n    axi.xaxis.set_major_locator(plt.MultipleLocator(1))\n    axi.yaxis.set_major_locator(plt.NullLocator())\n\n    axi.set_ylim(0, 1.05)\n    axi.set_xlim(-2.9, 2.9)\n\nax[0, 1].set_title(\"Available Kernels\")\n\n# ----------------------------------------------------------------------\n# Plot a 1D density example\nN = 100\nnp.random.seed(1)\nX = np.concatenate(\n    (np.random.normal(0, 1, int(0.3 * N)), np.random.normal(5, 1, int(0.7 * N)))\n)[:, np.newaxis]\n\nX_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\n\ntrue_dens = 0.3 * norm(0, 1).pdf(X_plot[:, 0]) + 0.7 * norm(5, 1).pdf(X_plot[:, 0])\n\nfig, ax = plt.subplots()\nax.fill(X_plot[:, 0], true_dens, fc=\"black\", alpha=0.2, label=\"input distribution\")\ncolors = [\"navy\", \"cornflowerblue\", \"darkorange\"]\nkernels = [\"gaussian\", \"tophat\", \"epanechnikov\"]\nlw = 2\n\nfor color, kernel in zip(colors, kernels):\n    kde = KernelDensity(kernel=kernel, bandwidth=0.5).fit(X)\n    log_dens = kde.score_samples(X_plot)\n    ax.plot(\n        X_plot[:, 0],\n        np.exp(log_dens),\n        color=color,\n        lw=lw,\n        linestyle=\"-\",\n        label=\"kernel = '{0}'\".format(kernel),\n    )\n\nax.text(6, 0.38, \"N={0} points\".format(N))\n\nax.legend(loc=\"upper left\")\nax.plot(X[:, 0], -0.005 - 0.01 * np.random.random(X.shape[0]), \"+k\")\n\nax.set_xlim(-4, 9)\nax.set_ylim(-0.02, 0.4)\nplt.show()",
                "filename": "examples/neighbors/plot_kde_1d.py",
                "start_index": 2956,
                "end_index": 5223,
                "start_line": 77,
                "end_line": 157,
                "max_line": 157,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            },
            {
                "code": "name: Feature request\ndescription: Suggest a new algorithm, enhancement to an existing algorithm, etc.\nlabels: ['New Feature', 'Needs Triage']\n\nbody:\n- type: markdown\n  attributes:\n    value: >\n      #### If you want to propose a new algorithm, please refer first to the [scikit-learn inclusion criterion](https://scikit-learn.org/stable/faq.html#what-are-the-inclusion-criteria-for-new-algorithms).\n- type: textarea\n  attributes:\n    label: Describe the workflow you want to enable\n  validations:\n    required: true\n- type: textarea\n  attributes:\n    label: Describe your proposed solution\n  validations:\n    required: true\n- type: textarea\n  attributes:\n    label: Describe alternatives you've considered, if relevant\n- type: textarea\n  attributes:\n    label: Additional context",
                "filename": ".github/ISSUE_TEMPLATE/feature_request.yml",
                "start_index": 0,
                "end_index": 780,
                "start_line": 1,
                "end_line": 25,
                "max_line": 25,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": ""
            }
        ],
        "classification": "feature request",
        "snippet_processor_task": {
            "sklearn/neighbors/_kde.py": [
                {
                    "chunk": {
                        "code": "\"\"\"\nKernel Density Estimation\n-------------------------\n\"\"\"\n# Author: Jake Vanderplas <jakevdp@cs.washington.edu>\nimport itertools\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy.special import gammainc\n\nfrom ..base import BaseEstimator, _fit_context\nfrom ..neighbors._base import VALID_METRICS\nfrom ..utils import check_random_state\nfrom ..utils._param_validation import Interval, StrOptions\nfrom ..utils.extmath import row_norms\nfrom ..utils.validation import _check_sample_weight, check_is_fitted\nfrom ._ball_tree import BallTree\nfrom ._kd_tree import KDTree\n\nVALID_KERNELS = [\n    \"gaussian\",\n    \"tophat\",\n    \"epanechnikov\",\n    \"exponential\",\n    \"linear\",\n    \"cosine\",\n]\n\nTREE_DICT = {\"ball_tree\": BallTree, \"kd_tree\": KDTree}\n\n\n# TODO: implement a brute force version for testing purposes\n# TODO: create a density estimation base class?",
                        "filename": "sklearn/neighbors/_kde.py",
                        "start_index": 0,
                        "end_index": 864,
                        "start_line": 1,
                        "end_line": 34,
                        "max_line": 365,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet is from the file where the Kernel Density Estimation (KDE) is implemented in scikit-learn. Since the issue is about adding the ability to pass a weight for each sample in KDE, this snippet is directly relevant to the issue."
                },
                {
                    "chunk": {
                        "code": "class KernelDensity(BaseEstimator):",
                        "filename": "sklearn/neighbors/_kde.py",
                        "start_index": 865,
                        "end_index": 900,
                        "start_line": 35,
                        "end_line": 35,
                        "max_line": 365,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet defines the KernelDensity class which is likely where the changes need to be made to support weighted samples in KDE. It is necessary to look at this class definition to understand how to integrate the new feature."
                }
            ],
            "examples/neighbors/plot_digits_kde_sampling.py": [
                {
                    "chunk": {
                        "code": "\"\"\"\n=========================\nKernel Density Estimation\n=========================\n\nThis example shows how kernel density estimation (KDE), a powerful\nnon-parametric density estimation technique, can be used to learn\na generative model for a dataset.  With this generative model in place,\nnew samples can be drawn.  These new samples reflect the underlying model\nof the data.\n\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KernelDensity\n\n# load the data\ndigits = load_digits()\n\n# project the 64-dimensional data to a lower dimension\npca = PCA(n_components=15, whiten=False)\ndata = pca.fit_transform(digits.data)\n\n# use grid search cross-validation to optimize the bandwidth\nparams = {\"bandwidth\": np.logspace(-1, 1, 20)}\ngrid = GridSearchCV(KernelDensity(), params)\ngrid.fit(data)\n\nprint(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\n\n# use the best estimator to compute the kernel density estimate\nkde = grid.best_estimator_\n\n# sample 44 new points from the data\nnew_data = kde.sample(44, random_state=0)\nnew_data = pca.inverse_transform(new_data)\n\n# turn data into a 4x11 grid\nnew_data = new_data.reshape((4, 11, -1))\nreal_data = digits.data[:44].reshape((4, 11, -1))\n\n# plot real digits and resampled digits\nfig, ax = plt.subplots(9, 11, subplot_kw=dict(xticks=[], yticks=[]))\nfor j in range(11):\n    ax[4, j].set_visible(False)\n    for i in range(4):\n        im = ax[i, j].imshow(\n            real_data[i, j].reshape((8, 8)), cmap=plt.cm.binary, interpolation=\"nearest\"\n        )\n        im.set_clim(0, 16)\n        im = ax[i + 5, j].imshow(\n            new_data[i, j].reshape((8, 8)), cmap=plt.cm.binary, interpolation=\"nearest\"\n        )\n        im.set_clim(0, 16)\n\nax[0, 5].set_title(\"Selection from the input data\")\nax[5, 5].set_title('\"New\" digits drawn from the kernel density model')\n\nplt.show()",
                        "filename": "examples/neighbors/plot_digits_kde_sampling.py",
                        "start_index": 0,
                        "end_index": 1989,
                        "start_line": 1,
                        "end_line": 64,
                        "max_line": 64,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows an example of using KernelDensity in practice. It may be useful to see how weights could be integrated into actual usage scenarios and to update the example accordingly once the feature is implemented."
                }
            ]
        },
        "solution_task": "To implement weighted KDE in the `KernelDensity` class within `sklearn.neighbors._kde.py`, you can follow these steps:\n\n- Modify the `KernelDensity` class to accept sample weights.\n- Update the `fit` method to handle the weights.\n- Adjust the kernel density estimation calculation to incorporate the weights.\n\nHere's a high-level overview of the changes needed:\n\n1. Modify the `KernelDensity` class to accept `sample_weight` as an optional parameter:\n\n```python\nclass KernelDensity(BaseEstimator):\n    def __init__(self, bandwidth=1.0, algorithm='auto', kernel='gaussian',\n                 metric='euclidean', atol=0, rtol=0, breadth_first=True,\n                 leaf_size=40, metric_params=None, sample_weight=None):\n        self.sample_weight = sample_weight\n        # ... rest of the __init__ code ...\n```\n\n2. Update the `fit` method to handle the `sample_weight` parameter:\n\n```python\nclass KernelDensity(BaseEstimator):\n    # ... rest of the class code ...\n\n    def fit(self, X, y=None, sample_weight=None):\n        # Store the weights if provided\n        if sample_weight is not None:\n            self.sample_weight = _check_sample_weight(sample_weight, X)\n        # ... rest of the fit code ...\n```\n\n3. Adjust the kernel density estimation calculation to incorporate the weights. This will likely involve modifying the `_kde.py` file where the actual KDE computation is performed. Since the code for this part is not provided, I can't give a precise implementation, but the general idea is to use the weights to scale the contribution of each point to the density estimate.\n\nHere's a pseudo-code snippet to illustrate how you might adjust the density estimation:\n\n```python\nclass KernelDensity(BaseEstimator):\n    # ... rest of the class code ...\n\n    def _compute_density(self, X):\n        # This is a pseudo-code function to illustrate the concept\n        # You will need to find where the density is actually computed and modify it\n        weighted_density = np.zeros_like(X)\n        for i, x in enumerate(X):\n            weight = self.sample_weight[i] if self.sample_weight is not None else 1\n            # Compute the density contribution of point x, scaled by its weight\n            density_contribution = weight * self._kernel_function(x)\n            weighted_density += density_contribution\n        return weighted_density / np.sum(self.sample_weight)\n```\n\n4. Ensure that all methods that rely on the density estimation are updated to handle the weighted density correctly.\n\n5. Add tests to verify that the weighted KDE works as expected.\n\nFor further research, you can look into the following:\n\n- The `scipy.stats.gaussian_kde` function, which already supports weighted KDE, to understand how it handles weights.\n- The `KernelDensity` implementation in scikit-learn to identify where the density is computed and how to incorporate weights into that computation.\n- The `_check_sample_weight` utility function from `sklearn.utils.validation` to properly validate and use the sample weights.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant suggestions that can directly lead to the successful implementation or improvement of the requested feature.",
            "supporting_evidence": "The RESPONSE provides a detailed, step-by-step guide on how to implement weighted KDE in the `KernelDensity` class of `sklearn.neighbors._kde.py`. It includes code snippets for modifying the class to accept sample weights, updating the `fit` method, and adjusting the kernel density estimation calculation. The RESPONSE also suggests looking into the `scipy.stats.gaussian_kde` function for further research and provides links to relevant code snippets in the scikit-learn repository. The RESPONSE is comprehensive and addresses all aspects of the feature request, providing actionable guidance for developers to follow.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711742505.940662,
        "result": "To implement weighted KDE in the `KernelDensity` class within `sklearn.neighbors._kde.py`, you can follow these steps:\n\n- Modify the `KernelDensity` class to accept sample weights.\n- Update the `fit` method to handle the weights.\n- Adjust the kernel density estimation calculation to incorporate the weights.\n\nHere's a high-level overview of the changes needed:\n\n1. Modify the `KernelDensity` class to accept `sample_weight` as an optional parameter:\n\n```python\nclass KernelDensity(BaseEstimator):\n    def __init__(self, bandwidth=1.0, algorithm='auto', kernel='gaussian',\n                 metric='euclidean', atol=0, rtol=0, breadth_first=True,\n                 leaf_size=40, metric_params=None, sample_weight=None):\n        self.sample_weight = sample_weight\n        # ... rest of the __init__ code ...\n```\n\n2. Update the `fit` method to handle the `sample_weight` parameter:\n\n```python\nclass KernelDensity(BaseEstimator):\n    # ... rest of the class code ...\n\n    def fit(self, X, y=None, sample_weight=None):\n        # Store the weights if provided\n        if sample_weight is not None:\n            self.sample_weight = _check_sample_weight(sample_weight, X)\n        # ... rest of the fit code ...\n```\n\n3. Adjust the kernel density estimation calculation to incorporate the weights. This will likely involve modifying the `_kde.py` file where the actual KDE computation is performed. Since the code for this part is not provided, I can't give a precise implementation, but the general idea is to use the weights to scale the contribution of each point to the density estimate.\n\nHere's a pseudo-code snippet to illustrate how you might adjust the density estimation:\n\n```python\nclass KernelDensity(BaseEstimator):\n    # ... rest of the class code ...\n\n    def _compute_density(self, X):\n        # This is a pseudo-code function to illustrate the concept\n        # You will need to find where the density is actually computed and modify it\n        weighted_density = np.zeros_like(X)\n        for i, x in enumerate(X):\n            weight = self.sample_weight[i] if self.sample_weight is not None else 1\n            # Compute the density contribution of point x, scaled by its weight\n            density_contribution = weight * self._kernel_function(x)\n            weighted_density += density_contribution\n        return weighted_density / np.sum(self.sample_weight)\n```\n\n4. Ensure that all methods that rely on the density estimation are updated to handle the weighted density correctly.\n\n5. Add tests to verify that the weighted KDE works as expected.\n\nFor further research, you can look into the following:\n\n- The `scipy.stats.gaussian_kde` function, which already supports weighted KDE, to understand how it handles weights.\n- The `KernelDensity` implementation in scikit-learn to identify where the density is computed and how to incorporate weights into that computation.\n- The `_check_sample_weight` utility function from `sklearn.utils.validation` to properly validate and use the sample weights.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [neighbors](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/neighbors)- [Lines 1 - 34](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/neighbors/_kde.py#L1-L34)\n\n This snippet is from the file where the Kernel Density Estimation (KDE) is implemented in scikit-learn. Since the issue is about adding the ability to pass a weight for each sample in KDE, this snippet is directly relevant to the issue. \n\n\n   - [Lines 35 - 35](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/neighbors/_kde.py#L35-L35)\n\n This snippet defines the KernelDensity class which is likely where the changes need to be made to support weighted samples in KDE. It is necessary to look at this class definition to understand how to integrate the new feature.\n-  \u2192 [examples](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//examples) \u2192 [neighbors](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//examples/neighbors)- [Lines 1 - 64](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//examples/neighbors/plot_digits_kde_sampling.py#L1-L64)\n\n This snippet shows an example of using KernelDensity in practice. It may be useful to see how weights could be integrated into actual usage scenarios and to update the example accordingly once the feature is implemented."
    }
}