{
    "id": "scikit-learn__scikit-learn-26400",
    "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
    "task_id": 7793044569854817998,
    "title": "PowerTransformer fails with unhelpful stack trace with all-nan feature and method='box-cox'",
    "body": "### Describe the bug\r\n\r\n`PowerTransformer(\"box-cox\").fit(x)` throws a difficult-to-debug error if x contains an all-nan column. \r\n\r\n### Steps/Code to Reproduce\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler\r\n\r\nx = np.ones((20, 5))\r\ny = np.ones((20, 1))\r\n\r\nx[:, 0] = np.nan\r\n\r\nPowerTransformer().fit_transform(x)  # preserves all-nan column\r\nPowerTransformer('box-cox').fit_transform(x)  # Throws an error when calling stats.boxcox\r\n```\r\n\r\n### Expected Results\r\n\r\nEither no error is thrown and the all-nan column is preserved, or a descriptive error is thrown indicating that there is an unfittable column \r\n\r\n### Actual Results\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\n\r\n[<ipython-input-12-563273596add>](https://localhost:8080/#) in <cell line: 1>()\r\n----> 1 PowerTransformer('box-cox').fit_transform(x)\r\n\r\n4 frames\r\n\r\n[/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py](https://localhost:8080/#) in wrapped(self, X, *args, **kwargs)\r\n    138     @wraps(f)\r\n    139     def wrapped(self, X, *args, **kwargs):\r\n--> 140         data_to_wrap = f(self, X, *args, **kwargs)\r\n    141         if isinstance(data_to_wrap, tuple):\r\n    142             # only wrap the first output for cross decomposition\r\n\r\n[/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py](https://localhost:8080/#) in fit_transform(self, X, y)\r\n   3101         \"\"\"\r\n   3102         self._validate_params()\r\n-> 3103         return self._fit(X, y, force_transform=True)\r\n   3104 \r\n   3105     def _fit(self, X, y=None, force_transform=False):\r\n\r\n[/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py](https://localhost:8080/#) in _fit(self, X, y, force_transform)\r\n   3114         }[self.method]\r\n   3115         with np.errstate(invalid=\"ignore\"):  # hide NaN warnings\r\n-> 3116             self.lambdas_ = np.array([optim_function(col) for col in X.T])\r\n   3117 \r\n   3118         if self.standardize or force_transform:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py](https://localhost:8080/#) in <listcomp>(.0)\r\n   3114         }[self.method]\r\n   3115         with np.errstate(invalid=\"ignore\"):  # hide NaN warnings\r\n-> 3116             self.lambdas_ = np.array([optim_function(col) for col in X.T])\r\n   3117 \r\n   3118         if self.standardize or force_transform:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py](https://localhost:8080/#) in _box_cox_optimize(self, x)\r\n   3272         # the computation of lambda is influenced by NaNs so we need to\r\n   3273         # get rid of them\r\n-> 3274         _, lmbda = stats.boxcox(x[~np.isnan(x)], lmbda=None)\r\n   3275 \r\n   3276         return lmbda\r\n\r\nValueError: not enough values to unpack (expected 2, got 0)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.11 (main, Apr  5 2023, 14:15:10) [GCC 9.4.0]\r\nexecutable: /usr/bin/python3\r\n   machine: Linux-5.10.147+-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.2\r\n          pip: 23.0.1\r\n   setuptools: 67.7.2\r\n        numpy: 1.22.4\r\n        scipy: 1.10.1\r\n       Cython: 0.29.34\r\n       pandas: 1.5.3\r\n   matplotlib: 3.7.1\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n```\r\n```\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__1.3.git",
    "requested_commit": "",
    "mayil_response": "To resolve the issue with the `PowerTransformer` when fitting to data with an all-nan column, you should:\n\n- Add a check in the `_fit` method of the `PowerTransformer` class to handle all-nan columns before applying the transformation.\n- If an all-nan column is detected, you can either:\n  - Skip the transformation for that column and set the corresponding lambda to a default value (e.g., 1 for Yeo-Johnson which corresponds to the identity transformation).\n  - Raise a more descriptive error message indicating that all-nan columns are not supported for transformation.\n\nHere is a possible solution:\n\n```python\ndef _fit(self, X, y=None, force_transform=False):\n    X = self._check_input(X, in_fit=True, check_positive=True)\n\n    if not self.copy and not force_transform:  # if call from fit()\n        X = X.copy()  # force copy so that fit does not change X inplace\n\n    n_samples, n_features = X.shape\n    self.lambdas_ = np.empty(n_features, dtype=X.dtype)\n\n    for i in range(n_features):\n        # Check for all-nan column\n        if np.isnan(X[:, i]).all():\n            # Option 1: Set lambda to a default value, e.g., 1 for Yeo-Johnson\n            self.lambdas_[i] = 1.0\n            # Option 2: Raise an error with a descriptive message\n            # raise ValueError(f\"All values are NaN in column {i}\")\n            continue  # Skip the rest of the loop for this column\n\n        # Existing code for fitting continues here...\n```\n\nChoose either Option 1 or Option 2 based on the desired behavior. If you choose Option 1, make sure to handle the transformation accordingly in the `transform` method as well. If you choose Option 2, ensure that the error message is clear and informative.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn) \u2192 [preprocessing](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn/preprocessing)- [Lines 2999 - 3517](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/preprocessing/_data.py#L2999-L3517)\n\n This snippet contains the PowerTransformer class implementation, which is directly related to the issue as the error occurs during the fit_transform method call. \n\n\n   - [Lines 3147 - 3411](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/preprocessing/_data.py#L3147-L3411)\n\n This snippet includes the _fit method of the PowerTransformer class where the error is raised when calling stats.boxcox on an all-nan column. \n\n\n   - [Lines 3422 - 3517](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/preprocessing/_data.py#L3422-L3517)\n\n This snippet provides documentation for the power_transform function, which is relevant for understanding the expected behavior of the PowerTransformer and how it handles NaN values.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0003495,
        "snippet_processor": 0.04503,
        "issue_star_creation": 0.0381,
        "issue_star_solver": 0.06981000000000001,
        "bouncer": 0.02748
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711737892.950664,
        "relevant_snippets": [
            {
                "code": "\"\"\"\n=================================\nMap data to a normal distribution\n=================================\n\n.. currentmodule:: sklearn.preprocessing\n\nThis example demonstrates the use of the Box-Cox and Yeo-Johnson transforms\nthrough :class:`~PowerTransformer` to map data from various\ndistributions to a normal distribution.\n\nThe power transform is useful as a transformation in modeling problems where\nhomoscedasticity and normality are desired. Below are examples of Box-Cox and\nYeo-Johnwon applied to six different probability distributions: Lognormal,\nChi-squared, Weibull, Gaussian, Uniform, and Bimodal.\n\nNote that the transformations successfully map the data to a normal\ndistribution when applied to certain datasets, but are ineffective with others.\nThis highlights the importance of visualizing the data before and after\ntransformation.\n\nAlso note that even though Box-Cox seems to perform better than Yeo-Johnson for\nlognormal and chi-squared distributions, keep in mind that Box-Cox does not\nsupport inputs with negative values.\n\nFor comparison, we also add the output from\n:class:`~QuantileTransformer`. It can force any arbitrary\ndistribution into a gaussian, provided that there are enough training samples\n(thousands). Because it is a non-parametric method, it is harder to interpret\nthan the parametric ones (Box-Cox and Yeo-Johnson).\n\nOn \"small\" datasets (less than a few hundred points), the quantile transformer\nis prone to overfitting. The use of the power transform is then recommended.\n\n\"\"\"\n\n# Author: Eric Chang <ericchang2017@u.northwestern.edu>\n#         Nicolas Hug <contact@nicolas-hug.com>\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PowerTransformer, QuantileTransformer\n\nN_SAMPLES = 1000\nFONT_SIZE = 6\nBINS = 30\n\n\nrng = np.random.RandomState(304)\nbc = PowerTransformer(method=\"box-cox\")\nyj = PowerTransformer(method=\"yeo-johnson\")\n# n_quantiles is set to the training set size rather than the default value\n# to avoid a warning being raised by this example\nqt = QuantileTransformer(\n    n_quantiles=500, output_distribution=\"normal\", random_state=rng\n)\nsize = (N_SAMPLES, 1)\n\n\n# lognormal distribution\nX_lognormal = rng.lognormal(size=size)\n\n# chi-squared distribution\ndf = 3\nX_chisq = rng.chisquare(df=df, size=size)\n\n# weibull distribution\na = 50\nX_weibull = rng.weibull(a=a, size=size)\n\n# gaussian distribution\nloc = 100\nX_gaussian = rng.normal(loc=loc, size=size)\n\n# uniform distribution\nX_uniform = rng.uniform(low=0, high=1, size=size)\n\n# bimodal distribution\nloc_a, loc_b = 100, 105\nX_a, X_b = rng.normal(loc=loc_a, size=size), rng.normal(loc=loc_b, size=size)\nX_bimodal = np.concatenate([X_a, X_b], axis=0)\n\n\n# create plots\ndistributions = [\n    (\"Lognormal\", X_lognormal),\n    (\"Chi-squared\", X_chisq),\n    (\"Weibull\", X_weibull),\n    (\"Gaussian\", X_gaussian),\n    (\"Uniform\", X_uniform),\n    (\"Bimodal\", X_bimodal),\n]",
                "filename": "examples/preprocessing/plot_map_data_to_normal.py",
                "start_index": 0,
                "end_index": 2978,
                "start_line": 1,
                "end_line": 141,
                "max_line": 145,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "# transformed values lie in a [-2, 3] range as seen in the zoomed-in figure.\n# Note that the outliers themselves are still present in the transformed data.\n# If a separate outlier clipping is desirable, a non-linear transformation is\n# required (see below).\n\nmake_plot(4)\n\n# %%\n# PowerTransformer\n# ----------------\n#\n# :class:`~sklearn.preprocessing.PowerTransformer` applies a power\n# transformation to each feature to make the data more Gaussian-like in order\n# to stabilize variance and minimize skewness. Currently the Yeo-Johnson\n# and Box-Cox transforms are supported and the optimal\n# scaling factor is determined via maximum likelihood estimation in both\n# methods. By default, :class:`~sklearn.preprocessing.PowerTransformer` applies\n# zero-mean, unit variance normalization. Note that\n# Box-Cox can only be applied to strictly positive data. Income and average\n# house occupancy happen to be strictly positive, but if negative values are\n# present the Yeo-Johnson transformed is preferred.\n\nmake_plot(5)\nmake_plot(6)\n\n# %%\n# QuantileTransformer (uniform output)\n# ------------------------------------\n#\n# :class:`~sklearn.preprocessing.QuantileTransformer` applies a non-linear\n# transformation such that the\n# probability density function of each feature will be mapped to a uniform\n# or Gaussian distribution. In this case, all the data, including outliers,\n# will be mapped to a uniform distribution with the range [0, 1], making\n# outliers indistinguishable from inliers.\n#\n# :class:`~sklearn.preprocessing.RobustScaler` and\n# :class:`~sklearn.preprocessing.QuantileTransformer` are robust to outliers in\n# the sense that adding or removing outliers in the training set will yield\n# approximately the same transformation. But contrary to\n# :class:`~sklearn.preprocessing.RobustScaler`,\n# :class:`~sklearn.preprocessing.QuantileTransformer` will also automatically\n# collapse any outlier by setting them to the a priori defined range boundaries\n# (0 and 1). This can result in saturation artifacts for extreme values.\n\nmake_plot(7)\n\n##############################################################################\n# QuantileTransformer (Gaussian output)\n# -------------------------------------\n#\n# To map to a Gaussian distribution, set the parameter\n# ``output_distribution='normal'``.\n\nmake_plot(8)\n\n# %%\n# Normalizer\n# ----------\n#\n# The :class:`~sklearn.preprocessing.Normalizer` rescales the vector for each\n# sample to have unit norm,\n# independently of the distribution of the samples. It can be seen on both\n# figures below where all samples are mapped onto the unit circle. In our\n# example the two selected features have only positive values; therefore the\n# transformed data only lie in the positive quadrant. This would not be the\n# case if some original features had a mix of positive and negative values.\n\nmake_plot(9)\n\nplt.show()",
                "filename": "examples/preprocessing/plot_all_scaling.py",
                "start_index": 11499,
                "end_index": 14355,
                "start_line": 330,
                "end_line": 400,
                "max_line": 400,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "\"\"\"Parametric, monotonic transformation to make data more Gaussian-like.\n\n    Power transforms are a family of parametric, monotonic transformations\n    that are applied to make data more Gaussian-like. This is useful for\n    modeling issues related to heteroscedasticity (non-constant variance),\n    or other situations where normality is desired.\n\n    Currently, power_transform supports the Box-Cox transform and the\n    Yeo-Johnson transform. The optimal parameter for stabilizing variance and\n    minimizing skewness is estimated through maximum likelihood.\n\n    Box-Cox requires input data to be strictly positive, while Yeo-Johnson\n    supports both positive or negative data.\n\n    By default, zero-mean, unit-variance normalization is applied to the\n    transformed data.\n\n    Read more in the :ref:`User Guide <preprocessing_transformer>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to be transformed using a power transformation.\n\n    method : {'yeo-johnson', 'box-cox'}, default='yeo-johnson'\n        The power transform method. Available methods are:\n\n        - 'yeo-johnson' [1]_, works with positive and negative values\n        - 'box-cox' [2]_, only works with strictly positive values\n\n        .. versionchanged:: 0.23\n            The default value of the `method` parameter changed from\n            'box-cox' to 'yeo-johnson' in 0.23.\n\n    standardize : bool, default=True\n        Set to True to apply zero-mean, unit-variance normalization to the\n        transformed output.\n\n    copy : bool, default=True\n        Set to False to perform inplace computation during transformation.\n\n    Returns\n    -------\n    X_trans : ndarray of shape (n_samples, n_features)\n        The transformed data.\n\n    See Also\n    --------\n    PowerTransformer : Equivalent transformation with the\n        Transformer API (e.g. as part of a preprocessing\n        :class:`~sklearn.pipeline.Pipeline`).\n\n    quantile_transform : Maps data to a standard normal distribution with\n        the parameter `output_distribution='normal'`.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in ``fit``, and maintained\n    in ``transform``.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\n    References\n    ----------\n\n    .. [1] I.K. Yeo and R.A. Johnson, \"A new family of power transformations to\n           improve normality or symmetry.\" Biometrika, 87(4), pp.954-959,\n           (2000).\n\n    .. [2] G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal\n           of the Royal Statistical Society B, 26, 211-252 (1964).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.preprocessing import power_transform\n    >>> data = [[1, 2], [3, 2], [4, 5]]\n    >>> print(power_transform(data, method='box-cox'))\n    [[-1.332... -0.707...]\n     [ 0.256... -0.707...]\n     [ 1.076...  1.414...]]\n\n    .. warning:: Risk of data leak.\n        Do not use :func:`~sklearn.preprocessing.power_transform` unless you\n        know what you are doing. A common mistake is to apply it to the entire\n        data *before* splitting into training and test sets. This will bias the\n        model evaluation because information would have leaked from the test\n        set to the training set.\n        In general, we recommend using\n        :class:`~sklearn.preprocessing.PowerTransformer` within a\n        :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n        leaking, e.g.: `pipe = make_pipeline(PowerTransformer(),\n        LogisticRegression())`.\n    \"\"\"",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 117994,
                "end_index": 121706,
                "start_line": 3422,
                "end_line": 3517,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "@ignore_warnings\ndef check_fit2d_1feature(name, estimator_orig):\n    # check fitting a 2d array with only 1 feature either works or returns\n    # informative message\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(10, 1))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    y = X[:, 0].astype(int)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n    # ensure two labels in subsample for RandomizedLogisticRegression\n    if name == \"RandomizedLogisticRegression\":\n        estimator.sample_fraction = 1\n    # ensure non skipped trials for RANSACRegressor\n    if name == \"RANSACRegressor\":\n        estimator.residual_threshold = 0.5\n\n    y = _enforce_estimator_tags_y(estimator, y)\n    set_random_state(estimator, 1)\n\n    msgs = [r\"1 feature\\(s\\)\", \"n_features = 1\", \"n_features=1\"]\n\n    with raises(ValueError, match=msgs, may_pass=True):\n        estimator.fit(X, y)\n\n\n@ignore_warnings\ndef check_fit1d(name, estimator_orig):\n    # check fitting 1d X array raises a ValueError\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20))\n    y = X.astype(int)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n    with raises(ValueError):\n        estimator.fit(X, y)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_transformer_general(name, transformer, readonly_memmap=False):\n    X, y = make_blobs(\n        n_samples=30,\n        centers=[[0, 0, 0], [1, 1, 1]],\n        random_state=0,\n        n_features=2,\n        cluster_std=0.1,\n    )\n    X = StandardScaler().fit_transform(X)\n    X = _enforce_estimator_tags_X(transformer, X)\n\n    if readonly_memmap:\n        X, y = create_memmap_backed_data([X, y])\n\n    _check_transformer(name, transformer, X, y)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_transformer_data_not_an_array(name, transformer):\n    X, y = make_blobs(\n        n_samples=30,\n        centers=[[0, 0, 0], [1, 1, 1]],\n        random_state=0,\n        n_features=2,\n        cluster_std=0.1,\n    )\n    X = StandardScaler().fit_transform(X)\n    X = _enforce_estimator_tags_X(transformer, X)\n    this_X = _NotAnArray(X)\n    this_y = _NotAnArray(np.asarray(y))\n    _check_transformer(name, transformer, this_X, this_y)\n    # try the same with some list\n    _check_transformer(name, transformer, X.tolist(), y.tolist())",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 55999,
                "end_index": 58685,
                "start_line": 181,
                "end_line": 1729,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "def check_transformer_get_feature_names_out_pandas(name, transformer_orig):\n    try:\n        import pandas as pd\n    except ImportError:\n        raise SkipTest(\n            \"pandas is not installed: not checking column name consistency for pandas\"\n        )\n\n    tags = transformer_orig._get_tags()\n    if \"2darray\" not in tags[\"X_types\"] or tags[\"no_validation\"]:\n        return\n\n    X, y = make_blobs(\n        n_samples=30,\n        centers=[[0, 0, 0], [1, 1, 1]],\n        random_state=0,\n        n_features=2,\n        cluster_std=0.1,\n    )\n    X = StandardScaler().fit_transform(X)\n\n    transformer = clone(transformer_orig)\n    X = _enforce_estimator_tags_X(transformer, X)\n\n    n_features = X.shape[1]\n    set_random_state(transformer)\n\n    y_ = y\n    if name in CROSS_DECOMPOSITION:\n        y_ = np.c_[np.asarray(y), np.asarray(y)]\n        y_[::2, 1] *= 2\n\n    feature_names_in = [f\"col{i}\" for i in range(n_features)]\n    df = pd.DataFrame(X, columns=feature_names_in, copy=False)\n    X_transform = transformer.fit_transform(df, y=y_)\n\n    # error is raised when `input_features` do not match feature_names_in\n    invalid_feature_names = [f\"bad{i}\" for i in range(n_features)]\n    with raises(ValueError, match=\"input_features is not equal to feature_names_in_\"):\n        transformer.get_feature_names_out(invalid_feature_names)\n\n    feature_names_out_default = transformer.get_feature_names_out()\n    feature_names_in_explicit_names = transformer.get_feature_names_out(\n        feature_names_in\n    )\n    assert_array_equal(feature_names_out_default, feature_names_in_explicit_names)\n\n    if isinstance(X_transform, tuple):\n        n_features_out = X_transform[0].shape[1]\n    else:\n        n_features_out = X_transform.shape[1]\n\n    assert (\n        len(feature_names_out_default) == n_features_out\n    ), f\"Expected {n_features_out} feature names, got {len(feature_names_out_default)}\"",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 149921,
                "end_index": 151816,
                "start_line": 4258,
                "end_line": 4312,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "\"\"\"Apply a power transform featurewise to make data more Gaussian-like.\n\n    Power transforms are a family of parametric, monotonic transformations\n    that are applied to make data more Gaussian-like. This is useful for\n    modeling issues related to heteroscedasticity (non-constant variance),\n    or other situations where normality is desired.\n\n    Currently, PowerTransformer supports the Box-Cox transform and the\n    Yeo-Johnson transform. The optimal parameter for stabilizing variance and\n    minimizing skewness is estimated through maximum likelihood.\n\n    Box-Cox requires input data to be strictly positive, while Yeo-Johnson\n    supports both positive or negative data.\n\n    By default, zero-mean, unit-variance normalization is applied to the\n    transformed data.\n\n    Read more in the :ref:`User Guide <preprocessing_transformer>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    method : {'yeo-johnson', 'box-cox'}, default='yeo-johnson'\n        The power transform method. Available methods are:\n\n        - 'yeo-johnson' [1]_, works with positive and negative values\n        - 'box-cox' [2]_, only works with strictly positive values\n\n    standardize : bool, default=True\n        Set to True to apply zero-mean, unit-variance normalization to the\n        transformed output.\n\n    copy : bool, default=True\n        Set to False to perform inplace computation during transformation.\n\n    Attributes\n    ----------\n    lambdas_ : ndarray of float of shape (n_features,)\n        The parameters of the power transformation for the selected features.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    power_transform : Equivalent function without the estimator API.\n\n    QuantileTransformer : Maps data to a standard normal distribution with\n        the parameter `output_distribution='normal'`.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in ``fit``, and maintained\n    in ``transform``.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\n    References\n    ----------\n\n    .. [1] I.K. Yeo and R.A. Johnson, \"A new family of power transformations to\n           improve normality or symmetry.\" Biometrika, 87(4), pp.954-959,\n           (2000).\n\n    .. [2] G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal\n           of the Royal Statistical Society B, 26, 211-252 (1964).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.preprocessing import PowerTransformer\n    >>> pt = PowerTransformer()\n    >>> data = [[1, 2], [3, 2], [4, 5]]\n    >>> print(pt.fit(data))\n    PowerTransformer()\n    >>> print(pt.lambdas_)\n    [ 1.386... -3.100...]\n    >>> print(pt.transform(data))\n    [[-1.316... -0.707...]\n     [ 0.209... -0.707...]\n     [ 1.106...  1.414...]]\n    \"\"\"",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 103716,
                "end_index": 106931,
                "start_line": 2999,
                "end_line": 3517,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "def _assert_all_finite_element_wise(\n    X, *, xp, allow_nan, msg_dtype=None, estimator_name=None, input_name=\"\"\n):\n    # Cython implementation doesn't support FP16 or complex numbers\n    use_cython = (\n        xp is np and X.data.contiguous and X.dtype.type in {np.float32, np.float64}\n    )\n    if use_cython:\n        out = cy_isfinite(X.reshape(-1), allow_nan=allow_nan)\n        has_nan_error = False if allow_nan else out == FiniteStatus.has_nan\n        has_inf = out == FiniteStatus.has_infinite\n    else:\n        has_inf = xp.any(xp.isinf(X))\n        has_nan_error = False if allow_nan else xp.any(xp.isnan(X))\n    if has_inf or has_nan_error:\n        if has_nan_error:\n            type_err = \"NaN\"\n        else:\n            msg_dtype = msg_dtype if msg_dtype is not None else X.dtype\n            type_err = f\"infinity or a value too large for {msg_dtype!r}\"\n        padded_input_name = input_name + \" \" if input_name else \"\"\n        msg_err = f\"Input {padded_input_name}contains {type_err}.\"\n        if estimator_name and input_name == \"X\" and has_nan_error:\n            # Improve the error message on how to handle missing values in\n            # scikit-learn.\n            msg_err += (\n                f\"\\n{estimator_name} does not accept missing values\"\n                \" encoded as NaN natively. For supervised learning, you might want\"\n                \" to consider sklearn.ensemble.HistGradientBoostingClassifier and\"\n                \" Regressor which accept missing values encoded as NaNs natively.\"\n                \" Alternatively, it is possible to preprocess the data, for\"\n                \" instance by using an imputer transformer in a pipeline or drop\"\n                \" samples with missing values. See\"\n                \" https://scikit-learn.org/stable/modules/impute.html\"\n                \" You can find a list of all estimators that handle NaN values\"\n                \" at the following page:\"\n                \" https://scikit-learn.org/stable/modules/impute.html\"\n                \"#estimators-that-handle-nan-values\"\n            )\n        raise ValueError(msg_err)\n\n\ndef assert_all_finite(\n    X,\n    *,\n    allow_nan=False,\n    estimator_name=None,\n    input_name=\"\",\n):\n    \"\"\"Throw a ValueError if X contains NaN or infinity.\n\n    Parameters\n    ----------\n    X : {ndarray, sparse matrix}\n        The input data.\n\n    allow_nan : bool, default=False\n        If True, do not throw error when `X` contains NaN.\n\n    estimator_name : str, default=None\n        The estimator name, used to construct the error message.\n\n    input_name : str, default=\"\"\n        The data name used to construct the error message. In particular\n        if `input_name` is \"X\" and the data has NaN values and\n        allow_nan is False, the error message will link to the imputer\n        documentation.\n    \"\"\"\n    _assert_all_finite(\n        X.data if sp.issparse(X) else X,\n        allow_nan=allow_nan,\n        estimator_name=estimator_name,\n        input_name=input_name,\n    )",
                "filename": "sklearn/utils/validation.py",
                "start_index": 4244,
                "end_index": 7227,
                "start_line": 135,
                "end_line": 2278,
                "max_line": 2282,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_estimators_empty_data_messages(name, estimator_orig):\n    e = clone(estimator_orig)\n    set_random_state(e, 1)\n\n    X_zero_samples = np.empty(0).reshape(0, 3)\n    # The precise message can change depending on whether X or y is\n    # validated first. Let us test the type of exception only:\n    err_msg = (\n        f\"The estimator {name} does not raise a ValueError when an \"\n        \"empty data is used to train. Perhaps use check_array in train.\"\n    )\n    with raises(ValueError, err_msg=err_msg):\n        e.fit(X_zero_samples, [])\n\n    X_zero_features = np.empty(0).reshape(12, 0)\n    # the following y should be accepted by both classifiers and regressors\n    # and ignored by unsupervised models\n    y = _enforce_estimator_tags_y(e, np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]))\n    msg = r\"0 feature\\(s\\) \\(shape=\\(\\d*, 0\\)\\) while a minimum of \\d* \" \"is required.\"\n    with raises(ValueError, match=msg):\n        e.fit(X_zero_features, y)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_estimators_nan_inf(name, estimator_orig):\n    # Checks that Estimator X's do not contain NaN or inf.\n    rnd = np.random.RandomState(0)\n    X_train_finite = _enforce_estimator_tags_X(\n        estimator_orig, rnd.uniform(size=(10, 3))\n    )\n    X_train_nan = rnd.uniform(size=(10, 3))\n    X_train_nan[0, 0] = np.nan\n    X_train_inf = rnd.uniform(size=(10, 3))\n    X_train_inf[0, 0] = np.inf\n    y = np.ones(10)\n    y[:5] = 0\n    y = _enforce_estimator_tags_y(estimator_orig, y)\n    error_string_fit = f\"Estimator {name} doesn't check for NaN and inf in fit.\"\n    error_string_predict = f\"Estimator {name} doesn't check for NaN and inf in predict.\"\n    error_string_transform = (\n        f\"Estimator {name} doesn't check for NaN and inf in transform.\"\n    )\n    for X_train in [X_train_nan, X_train_inf]:\n        # catch deprecation warnings\n        with ignore_warnings(category=FutureWarning):\n            estimator = clone(estimator_orig)\n            set_random_state(estimator, 1)\n            # try to fit\n            with raises(ValueError, match=[\"inf\", \"NaN\"], err_msg=error_string_fit):\n                estimator.fit(X_train, y)\n            # actually fit\n            estimator.fit(X_train_finite, y)\n\n            # predict\n            if hasattr(estimator, \"predict\"):\n                with raises(\n                    ValueError,\n                    match=[\"inf\", \"NaN\"],\n                    err_msg=error_string_predict,\n                ):\n                    estimator.predict(X_train)\n\n            # transform\n            if hasattr(estimator, \"transform\"):\n                with raises(\n                    ValueError,\n                    match=[\"inf\", \"NaN\"],\n                    err_msg=error_string_transform,\n                ):\n                    estimator.transform(X_train)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 67270,
                "end_index": 70112,
                "start_line": 181,
                "end_line": 2043,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "def _fit(self, X, y=None, force_transform=False):\n        X = self._check_input(X, in_fit=True, check_positive=True)\n\n        if not self.copy and not force_transform:  # if call from fit()\n            X = X.copy()  # force copy so that fit does not change X inplace\n\n        n_samples = X.shape[0]\n        mean = np.mean(X, axis=0, dtype=np.float64)\n        var = np.var(X, axis=0, dtype=np.float64)\n\n        optim_function = {\n            \"box-cox\": self._box_cox_optimize,\n            \"yeo-johnson\": self._yeo_johnson_optimize,\n        }[self.method]\n\n        transform_function = {\n            \"box-cox\": boxcox,\n            \"yeo-johnson\": self._yeo_johnson_transform,\n        }[self.method]\n\n        with np.errstate(invalid=\"ignore\"):  # hide NaN warnings\n            self.lambdas_ = np.empty(X.shape[1], dtype=X.dtype)\n            for i, col in enumerate(X.T):\n                # For yeo-johnson, leave constant features unchanged\n                # lambda=1 corresponds to the identity transformation\n                is_constant_feature = _is_constant_feature(var[i], mean[i], n_samples)\n                if self.method == \"yeo-johnson\" and is_constant_feature:\n                    self.lambdas_[i] = 1.0\n                    continue\n\n                self.lambdas_[i] = optim_function(col)\n\n                if self.standardize or force_transform:\n                    X[:, i] = transform_function(X[:, i], self.lambdas_[i])\n\n        if self.standardize:\n            self._scaler = StandardScaler(copy=False).set_output(transform=\"default\")\n            if force_transform:\n                X = self._scaler.fit_transform(X)\n            else:\n                self._scaler.fit(X)\n\n        return X\n\n    def transform(self, X):\n        \"\"\"Apply the power transform to each feature using the fitted lambdas.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to be transformed using a power transformation.\n\n        Returns\n        -------\n        X_trans : ndarray of shape (n_samples, n_features)\n            The transformed data.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_input(X, in_fit=False, check_positive=True, check_shape=True)\n\n        transform_function = {\n            \"box-cox\": boxcox,\n            \"yeo-johnson\": self._yeo_johnson_transform,\n        }[self.method]\n        for i, lmbda in enumerate(self.lambdas_):\n            with np.errstate(invalid=\"ignore\"):  # hide NaN warnings\n                X[:, i] = transform_function(X[:, i], lmbda)\n\n        if self.standardize:\n            X = self._scaler.transform(X)\n\n        return X",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 108611,
                "end_index": 111246,
                "start_line": 3147,
                "end_line": 3411,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            },
            {
                "code": "# Authors: Nicolas Tresegnie <nicolas.tresegnie@gmail.com>\n#          Sergey Feldman <sergeyfeldman@gmail.com>\n# License: BSD 3 clause\n\nimport numbers\nimport warnings\nfrom collections import Counter\n\nimport numpy as np\nimport numpy.ma as ma\nfrom scipy import sparse as sp\n\nfrom ..base import BaseEstimator, TransformerMixin, _fit_context\nfrom ..utils import _is_pandas_na, is_scalar_nan\nfrom ..utils._mask import _get_mask\nfrom ..utils._param_validation import MissingValues, StrOptions\nfrom ..utils.fixes import _mode\nfrom ..utils.sparsefuncs import _get_median\nfrom ..utils.validation import FLOAT_DTYPES, _check_feature_names_in, check_is_fitted\n\n\ndef _check_inputs_dtype(X, missing_values):\n    if _is_pandas_na(missing_values):\n        # Allow using `pd.NA` as missing values to impute numerical arrays.\n        return\n    if X.dtype.kind in (\"f\", \"i\", \"u\") and not isinstance(missing_values, numbers.Real):\n        raise ValueError(\n            \"'X' and 'missing_values' types are expected to be\"\n            \" both numerical. Got X.dtype={} and \"\n            \" type(missing_values)={}.\".format(X.dtype, type(missing_values))\n        )\n\n\ndef _most_frequent(array, extra_value, n_repeat):\n    \"\"\"Compute the most frequent value in a 1d array extended with\n    [extra_value] * n_repeat, where extra_value is assumed to be not part\n    of the array.\"\"\"\n    # Compute the most frequent value in array only\n    if array.size > 0:\n        if array.dtype == object:\n            # scipy.stats.mode is slow with object dtype array.\n            # Python Counter is more efficient\n            counter = Counter(array)\n            most_frequent_count = counter.most_common(1)[0][1]\n            # tie breaking similarly to scipy.stats.mode\n            most_frequent_value = min(\n                value\n                for value, count in counter.items()\n                if count == most_frequent_count\n            )\n        else:\n            mode = _mode(array)\n            most_frequent_value = mode[0][0]\n            most_frequent_count = mode[1][0]\n    else:\n        most_frequent_value = 0\n        most_frequent_count = 0\n\n    # Compare to array + [extra_value] * n_repeat\n    if most_frequent_count == 0 and n_repeat == 0:\n        return np.nan\n    elif most_frequent_count < n_repeat:\n        return extra_value\n    elif most_frequent_count > n_repeat:\n        return most_frequent_value\n    elif most_frequent_count == n_repeat:\n        # tie breaking similarly to scipy.stats.mode\n        return min(most_frequent_value, extra_value)",
                "filename": "sklearn/impute/_base.py",
                "start_index": 0,
                "end_index": 2533,
                "start_line": 1,
                "end_line": 68,
                "max_line": 1053,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/preprocessing/_data.py": [
                {
                    "chunk": {
                        "code": "\"\"\"Apply a power transform featurewise to make data more Gaussian-like.\n\n    Power transforms are a family of parametric, monotonic transformations\n    that are applied to make data more Gaussian-like. This is useful for\n    modeling issues related to heteroscedasticity (non-constant variance),\n    or other situations where normality is desired.\n\n    Currently, PowerTransformer supports the Box-Cox transform and the\n    Yeo-Johnson transform. The optimal parameter for stabilizing variance and\n    minimizing skewness is estimated through maximum likelihood.\n\n    Box-Cox requires input data to be strictly positive, while Yeo-Johnson\n    supports both positive or negative data.\n\n    By default, zero-mean, unit-variance normalization is applied to the\n    transformed data.\n\n    Read more in the :ref:`User Guide <preprocessing_transformer>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    method : {'yeo-johnson', 'box-cox'}, default='yeo-johnson'\n        The power transform method. Available methods are:\n\n        - 'yeo-johnson' [1]_, works with positive and negative values\n        - 'box-cox' [2]_, only works with strictly positive values\n\n    standardize : bool, default=True\n        Set to True to apply zero-mean, unit-variance normalization to the\n        transformed output.\n\n    copy : bool, default=True\n        Set to False to perform inplace computation during transformation.\n\n    Attributes\n    ----------\n    lambdas_ : ndarray of float of shape (n_features,)\n        The parameters of the power transformation for the selected features.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    power_transform : Equivalent function without the estimator API.\n\n    QuantileTransformer : Maps data to a standard normal distribution with\n        the parameter `output_distribution='normal'`.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in ``fit``, and maintained\n    in ``transform``.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\n    References\n    ----------\n\n    .. [1] I.K. Yeo and R.A. Johnson, \"A new family of power transformations to\n           improve normality or symmetry.\" Biometrika, 87(4), pp.954-959,\n           (2000).\n\n    .. [2] G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal\n           of the Royal Statistical Society B, 26, 211-252 (1964).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.preprocessing import PowerTransformer\n    >>> pt = PowerTransformer()\n    >>> data = [[1, 2], [3, 2], [4, 5]]\n    >>> print(pt.fit(data))\n    PowerTransformer()\n    >>> print(pt.lambdas_)\n    [ 1.386... -3.100...]\n    >>> print(pt.transform(data))\n    [[-1.316... -0.707...]\n     [ 0.209... -0.707...]\n     [ 1.106...  1.414...]]\n    \"\"\"",
                        "filename": "sklearn/preprocessing/_data.py",
                        "start_index": 103716,
                        "end_index": 106931,
                        "start_line": 2999,
                        "end_line": 3517,
                        "max_line": 3519,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the PowerTransformer class implementation, which is directly related to the issue as the error occurs during the fit_transform method call."
                },
                {
                    "chunk": {
                        "code": "def _fit(self, X, y=None, force_transform=False):\n        X = self._check_input(X, in_fit=True, check_positive=True)\n\n        if not self.copy and not force_transform:  # if call from fit()\n            X = X.copy()  # force copy so that fit does not change X inplace\n\n        n_samples = X.shape[0]\n        mean = np.mean(X, axis=0, dtype=np.float64)\n        var = np.var(X, axis=0, dtype=np.float64)\n\n        optim_function = {\n            \"box-cox\": self._box_cox_optimize,\n            \"yeo-johnson\": self._yeo_johnson_optimize,\n        }[self.method]\n\n        transform_function = {\n            \"box-cox\": boxcox,\n            \"yeo-johnson\": self._yeo_johnson_transform,\n        }[self.method]\n\n        with np.errstate(invalid=\"ignore\"):  # hide NaN warnings\n            self.lambdas_ = np.empty(X.shape[1], dtype=X.dtype)\n            for i, col in enumerate(X.T):\n                # For yeo-johnson, leave constant features unchanged\n                # lambda=1 corresponds to the identity transformation\n                is_constant_feature = _is_constant_feature(var[i], mean[i], n_samples)\n                if self.method == \"yeo-johnson\" and is_constant_feature:\n                    self.lambdas_[i] = 1.0\n                    continue\n\n                self.lambdas_[i] = optim_function(col)\n\n                if self.standardize or force_transform:\n                    X[:, i] = transform_function(X[:, i], self.lambdas_[i])\n\n        if self.standardize:\n            self._scaler = StandardScaler(copy=False).set_output(transform=\"default\")\n            if force_transform:\n                X = self._scaler.fit_transform(X)\n            else:\n                self._scaler.fit(X)\n\n        return X\n\n    def transform(self, X):\n        \"\"\"Apply the power transform to each feature using the fitted lambdas.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The data to be transformed using a power transformation.\n\n        Returns\n        -------\n        X_trans : ndarray of shape (n_samples, n_features)\n            The transformed data.\n        \"\"\"\n        check_is_fitted(self)\n        X = self._check_input(X, in_fit=False, check_positive=True, check_shape=True)\n\n        transform_function = {\n            \"box-cox\": boxcox,\n            \"yeo-johnson\": self._yeo_johnson_transform,\n        }[self.method]\n        for i, lmbda in enumerate(self.lambdas_):\n            with np.errstate(invalid=\"ignore\"):  # hide NaN warnings\n                X[:, i] = transform_function(X[:, i], lmbda)\n\n        if self.standardize:\n            X = self._scaler.transform(X)\n\n        return X",
                        "filename": "sklearn/preprocessing/_data.py",
                        "start_index": 108611,
                        "end_index": 111246,
                        "start_line": 3147,
                        "end_line": 3411,
                        "max_line": 3519,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes the _fit method of the PowerTransformer class where the error is raised when calling stats.boxcox on an all-nan column."
                },
                {
                    "chunk": {
                        "code": "\"\"\"Parametric, monotonic transformation to make data more Gaussian-like.\n\n    Power transforms are a family of parametric, monotonic transformations\n    that are applied to make data more Gaussian-like. This is useful for\n    modeling issues related to heteroscedasticity (non-constant variance),\n    or other situations where normality is desired.\n\n    Currently, power_transform supports the Box-Cox transform and the\n    Yeo-Johnson transform. The optimal parameter for stabilizing variance and\n    minimizing skewness is estimated through maximum likelihood.\n\n    Box-Cox requires input data to be strictly positive, while Yeo-Johnson\n    supports both positive or negative data.\n\n    By default, zero-mean, unit-variance normalization is applied to the\n    transformed data.\n\n    Read more in the :ref:`User Guide <preprocessing_transformer>`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data to be transformed using a power transformation.\n\n    method : {'yeo-johnson', 'box-cox'}, default='yeo-johnson'\n        The power transform method. Available methods are:\n\n        - 'yeo-johnson' [1]_, works with positive and negative values\n        - 'box-cox' [2]_, only works with strictly positive values\n\n        .. versionchanged:: 0.23\n            The default value of the `method` parameter changed from\n            'box-cox' to 'yeo-johnson' in 0.23.\n\n    standardize : bool, default=True\n        Set to True to apply zero-mean, unit-variance normalization to the\n        transformed output.\n\n    copy : bool, default=True\n        Set to False to perform inplace computation during transformation.\n\n    Returns\n    -------\n    X_trans : ndarray of shape (n_samples, n_features)\n        The transformed data.\n\n    See Also\n    --------\n    PowerTransformer : Equivalent transformation with the\n        Transformer API (e.g. as part of a preprocessing\n        :class:`~sklearn.pipeline.Pipeline`).\n\n    quantile_transform : Maps data to a standard normal distribution with\n        the parameter `output_distribution='normal'`.\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded in ``fit``, and maintained\n    in ``transform``.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see :ref:`examples/preprocessing/plot_all_scaling.py\n    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n\n    References\n    ----------\n\n    .. [1] I.K. Yeo and R.A. Johnson, \"A new family of power transformations to\n           improve normality or symmetry.\" Biometrika, 87(4), pp.954-959,\n           (2000).\n\n    .. [2] G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal\n           of the Royal Statistical Society B, 26, 211-252 (1964).\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.preprocessing import power_transform\n    >>> data = [[1, 2], [3, 2], [4, 5]]\n    >>> print(power_transform(data, method='box-cox'))\n    [[-1.332... -0.707...]\n     [ 0.256... -0.707...]\n     [ 1.076...  1.414...]]\n\n    .. warning:: Risk of data leak.\n        Do not use :func:`~sklearn.preprocessing.power_transform` unless you\n        know what you are doing. A common mistake is to apply it to the entire\n        data *before* splitting into training and test sets. This will bias the\n        model evaluation because information would have leaked from the test\n        set to the training set.\n        In general, we recommend using\n        :class:`~sklearn.preprocessing.PowerTransformer` within a\n        :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n        leaking, e.g.: `pipe = make_pipeline(PowerTransformer(),\n        LogisticRegression())`.\n    \"\"\"",
                        "filename": "sklearn/preprocessing/_data.py",
                        "start_index": 117994,
                        "end_index": 121706,
                        "start_line": 3422,
                        "end_line": 3517,
                        "max_line": 3519,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet provides documentation for the power_transform function, which is relevant for understanding the expected behavior of the PowerTransformer and how it handles NaN values."
                }
            ]
        },
        "solution_task": "To resolve the issue with the `PowerTransformer` when fitting to data with an all-nan column, you should:\n\n- Add a check in the `_fit` method of the `PowerTransformer` class to handle all-nan columns before applying the transformation.\n- If an all-nan column is detected, you can either:\n  - Skip the transformation for that column and set the corresponding lambda to a default value (e.g., 1 for Yeo-Johnson which corresponds to the identity transformation).\n  - Raise a more descriptive error message indicating that all-nan columns are not supported for transformation.\n\nHere is a possible solution:\n\n```python\ndef _fit(self, X, y=None, force_transform=False):\n    X = self._check_input(X, in_fit=True, check_positive=True)\n\n    if not self.copy and not force_transform:  # if call from fit()\n        X = X.copy()  # force copy so that fit does not change X inplace\n\n    n_samples, n_features = X.shape\n    self.lambdas_ = np.empty(n_features, dtype=X.dtype)\n\n    for i in range(n_features):\n        # Check for all-nan column\n        if np.isnan(X[:, i]).all():\n            # Option 1: Set lambda to a default value, e.g., 1 for Yeo-Johnson\n            self.lambdas_[i] = 1.0\n            # Option 2: Raise an error with a descriptive message\n            # raise ValueError(f\"All values are NaN in column {i}\")\n            continue  # Skip the rest of the loop for this column\n\n        # Existing code for fitting continues here...\n```\n\nChoose either Option 1 or Option 2 based on the desired behavior. If you choose Option 1, make sure to handle the transformation accordingly in the `transform` method as well. If you choose Option 2, ensure that the error message is clear and informative.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE provides a detailed and actionable solution to the bug described in the PROMPT. It suggests adding a check in the `_fit` method of the `PowerTransformer` class to handle all-nan columns before applying the transformation. The RESPONSE offers two options: setting a default lambda value for columns with all NaNs or raising a descriptive error message. It also includes a code snippet demonstrating how to implement the check and handle the all-nan column, which is directly relevant to the issue. Furthermore, the RESPONSE provides links to the relevant sections of the code in the sklearn repository, which would be useful for a developer looking to implement the fix. The only reason it does not score a 10 is that it does not explicitly mention the need to handle the transformation in the `transform` method if Option 1 is chosen, which is a necessary part of the solution.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711738089.925478,
        "result": "To resolve the issue with the `PowerTransformer` when fitting to data with an all-nan column, you should:\n\n- Add a check in the `_fit` method of the `PowerTransformer` class to handle all-nan columns before applying the transformation.\n- If an all-nan column is detected, you can either:\n  - Skip the transformation for that column and set the corresponding lambda to a default value (e.g., 1 for Yeo-Johnson which corresponds to the identity transformation).\n  - Raise a more descriptive error message indicating that all-nan columns are not supported for transformation.\n\nHere is a possible solution:\n\n```python\ndef _fit(self, X, y=None, force_transform=False):\n    X = self._check_input(X, in_fit=True, check_positive=True)\n\n    if not self.copy and not force_transform:  # if call from fit()\n        X = X.copy()  # force copy so that fit does not change X inplace\n\n    n_samples, n_features = X.shape\n    self.lambdas_ = np.empty(n_features, dtype=X.dtype)\n\n    for i in range(n_features):\n        # Check for all-nan column\n        if np.isnan(X[:, i]).all():\n            # Option 1: Set lambda to a default value, e.g., 1 for Yeo-Johnson\n            self.lambdas_[i] = 1.0\n            # Option 2: Raise an error with a descriptive message\n            # raise ValueError(f\"All values are NaN in column {i}\")\n            continue  # Skip the rest of the loop for this column\n\n        # Existing code for fitting continues here...\n```\n\nChoose either Option 1 or Option 2 based on the desired behavior. If you choose Option 1, make sure to handle the transformation accordingly in the `transform` method as well. If you choose Option 2, ensure that the error message is clear and informative.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn) \u2192 [preprocessing](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn/preprocessing)- [Lines 2999 - 3517](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/preprocessing/_data.py#L2999-L3517)\n\n This snippet contains the PowerTransformer class implementation, which is directly related to the issue as the error occurs during the fit_transform method call. \n\n\n   - [Lines 3147 - 3411](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/preprocessing/_data.py#L3147-L3411)\n\n This snippet includes the _fit method of the PowerTransformer class where the error is raised when calling stats.boxcox on an all-nan column. \n\n\n   - [Lines 3422 - 3517](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/preprocessing/_data.py#L3422-L3517)\n\n This snippet provides documentation for the power_transform function, which is relevant for understanding the expected behavior of the PowerTransformer and how it handles NaN values."
    }
}