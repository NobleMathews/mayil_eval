{
    "id": "scikit-learn__scikit-learn-13013",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
    "task_id": 2160865756093999784,
    "title": "Make use of check_is_fitted instead of manual checks",
    "body": "#### Description\r\nIn some places, a manual check is performed to check whether an estimator has been fitted, instead of using the `check_is_fitted` method. Due to this, the NotFittedError messages are often inconsistent.\r\n\r\nSome examples include:\r\nhttps://github.com/scikit-learn/scikit-learn/blob/486f8fc5438d4625ec05d22bb24ca5afb3c396fd/sklearn/linear_model/base.py#L253-L255\r\nhttps://github.com/scikit-learn/scikit-learn/blob/486f8fc5438d4625ec05d22bb24ca5afb3c396fd/sklearn/linear_model/logistic.py#L1645-L1646\r\n\r\n#### Steps/Code to Reproduce\r\nLook at the code in the examples above.\r\n\r\n#### Expected Results\r\nCode should be using the `check_is_fitted` method from the `utils.validation` submodule. \r\n\r\n#### Actual Results\r\nThis check is re-implemented in various places. Error messages are not consistent.\r\n\r\n#### Versions\r\nn/a\r\n\r\n#### TODO\r\nI am happy to submit a PR to fix this. Planning to identify the places where the method is re-implemented using the search functionality on github. Please let me know if there is more clever way of doing this.\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.22.git",
    "requested_commit": "",
    "mayil_response": "- Search for instances in the codebase where manual checks for fitting are performed, typically by checking if certain attributes like `coef_` or `intercept_` are present.\n- Replace these manual checks with calls to `check_is_fitted(self)` within the relevant methods of estimator classes.\n- Ensure that the error message for `NotFittedError` is consistent across the codebase. If custom messages are necessary, use the `msg` parameter of `check_is_fitted` to provide specific error details.\n- Remove any redundant error handling code that becomes unnecessary after using `check_is_fitted`.\n- Submit changes for review, ensuring that the refactoring does not alter the functionality of the estimators.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/utils)- [Lines 3892 - 3980](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/estimator_checks.py#L3892-L3980)\n\n This snippet contains a test that checks the use of 'check_is_fitted' before and after fitting an estimator, which is directly related to the issue of replacing manual checks with 'check_is_fitted'.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/utils)- [Lines 1406 - 1491](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/validation.py#L1406-L1491)\n\n This snippet defines the 'check_is_fitted' function, which should be used instead of manual checks to determine if an estimator has been fitted, as described in the issue.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0005394999999999999,
        "snippet_processor": 0.08021,
        "issue_star_creation": 0.03338,
        "issue_star_solver": 0.08541,
        "bouncer": 0.023790000000000002
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711739737.098112,
        "relevant_snippets": [
            {
                "code": "@ignore_warnings\ndef check_estimators_unfitted(name, estimator_orig):\n    \"\"\"Check that predict raises an exception in an unfitted estimator.\n\n    Unfitted estimators should raise a NotFittedError.\n    \"\"\"\n    # Common test for Regressors, Classifiers and Outlier detection estimators\n    X, y = _regression_dataset()\n\n    estimator = clone(estimator_orig)\n    for method in (\n        \"decision_function\",\n        \"predict\",\n        \"predict_proba\",\n        \"predict_log_proba\",\n    ):\n        if hasattr(estimator, method):\n            with raises(NotFittedError):\n                getattr(estimator, method)(X)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_supervised_y_2d(name, estimator_orig):\n    tags = _safe_tags(estimator_orig)\n    rnd = np.random.RandomState(0)\n    n_samples = 30\n    X = _enforce_estimator_tags_X(estimator_orig, rnd.uniform(size=(n_samples, 3)))\n    y = np.arange(n_samples) % 3\n    y = _enforce_estimator_tags_y(estimator_orig, y)\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    # fit\n    estimator.fit(X, y)\n    y_pred = estimator.predict(X)\n\n    set_random_state(estimator)\n    # Check that when a 2D y is given, a DataConversionWarning is\n    # raised\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\", DataConversionWarning)\n        warnings.simplefilter(\"ignore\", RuntimeWarning)\n        estimator.fit(X, y[:, np.newaxis])\n    y_pred_2d = estimator.predict(X)\n    msg = \"expected 1 DataConversionWarning, got: %s\" % \", \".join(\n        [str(w_x) for w_x in w]\n    )\n    if not tags[\"multioutput\"]:\n        # check that we warned if we don't support multi-output\n        assert len(w) > 0, msg\n        assert (\n            \"DataConversionWarning('A column-vector y\"\n            \" was passed when a 1d array was expected\"\n            in msg\n        )\n    assert_allclose(y_pred.ravel(), y_pred_2d.ravel())",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 101202,
                "end_index": 103111,
                "start_line": 181,
                "end_line": 2929,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "def check_fit_check_is_fitted(name, estimator_orig):\n    # Make sure that estimator doesn't pass check_is_fitted before calling fit\n    # and that passes check_is_fitted once it's fit.\n\n    rng = np.random.RandomState(42)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    if \"warm_start\" in estimator.get_params():\n        estimator.set_params(warm_start=False)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n    if is_regressor(estimator_orig):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(low=0, high=2, size=n_samples)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if not _safe_tags(estimator).get(\"stateless\", False):\n        # stateless estimators (such as FunctionTransformer) are always \"fit\"!\n        try:\n            check_is_fitted(estimator)\n            raise AssertionError(\n                f\"{estimator.__class__.__name__} passes check_is_fitted before being\"\n                \" fit!\"\n            )\n        except NotFittedError:\n            pass\n    estimator.fit(X, y)\n    try:\n        check_is_fitted(estimator)\n    except NotFittedError as e:\n        raise NotFittedError(\n            \"Estimator fails to pass `check_is_fitted` even though it has been fit.\"\n        ) from e\n\n\ndef check_n_features_in(name, estimator_orig):\n    # Make sure that n_features_in_ attribute doesn't exist until fit is\n    # called, and that its value is correct.\n\n    rng = np.random.RandomState(0)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    if \"warm_start\" in estimator.get_params():\n        estimator.set_params(warm_start=False)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n    if is_regressor(estimator_orig):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(low=0, high=2, size=n_samples)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    assert not hasattr(estimator, \"n_features_in_\")\n    estimator.fit(X, y)\n    assert hasattr(estimator, \"n_features_in_\")\n    assert estimator.n_features_in_ == X.shape[1]\n\n\ndef check_requires_y_none(name, estimator_orig):\n    # Make sure that an estimator with requires_y=True fails gracefully when\n    # given y=None\n\n    rng = np.random.RandomState(0)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n\n    expected_err_msgs = (\n        \"requires y to be passed, but the target y is None\",\n        \"Expected array-like (array or non-string sequence), got None\",\n        \"y should be a 1d array\",\n    )\n\n    try:\n        estimator.fit(X, None)\n    except ValueError as ve:\n        if not any(msg in str(ve) for msg in expected_err_msgs):\n            raise ve",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 138164,
                "end_index": 141063,
                "start_line": 3892,
                "end_line": 3980,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):\n    \"\"\"Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a NotFittedError with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to specify if the\n    estimator is fitted or not.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.\n    \"\"\"\n    if isclass(estimator):\n        raise TypeError(\"{} is a class, not an instance.\".format(estimator))\n    if msg is None:\n        msg = (\n            \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using this estimator.\"\n        )\n\n    if not hasattr(estimator, \"fit\"):\n        raise TypeError(\"%s is not an estimator instance.\" % (estimator))\n\n    if not _is_fitted(estimator, attributes, all_or_any):\n        raise NotFittedError(msg % {\"name\": type(estimator).__name__})\n\n\ndef check_non_negative(X, whom):\n    \"\"\"\n    Check if there is any negative value in an array.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}\n        Input data.\n\n    whom : str\n        Who passed X to this function.\n    \"\"\"\n    xp, _ = get_namespace(X)\n    # avoid X.min() on sparse matrix since it also sorts the indices\n    if sp.issparse(X):\n        if X.format in [\"lil\", \"dok\"]:\n            X = X.tocsr()\n        if X.data.size == 0:\n            X_min = 0\n        else:\n            X_min = X.data.min()\n    else:\n        X_min = xp.min(X)\n\n    if X_min < 0:\n        raise ValueError(\"Negative values in data passed to %s\" % whom)",
                "filename": "sklearn/utils/validation.py",
                "start_index": 49245,
                "end_index": 52119,
                "start_line": 1406,
                "end_line": 1491,
                "max_line": 2282,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "def check_fit_idempotent(name, estimator_orig):\n    # Check that est.fit(X) is the same as est.fit(X).fit(X). Ideally we would\n    # check that the estimated parameters during training (e.g. coefs_) are\n    # the same, but having a universal comparison function for those\n    # attributes is difficult and full of edge cases. So instead we check that\n    # predict(), predict_proba(), decision_function() and transform() return\n    # the same results.\n\n    check_methods = [\"predict\", \"transform\", \"decision_function\", \"predict_proba\"]\n    rng = np.random.RandomState(0)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    if \"warm_start\" in estimator.get_params().keys():\n        estimator.set_params(warm_start=False)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n    if is_regressor(estimator_orig):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(low=0, high=2, size=n_samples)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    train, test = next(ShuffleSplit(test_size=0.2, random_state=rng).split(X))\n    X_train, y_train = _safe_split(estimator, X, y, train)\n    X_test, y_test = _safe_split(estimator, X, y, test, train)\n\n    # Fit for the first time\n    estimator.fit(X_train, y_train)\n\n    result = {\n        method: getattr(estimator, method)(X_test)\n        for method in check_methods\n        if hasattr(estimator, method)\n    }\n\n    # Fit again\n    set_random_state(estimator)\n    estimator.fit(X_train, y_train)\n\n    for method in check_methods:\n        if hasattr(estimator, method):\n            new_result = getattr(estimator, method)(X_test)\n            if np.issubdtype(new_result.dtype, np.floating):\n                tol = 2 * np.finfo(new_result.dtype).eps\n            else:\n                tol = 2 * np.finfo(np.float64).eps\n            assert_allclose_dense_sparse(\n                result[method],\n                new_result,\n                atol=max(tol, 1e-9),\n                rtol=max(tol, 1e-7),\n                err_msg=\"Idempotency check failed for method {}\".format(method),\n            )",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 136021,
                "end_index": 138161,
                "start_line": 3834,
                "end_line": 4630,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "def check_outliers_fit_predict(name, estimator_orig):\n    # Check fit_predict for outlier detectors.\n\n    n_samples = 300\n    X, _ = make_blobs(n_samples=n_samples, random_state=0)\n    X = shuffle(X, random_state=7)\n    n_samples, n_features = X.shape\n    estimator = clone(estimator_orig)\n\n    set_random_state(estimator)\n\n    y_pred = estimator.fit_predict(X)\n    assert y_pred.shape == (n_samples,)\n    assert y_pred.dtype.kind == \"i\"\n    assert_array_equal(np.unique(y_pred), np.array([-1, 1]))\n\n    # check fit_predict = fit.predict when the estimator has both a predict and\n    # a fit_predict method. recall that it is already assumed here that the\n    # estimator has a fit_predict method\n    if hasattr(estimator, \"predict\"):\n        y_pred_2 = estimator.fit(X).predict(X)\n        assert_array_equal(y_pred, y_pred_2)\n\n    if hasattr(estimator, \"contamination\"):\n        # proportion of outliers equal to contamination parameter when not\n        # set to 'auto'\n        expected_outliers = 30\n        contamination = float(expected_outliers) / n_samples\n        estimator.set_params(contamination=contamination)\n        y_pred = estimator.fit_predict(X)\n\n        num_outliers = np.sum(y_pred != 1)\n        # num_outliers should be equal to expected_outliers unless\n        # there are ties in the decision_function values. this can\n        # only be tested for estimators with a decision_function\n        # method\n        if num_outliers != expected_outliers and hasattr(\n            estimator, \"decision_function\"\n        ):\n            decision = estimator.decision_function(X)\n            check_outlier_corruption(num_outliers, expected_outliers, decision)\n\n\ndef check_fit_non_negative(name, estimator_orig):\n    # Check that proper warning is raised for non-negative X\n    # when tag requires_positive_X is present\n    X = np.array([[-1.0, 1], [-1.0, 1]])\n    y = np.array([1, 2])\n    estimator = clone(estimator_orig)\n    with raises(ValueError):\n        estimator.fit(X, y)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 134030,
                "end_index": 136018,
                "start_line": 3781,
                "end_line": 4110,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_classifiers_multilabel_output_format_decision_function(name, classifier_orig):\n    \"\"\"Check the output of the `decision_function` method for classifiers supporting\n    multilabel-indicator targets.\"\"\"\n    classifier = clone(classifier_orig)\n    set_random_state(classifier)\n\n    n_samples, test_size, n_outputs = 100, 25, 5\n    X, y = make_multilabel_classification(\n        n_samples=n_samples,\n        n_features=2,\n        n_classes=n_outputs,\n        n_labels=3,\n        length=50,\n        allow_unlabeled=True,\n        random_state=0,\n    )\n    X = scale(X)\n\n    X_train, X_test = X[:-test_size], X[-test_size:]\n    y_train = y[:-test_size]\n    classifier.fit(X_train, y_train)\n\n    response_method_name = \"decision_function\"\n    decision_function_method = getattr(classifier, response_method_name, None)\n    if decision_function_method is None:\n        raise SkipTest(f\"{name} does not have a {response_method_name} method.\")\n\n    y_pred = decision_function_method(X_test)\n\n    # y_pred.shape -> y_test.shape with floating dtype\n    assert isinstance(y_pred, np.ndarray), (\n        f\"{name}.decision_function is expected to output a NumPy array.\"\n        f\" Got {type(y_pred)} instead.\"\n    )\n    assert y_pred.shape == (test_size, n_outputs), (\n        f\"{name}.decision_function is expected to provide a NumPy array \"\n        f\"of shape (n_samples, n_outputs). Got {y_pred.shape} instead of \"\n        f\"{(test_size, n_outputs)}.\"\n    )\n    assert y_pred.dtype.kind == \"f\", (\n        f\"{name}.decision_function is expected to output a floating dtype.\"\n        f\" Got {y_pred.dtype} instead.\"\n    )\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_get_feature_names_out_error(name, estimator_orig):\n    \"\"\"Check the error raised by get_feature_names_out when called before fit.\n\n    Unfitted estimators with get_feature_names_out should raise a NotFittedError.\n    \"\"\"\n\n    estimator = clone(estimator_orig)\n    err_msg = (\n        f\"Estimator {name} should have raised a NotFitted error when fit is called\"\n        \" before get_feature_names_out\"\n    )\n    with raises(NotFittedError, err_msg=err_msg):\n        estimator.get_feature_names_out()\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_estimators_fit_returns_self(name, estimator_orig, readonly_memmap=False):\n    \"\"\"Check if self is returned when calling fit.\"\"\"\n    X, y = make_blobs(random_state=0, n_samples=21)\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if readonly_memmap:\n        X, y = create_memmap_backed_data([X, y])\n\n    set_random_state(estimator)\n    assert estimator.fit(X, y) is estimator",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 98469,
                "end_index": 101199,
                "start_line": 181,
                "end_line": 2872,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "invalid_names = [\n        (names[::-1], \"Feature names must be in the same order as they were in fit.\"),\n        (\n            [f\"another_prefix_{i}\" for i in range(n_features)],\n            (\n                \"Feature names unseen at fit time:\\n- another_prefix_0\\n-\"\n                \" another_prefix_1\\n\"\n            ),\n        ),\n        (\n            names[:3],\n            f\"Feature names seen at fit time, yet now missing:\\n- {min(names[3:])}\\n\",\n        ),\n    ]\n    params = {\n        key: value\n        for key, value in estimator.get_params().items()\n        if \"early_stopping\" in key\n    }\n    early_stopping_enabled = any(value is True for value in params.values())\n\n    for invalid_name, additional_message in invalid_names:\n        X_bad = pd.DataFrame(X, columns=invalid_name, copy=False)\n\n        expected_msg = re.escape(\n            \"The feature names should match those that were passed during fit.\\n\"\n            f\"{additional_message}\"\n        )\n        for name, method in check_methods:\n            with raises(\n                ValueError, match=expected_msg, err_msg=f\"{name} did not raise\"\n            ):\n                method(X_bad)\n\n        # partial_fit checks on second call\n        # Do not call partial fit if early_stopping is on\n        if not hasattr(estimator, \"partial_fit\") or early_stopping_enabled:\n            continue\n\n        estimator = clone(estimator_orig)\n        if is_classifier(estimator):\n            classes = np.unique(y)\n            estimator.partial_fit(X, y, classes=classes)\n        else:\n            estimator.partial_fit(X, y)\n\n        with raises(ValueError, match=expected_msg):\n            estimator.partial_fit(X_bad, y)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 146663,
                "end_index": 148346,
                "start_line": 4160,
                "end_line": 4207,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_dont_overwrite_parameters(name, estimator_orig):\n    # check that fit method only changes or sets private attributes\n    if hasattr(estimator_orig.__init__, \"deprecated_original\"):\n        # to not check deprecated classes\n        return\n    estimator = clone(estimator_orig)\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20, 3))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    y = X[:, 0].astype(int)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n    dict_before_fit = estimator.__dict__.copy()\n    estimator.fit(X, y)\n\n    dict_after_fit = estimator.__dict__\n\n    public_keys_after_fit = [\n        key for key in dict_after_fit.keys() if _is_public_parameter(key)\n    ]\n\n    attrs_added_by_fit = [\n        key for key in public_keys_after_fit if key not in dict_before_fit.keys()\n    ]\n\n    # check that fit doesn't add any public attribute\n    assert not attrs_added_by_fit, (\n        \"Estimator adds public attribute(s) during\"\n        \" the fit method.\"\n        \" Estimators are only allowed to add private attributes\"\n        \" either started with _ or ended\"\n        \" with _ but %s added\"\n        % \", \".join(attrs_added_by_fit)\n    )\n\n    # check that fit doesn't change any public attribute\n    attrs_changed_by_fit = [\n        key\n        for key in public_keys_after_fit\n        if (dict_before_fit[key] is not dict_after_fit[key])\n    ]\n\n    assert not attrs_changed_by_fit, (\n        \"Estimator changes public attribute(s) during\"\n        \" the fit method. Estimators are only allowed\"\n        \" to change attributes started\"\n        \" or ended with _, but\"\n        \" %s changed\"\n        % \", \".join(attrs_changed_by_fit)\n    )\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_fit2d_predict1d(name, estimator_orig):\n    # check by fitting a 2d array and predicting with a 1d array\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20, 3))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    y = X[:, 0].astype(int)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n    estimator.fit(X, y)\n\n    for method in [\"predict\", \"transform\", \"decision_function\", \"predict_proba\"]:\n        if hasattr(estimator, method):\n            assert_raise_message(\n                ValueError, \"Reshape your data\", getattr(estimator, method), X[0]\n            )",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 48853,
                "end_index": 51626,
                "start_line": 181,
                "end_line": 4630,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_estimators_empty_data_messages(name, estimator_orig):\n    e = clone(estimator_orig)\n    set_random_state(e, 1)\n\n    X_zero_samples = np.empty(0).reshape(0, 3)\n    # The precise message can change depending on whether X or y is\n    # validated first. Let us test the type of exception only:\n    err_msg = (\n        f\"The estimator {name} does not raise a ValueError when an \"\n        \"empty data is used to train. Perhaps use check_array in train.\"\n    )\n    with raises(ValueError, err_msg=err_msg):\n        e.fit(X_zero_samples, [])\n\n    X_zero_features = np.empty(0).reshape(12, 0)\n    # the following y should be accepted by both classifiers and regressors\n    # and ignored by unsupervised models\n    y = _enforce_estimator_tags_y(e, np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]))\n    msg = r\"0 feature\\(s\\) \\(shape=\\(\\d*, 0\\)\\) while a minimum of \\d* \" \"is required.\"\n    with raises(ValueError, match=msg):\n        e.fit(X_zero_features, y)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_estimators_nan_inf(name, estimator_orig):\n    # Checks that Estimator X's do not contain NaN or inf.\n    rnd = np.random.RandomState(0)\n    X_train_finite = _enforce_estimator_tags_X(\n        estimator_orig, rnd.uniform(size=(10, 3))\n    )\n    X_train_nan = rnd.uniform(size=(10, 3))\n    X_train_nan[0, 0] = np.nan\n    X_train_inf = rnd.uniform(size=(10, 3))\n    X_train_inf[0, 0] = np.inf\n    y = np.ones(10)\n    y[:5] = 0\n    y = _enforce_estimator_tags_y(estimator_orig, y)\n    error_string_fit = f\"Estimator {name} doesn't check for NaN and inf in fit.\"\n    error_string_predict = f\"Estimator {name} doesn't check for NaN and inf in predict.\"\n    error_string_transform = (\n        f\"Estimator {name} doesn't check for NaN and inf in transform.\"\n    )\n    for X_train in [X_train_nan, X_train_inf]:\n        # catch deprecation warnings\n        with ignore_warnings(category=FutureWarning):\n            estimator = clone(estimator_orig)\n            set_random_state(estimator, 1)\n            # try to fit\n            with raises(ValueError, match=[\"inf\", \"NaN\"], err_msg=error_string_fit):\n                estimator.fit(X_train, y)\n            # actually fit\n            estimator.fit(X_train_finite, y)\n\n            # predict\n            if hasattr(estimator, \"predict\"):\n                with raises(\n                    ValueError,\n                    match=[\"inf\", \"NaN\"],\n                    err_msg=error_string_predict,\n                ):\n                    estimator.predict(X_train)\n\n            # transform\n            if hasattr(estimator, \"transform\"):\n                with raises(\n                    ValueError,\n                    match=[\"inf\", \"NaN\"],\n                    err_msg=error_string_transform,\n                ):\n                    estimator.transform(X_train)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 67270,
                "end_index": 70112,
                "start_line": 181,
                "end_line": 2043,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings\ndef check_fit2d_1feature(name, estimator_orig):\n    # check fitting a 2d array with only 1 feature either works or returns\n    # informative message\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(10, 1))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    y = X[:, 0].astype(int)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n    # ensure two labels in subsample for RandomizedLogisticRegression\n    if name == \"RandomizedLogisticRegression\":\n        estimator.sample_fraction = 1\n    # ensure non skipped trials for RANSACRegressor\n    if name == \"RANSACRegressor\":\n        estimator.residual_threshold = 0.5\n\n    y = _enforce_estimator_tags_y(estimator, y)\n    set_random_state(estimator, 1)\n\n    msgs = [r\"1 feature\\(s\\)\", \"n_features = 1\", \"n_features=1\"]\n\n    with raises(ValueError, match=msgs, may_pass=True):\n        estimator.fit(X, y)\n\n\n@ignore_warnings\ndef check_fit1d(name, estimator_orig):\n    # check fitting 1d X array raises a ValueError\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20))\n    y = X.astype(int)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n    with raises(ValueError):\n        estimator.fit(X, y)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_transformer_general(name, transformer, readonly_memmap=False):\n    X, y = make_blobs(\n        n_samples=30,\n        centers=[[0, 0, 0], [1, 1, 1]],\n        random_state=0,\n        n_features=2,\n        cluster_std=0.1,\n    )\n    X = StandardScaler().fit_transform(X)\n    X = _enforce_estimator_tags_X(transformer, X)\n\n    if readonly_memmap:\n        X, y = create_memmap_backed_data([X, y])\n\n    _check_transformer(name, transformer, X, y)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_transformer_data_not_an_array(name, transformer):\n    X, y = make_blobs(\n        n_samples=30,\n        centers=[[0, 0, 0], [1, 1, 1]],\n        random_state=0,\n        n_features=2,\n        cluster_std=0.1,\n    )\n    X = StandardScaler().fit_transform(X)\n    X = _enforce_estimator_tags_X(transformer, X)\n    this_X = _NotAnArray(X)\n    this_y = _NotAnArray(np.asarray(y))\n    _check_transformer(name, transformer, this_X, this_y)\n    # try the same with some list\n    _check_transformer(name, transformer, X.tolist(), y.tolist())",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 55999,
                "end_index": 58685,
                "start_line": 181,
                "end_line": 1729,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/utils/estimator_checks.py": [
                {
                    "chunk": {
                        "code": "def check_fit_check_is_fitted(name, estimator_orig):\n    # Make sure that estimator doesn't pass check_is_fitted before calling fit\n    # and that passes check_is_fitted once it's fit.\n\n    rng = np.random.RandomState(42)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    if \"warm_start\" in estimator.get_params():\n        estimator.set_params(warm_start=False)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n    if is_regressor(estimator_orig):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(low=0, high=2, size=n_samples)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if not _safe_tags(estimator).get(\"stateless\", False):\n        # stateless estimators (such as FunctionTransformer) are always \"fit\"!\n        try:\n            check_is_fitted(estimator)\n            raise AssertionError(\n                f\"{estimator.__class__.__name__} passes check_is_fitted before being\"\n                \" fit!\"\n            )\n        except NotFittedError:\n            pass\n    estimator.fit(X, y)\n    try:\n        check_is_fitted(estimator)\n    except NotFittedError as e:\n        raise NotFittedError(\n            \"Estimator fails to pass `check_is_fitted` even though it has been fit.\"\n        ) from e\n\n\ndef check_n_features_in(name, estimator_orig):\n    # Make sure that n_features_in_ attribute doesn't exist until fit is\n    # called, and that its value is correct.\n\n    rng = np.random.RandomState(0)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    if \"warm_start\" in estimator.get_params():\n        estimator.set_params(warm_start=False)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n    if is_regressor(estimator_orig):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(low=0, high=2, size=n_samples)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    assert not hasattr(estimator, \"n_features_in_\")\n    estimator.fit(X, y)\n    assert hasattr(estimator, \"n_features_in_\")\n    assert estimator.n_features_in_ == X.shape[1]\n\n\ndef check_requires_y_none(name, estimator_orig):\n    # Make sure that an estimator with requires_y=True fails gracefully when\n    # given y=None\n\n    rng = np.random.RandomState(0)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _enforce_estimator_tags_X(estimator, X)\n\n    expected_err_msgs = (\n        \"requires y to be passed, but the target y is None\",\n        \"Expected array-like (array or non-string sequence), got None\",\n        \"y should be a 1d array\",\n    )\n\n    try:\n        estimator.fit(X, None)\n    except ValueError as ve:\n        if not any(msg in str(ve) for msg in expected_err_msgs):\n            raise ve",
                        "filename": "sklearn/utils/estimator_checks.py",
                        "start_index": 138164,
                        "end_index": 141063,
                        "start_line": 3892,
                        "end_line": 3980,
                        "max_line": 4639,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains a test that checks the use of 'check_is_fitted' before and after fitting an estimator, which is directly related to the issue of replacing manual checks with 'check_is_fitted'."
                }
            ],
            "sklearn/utils/validation.py": [
                {
                    "chunk": {
                        "code": "def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):\n    \"\"\"Perform is_fitted validation for estimator.\n\n    Checks if the estimator is fitted by verifying the presence of\n    fitted attributes (ending with a trailing underscore) and otherwise\n    raises a NotFittedError with the given message.\n\n    If an estimator does not set any attributes with a trailing underscore, it\n    can define a ``__sklearn_is_fitted__`` method returning a boolean to specify if the\n    estimator is fitted or not.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        Estimator instance for which the check is performed.\n\n    attributes : str, list or tuple of str, default=None\n        Attribute name(s) given as string or a list/tuple of strings\n        Eg.: ``[\"coef_\", \"estimator_\", ...], \"coef_\"``\n\n        If `None`, `estimator` is considered fitted if there exist an\n        attribute that ends with a underscore and does not start with double\n        underscore.\n\n    msg : str, default=None\n        The default error message is, \"This %(name)s instance is not fitted\n        yet. Call 'fit' with appropriate arguments before using this\n        estimator.\"\n\n        For custom messages if \"%(name)s\" is present in the message string,\n        it is substituted for the estimator name.\n\n        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n\n    all_or_any : callable, {all, any}, default=all\n        Specify whether all or any of the given attributes must exist.\n\n    Raises\n    ------\n    TypeError\n        If the estimator is a class or not an estimator instance\n\n    NotFittedError\n        If the attributes are not found.\n    \"\"\"\n    if isclass(estimator):\n        raise TypeError(\"{} is a class, not an instance.\".format(estimator))\n    if msg is None:\n        msg = (\n            \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n            \"appropriate arguments before using this estimator.\"\n        )\n\n    if not hasattr(estimator, \"fit\"):\n        raise TypeError(\"%s is not an estimator instance.\" % (estimator))\n\n    if not _is_fitted(estimator, attributes, all_or_any):\n        raise NotFittedError(msg % {\"name\": type(estimator).__name__})\n\n\ndef check_non_negative(X, whom):\n    \"\"\"\n    Check if there is any negative value in an array.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}\n        Input data.\n\n    whom : str\n        Who passed X to this function.\n    \"\"\"\n    xp, _ = get_namespace(X)\n    # avoid X.min() on sparse matrix since it also sorts the indices\n    if sp.issparse(X):\n        if X.format in [\"lil\", \"dok\"]:\n            X = X.tocsr()\n        if X.data.size == 0:\n            X_min = 0\n        else:\n            X_min = X.data.min()\n    else:\n        X_min = xp.min(X)\n\n    if X_min < 0:\n        raise ValueError(\"Negative values in data passed to %s\" % whom)",
                        "filename": "sklearn/utils/validation.py",
                        "start_index": 49245,
                        "end_index": 52119,
                        "start_line": 1406,
                        "end_line": 1491,
                        "max_line": 2282,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet defines the 'check_is_fitted' function, which should be used instead of manual checks to determine if an estimator has been fitted, as described in the issue."
                }
            ]
        },
        "solution_task": "- Search for instances in the codebase where manual checks for fitting are performed, typically by checking if certain attributes like `coef_` or `intercept_` are present.\n- Replace these manual checks with calls to `check_is_fitted(self)` within the relevant methods of estimator classes.\n- Ensure that the error message for `NotFittedError` is consistent across the codebase. If custom messages are necessary, use the `msg` parameter of `check_is_fitted` to provide specific error details.\n- Remove any redundant error handling code that becomes unnecessary after using `check_is_fitted`.\n- Submit changes for review, ensuring that the refactoring does not alter the functionality of the estimators.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE provides a clear and actionable plan for addressing the issue of inconsistent NotFittedError messages due to manual checks in the scikit-learn codebase. It suggests searching for instances where manual checks are performed, replacing them with the `check_is_fitted` method, ensuring consistent error messages, and removing redundant error handling code. Additionally, the RESPONSE includes specific code snippets that are relevant to the issue, which shows a deep understanding of the problem and the codebase. The only reason it does not score a 10 is that it does not address the 'more clever way' of identifying the places where the method is re-implemented, which was part of the TODO in the PROMPT.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711739935.5452049,
        "result": "- Search for instances in the codebase where manual checks for fitting are performed, typically by checking if certain attributes like `coef_` or `intercept_` are present.\n- Replace these manual checks with calls to `check_is_fitted(self)` within the relevant methods of estimator classes.\n- Ensure that the error message for `NotFittedError` is consistent across the codebase. If custom messages are necessary, use the `msg` parameter of `check_is_fitted` to provide specific error details.\n- Remove any redundant error handling code that becomes unnecessary after using `check_is_fitted`.\n- Submit changes for review, ensuring that the refactoring does not alter the functionality of the estimators.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/utils)- [Lines 3892 - 3980](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/estimator_checks.py#L3892-L3980)\n\n This snippet contains a test that checks the use of 'check_is_fitted' before and after fitting an estimator, which is directly related to the issue of replacing manual checks with 'check_is_fitted'.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/utils)- [Lines 1406 - 1491](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/validation.py#L1406-L1491)\n\n This snippet defines the 'check_is_fitted' function, which should be used instead of manual checks to determine if an estimator has been fitted, as described in the issue."
    }
}