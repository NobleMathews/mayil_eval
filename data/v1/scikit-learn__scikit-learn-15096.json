{
    "id": "scikit-learn__scikit-learn-15096",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
    "task_id": 6109107500760834907,
    "title": "GridSearchCV saves all fitted estimator in cv_results['params'] when params are estimators",
    "body": "#### Description\r\nI use GridSearchCV to optimize the hyperparameters of a pipeline. I set the param grid by inputing transformers or estimators at different steps of the pipeline, following the Pipeline documentation:\r\n\r\n> A step\u2019s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting to None.\r\n\r\nI couldn't figure why dumping cv_results_ would take so much memory on disk. It happens that cv_results_['params'] and all cv_results_['param_*'] objects contains fitted estimators, as much as there are points on my grid.\r\n\r\nThis bug should happen only when n_jobs = 1 (which is my usecase).\r\n\r\nI don't think this is intended (else the arguments and attributes _refit_ and _best_\\__estimator_ wouldn't be used).\r\n\r\nMy guess is that during the grid search, those estimator's aren't cloned before use (which could be a problem if using the same grid search several times, because estimators passed in the next grid would be fitted...).\r\n\r\n#### Version: 0.19.0\r\n\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.22.git",
    "requested_commit": "",
    "mayil_response": "Based on the provided issue summary and code snippets, the following solution can be proposed:\n\n- Ensure that the `cv_results_` attribute does not store fitted estimators within `cv_results_['params']` to reduce memory usage. This can be done by modifying the `evaluate_candidates` function in `_search.py` to only store the parameters used for fitting, not the fitted estimators themselves.\n- Modify the `evaluate_candidates` function to clone the base estimator before fitting it with the current set of parameters. This ensures that the original estimator is not modified during the grid search process.\n- Since the issue occurs when `n_jobs=1`, it is important to verify that the cloning of the estimator is done properly when `n_jobs` is set to 1. This can be checked in the `evaluate_candidates` function where the parallel execution is managed.\n- Update the `best_estimator_` attribute in `_search.py` to ensure that it is a clone of the estimator with the best found parameters, not a fitted estimator. This is to prevent the `best_estimator_` from being a reference to a fitted estimator which could lead to high memory usage when dumping `cv_results_`.\n- Since the issue mentions version 0.19.0, ensure that the solution is compatible with this version or recommend an upgrade if the solution requires a newer version of the library.\n\nHere is a concise solution:\n\n- In `_search.py`, within the `evaluate_candidates` function, modify the code to store only parameter values, not fitted estimators, in `cv_results_['params']`.\n- Ensure that `clone(base_estimator)` is called before fitting the estimator with parameters within the `evaluate_candidates` function.\n- In the `GridSearchCV` class, ensure that `best_estimator_` is set to a clone of the base estimator with the best parameters applied, not the actual fitted estimator.\n- Test the changes with `n_jobs=1` to confirm that the memory usage issue is resolved.\n- If the issue persists or if the changes are not compatible with version 0.19.0, consider recommending an upgrade to a newer version where the issue might be already addressed.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [model_selection](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/model_selection)- [Lines 1060 - 1808](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/model_selection/_search.py#L1060-L1808)\n\n This snippet contains the definition of GridSearchCV, including the 'cv_results_' attribute where the issue with storing fitted estimators is occurring. \n\n\n   - [Lines 910 - 951](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/model_selection/_search.py#L910-L951)\n\n This snippet includes the code where the best estimator is cloned and refitted, which is relevant to the issue of fitted estimators being stored in 'cv_results_'. \n\n\n   - [Lines 828 - 898](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/model_selection/_search.py#L828-L898)\n\n This snippet shows the evaluation of candidates within GridSearchCV, which may be relevant to understanding how the fitted estimators are handled during the search process. \n\n\n   - [Lines 1386 - 1422](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/model_selection/_search.py#L1386-L1422)\n\n This snippet shows the initialization of GridSearchCV, which may be relevant to the issue if the problem is related to how the 'param_grid' is being handled.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0003105,
        "snippet_processor": 0.11024,
        "issue_star_creation": 0.03695,
        "issue_star_solver": 0.1084,
        "bouncer": 0.03046
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711738738.605682,
        "relevant_snippets": [
            {
                "code": "\"\"\"Exhaustive search over specified parameter values for an estimator.\n\n    Important members are fit, predict.\n\n    GridSearchCV implements a \"fit\" and a \"score\" method.\n    It also implements \"score_samples\", \"predict\", \"predict_proba\",\n    \"decision_function\", \"transform\" and \"inverse_transform\" if they are\n    implemented in the estimator used.\n\n    The parameters of the estimator used to apply these methods are optimized\n    by cross-validated grid-search over a parameter grid.\n\n    Read more in the :ref:`User Guide <grid_search>`.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        This is assumed to implement the scikit-learn estimator interface.\n        Either estimator needs to provide a ``score`` function,\n        or ``scoring`` must be passed.\n\n    param_grid : dict or list of dictionaries\n        Dictionary with parameters names (`str`) as keys and lists of\n        parameter settings to try as values, or a list of such\n        dictionaries, in which case the grids spanned by each dictionary\n        in the list are explored. This enables searching over any sequence\n        of parameter settings.\n\n    scoring : str, callable, list, tuple or dict, default=None\n        Strategy to evaluate the performance of the cross-validated model on\n        the test set.\n\n        If `scoring` represents a single score, one can use:\n\n        - a single string (see :ref:`scoring_parameter`);\n        - a callable (see :ref:`scoring`) that returns a single value.\n\n        If `scoring` represents multiple scores, one can use:\n\n        - a list or tuple of unique strings;\n        - a callable returning a dictionary where the keys are the metric\n          names and the values are the metric scores;\n        - a dictionary with metric names as keys and callables a values.\n\n        See :ref:`multimetric_grid_search` for an example.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionchanged:: v0.20\n           `n_jobs` default changed from 1 to None\n\n    refit : bool, str, or callable, default=True\n        Refit an estimator using the best found parameters on the whole\n        dataset.\n\n        For multiple metric evaluation, this needs to be a `str` denoting the\n        scorer that would be used to find the best parameters for refitting\n        the estimator at the end.\n\n        Where there are considerations other than maximum score in\n        choosing a best estimator, ``refit`` can be set to a function which\n        returns the selected ``best_index_`` given ``cv_results_``. In that\n        case, the ``best_estimator_`` and ``best_params_`` will be set\n        according to the returned ``best_index_`` while the ``best_score_``\n        attribute will not be available.\n\n        The refitted estimator is made available at the ``best_estimator_``\n        attribute and permits using ``predict`` directly on this\n        ``GridSearchCV`` instance.\n\n        Also for multiple metric evaluation, the attributes ``best_index_``,\n        ``best_score_`` and ``best_params_`` will only be available if\n        ``refit`` is set and all of them will be determined w.r.t this specific\n        scorer.\n\n        See ``scoring`` parameter to know more about multiple metric\n        evaluation.\n\n        See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`\n        to see how to design a custom selection strategy using a callable\n        via `refit`.\n\n        .. versionchanged:: 0.20\n            Support for callable added.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used. These splitters are instantiated\n        with `shuffle=False` so the splits will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    verbose : int\n        Controls the verbosity: the higher, the more messages.\n\n        - >1 : the computation time for each fold and parameter candidate is\n          displayed;\n        - >2 : the score is also displayed;\n        - >3 : the fold and candidate parameter indexes are also displayed\n          together with the starting time of the computation.\n\n    pre_dispatch : int, or str, default='2*n_jobs'\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A str, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    error_score : 'raise' or numeric, default=np.nan\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised. If a numeric value is given,\n        FitFailedWarning is raised. This parameter does not affect the refit\n        step, which will always raise the error.\n\n    return_train_score : bool, default=False\n        If ``False``, the ``cv_results_`` attribute will not include training\n        scores.\n        Computing training scores is used to get insights on how different\n        parameter settings impact the overfitting/underfitting trade-off.\n        However computing the scores on the training set can be computationally\n        expensive and is not strictly required to select the parameters that\n        yield the best generalization performance.\n\n        .. versionadded:: 0.19\n\n        .. versionchanged:: 0.21\n            Default value was changed from ``True`` to ``False``\n\n    Attributes\n    ----------\n    cv_results_ : dict of numpy (masked) ndarrays\n        A dict with keys as column headers and values as columns, that can be\n        imported into a pandas ``DataFrame``.\n\n        For instance the below given table\n\n        +------------+-----------+------------+-----------------+---+---------+\n        |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n        +============+===========+============+=================+===+=========+\n        |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n        +------------+-----------+------------+-----------------+---+---------+\n\n        will be represented by a ``cv_results_`` dict of::\n\n            {\n            'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n                                         mask = [False False False False]...)\n            'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n                                        mask = [ True  True False False]...),\n            'param_degree': masked_array(data = [2.0 3.0 -- --],\n                                         mask = [False False  True  True]...),\n            'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n            'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n            'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n            'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n            'rank_test_score'    : [2, 4, 3, 1],\n            'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n            'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n            'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n            'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n            'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n            'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n            'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n            'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n            'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n            }\n\n        NOTE\n\n        The key ``'params'`` is used to store a list of parameter\n        settings dicts for all the parameter candidates.\n\n        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n        ``std_score_time`` are all in seconds.\n\n        For multi-metric evaluation, the scores for all the scorers are\n        available in the ``cv_results_`` dict at the keys ending with that\n        scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n        above. ('split0_test_precision', 'mean_train_precision' etc.)\n\n    best_estimator_ : estimator\n        Estimator that was chosen by the search, i.e. estimator\n        which gave highest score (or smallest loss if specified)\n        on the left out data. Not available if ``refit=False``.\n\n        See ``refit`` parameter for more information on allowed values.\n\n    best_score_ : float\n        Mean cross-validated score of the best_estimator\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n        This attribute is not available if ``refit`` is a function.\n\n    best_params_ : dict\n        Parameter setting that gave the best results on the hold out data.\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n    best_index_ : int\n        The index (of the ``cv_results_`` arrays) which corresponds to the best\n        candidate parameter setting.\n\n        The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n        the parameter setting for the best model, that gives the highest\n        mean score (``search.best_score_``).\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n    scorer_ : function or a dict\n        Scorer function used on the held out data to choose the best\n        parameters for the model.\n\n        For multi-metric evaluation, this attribute holds the validated\n        ``scoring`` dict which maps the scorer key to the scorer callable.\n\n    n_splits_ : int\n        The number of cross-validation splits (folds/iterations).\n\n    refit_time_ : float\n        Seconds used for refitting the best model on the whole dataset.\n\n        This is present only if ``refit`` is not False.\n\n        .. versionadded:: 0.20\n\n    multimetric_ : bool\n        Whether or not the scorers compute several metrics.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels. This is present only if ``refit`` is specified and\n        the underlying estimator is a classifier.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if\n        `best_estimator_` is defined (see the documentation for the `refit`\n        parameter for more details) and that `best_estimator_` exposes\n        `n_features_in_` when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if\n        `best_estimator_` is defined (see the documentation for the `refit`\n        parameter for more details) and that `best_estimator_` exposes\n        `feature_names_in_` when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    ParameterGrid : Generates all the combinations of a hyperparameter grid.\n    train_test_split : Utility function to split the data into a development\n        set usable for fitting a GridSearchCV instance and an evaluation set\n        for its final evaluation.\n    sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n        loss function.\n\n    Notes\n    -----\n    The parameters selected are those that maximize the score of the left out\n    data, unless an explicit score is passed in which case it is used instead.\n\n    If `n_jobs` was set to a value higher than one, the data is copied for each\n    point in the grid (and not `n_jobs` times). This is done for efficiency\n    reasons if individual jobs take very little time, but may raise errors if\n    the dataset is large and not enough memory is available.  A workaround in\n    this case is to set `pre_dispatch`. Then, the memory is copied only\n    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n    n_jobs`.\n\n    Examples\n    --------\n    >>> from sklearn import svm, datasets\n    >>> from sklearn.model_selection import GridSearchCV\n    >>> iris = datasets.load_iris()\n    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n    >>> svc = svm.SVC()\n    >>> clf = GridSearchCV(svc, parameters)\n    >>> clf.fit(iris.data, iris.target)\n    GridSearchCV(estimator=SVC(),\n                 param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n    >>> sorted(clf.cv_results_.keys())\n    ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n     'param_C', 'param_kernel', 'params',...\n     'rank_test_score', 'split0_test_score',...\n     'split2_test_score', ...\n     'std_fit_time', 'std_score_time', 'std_test_score']\n    \"\"\"",
                "filename": "sklearn/model_selection/_search.py",
                "start_index": 40693,
                "end_index": 54899,
                "start_line": 1060,
                "end_line": 1808,
                "max_line": 1813,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "all_candidate_params = []\n            all_out = []\n            all_more_results = defaultdict(list)\n\n            def evaluate_candidates(candidate_params, cv=None, more_results=None):\n                cv = cv or cv_orig\n                candidate_params = list(candidate_params)\n                n_candidates = len(candidate_params)\n\n                if self.verbose > 0:\n                    print(\n                        \"Fitting {0} folds for each of {1} candidates,\"\n                        \" totalling {2} fits\".format(\n                            n_splits, n_candidates, n_candidates * n_splits\n                        )\n                    )\n\n                out = parallel(\n                    delayed(_fit_and_score)(\n                        clone(base_estimator),\n                        X,\n                        y,\n                        train=train,\n                        test=test,\n                        parameters=parameters,\n                        split_progress=(split_idx, n_splits),\n                        candidate_progress=(cand_idx, n_candidates),\n                        **fit_and_score_kwargs,\n                    )\n                    for (cand_idx, parameters), (split_idx, (train, test)) in product(\n                        enumerate(candidate_params), enumerate(cv.split(X, y, groups))\n                    )\n                )\n\n                if len(out) < 1:\n                    raise ValueError(\n                        \"No fits were performed. \"\n                        \"Was the CV iterator empty? \"\n                        \"Were there no candidates?\"\n                    )\n                elif len(out) != n_candidates * n_splits:\n                    raise ValueError(\n                        \"cv.split and cv.get_n_splits returned \"\n                        \"inconsistent results. Expected {} \"\n                        \"splits, got {}\".format(n_splits, len(out) // n_candidates)\n                    )\n\n                _warn_or_raise_about_fit_failures(out, self.error_score)\n\n                # For callable self.scoring, the return type is only know after\n                # calling. If the return type is a dictionary, the error scores\n                # can now be inserted with the correct key. The type checking\n                # of out will be done in `_insert_error_scores`.\n                if callable(self.scoring):\n                    _insert_error_scores(out, self.error_score)\n\n                all_candidate_params.extend(candidate_params)\n                all_out.extend(out)\n\n                if more_results is not None:\n                    for key, value in more_results.items():\n                        all_more_results[key].extend(value)\n\n                nonlocal results\n                results = self._format_results(\n                    all_candidate_params, n_splits, all_out, all_more_results\n                )\n\n                return results\n\n            self._run_search(evaluate_candidates)",
                "filename": "sklearn/model_selection/_search.py",
                "start_index": 30860,
                "end_index": 33807,
                "start_line": 828,
                "end_line": 898,
                "max_line": 1813,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "# Caching transformers within a ``Pipeline``\n###############################################################################\n# It is sometimes worthwhile storing the state of a specific transformer\n# since it could be used again. Using a pipeline in ``GridSearchCV`` triggers\n# such situations. Therefore, we use the argument ``memory`` to enable caching.\n#\n# .. warning::\n#     Note that this example is, however, only an illustration since for this\n#     specific case fitting PCA is not necessarily slower than loading the\n#     cache. Hence, use the ``memory`` constructor parameter when the fitting\n#     of a transformer is costly.\n\nfrom shutil import rmtree\n\nfrom joblib import Memory\n\n# Create a temporary folder to store the transformers of the pipeline\nlocation = \"cachedir\"\nmemory = Memory(location=location, verbose=10)\ncached_pipe = Pipeline(\n    [(\"reduce_dim\", PCA()), (\"classify\", LinearSVC(dual=False, max_iter=10000))],\n    memory=memory,\n)\n\n# This time, a cached pipeline will be used within the grid search\n\n\n# Delete the temporary cache before exiting\nmemory.clear(warn=False)\nrmtree(location)\n\n# %%\n# The ``PCA`` fitting is only computed at the evaluation of the first\n# configuration of the ``C`` parameter of the ``LinearSVC`` classifier. The\n# other configurations of ``C`` will trigger the loading of the cached ``PCA``\n# estimator data, leading to save processing time. Therefore, the use of\n# caching the pipeline using ``memory`` is highly beneficial when fitting\n# a transformer is costly.",
                "filename": "examples/compose/plot_compare_reduction.py",
                "start_index": 2978,
                "end_index": 4497,
                "start_line": 95,
                "end_line": 132,
                "max_line": 132,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_dont_overwrite_parameters(name, estimator_orig):\n    # check that fit method only changes or sets private attributes\n    if hasattr(estimator_orig.__init__, \"deprecated_original\"):\n        # to not check deprecated classes\n        return\n    estimator = clone(estimator_orig)\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20, 3))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    y = X[:, 0].astype(int)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n    dict_before_fit = estimator.__dict__.copy()\n    estimator.fit(X, y)\n\n    dict_after_fit = estimator.__dict__\n\n    public_keys_after_fit = [\n        key for key in dict_after_fit.keys() if _is_public_parameter(key)\n    ]\n\n    attrs_added_by_fit = [\n        key for key in public_keys_after_fit if key not in dict_before_fit.keys()\n    ]\n\n    # check that fit doesn't add any public attribute\n    assert not attrs_added_by_fit, (\n        \"Estimator adds public attribute(s) during\"\n        \" the fit method.\"\n        \" Estimators are only allowed to add private attributes\"\n        \" either started with _ or ended\"\n        \" with _ but %s added\"\n        % \", \".join(attrs_added_by_fit)\n    )\n\n    # check that fit doesn't change any public attribute\n    attrs_changed_by_fit = [\n        key\n        for key in public_keys_after_fit\n        if (dict_before_fit[key] is not dict_after_fit[key])\n    ]\n\n    assert not attrs_changed_by_fit, (\n        \"Estimator changes public attribute(s) during\"\n        \" the fit method. Estimators are only allowed\"\n        \" to change attributes started\"\n        \" or ended with _, but\"\n        \" %s changed\"\n        % \", \".join(attrs_changed_by_fit)\n    )\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_fit2d_predict1d(name, estimator_orig):\n    # check by fitting a 2d array and predicting with a 1d array\n    rnd = np.random.RandomState(0)\n    X = 3 * rnd.uniform(size=(20, 3))\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    y = X[:, 0].astype(int)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    if hasattr(estimator, \"n_components\"):\n        estimator.n_components = 1\n    if hasattr(estimator, \"n_clusters\"):\n        estimator.n_clusters = 1\n\n    set_random_state(estimator, 1)\n    estimator.fit(X, y)\n\n    for method in [\"predict\", \"transform\", \"decision_function\", \"predict_proba\"]:\n        if hasattr(estimator, method):\n            assert_raise_message(\n                ValueError, \"Reshape your data\", getattr(estimator, method), X[0]\n            )",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 48853,
                "end_index": 51626,
                "start_line": 181,
                "end_line": 4630,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "# For multi-metric evaluation, store the best_index_, best_params_ and\n        # best_score_ iff refit is one of the scorer names\n        # In single metric evaluation, refit_metric is \"score\"\n        if self.refit or not self.multimetric_:\n            self.best_index_ = self._select_best_index(\n                self.refit, refit_metric, results\n            )\n            if not callable(self.refit):\n                # With a non-custom callable, we can select the best score\n                # based on the best index\n                self.best_score_ = results[f\"mean_test_{refit_metric}\"][\n                    self.best_index_\n                ]\n            self.best_params_ = results[\"params\"][self.best_index_]\n\n        if self.refit:\n            # here we clone the estimator as well as the parameters, since\n            # sometimes the parameters themselves might be estimators, e.g.\n            # when we search over different estimators in a pipeline.\n            # ref: https://github.com/scikit-learn/scikit-learn/pull/26786\n            self.best_estimator_ = clone(base_estimator).set_params(\n                **clone(self.best_params_, safe=False)\n            )\n\n            refit_start_time = time.time()\n            if y is not None:\n                self.best_estimator_.fit(X, y, **fit_params)\n            else:\n                self.best_estimator_.fit(X, **fit_params)\n            refit_end_time = time.time()\n            self.refit_time_ = refit_end_time - refit_start_time\n\n            if hasattr(self.best_estimator_, \"feature_names_in_\"):\n                self.feature_names_in_ = self.best_estimator_.feature_names_in_\n\n        # Store the only scorer not as a dict for single metric evaluation\n        self.scorer_ = scorers\n\n        self.cv_results_ = results\n        self.n_splits_ = n_splits\n\n        return self",
                "filename": "sklearn/model_selection/_search.py",
                "start_index": 34342,
                "end_index": 36177,
                "start_line": 910,
                "end_line": 951,
                "max_line": 1813,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "estimator = _construct_instance(Estimator)\n        # test cloning\n        clone(estimator)\n        # test __repr__\n        repr(estimator)\n        # test that set_params returns self\n        assert estimator.set_params() is estimator\n\n        # test if init does nothing but set parameters\n        # this is important for grid_search etc.\n        # We get the default parameters from init and then\n        # compare these against the actual values of the attributes.\n\n        # this comes from getattr. Gets rid of deprecation decorator.\n        init = getattr(estimator.__init__, \"deprecated_original\", estimator.__init__)\n\n        try:\n\n            def param_filter(p):\n                \"\"\"Identify hyper parameters of an estimator.\"\"\"\n                return (\n                    p.name != \"self\"\n                    and p.kind != p.VAR_KEYWORD\n                    and p.kind != p.VAR_POSITIONAL\n                )\n\n            init_params = [\n                p for p in signature(init).parameters.values() if param_filter(p)\n            ]\n\n        except (TypeError, ValueError):\n            # init is not a python function.\n            # true for mixins\n            return\n        params = estimator.get_params()\n        # they can need a non-default argument\n        init_params = init_params[len(getattr(estimator, \"_required_parameters\", [])) :]",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 120802,
                "end_index": 122153,
                "start_line": 3440,
                "end_line": 3476,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "_required_parameters = [\"estimator\", \"param_grid\"]\n\n    _parameter_constraints: dict = {\n        **BaseSearchCV._parameter_constraints,\n        \"param_grid\": [dict, list],\n    }\n\n    def __init__(\n        self,\n        estimator,\n        param_grid,\n        *,\n        scoring=None,\n        n_jobs=None,\n        refit=True,\n        cv=None,\n        verbose=0,\n        pre_dispatch=\"2*n_jobs\",\n        error_score=np.nan,\n        return_train_score=False,\n    ):\n        super().__init__(\n            estimator=estimator,\n            scoring=scoring,\n            n_jobs=n_jobs,\n            refit=refit,\n            cv=cv,\n            verbose=verbose,\n            pre_dispatch=pre_dispatch,\n            error_score=error_score,\n            return_train_score=return_train_score,\n        )\n        self.param_grid = param_grid\n\n    def _run_search(self, evaluate_candidates):\n        \"\"\"Search all candidates in param_grid\"\"\"\n        evaluate_candidates(ParameterGrid(self.param_grid))",
                "filename": "sklearn/model_selection/_search.py",
                "start_index": 54905,
                "end_index": 55887,
                "start_line": 1386,
                "end_line": 1422,
                "max_line": 1813,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "invalid_names = [\n        (names[::-1], \"Feature names must be in the same order as they were in fit.\"),\n        (\n            [f\"another_prefix_{i}\" for i in range(n_features)],\n            (\n                \"Feature names unseen at fit time:\\n- another_prefix_0\\n-\"\n                \" another_prefix_1\\n\"\n            ),\n        ),\n        (\n            names[:3],\n            f\"Feature names seen at fit time, yet now missing:\\n- {min(names[3:])}\\n\",\n        ),\n    ]\n    params = {\n        key: value\n        for key, value in estimator.get_params().items()\n        if \"early_stopping\" in key\n    }\n    early_stopping_enabled = any(value is True for value in params.values())\n\n    for invalid_name, additional_message in invalid_names:\n        X_bad = pd.DataFrame(X, columns=invalid_name, copy=False)\n\n        expected_msg = re.escape(\n            \"The feature names should match those that were passed during fit.\\n\"\n            f\"{additional_message}\"\n        )\n        for name, method in check_methods:\n            with raises(\n                ValueError, match=expected_msg, err_msg=f\"{name} did not raise\"\n            ):\n                method(X_bad)\n\n        # partial_fit checks on second call\n        # Do not call partial fit if early_stopping is on\n        if not hasattr(estimator, \"partial_fit\") or early_stopping_enabled:\n            continue\n\n        estimator = clone(estimator_orig)\n        if is_classifier(estimator):\n            classes = np.unique(y)\n            estimator.partial_fit(X, y, classes=classes)\n        else:\n            estimator.partial_fit(X, y)\n\n        with raises(ValueError, match=expected_msg):\n            estimator.partial_fit(X_bad, y)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 146663,
                "end_index": 148346,
                "start_line": 4160,
                "end_line": 4207,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"Randomized search on hyper parameters.\n\n    RandomizedSearchCV implements a \"fit\" and a \"score\" method.\n    It also implements \"score_samples\", \"predict\", \"predict_proba\",\n    \"decision_function\", \"transform\" and \"inverse_transform\" if they are\n    implemented in the estimator used.\n\n    The parameters of the estimator used to apply these methods are optimized\n    by cross-validated search over parameter settings.\n\n    In contrast to GridSearchCV, not all parameter values are tried out, but\n    rather a fixed number of parameter settings is sampled from the specified\n    distributions. The number of parameter settings that are tried is\n    given by n_iter.\n\n    If all parameters are presented as a list,\n    sampling without replacement is performed. If at least one parameter\n    is given as a distribution, sampling with replacement is used.\n    It is highly recommended to use continuous distributions for continuous\n    parameters.\n\n    Read more in the :ref:`User Guide <randomized_parameter_search>`.\n\n    .. versionadded:: 0.14\n\n    Parameters\n    ----------\n    estimator : estimator object\n        An object of that type is instantiated for each grid point.\n        This is assumed to implement the scikit-learn estimator interface.\n        Either estimator needs to provide a ``score`` function,\n        or ``scoring`` must be passed.\n\n    param_distributions : dict or list of dicts\n        Dictionary with parameters names (`str`) as keys and distributions\n        or lists of parameters to try. Distributions must provide a ``rvs``\n        method for sampling (such as those from scipy.stats.distributions).\n        If a list is given, it is sampled uniformly.\n        If a list of dicts is given, first a dict is sampled uniformly, and\n        then a parameter is sampled using that dict as above.\n\n    n_iter : int, default=10\n        Number of parameter settings that are sampled. n_iter trades\n        off runtime vs quality of the solution.\n\n    scoring : str, callable, list, tuple or dict, default=None\n        Strategy to evaluate the performance of the cross-validated model on\n        the test set.\n\n        If `scoring` represents a single score, one can use:\n\n        - a single string (see :ref:`scoring_parameter`);\n        - a callable (see :ref:`scoring`) that returns a single value.\n\n        If `scoring` represents multiple scores, one can use:\n\n        - a list or tuple of unique strings;\n        - a callable returning a dictionary where the keys are the metric\n          names and the values are the metric scores;\n        - a dictionary with metric names as keys and callables a values.\n\n        See :ref:`multimetric_grid_search` for an example.\n\n        If None, the estimator's score method is used.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionchanged:: v0.20\n           `n_jobs` default changed from 1 to None\n\n    refit : bool, str, or callable, default=True\n        Refit an estimator using the best found parameters on the whole\n        dataset.\n\n        For multiple metric evaluation, this needs to be a `str` denoting the\n        scorer that would be used to find the best parameters for refitting\n        the estimator at the end.\n\n        Where there are considerations other than maximum score in\n        choosing a best estimator, ``refit`` can be set to a function which\n        returns the selected ``best_index_`` given the ``cv_results``. In that\n        case, the ``best_estimator_`` and ``best_params_`` will be set\n        according to the returned ``best_index_`` while the ``best_score_``\n        attribute will not be available.\n\n        The refitted estimator is made available at the ``best_estimator_``\n        attribute and permits using ``predict`` directly on this\n        ``RandomizedSearchCV`` instance.\n\n        Also for multiple metric evaluation, the attributes ``best_index_``,\n        ``best_score_`` and ``best_params_`` will only be available if\n        ``refit`` is set and all of them will be determined w.r.t this specific\n        scorer.\n\n        See ``scoring`` parameter to know more about multiple metric\n        evaluation.\n\n        .. versionchanged:: 0.20\n            Support for callable added.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used. These splitters are instantiated\n        with `shuffle=False` so the splits will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    verbose : int\n        Controls the verbosity: the higher, the more messages.\n\n        - >1 : the computation time for each fold and parameter candidate is\n          displayed;\n        - >2 : the score is also displayed;\n        - >3 : the fold and candidate parameter indexes are also displayed\n          together with the starting time of the computation.\n\n    pre_dispatch : int, or str, default='2*n_jobs'\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A str, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    random_state : int, RandomState instance or None, default=None\n        Pseudo random number generator state used for random uniform sampling\n        from lists of possible values instead of scipy.stats distributions.\n        Pass an int for reproducible output across multiple\n        function calls.\n        See :term:`Glossary <random_state>`.\n\n    error_score : 'raise' or numeric, default=np.nan\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised. If a numeric value is given,\n        FitFailedWarning is raised. This parameter does not affect the refit\n        step, which will always raise the error.\n\n    return_train_score : bool, default=False\n        If ``False``, the ``cv_results_`` attribute will not include training\n        scores.\n        Computing training scores is used to get insights on how different\n        parameter settings impact the overfitting/underfitting trade-off.\n        However computing the scores on the training set can be computationally\n        expensive and is not strictly required to select the parameters that\n        yield the best generalization performance.\n\n        .. versionadded:: 0.19\n\n        .. versionchanged:: 0.21\n            Default value was changed from ``True`` to ``False``\n\n    Attributes\n    ----------\n    cv_results_ : dict of numpy (masked) ndarrays\n        A dict with keys as column headers and values as columns, that can be\n        imported into a pandas ``DataFrame``.\n\n        For instance the below given table\n\n        +--------------+-------------+-------------------+---+---------------+\n        | param_kernel | param_gamma | split0_test_score |...|rank_test_score|\n        +==============+=============+===================+===+===============+\n        |    'rbf'     |     0.1     |       0.80        |...|       1       |\n        +--------------+-------------+-------------------+---+---------------+\n        |    'rbf'     |     0.2     |       0.84        |...|       3       |\n        +--------------+-------------+-------------------+---+---------------+\n        |    'rbf'     |     0.3     |       0.70        |...|       2       |\n        +--------------+-------------+-------------------+---+---------------+\n\n        will be represented by a ``cv_results_`` dict of::\n\n            {\n            'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],\n                                          mask = False),\n            'param_gamma'  : masked_array(data = [0.1 0.2 0.3], mask = False),\n            'split0_test_score'  : [0.80, 0.84, 0.70],\n            'split1_test_score'  : [0.82, 0.50, 0.70],\n            'mean_test_score'    : [0.81, 0.67, 0.70],\n            'std_test_score'     : [0.01, 0.24, 0.00],\n            'rank_test_score'    : [1, 3, 2],\n            'split0_train_score' : [0.80, 0.92, 0.70],\n            'split1_train_score' : [0.82, 0.55, 0.70],\n            'mean_train_score'   : [0.81, 0.74, 0.70],\n            'std_train_score'    : [0.01, 0.19, 0.00],\n            'mean_fit_time'      : [0.73, 0.63, 0.43],\n            'std_fit_time'       : [0.01, 0.02, 0.01],\n            'mean_score_time'    : [0.01, 0.06, 0.04],\n            'std_score_time'     : [0.00, 0.00, 0.00],\n            'params'             : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],\n            }\n\n        NOTE\n\n        The key ``'params'`` is used to store a list of parameter\n        settings dicts for all the parameter candidates.\n\n        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n        ``std_score_time`` are all in seconds.\n\n        For multi-metric evaluation, the scores for all the scorers are\n        available in the ``cv_results_`` dict at the keys ending with that\n        scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n        above. ('split0_test_precision', 'mean_train_precision' etc.)\n\n    best_estimator_ : estimator\n        Estimator that was chosen by the search, i.e. estimator\n        which gave highest score (or smallest loss if specified)\n        on the left out data. Not available if ``refit=False``.\n\n        For multi-metric evaluation, this attribute is present only if\n        ``refit`` is specified.\n\n        See ``refit`` parameter for more information on allowed values.\n\n    best_score_ : float\n        Mean cross-validated score of the best_estimator.\n\n        For multi-metric evaluation, this is not available if ``refit`` is\n        ``False``. See ``refit`` parameter for more information.\n\n        This attribute is not available if ``refit`` is a function.\n\n    best_params_ : dict\n        Parameter setting that gave the best results on the hold out data.\n\n        For multi-metric evaluation, this is not available if ``refit`` is\n        ``False``. See ``refit`` parameter for more information.\n\n    best_index_ : int\n        The index (of the ``cv_results_`` arrays) which corresponds to the best\n        candidate parameter setting.\n\n        The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n        the parameter setting for the best model, that gives the highest\n        mean score (``search.best_score_``).\n\n        For multi-metric evaluation, this is not available if ``refit`` is\n        ``False``. See ``refit`` parameter for more information.\n\n    scorer_ : function or a dict\n        Scorer function used on the held out data to choose the best\n        parameters for the model.\n\n        For multi-metric evaluation, this attribute holds the validated\n        ``scoring`` dict which maps the scorer key to the scorer callable.\n\n    n_splits_ : int\n        The number of cross-validation splits (folds/iterations).\n\n    refit_time_ : float\n        Seconds used for refitting the best model on the whole dataset.\n\n        This is present only if ``refit`` is not False.\n\n        .. versionadded:: 0.20\n\n    multimetric_ : bool\n        Whether or not the scorers compute several metrics.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels. This is present only if ``refit`` is specified and\n        the underlying estimator is a classifier.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if\n        `best_estimator_` is defined (see the documentation for the `refit`\n        parameter for more details) and that `best_estimator_` exposes\n        `n_features_in_` when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if\n        `best_estimator_` is defined (see the documentation for the `refit`\n        parameter for more details) and that `best_estimator_` exposes\n        `feature_names_in_` when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    GridSearchCV : Does exhaustive search over a grid of parameters.\n    ParameterSampler : A generator over parameter settings, constructed from\n        param_distributions.\n\n    Notes\n    -----\n    The parameters selected are those that maximize the score of the held-out\n    data, according to the scoring parameter.\n\n    If `n_jobs` was set to a value higher than one, the data is copied for each\n    parameter setting(and not `n_jobs` times). This is done for efficiency\n    reasons if individual jobs take very little time, but may raise errors if\n    the dataset is large and not enough memory is available.  A workaround in\n    this case is to set `pre_dispatch`. Then, the memory is copied only\n    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n    n_jobs`.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.linear_model import LogisticRegression\n    >>> from sklearn.model_selection import RandomizedSearchCV\n    >>> from scipy.stats import uniform\n    >>> iris = load_iris()\n    >>> logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n    ...                               random_state=0)\n    >>> distributions = dict(C=uniform(loc=0, scale=4),\n    ...                      penalty=['l2', 'l1'])\n    >>> clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n    >>> search = clf.fit(iris.data, iris.target)\n    >>> search.best_params_\n    {'C': 2..., 'penalty': 'l1'}\n    \"\"\"",
                "filename": "sklearn/model_selection/_search.py",
                "start_index": 55934,
                "end_index": 70710,
                "start_line": 1426,
                "end_line": 1808,
                "max_line": 1813,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            },
            {
                "code": "\"\"\"Search over specified parameter values with successive halving.\n\n    The search strategy starts evaluating all the candidates with a small\n    amount of resources and iteratively selects the best candidates, using\n    more and more resources.\n\n    Read more in the :ref:`User guide <successive_halving_user_guide>`.\n\n    .. note::\n\n      This estimator is still **experimental** for now: the predictions\n      and the API might change without any deprecation cycle. To use it,\n      you need to explicitly import ``enable_halving_search_cv``::\n\n        >>> # explicitly require this experimental feature\n        >>> from sklearn.experimental import enable_halving_search_cv # noqa\n        >>> # now you can import normally from model_selection\n        >>> from sklearn.model_selection import HalvingGridSearchCV\n\n    Parameters\n    ----------\n    estimator : estimator object\n        This is assumed to implement the scikit-learn estimator interface.\n        Either estimator needs to provide a ``score`` function,\n        or ``scoring`` must be passed.\n\n    param_grid : dict or list of dictionaries\n        Dictionary with parameters names (string) as keys and lists of\n        parameter settings to try as values, or a list of such\n        dictionaries, in which case the grids spanned by each dictionary\n        in the list are explored. This enables searching over any sequence\n        of parameter settings.\n\n    factor : int or float, default=3\n        The 'halving' parameter, which determines the proportion of candidates\n        that are selected for each subsequent iteration. For example,\n        ``factor=3`` means that only one third of the candidates are selected.\n\n    resource : ``'n_samples'`` or str, default='n_samples'\n        Defines the resource that increases with each iteration. By default,\n        the resource is the number of samples. It can also be set to any\n        parameter of the base estimator that accepts positive integer\n        values, e.g. 'n_iterations' or 'n_estimators' for a gradient\n        boosting estimator. In this case ``max_resources`` cannot be 'auto'\n        and must be set explicitly.\n\n    max_resources : int, default='auto'\n        The maximum amount of resource that any candidate is allowed to use\n        for a given iteration. By default, this is set to ``n_samples`` when\n        ``resource='n_samples'`` (default), else an error is raised.\n\n    min_resources : {'exhaust', 'smallest'} or int, default='exhaust'\n        The minimum amount of resource that any candidate is allowed to use\n        for a given iteration. Equivalently, this defines the amount of\n        resources `r0` that are allocated for each candidate at the first\n        iteration.\n\n        - 'smallest' is a heuristic that sets `r0` to a small value:\n\n            - ``n_splits * 2`` when ``resource='n_samples'`` for a regression\n              problem\n            - ``n_classes * n_splits * 2`` when ``resource='n_samples'`` for a\n              classification problem\n            - ``1`` when ``resource != 'n_samples'``\n\n        - 'exhaust' will set `r0` such that the **last** iteration uses as\n          much resources as possible. Namely, the last iteration will use the\n          highest value smaller than ``max_resources`` that is a multiple of\n          both ``min_resources`` and ``factor``. In general, using 'exhaust'\n          leads to a more accurate estimator, but is slightly more time\n          consuming.\n\n        Note that the amount of resources used at each iteration is always a\n        multiple of ``min_resources``.\n\n    aggressive_elimination : bool, default=False\n        This is only relevant in cases where there isn't enough resources to\n        reduce the remaining candidates to at most `factor` after the last\n        iteration. If ``True``, then the search process will 'replay' the\n        first iteration for as long as needed until the number of candidates\n        is small enough. This is ``False`` by default, which means that the\n        last iteration may evaluate more than ``factor`` candidates. See\n        :ref:`aggressive_elimination` for more details.\n\n    cv : int, cross-validation generator or iterable, default=5\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used. These splitters are instantiated\n        with `shuffle=False` so the splits will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. note::\n            Due to implementation details, the folds produced by `cv` must be\n            the same across multiple calls to `cv.split()`. For\n            built-in `scikit-learn` iterators, this can be achieved by\n            deactivating shuffling (`shuffle=False`), or by setting the\n            `cv`'s `random_state` parameter to an integer.\n\n    scoring : str, callable, or None, default=None\n        A single string (see :ref:`scoring_parameter`) or a callable\n        (see :ref:`scoring`) to evaluate the predictions on the test set.\n        If None, the estimator's score method is used.\n\n    refit : bool, default=True\n        If True, refit an estimator using the best found parameters on the\n        whole dataset.\n\n        The refitted estimator is made available at the ``best_estimator_``\n        attribute and permits using ``predict`` directly on this\n        ``HalvingGridSearchCV`` instance.\n\n    error_score : 'raise' or numeric\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised. If a numeric value is given,\n        FitFailedWarning is raised. This parameter does not affect the refit\n        step, which will always raise the error. Default is ``np.nan``.\n\n    return_train_score : bool, default=False\n        If ``False``, the ``cv_results_`` attribute will not include training\n        scores.\n        Computing training scores is used to get insights on how different\n        parameter settings impact the overfitting/underfitting trade-off.\n        However computing the scores on the training set can be computationally\n        expensive and is not strictly required to select the parameters that\n        yield the best generalization performance.\n\n    random_state : int, RandomState instance or None, default=None\n        Pseudo random number generator state used for subsampling the dataset\n        when `resources != 'n_samples'`. Ignored otherwise.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : int\n        Controls the verbosity: the higher, the more messages.\n\n    Attributes\n    ----------\n    n_resources_ : list of int\n        The amount of resources used at each iteration.\n\n    n_candidates_ : list of int\n        The number of candidate parameters that were evaluated at each\n        iteration.\n\n    n_remaining_candidates_ : int\n        The number of candidate parameters that are left after the last\n        iteration. It corresponds to `ceil(n_candidates[-1] / factor)`\n\n    max_resources_ : int\n        The maximum number of resources that any candidate is allowed to use\n        for a given iteration. Note that since the number of resources used\n        at each iteration must be a multiple of ``min_resources_``, the\n        actual number of resources used at the last iteration may be smaller\n        than ``max_resources_``.\n\n    min_resources_ : int\n        The amount of resources that are allocated for each candidate at the\n        first iteration.\n\n    n_iterations_ : int\n        The actual number of iterations that were run. This is equal to\n        ``n_required_iterations_`` if ``aggressive_elimination`` is ``True``.\n        Else, this is equal to ``min(n_possible_iterations_,\n        n_required_iterations_)``.\n\n    n_possible_iterations_ : int\n        The number of iterations that are possible starting with\n        ``min_resources_`` resources and without exceeding\n        ``max_resources_``.\n\n    n_required_iterations_ : int\n        The number of iterations that are required to end up with less than\n        ``factor`` candidates at the last iteration, starting with\n        ``min_resources_`` resources. This will be smaller than\n        ``n_possible_iterations_`` when there isn't enough resources.\n\n    cv_results_ : dict of numpy (masked) ndarrays\n        A dict with keys as column headers and values as columns, that can be\n        imported into a pandas ``DataFrame``. It contains lots of information\n        for analysing the results of a search.\n        Please refer to the :ref:`User guide<successive_halving_cv_results>`\n        for details.\n\n    best_estimator_ : estimator or dict\n        Estimator that was chosen by the search, i.e. estimator\n        which gave highest score (or smallest loss if specified)\n        on the left out data. Not available if ``refit=False``.\n\n    best_score_ : float\n        Mean cross-validated score of the best_estimator.\n\n    best_params_ : dict\n        Parameter setting that gave the best results on the hold out data.\n\n    best_index_ : int\n        The index (of the ``cv_results_`` arrays) which corresponds to the best\n        candidate parameter setting.\n\n        The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n        the parameter setting for the best model, that gives the highest\n        mean score (``search.best_score_``).\n\n    scorer_ : function or a dict\n        Scorer function used on the held out data to choose the best\n        parameters for the model.\n\n    n_splits_ : int\n        The number of cross-validation splits (folds/iterations).\n\n    refit_time_ : float\n        Seconds used for refitting the best model on the whole dataset.\n\n        This is present only if ``refit`` is not False.\n\n    multimetric_ : bool\n        Whether or not the scorers compute several metrics.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels. This is present only if ``refit`` is specified and\n        the underlying estimator is a classifier.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if\n        `best_estimator_` is defined (see the documentation for the `refit`\n        parameter for more details) and that `best_estimator_` exposes\n        `n_features_in_` when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if\n        `best_estimator_` is defined (see the documentation for the `refit`\n        parameter for more details) and that `best_estimator_` exposes\n        `feature_names_in_` when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    :class:`HalvingRandomSearchCV`:\n        Random search over a set of parameters using successive halving.\n\n    Notes\n    -----\n    The parameters selected are those that maximize the score of the held-out\n    data, according to the scoring parameter.\n\n    All parameter combinations scored with a NaN will share the lowest rank.\n\n    Examples\n    --------\n\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> from sklearn.experimental import enable_halving_search_cv  # noqa\n    >>> from sklearn.model_selection import HalvingGridSearchCV\n    ...\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = RandomForestClassifier(random_state=0)\n    ...\n    >>> param_grid = {\"max_depth\": [3, None],\n    ...               \"min_samples_split\": [5, 10]}\n    >>> search = HalvingGridSearchCV(clf, param_grid, resource='n_estimators',\n    ...                              max_resources=10,\n    ...                              random_state=0).fit(X, y)\n    >>> search.best_params_  # doctest: +SKIP\n    {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 9}\n    \"\"\"",
                "filename": "sklearn/model_selection/_search_successive_halving.py",
                "start_index": 14818,
                "end_index": 27400,
                "start_line": 391,
                "end_line": 1021,
                "max_line": 1083,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": ""
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/model_selection/_search.py": [
                {
                    "chunk": {
                        "code": "\"\"\"Exhaustive search over specified parameter values for an estimator.\n\n    Important members are fit, predict.\n\n    GridSearchCV implements a \"fit\" and a \"score\" method.\n    It also implements \"score_samples\", \"predict\", \"predict_proba\",\n    \"decision_function\", \"transform\" and \"inverse_transform\" if they are\n    implemented in the estimator used.\n\n    The parameters of the estimator used to apply these methods are optimized\n    by cross-validated grid-search over a parameter grid.\n\n    Read more in the :ref:`User Guide <grid_search>`.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        This is assumed to implement the scikit-learn estimator interface.\n        Either estimator needs to provide a ``score`` function,\n        or ``scoring`` must be passed.\n\n    param_grid : dict or list of dictionaries\n        Dictionary with parameters names (`str`) as keys and lists of\n        parameter settings to try as values, or a list of such\n        dictionaries, in which case the grids spanned by each dictionary\n        in the list are explored. This enables searching over any sequence\n        of parameter settings.\n\n    scoring : str, callable, list, tuple or dict, default=None\n        Strategy to evaluate the performance of the cross-validated model on\n        the test set.\n\n        If `scoring` represents a single score, one can use:\n\n        - a single string (see :ref:`scoring_parameter`);\n        - a callable (see :ref:`scoring`) that returns a single value.\n\n        If `scoring` represents multiple scores, one can use:\n\n        - a list or tuple of unique strings;\n        - a callable returning a dictionary where the keys are the metric\n          names and the values are the metric scores;\n        - a dictionary with metric names as keys and callables a values.\n\n        See :ref:`multimetric_grid_search` for an example.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionchanged:: v0.20\n           `n_jobs` default changed from 1 to None\n\n    refit : bool, str, or callable, default=True\n        Refit an estimator using the best found parameters on the whole\n        dataset.\n\n        For multiple metric evaluation, this needs to be a `str` denoting the\n        scorer that would be used to find the best parameters for refitting\n        the estimator at the end.\n\n        Where there are considerations other than maximum score in\n        choosing a best estimator, ``refit`` can be set to a function which\n        returns the selected ``best_index_`` given ``cv_results_``. In that\n        case, the ``best_estimator_`` and ``best_params_`` will be set\n        according to the returned ``best_index_`` while the ``best_score_``\n        attribute will not be available.\n\n        The refitted estimator is made available at the ``best_estimator_``\n        attribute and permits using ``predict`` directly on this\n        ``GridSearchCV`` instance.\n\n        Also for multiple metric evaluation, the attributes ``best_index_``,\n        ``best_score_`` and ``best_params_`` will only be available if\n        ``refit`` is set and all of them will be determined w.r.t this specific\n        scorer.\n\n        See ``scoring`` parameter to know more about multiple metric\n        evaluation.\n\n        See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`\n        to see how to design a custom selection strategy using a callable\n        via `refit`.\n\n        .. versionchanged:: 0.20\n            Support for callable added.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used. These splitters are instantiated\n        with `shuffle=False` so the splits will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    verbose : int\n        Controls the verbosity: the higher, the more messages.\n\n        - >1 : the computation time for each fold and parameter candidate is\n          displayed;\n        - >2 : the score is also displayed;\n        - >3 : the fold and candidate parameter indexes are also displayed\n          together with the starting time of the computation.\n\n    pre_dispatch : int, or str, default='2*n_jobs'\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A str, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    error_score : 'raise' or numeric, default=np.nan\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised. If a numeric value is given,\n        FitFailedWarning is raised. This parameter does not affect the refit\n        step, which will always raise the error.\n\n    return_train_score : bool, default=False\n        If ``False``, the ``cv_results_`` attribute will not include training\n        scores.\n        Computing training scores is used to get insights on how different\n        parameter settings impact the overfitting/underfitting trade-off.\n        However computing the scores on the training set can be computationally\n        expensive and is not strictly required to select the parameters that\n        yield the best generalization performance.\n\n        .. versionadded:: 0.19\n\n        .. versionchanged:: 0.21\n            Default value was changed from ``True`` to ``False``\n\n    Attributes\n    ----------\n    cv_results_ : dict of numpy (masked) ndarrays\n        A dict with keys as column headers and values as columns, that can be\n        imported into a pandas ``DataFrame``.\n\n        For instance the below given table\n\n        +------------+-----------+------------+-----------------+---+---------+\n        |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n        +============+===========+============+=================+===+=========+\n        |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n        +------------+-----------+------------+-----------------+---+---------+\n\n        will be represented by a ``cv_results_`` dict of::\n\n            {\n            'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n                                         mask = [False False False False]...)\n            'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n                                        mask = [ True  True False False]...),\n            'param_degree': masked_array(data = [2.0 3.0 -- --],\n                                         mask = [False False  True  True]...),\n            'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n            'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n            'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n            'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n            'rank_test_score'    : [2, 4, 3, 1],\n            'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n            'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n            'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n            'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n            'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n            'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n            'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n            'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n            'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n            }\n\n        NOTE\n\n        The key ``'params'`` is used to store a list of parameter\n        settings dicts for all the parameter candidates.\n\n        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n        ``std_score_time`` are all in seconds.\n\n        For multi-metric evaluation, the scores for all the scorers are\n        available in the ``cv_results_`` dict at the keys ending with that\n        scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n        above. ('split0_test_precision', 'mean_train_precision' etc.)\n\n    best_estimator_ : estimator\n        Estimator that was chosen by the search, i.e. estimator\n        which gave highest score (or smallest loss if specified)\n        on the left out data. Not available if ``refit=False``.\n\n        See ``refit`` parameter for more information on allowed values.\n\n    best_score_ : float\n        Mean cross-validated score of the best_estimator\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n        This attribute is not available if ``refit`` is a function.\n\n    best_params_ : dict\n        Parameter setting that gave the best results on the hold out data.\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n    best_index_ : int\n        The index (of the ``cv_results_`` arrays) which corresponds to the best\n        candidate parameter setting.\n\n        The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n        the parameter setting for the best model, that gives the highest\n        mean score (``search.best_score_``).\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n    scorer_ : function or a dict\n        Scorer function used on the held out data to choose the best\n        parameters for the model.\n\n        For multi-metric evaluation, this attribute holds the validated\n        ``scoring`` dict which maps the scorer key to the scorer callable.\n\n    n_splits_ : int\n        The number of cross-validation splits (folds/iterations).\n\n    refit_time_ : float\n        Seconds used for refitting the best model on the whole dataset.\n\n        This is present only if ``refit`` is not False.\n\n        .. versionadded:: 0.20\n\n    multimetric_ : bool\n        Whether or not the scorers compute several metrics.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels. This is present only if ``refit`` is specified and\n        the underlying estimator is a classifier.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if\n        `best_estimator_` is defined (see the documentation for the `refit`\n        parameter for more details) and that `best_estimator_` exposes\n        `n_features_in_` when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if\n        `best_estimator_` is defined (see the documentation for the `refit`\n        parameter for more details) and that `best_estimator_` exposes\n        `feature_names_in_` when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    ParameterGrid : Generates all the combinations of a hyperparameter grid.\n    train_test_split : Utility function to split the data into a development\n        set usable for fitting a GridSearchCV instance and an evaluation set\n        for its final evaluation.\n    sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n        loss function.\n\n    Notes\n    -----\n    The parameters selected are those that maximize the score of the left out\n    data, unless an explicit score is passed in which case it is used instead.\n\n    If `n_jobs` was set to a value higher than one, the data is copied for each\n    point in the grid (and not `n_jobs` times). This is done for efficiency\n    reasons if individual jobs take very little time, but may raise errors if\n    the dataset is large and not enough memory is available.  A workaround in\n    this case is to set `pre_dispatch`. Then, the memory is copied only\n    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n    n_jobs`.\n\n    Examples\n    --------\n    >>> from sklearn import svm, datasets\n    >>> from sklearn.model_selection import GridSearchCV\n    >>> iris = datasets.load_iris()\n    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n    >>> svc = svm.SVC()\n    >>> clf = GridSearchCV(svc, parameters)\n    >>> clf.fit(iris.data, iris.target)\n    GridSearchCV(estimator=SVC(),\n                 param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n    >>> sorted(clf.cv_results_.keys())\n    ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n     'param_C', 'param_kernel', 'params',...\n     'rank_test_score', 'split0_test_score',...\n     'split2_test_score', ...\n     'std_fit_time', 'std_score_time', 'std_test_score']\n    \"\"\"",
                        "filename": "sklearn/model_selection/_search.py",
                        "start_index": 40693,
                        "end_index": 54899,
                        "start_line": 1060,
                        "end_line": 1808,
                        "max_line": 1813,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the definition of GridSearchCV, including the 'cv_results_' attribute where the issue with storing fitted estimators is occurring."
                },
                {
                    "chunk": {
                        "code": "# For multi-metric evaluation, store the best_index_, best_params_ and\n        # best_score_ iff refit is one of the scorer names\n        # In single metric evaluation, refit_metric is \"score\"\n        if self.refit or not self.multimetric_:\n            self.best_index_ = self._select_best_index(\n                self.refit, refit_metric, results\n            )\n            if not callable(self.refit):\n                # With a non-custom callable, we can select the best score\n                # based on the best index\n                self.best_score_ = results[f\"mean_test_{refit_metric}\"][\n                    self.best_index_\n                ]\n            self.best_params_ = results[\"params\"][self.best_index_]\n\n        if self.refit:\n            # here we clone the estimator as well as the parameters, since\n            # sometimes the parameters themselves might be estimators, e.g.\n            # when we search over different estimators in a pipeline.\n            # ref: https://github.com/scikit-learn/scikit-learn/pull/26786\n            self.best_estimator_ = clone(base_estimator).set_params(\n                **clone(self.best_params_, safe=False)\n            )\n\n            refit_start_time = time.time()\n            if y is not None:\n                self.best_estimator_.fit(X, y, **fit_params)\n            else:\n                self.best_estimator_.fit(X, **fit_params)\n            refit_end_time = time.time()\n            self.refit_time_ = refit_end_time - refit_start_time\n\n            if hasattr(self.best_estimator_, \"feature_names_in_\"):\n                self.feature_names_in_ = self.best_estimator_.feature_names_in_\n\n        # Store the only scorer not as a dict for single metric evaluation\n        self.scorer_ = scorers\n\n        self.cv_results_ = results\n        self.n_splits_ = n_splits\n\n        return self",
                        "filename": "sklearn/model_selection/_search.py",
                        "start_index": 34342,
                        "end_index": 36177,
                        "start_line": 910,
                        "end_line": 951,
                        "max_line": 1813,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes the code where the best estimator is cloned and refitted, which is relevant to the issue of fitted estimators being stored in 'cv_results_'."
                },
                {
                    "chunk": {
                        "code": "all_candidate_params = []\n            all_out = []\n            all_more_results = defaultdict(list)\n\n            def evaluate_candidates(candidate_params, cv=None, more_results=None):\n                cv = cv or cv_orig\n                candidate_params = list(candidate_params)\n                n_candidates = len(candidate_params)\n\n                if self.verbose > 0:\n                    print(\n                        \"Fitting {0} folds for each of {1} candidates,\"\n                        \" totalling {2} fits\".format(\n                            n_splits, n_candidates, n_candidates * n_splits\n                        )\n                    )\n\n                out = parallel(\n                    delayed(_fit_and_score)(\n                        clone(base_estimator),\n                        X,\n                        y,\n                        train=train,\n                        test=test,\n                        parameters=parameters,\n                        split_progress=(split_idx, n_splits),\n                        candidate_progress=(cand_idx, n_candidates),\n                        **fit_and_score_kwargs,\n                    )\n                    for (cand_idx, parameters), (split_idx, (train, test)) in product(\n                        enumerate(candidate_params), enumerate(cv.split(X, y, groups))\n                    )\n                )\n\n                if len(out) < 1:\n                    raise ValueError(\n                        \"No fits were performed. \"\n                        \"Was the CV iterator empty? \"\n                        \"Were there no candidates?\"\n                    )\n                elif len(out) != n_candidates * n_splits:\n                    raise ValueError(\n                        \"cv.split and cv.get_n_splits returned \"\n                        \"inconsistent results. Expected {} \"\n                        \"splits, got {}\".format(n_splits, len(out) // n_candidates)\n                    )\n\n                _warn_or_raise_about_fit_failures(out, self.error_score)\n\n                # For callable self.scoring, the return type is only know after\n                # calling. If the return type is a dictionary, the error scores\n                # can now be inserted with the correct key. The type checking\n                # of out will be done in `_insert_error_scores`.\n                if callable(self.scoring):\n                    _insert_error_scores(out, self.error_score)\n\n                all_candidate_params.extend(candidate_params)\n                all_out.extend(out)\n\n                if more_results is not None:\n                    for key, value in more_results.items():\n                        all_more_results[key].extend(value)\n\n                nonlocal results\n                results = self._format_results(\n                    all_candidate_params, n_splits, all_out, all_more_results\n                )\n\n                return results\n\n            self._run_search(evaluate_candidates)",
                        "filename": "sklearn/model_selection/_search.py",
                        "start_index": 30860,
                        "end_index": 33807,
                        "start_line": 828,
                        "end_line": 898,
                        "max_line": 1813,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the evaluation of candidates within GridSearchCV, which may be relevant to understanding how the fitted estimators are handled during the search process."
                },
                {
                    "chunk": {
                        "code": "_required_parameters = [\"estimator\", \"param_grid\"]\n\n    _parameter_constraints: dict = {\n        **BaseSearchCV._parameter_constraints,\n        \"param_grid\": [dict, list],\n    }\n\n    def __init__(\n        self,\n        estimator,\n        param_grid,\n        *,\n        scoring=None,\n        n_jobs=None,\n        refit=True,\n        cv=None,\n        verbose=0,\n        pre_dispatch=\"2*n_jobs\",\n        error_score=np.nan,\n        return_train_score=False,\n    ):\n        super().__init__(\n            estimator=estimator,\n            scoring=scoring,\n            n_jobs=n_jobs,\n            refit=refit,\n            cv=cv,\n            verbose=verbose,\n            pre_dispatch=pre_dispatch,\n            error_score=error_score,\n            return_train_score=return_train_score,\n        )\n        self.param_grid = param_grid\n\n    def _run_search(self, evaluate_candidates):\n        \"\"\"Search all candidates in param_grid\"\"\"\n        evaluate_candidates(ParameterGrid(self.param_grid))",
                        "filename": "sklearn/model_selection/_search.py",
                        "start_index": 54905,
                        "end_index": 55887,
                        "start_line": 1386,
                        "end_line": 1422,
                        "max_line": 1813,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the initialization of GridSearchCV, which may be relevant to the issue if the problem is related to how the 'param_grid' is being handled."
                }
            ]
        },
        "solution_task": "Based on the provided issue summary and code snippets, the following solution can be proposed:\n\n- Ensure that the `cv_results_` attribute does not store fitted estimators within `cv_results_['params']` to reduce memory usage. This can be done by modifying the `evaluate_candidates` function in `_search.py` to only store the parameters used for fitting, not the fitted estimators themselves.\n- Modify the `evaluate_candidates` function to clone the base estimator before fitting it with the current set of parameters. This ensures that the original estimator is not modified during the grid search process.\n- Since the issue occurs when `n_jobs=1`, it is important to verify that the cloning of the estimator is done properly when `n_jobs` is set to 1. This can be checked in the `evaluate_candidates` function where the parallel execution is managed.\n- Update the `best_estimator_` attribute in `_search.py` to ensure that it is a clone of the estimator with the best found parameters, not a fitted estimator. This is to prevent the `best_estimator_` from being a reference to a fitted estimator which could lead to high memory usage when dumping `cv_results_`.\n- Since the issue mentions version 0.19.0, ensure that the solution is compatible with this version or recommend an upgrade if the solution requires a newer version of the library.\n\nHere is a concise solution:\n\n- In `_search.py`, within the `evaluate_candidates` function, modify the code to store only parameter values, not fitted estimators, in `cv_results_['params']`.\n- Ensure that `clone(base_estimator)` is called before fitting the estimator with parameters within the `evaluate_candidates` function.\n- In the `GridSearchCV` class, ensure that `best_estimator_` is set to a clone of the base estimator with the best parameters applied, not the actual fitted estimator.\n- Test the changes with `n_jobs=1` to confirm that the memory usage issue is resolved.\n- If the issue persists or if the changes are not compatible with version 0.19.0, consider recommending an upgrade to a newer version where the issue might be already addressed.",
        "bouncer_response": {
            "score": 7,
            "criteria": "The RESPONSE must provide actionable, relevant guidance to more parts of the PROMPT's described issue to score higher.",
            "supporting_evidence": "The RESPONSE provides a series of actionable steps that are directly related to the issue described in the PROMPT. It suggests modifying the 'evaluate_candidates' function to prevent storing fitted estimators and to clone the base estimator before fitting. It also addresses the specific condition where the bug occurs (when 'n_jobs=1'). The RESPONSE includes a recommendation to ensure compatibility with the mentioned version (0.19.0) or to upgrade if necessary. Additionally, it provides specific code snippets to check, which is valuable for someone looking to resolve the issue. However, the RESPONSE could have been more precise by providing actual code changes or pseudocode, which would have made it even more actionable. It also assumes that the user has access to modify the library's source code, which may not always be the case.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711738892.6979089,
        "result": "Based on the provided issue summary and code snippets, the following solution can be proposed:\n\n- Ensure that the `cv_results_` attribute does not store fitted estimators within `cv_results_['params']` to reduce memory usage. This can be done by modifying the `evaluate_candidates` function in `_search.py` to only store the parameters used for fitting, not the fitted estimators themselves.\n- Modify the `evaluate_candidates` function to clone the base estimator before fitting it with the current set of parameters. This ensures that the original estimator is not modified during the grid search process.\n- Since the issue occurs when `n_jobs=1`, it is important to verify that the cloning of the estimator is done properly when `n_jobs` is set to 1. This can be checked in the `evaluate_candidates` function where the parallel execution is managed.\n- Update the `best_estimator_` attribute in `_search.py` to ensure that it is a clone of the estimator with the best found parameters, not a fitted estimator. This is to prevent the `best_estimator_` from being a reference to a fitted estimator which could lead to high memory usage when dumping `cv_results_`.\n- Since the issue mentions version 0.19.0, ensure that the solution is compatible with this version or recommend an upgrade if the solution requires a newer version of the library.\n\nHere is a concise solution:\n\n- In `_search.py`, within the `evaluate_candidates` function, modify the code to store only parameter values, not fitted estimators, in `cv_results_['params']`.\n- Ensure that `clone(base_estimator)` is called before fitting the estimator with parameters within the `evaluate_candidates` function.\n- In the `GridSearchCV` class, ensure that `best_estimator_` is set to a clone of the base estimator with the best parameters applied, not the actual fitted estimator.\n- Test the changes with `n_jobs=1` to confirm that the memory usage issue is resolved.\n- If the issue persists or if the changes are not compatible with version 0.19.0, consider recommending an upgrade to a newer version where the issue might be already addressed.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [model_selection](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/model_selection)- [Lines 1060 - 1808](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/model_selection/_search.py#L1060-L1808)\n\n This snippet contains the definition of GridSearchCV, including the 'cv_results_' attribute where the issue with storing fitted estimators is occurring. \n\n\n   - [Lines 910 - 951](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/model_selection/_search.py#L910-L951)\n\n This snippet includes the code where the best estimator is cloned and refitted, which is relevant to the issue of fitted estimators being stored in 'cv_results_'. \n\n\n   - [Lines 828 - 898](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/model_selection/_search.py#L828-L898)\n\n This snippet shows the evaluation of candidates within GridSearchCV, which may be relevant to understanding how the fitted estimators are handled during the search process. \n\n\n   - [Lines 1386 - 1422](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/model_selection/_search.py#L1386-L1422)\n\n This snippet shows the initialization of GridSearchCV, which may be relevant to the issue if the problem is related to how the 'param_grid' is being handled."
    }
}