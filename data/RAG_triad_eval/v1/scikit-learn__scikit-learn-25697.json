{
    "id": "scikit-learn__scikit-learn-25697",
    "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
    "task_id": 1724387690406808849,
    "title": "Deprecate `n_iter` in favor of `max_iter` for consistency",
    "body": "`BayesianRidge` and `ARDRegression` are exposing the parameter `n_iter` instead of `max_iter` as in other models. I think that we should deprecate `n_iter` and rename it `max_iter` to be consistent.\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__1.3.git",
    "requested_commit": "",
    "mayil_response": "To deprecate `n_iter` in favor of `max_iter` for consistency in `BayesianRidge` and `ARDRegression`, follow these steps:\n\n- Update the constructor of both `BayesianRidge` and `ARDRegression` classes to include `max_iter` instead of `n_iter`.\n- Add a deprecation warning in the constructor if `n_iter` is used, and set `max_iter` to the value of `n_iter`.\n- Update the documentation to reflect the change.\n- Replace all internal uses of `n_iter` with `max_iter`.\n- Ensure that the `n_iter_` attribute (which stores the actual number of iterations run) remains unchanged, as it is not part of the deprecation.\n- Add a test case to check that the deprecation warning is raised when `n_iter` is used and that the correct value is set for `max_iter`.\n- Schedule the removal of `n_iter` in a future release (e.g., 1.5) as per the deprecation cycle.\n\nHere is a code snippet to illustrate the changes in the constructor and the deprecation warning:\n\n```python\nimport warnings\n\nclass BayesianRidge(LinearModel):\n    def __init__(self, max_iter=None, ..., n_iter=\"deprecated\"):\n        # Deprecation warning for n_iter\n        if n_iter != \"deprecated\":\n            warnings.warn(\n                \"'n_iter' was renamed to 'max_iter' in version 1.3 and \"\n                \"will be removed in 1.5. Use 'max_iter' instead.\",\n                FutureWarning\n            )\n            max_iter = n_iter\n        self.max_iter = max_iter if max_iter is not None else 300\n        # Other parameters ...\n\n# Similar changes for ARDRegression\n```\n\nRemember to update the documentation and comments to reflect the deprecation of `n_iter` and the introduction of `max_iter`.\n\nFor the test case, you can add a function in `sklearn/utils/estimator_checks.py` to check for the deprecation warning:\n\n```python\nfrom sklearn.utils._testing import assert_warns_message\n\ndef test_deprecation_n_iter_to_max_iter():\n    # Test that a deprecation warning is issued when using n_iter\n    assert_warns_message(\n        FutureWarning,\n        \"'n_iter' was renamed to 'max_iter' in version 1.3 and will be removed in 1.5. Use 'max_iter' instead.\",\n        BayesianRidge,\n        n_iter=100\n    )\n    # Similar test for ARDRegression\n```\n\nMake sure to run the test suite to ensure that your changes do not break existing functionality and that the deprecation warning is correctly issued.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn/linear_model)- [Lines 484 - 840](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/linear_model/_bayes.py#L484-L840)\n\n This snippet shows the deprecation of `n_iter` and introduction of `max_iter` in the Bayesian ARD regression documentation, which is directly related to the issue. \n\n\n   - [Lines 1 - 67](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/linear_model/_bayes.py#L1-L67)\n\n This snippet contains the `_deprecate_n_iter` function which handles the deprecation warning for `n_iter` and is relevant to the issue. \n\n\n   - [Lines 71 - 840](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/linear_model/_bayes.py#L71-L840)\n\n This snippet shows the deprecation of `n_iter` and introduction of `max_iter` in the Bayesian Ridge regression documentation, which is directly related to the issue.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0003495,
        "snippet_processor": 0.04503,
        "issue_star_creation": 0.0381,
        "issue_star_solver": 0.06981000000000001,
        "bouncer": 0.02748
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711737893.151541,
        "relevant_snippets": [
            {
                "code": "\"\"\"Bayesian ARD regression.\n\n    Fit the weights of a regression model, using an ARD prior. The weights of\n    the regression model are assumed to be in Gaussian distributions.\n    Also estimate the parameters lambda (precisions of the distributions of the\n    weights) and alpha (precision of the distribution of the noise).\n    The estimation is done by an iterative procedures (Evidence Maximization)\n\n    Read more in the :ref:`User Guide <bayesian_regression>`.\n\n    Parameters\n    ----------\n    max_iter : int, default=None\n        Maximum number of iterations. If `None`, it corresponds to `max_iter=300`.\n\n        .. versionchanged:: 1.3\n\n    tol : float, default=1e-3\n        Stop the algorithm if w has converged.\n\n    alpha_1 : float, default=1e-6\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the alpha parameter.\n\n    alpha_2 : float, default=1e-6\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the alpha parameter.\n\n    lambda_1 : float, default=1e-6\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the lambda parameter.\n\n    lambda_2 : float, default=1e-6\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the lambda parameter.\n\n    compute_score : bool, default=False\n        If True, compute the objective function at each step of the model.\n\n    threshold_lambda : float, default=10 000\n        Threshold for removing (pruning) weights with high precision from\n        the computation.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    verbose : bool, default=False\n        Verbose mode when fitting the model.\n\n    n_iter : int\n        Maximum number of iterations.\n\n        .. deprecated:: 1.3\n           `n_iter` is deprecated in 1.3 and will be removed in 1.5. Use\n           `max_iter` instead.\n\n    Attributes\n    ----------\n    coef_ : array-like of shape (n_features,)\n        Coefficients of the regression model (mean of distribution)\n\n    alpha_ : float\n       estimated precision of the noise.\n\n    lambda_ : array-like of shape (n_features,)\n       estimated precisions of the weights.\n\n    sigma_ : array-like of shape (n_features, n_features)\n        estimated variance-covariance matrix of the weights\n\n    scores_ : float\n        if computed, value of the objective function (to be maximized)\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n\n        .. versionadded:: 1.3\n\n    intercept_ : float\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    X_offset_ : float\n        If `fit_intercept=True`, offset subtracted for centering data to a\n        zero mean. Set to np.zeros(n_features) otherwise.\n\n    X_scale_ : float\n        Set to np.ones(n_features).\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    BayesianRidge : Bayesian ridge regression.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/linear_model/plot_ard.py\n    <sphx_glr_auto_examples_linear_model_plot_ard.py>`.\n\n    References\n    ----------\n    D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\n    competition, ASHRAE Transactions, 1994.\n\n    R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\n    http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\n    Their beta is our ``self.alpha_``\n    Their alpha is our ``self.lambda_``\n    ARD is a little different than the slide: only dimensions/features for\n    which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\n    discarded.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.ARDRegression()\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    ARDRegression()\n    >>> clf.predict([[1, 1]])\n    array([1.])\n    \"\"\"",
                "filename": "sklearn/linear_model/_bayes.py",
                "start_index": 16505,
                "end_index": 20980,
                "start_line": 484,
                "end_line": 840,
                "max_line": 848,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 1.0
            },
            {
                "code": "\"\"\"\nVarious bayesian regression\n\"\"\"\n\n# Authors: V. Michel, F. Pedregosa, A. Gramfort\n# License: BSD 3 clause\n\nimport warnings\nfrom math import log\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy import linalg\nfrom scipy.linalg import pinvh\n\nfrom ..base import RegressorMixin, _fit_context\nfrom ..utils._param_validation import Hidden, Interval, StrOptions\nfrom ..utils.extmath import fast_logdet\nfrom ..utils.validation import _check_sample_weight\nfrom ._base import LinearModel, _preprocess_data, _rescale_data\n\n\n# TODO(1.5) Remove\ndef _deprecate_n_iter(n_iter, max_iter):\n    \"\"\"Deprecates n_iter in favour of max_iter. Checks if the n_iter has been\n    used instead of max_iter and generates a deprecation warning if True.\n\n    Parameters\n    ----------\n    n_iter : int,\n        Value of n_iter attribute passed by the estimator.\n\n    max_iter : int, default=None\n        Value of max_iter attribute passed by the estimator.\n        If `None`, it corresponds to `max_iter=300`.\n\n    Returns\n    -------\n    max_iter : int,\n        Value of max_iter which shall further be used by the estimator.\n\n    Notes\n    -----\n    This function should be completely removed in 1.5.\n    \"\"\"\n    if n_iter != \"deprecated\":\n        if max_iter is not None:\n            raise ValueError(\n                \"Both `n_iter` and `max_iter` attributes were set. Attribute\"\n                \" `n_iter` was deprecated in version 1.3 and will be removed in\"\n                \" 1.5. To avoid this error, only set the `max_iter` attribute.\"\n            )\n        warnings.warn(\n            (\n                \"'n_iter' was renamed to 'max_iter' in version 1.3 and \"\n                \"will be removed in 1.5\"\n            ),\n            FutureWarning,\n        )\n        max_iter = n_iter\n    elif max_iter is None:\n        max_iter = 300\n    return max_iter\n\n\n###############################################################################\n# BayesianRidge regression",
                "filename": "sklearn/linear_model/_bayes.py",
                "start_index": 0,
                "end_index": 1955,
                "start_line": 1,
                "end_line": 67,
                "max_line": 848,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 1.0
            },
            {
                "code": "\"\"\"Bayesian ridge regression.\n\n    Fit a Bayesian ridge model. See the Notes section for details on this\n    implementation and the optimization of the regularization parameters\n    lambda (precision of the weights) and alpha (precision of the noise).\n\n    Read more in the :ref:`User Guide <bayesian_regression>`.\n\n    Parameters\n    ----------\n    max_iter : int, default=None\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion. If `None`, it\n        corresponds to `max_iter=300`.\n\n        .. versionchanged:: 1.3\n\n    tol : float, default=1e-3\n        Stop the algorithm if w has converged.\n\n    alpha_1 : float, default=1e-6\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the alpha parameter.\n\n    alpha_2 : float, default=1e-6\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the alpha parameter.\n\n    lambda_1 : float, default=1e-6\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the lambda parameter.\n\n    lambda_2 : float, default=1e-6\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the lambda parameter.\n\n    alpha_init : float, default=None\n        Initial value for alpha (precision of the noise).\n        If not set, alpha_init is 1/Var(y).\n\n            .. versionadded:: 0.22\n\n    lambda_init : float, default=None\n        Initial value for lambda (precision of the weights).\n        If not set, lambda_init is 1.\n\n            .. versionadded:: 0.22\n\n    compute_score : bool, default=False\n        If True, compute the log marginal likelihood at each iteration of the\n        optimization.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model.\n        The intercept is not treated as a probabilistic parameter\n        and thus has no associated variance. If set\n        to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    verbose : bool, default=False\n        Verbose mode when fitting the model.\n\n    n_iter : int\n        Maximum number of iterations. Should be greater than or equal to 1.\n\n        .. deprecated:: 1.3\n           `n_iter` is deprecated in 1.3 and will be removed in 1.5. Use\n           `max_iter` instead.\n\n    Attributes\n    ----------\n    coef_ : array-like of shape (n_features,)\n        Coefficients of the regression model (mean of distribution)\n\n    intercept_ : float\n        Independent term in decision function. Set to 0.0 if\n        `fit_intercept = False`.\n\n    alpha_ : float\n       Estimated precision of the noise.\n\n    lambda_ : float\n       Estimated precision of the weights.\n\n    sigma_ : array-like of shape (n_features, n_features)\n        Estimated variance-covariance matrix of the weights\n\n    scores_ : array-like of shape (n_iter_+1,)\n        If computed_score is True, value of the log marginal likelihood (to be\n        maximized) at each iteration of the optimization. The array starts\n        with the value of the log marginal likelihood obtained for the initial\n        values of alpha and lambda and ends with the value obtained for the\n        estimated alpha and lambda.\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n\n    X_offset_ : ndarray of shape (n_features,)\n        If `fit_intercept=True`, offset subtracted for centering data to a\n        zero mean. Set to np.zeros(n_features) otherwise.\n\n    X_scale_ : ndarray of shape (n_features,)\n        Set to np.ones(n_features).\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    ARDRegression : Bayesian ARD regression.\n\n    Notes\n    -----\n    There exist several strategies to perform Bayesian ridge regression. This\n    implementation is based on the algorithm described in Appendix A of\n    (Tipping, 2001) where updates of the regularization parameters are done as\n    suggested in (MacKay, 1992). Note that according to A New\n    View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\n    update rules do not guarantee that the marginal likelihood is increasing\n    between two consecutive iterations of the optimization.\n\n    References\n    ----------\n    D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\n    Vol. 4, No. 3, 1992.\n\n    M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\n    Journal of Machine Learning Research, Vol. 1, 2001.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.BayesianRidge()\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    BayesianRidge()\n    >>> clf.predict([[1, 1]])\n    array([1.])\n    \"\"\"",
                "filename": "sklearn/linear_model/_bayes.py",
                "start_index": 2012,
                "end_index": 7223,
                "start_line": 71,
                "end_line": 840,
                "max_line": 848,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 1.0
            },
            {
                "code": "for i in range(max_iter):",
                "filename": "sklearn/covariance/_graph_lasso.py",
                "start_index": 3750,
                "end_index": 3775,
                "start_line": 112,
                "end_line": 112,
                "max_line": 1094,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "while(iter < max_iter)",
                "filename": "sklearn/svm/src/liblinear/linear.cpp",
                "start_index": 11787,
                "end_index": 11809,
                "start_line": 624,
                "end_line": 1915,
                "max_line": 3075,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "self.n_iter_ = 0",
                "filename": "sklearn/neural_network/_multilayer_perceptron.py",
                "start_index": 13118,
                "end_index": 13134,
                "start_line": 370,
                "end_line": 616,
                "max_line": 1646,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_non_transformer_estimators_n_iter(name, estimator_orig):\n    # Test that estimators that are not transformers with a parameter\n    # max_iter, return the attribute of n_iter_ at least 1.\n\n    # These models are dependent on external solvers like\n    # libsvm and accessing the iter parameter is non-trivial.\n    # SelfTrainingClassifier does not perform an iteration if all samples are\n    # labeled, hence n_iter_ = 0 is valid.\n    not_run_check_n_iter = [\n        \"Ridge\",\n        \"RidgeClassifier\",\n        \"RandomizedLasso\",\n        \"LogisticRegressionCV\",\n        \"LinearSVC\",\n        \"LogisticRegression\",\n        \"SelfTrainingClassifier\",\n    ]\n\n    # Tested in test_transformer_n_iter\n    not_run_check_n_iter += CROSS_DECOMPOSITION\n    if name in not_run_check_n_iter:\n        return\n\n    # LassoLars stops early for the default alpha=1.0 the iris dataset.\n    if name == \"LassoLars\":\n        estimator = clone(estimator_orig).set_params(alpha=0.0)\n    else:\n        estimator = clone(estimator_orig)\n    if hasattr(estimator, \"max_iter\"):\n        iris = load_iris()\n        X, y_ = iris.data, iris.target\n        y_ = _enforce_estimator_tags_y(estimator, y_)\n\n        set_random_state(estimator, 0)\n\n        X = _enforce_estimator_tags_X(estimator_orig, X)\n\n        estimator.fit(X, y_)\n\n        assert np.all(estimator.n_iter_ >= 1)\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_transformer_n_iter(name, estimator_orig):\n    # Test that transformers with a parameter max_iter, return the\n    # attribute of n_iter_ at least 1.\n    estimator = clone(estimator_orig)\n    if hasattr(estimator, \"max_iter\"):\n        if name in CROSS_DECOMPOSITION:\n            # Check using default data\n            X = [[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [2.0, 2.0, 2.0], [2.0, 5.0, 4.0]]\n            y_ = [[0.1, -0.2], [0.9, 1.1], [0.1, -0.5], [0.3, -0.2]]\n\n        else:\n            X, y_ = make_blobs(\n                n_samples=30,\n                centers=[[0, 0, 0], [1, 1, 1]],\n                random_state=0,\n                n_features=2,\n                cluster_std=0.1,\n            )\n            X = _enforce_estimator_tags_X(estimator_orig, X)\n        set_random_state(estimator, 0)\n        estimator.fit(X, y_)\n\n        # These return a n_iter per component.\n        if name in CROSS_DECOMPOSITION:\n            for iter_ in estimator.n_iter_:\n                assert iter_ >= 1\n        else:\n            assert estimator.n_iter_ >= 1\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_get_params_invariance(name, estimator_orig):\n    # Checks if get_params(deep=False) is a subset of get_params(deep=True)\n    e = clone(estimator_orig)\n\n    shallow_params = e.get_params(deep=False)\n    deep_params = e.get_params(deep=True)\n\n    assert all(item in deep_params.items() for item in shallow_params.items())",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 126361,
                "end_index": 129224,
                "start_line": 181,
                "end_line": 3662,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 0.7
            },
            {
                "code": "try:\n        # be robust to the max_iter=0 edge case, see:\n        # https://github.com/scikit-learn/scikit-learn/issues/4134",
                "filename": "sklearn/covariance/_graph_lasso.py",
                "start_index": 3490,
                "end_index": 3615,
                "start_line": 106,
                "end_line": 108,
                "max_line": 1094,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "# all models always have more than 10 non-zero coefficients. Compared to the OLS\n# estimator, the coefficients using a Bayesian Ridge regression are slightly\n# shifted toward zero, which stabilises them. The ARD regression provides a\n# sparser solution: some of the non-informative coefficients are set exactly to\n# zero, while shifting others closer to zero. Some non-informative coefficients\n# are still present and retain large values.\n\n# %%\n# Plot the marginal log-likelihood\n# --------------------------------\nimport numpy as np\n\nard_scores = -np.array(ard.scores_)\nbrr_scores = -np.array(brr.scores_)\nplt.plot(ard_scores, color=\"navy\", label=\"ARD\")\nplt.plot(brr_scores, color=\"red\", label=\"BayesianRidge\")\nplt.ylabel(\"Log-likelihood\")\nplt.xlabel(\"Iterations\")\nplt.xlim(1, 30)\nplt.legend()\n_ = plt.title(\"Models log-likelihood\")\n\n# %%\n# Indeed, both models minimize the log-likelihood up to an arbitrary cutoff\n# defined by the `n_iter` parameter.\n#\n# Bayesian regressions with polynomial feature expansion\n# ======================================================\n# Generate synthetic dataset\n# --------------------------\n# We create a target that is a non-linear function of the input feature.\n# Noise following a standard uniform distribution is added.\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\n\nrng = np.random.RandomState(0)\nn_samples = 110\n\n# sort the data to make plotting easier later\nX = np.sort(-10 * rng.rand(n_samples) + 10)\nnoise = rng.normal(0, 1, n_samples) * 1.35\ny = np.sqrt(X) * np.sin(X) + noise\nfull_data = pd.DataFrame({\"input_feature\": X, \"target\": y})\nX = X.reshape((-1, 1))\n\n# extrapolation\nX_plot = np.linspace(10, 10.4, 10)\ny_plot = np.sqrt(X_plot) * np.sin(X_plot)\nX_plot = np.concatenate((X, X_plot.reshape((-1, 1))))\ny_plot = np.concatenate((y - noise, y_plot))\n\n# %%\n# Fit the regressors\n# ------------------\n#\n# Here we try a degree 10 polynomial to potentially overfit, though the bayesian\n# linear models regularize the size of the polynomial coefficients. As\n# `fit_intercept=True` by default for\n# :class:`~sklearn.linear_model.ARDRegression` and\n# :class:`~sklearn.linear_model.BayesianRidge`, then\n# :class:`~sklearn.preprocessing.PolynomialFeatures` should not introduce an\n# additional bias feature. By setting `return_std=True`, the bayesian regressors\n# return the standard deviation of the posterior distribution for the model\n# parameters.\n\nard_poly = make_pipeline(\n    PolynomialFeatures(degree=10, include_bias=False),\n    StandardScaler(),\n    ARDRegression(),\n).fit(X, y)\nbrr_poly = make_pipeline(\n    PolynomialFeatures(degree=10, include_bias=False),\n    StandardScaler(),\n    BayesianRidge(),\n).fit(X, y)\n\ny_ard, y_ard_std = ard_poly.predict(X_plot, return_std=True)\ny_brr, y_brr_std = brr_poly.predict(X_plot, return_std=True)\n\n# %%\n# Plotting polynomial regressions with std errors of the scores\n# -------------------------------------------------------------",
                "filename": "examples/linear_model/plot_ard.py",
                "start_index": 2940,
                "end_index": 5927,
                "start_line": 96,
                "end_line": 178,
                "max_line": 211,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "for iter_ in range(max_iter):\n            # update posterior mean coef_ based on alpha_ and lambda_ and\n            # compute corresponding rmse\n            coef_, rmse_ = self._update_coef_(\n                X, y, n_samples, n_features, XT_y, U, Vh, eigen_vals_, alpha_, lambda_\n            )\n            if self.compute_score:\n                # compute the log marginal likelihood\n                s = self._log_marginal_likelihood(\n                    n_samples, n_features, eigen_vals_, alpha_, lambda_, coef_, rmse_\n                )\n                self.scores_.append(s)\n\n            # Update alpha and lambda according to (MacKay, 1992)\n            gamma_ = np.sum((alpha_ * eigen_vals_) / (lambda_ + alpha_ * eigen_vals_))\n            lambda_ = (gamma_ + 2 * lambda_1) / (np.sum(coef_**2) + 2 * lambda_2)\n            alpha_ = (n_samples - gamma_ + 2 * alpha_1) / (rmse_ + 2 * alpha_2)\n\n            # Check for convergence\n            if iter_ != 0 and np.sum(np.abs(coef_old_ - coef_)) < self.tol:\n                if verbose:\n                    print(\"Convergence after \", str(iter_), \" iterations\")\n                break\n            coef_old_ = np.copy(coef_)\n\n        self.n_iter_ = iter_ + 1\n\n        # return regularization parameters and corresponding posterior mean,\n        # log marginal likelihood and posterior covariance\n        self.alpha_ = alpha_\n        self.lambda_ = lambda_\n        self.coef_, rmse_ = self._update_coef_(\n            X, y, n_samples, n_features, XT_y, U, Vh, eigen_vals_, alpha_, lambda_\n        )\n        if self.compute_score:\n            # compute the log marginal likelihood\n            s = self._log_marginal_likelihood(\n                n_samples, n_features, eigen_vals_, alpha_, lambda_, coef_, rmse_\n            )\n            self.scores_.append(s)\n            self.scores_ = np.array(self.scores_)\n\n        # posterior covariance is given by 1/alpha_ * scaled_sigma_\n        scaled_sigma_ = np.dot(\n            Vh.T, Vh / (eigen_vals_ + lambda_ / alpha_)[:, np.newaxis]\n        )\n        self.sigma_ = (1.0 / alpha_) * scaled_sigma_\n\n        self._set_intercept(X_offset_, y_offset_, X_scale_)\n\n        return self",
                "filename": "sklearn/linear_model/_bayes.py",
                "start_index": 11143,
                "end_index": 13309,
                "start_line": 341,
                "end_line": 788,
                "max_line": 848,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 1.0
            }
        ],
        "classification": "feature request",
        "snippet_processor_task": {
            "sklearn/linear_model/_bayes.py": [
                {
                    "chunk": {
                        "code": "\"\"\"Bayesian ARD regression.\n\n    Fit the weights of a regression model, using an ARD prior. The weights of\n    the regression model are assumed to be in Gaussian distributions.\n    Also estimate the parameters lambda (precisions of the distributions of the\n    weights) and alpha (precision of the distribution of the noise).\n    The estimation is done by an iterative procedures (Evidence Maximization)\n\n    Read more in the :ref:`User Guide <bayesian_regression>`.\n\n    Parameters\n    ----------\n    max_iter : int, default=None\n        Maximum number of iterations. If `None`, it corresponds to `max_iter=300`.\n\n        .. versionchanged:: 1.3\n\n    tol : float, default=1e-3\n        Stop the algorithm if w has converged.\n\n    alpha_1 : float, default=1e-6\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the alpha parameter.\n\n    alpha_2 : float, default=1e-6\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the alpha parameter.\n\n    lambda_1 : float, default=1e-6\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the lambda parameter.\n\n    lambda_2 : float, default=1e-6\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the lambda parameter.\n\n    compute_score : bool, default=False\n        If True, compute the objective function at each step of the model.\n\n    threshold_lambda : float, default=10 000\n        Threshold for removing (pruning) weights with high precision from\n        the computation.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model. If set\n        to false, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    verbose : bool, default=False\n        Verbose mode when fitting the model.\n\n    n_iter : int\n        Maximum number of iterations.\n\n        .. deprecated:: 1.3\n           `n_iter` is deprecated in 1.3 and will be removed in 1.5. Use\n           `max_iter` instead.\n\n    Attributes\n    ----------\n    coef_ : array-like of shape (n_features,)\n        Coefficients of the regression model (mean of distribution)\n\n    alpha_ : float\n       estimated precision of the noise.\n\n    lambda_ : array-like of shape (n_features,)\n       estimated precisions of the weights.\n\n    sigma_ : array-like of shape (n_features, n_features)\n        estimated variance-covariance matrix of the weights\n\n    scores_ : float\n        if computed, value of the objective function (to be maximized)\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n\n        .. versionadded:: 1.3\n\n    intercept_ : float\n        Independent term in decision function. Set to 0.0 if\n        ``fit_intercept = False``.\n\n    X_offset_ : float\n        If `fit_intercept=True`, offset subtracted for centering data to a\n        zero mean. Set to np.zeros(n_features) otherwise.\n\n    X_scale_ : float\n        Set to np.ones(n_features).\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    BayesianRidge : Bayesian ridge regression.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/linear_model/plot_ard.py\n    <sphx_glr_auto_examples_linear_model_plot_ard.py>`.\n\n    References\n    ----------\n    D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\n    competition, ASHRAE Transactions, 1994.\n\n    R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\n    http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\n    Their beta is our ``self.alpha_``\n    Their alpha is our ``self.lambda_``\n    ARD is a little different than the slide: only dimensions/features for\n    which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\n    discarded.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.ARDRegression()\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    ARDRegression()\n    >>> clf.predict([[1, 1]])\n    array([1.])\n    \"\"\"",
                        "filename": "sklearn/linear_model/_bayes.py",
                        "start_index": 16505,
                        "end_index": 20980,
                        "start_line": 484,
                        "end_line": 840,
                        "max_line": 848,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the deprecation of `n_iter` and introduction of `max_iter` in the Bayesian ARD regression documentation, which is directly related to the issue."
                },
                {
                    "chunk": {
                        "code": "\"\"\"\nVarious bayesian regression\n\"\"\"\n\n# Authors: V. Michel, F. Pedregosa, A. Gramfort\n# License: BSD 3 clause\n\nimport warnings\nfrom math import log\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy import linalg\nfrom scipy.linalg import pinvh\n\nfrom ..base import RegressorMixin, _fit_context\nfrom ..utils._param_validation import Hidden, Interval, StrOptions\nfrom ..utils.extmath import fast_logdet\nfrom ..utils.validation import _check_sample_weight\nfrom ._base import LinearModel, _preprocess_data, _rescale_data\n\n\n# TODO(1.5) Remove\ndef _deprecate_n_iter(n_iter, max_iter):\n    \"\"\"Deprecates n_iter in favour of max_iter. Checks if the n_iter has been\n    used instead of max_iter and generates a deprecation warning if True.\n\n    Parameters\n    ----------\n    n_iter : int,\n        Value of n_iter attribute passed by the estimator.\n\n    max_iter : int, default=None\n        Value of max_iter attribute passed by the estimator.\n        If `None`, it corresponds to `max_iter=300`.\n\n    Returns\n    -------\n    max_iter : int,\n        Value of max_iter which shall further be used by the estimator.\n\n    Notes\n    -----\n    This function should be completely removed in 1.5.\n    \"\"\"\n    if n_iter != \"deprecated\":\n        if max_iter is not None:\n            raise ValueError(\n                \"Both `n_iter` and `max_iter` attributes were set. Attribute\"\n                \" `n_iter` was deprecated in version 1.3 and will be removed in\"\n                \" 1.5. To avoid this error, only set the `max_iter` attribute.\"\n            )\n        warnings.warn(\n            (\n                \"'n_iter' was renamed to 'max_iter' in version 1.3 and \"\n                \"will be removed in 1.5\"\n            ),\n            FutureWarning,\n        )\n        max_iter = n_iter\n    elif max_iter is None:\n        max_iter = 300\n    return max_iter\n\n\n###############################################################################\n# BayesianRidge regression",
                        "filename": "sklearn/linear_model/_bayes.py",
                        "start_index": 0,
                        "end_index": 1955,
                        "start_line": 1,
                        "end_line": 67,
                        "max_line": 848,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the `_deprecate_n_iter` function which handles the deprecation warning for `n_iter` and is relevant to the issue."
                },
                {
                    "chunk": {
                        "code": "\"\"\"Bayesian ridge regression.\n\n    Fit a Bayesian ridge model. See the Notes section for details on this\n    implementation and the optimization of the regularization parameters\n    lambda (precision of the weights) and alpha (precision of the noise).\n\n    Read more in the :ref:`User Guide <bayesian_regression>`.\n\n    Parameters\n    ----------\n    max_iter : int, default=None\n        Maximum number of iterations over the complete dataset before\n        stopping independently of any early stopping criterion. If `None`, it\n        corresponds to `max_iter=300`.\n\n        .. versionchanged:: 1.3\n\n    tol : float, default=1e-3\n        Stop the algorithm if w has converged.\n\n    alpha_1 : float, default=1e-6\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the alpha parameter.\n\n    alpha_2 : float, default=1e-6\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the alpha parameter.\n\n    lambda_1 : float, default=1e-6\n        Hyper-parameter : shape parameter for the Gamma distribution prior\n        over the lambda parameter.\n\n    lambda_2 : float, default=1e-6\n        Hyper-parameter : inverse scale parameter (rate parameter) for the\n        Gamma distribution prior over the lambda parameter.\n\n    alpha_init : float, default=None\n        Initial value for alpha (precision of the noise).\n        If not set, alpha_init is 1/Var(y).\n\n            .. versionadded:: 0.22\n\n    lambda_init : float, default=None\n        Initial value for lambda (precision of the weights).\n        If not set, lambda_init is 1.\n\n            .. versionadded:: 0.22\n\n    compute_score : bool, default=False\n        If True, compute the log marginal likelihood at each iteration of the\n        optimization.\n\n    fit_intercept : bool, default=True\n        Whether to calculate the intercept for this model.\n        The intercept is not treated as a probabilistic parameter\n        and thus has no associated variance. If set\n        to False, no intercept will be used in calculations\n        (i.e. data is expected to be centered).\n\n    copy_X : bool, default=True\n        If True, X will be copied; else, it may be overwritten.\n\n    verbose : bool, default=False\n        Verbose mode when fitting the model.\n\n    n_iter : int\n        Maximum number of iterations. Should be greater than or equal to 1.\n\n        .. deprecated:: 1.3\n           `n_iter` is deprecated in 1.3 and will be removed in 1.5. Use\n           `max_iter` instead.\n\n    Attributes\n    ----------\n    coef_ : array-like of shape (n_features,)\n        Coefficients of the regression model (mean of distribution)\n\n    intercept_ : float\n        Independent term in decision function. Set to 0.0 if\n        `fit_intercept = False`.\n\n    alpha_ : float\n       Estimated precision of the noise.\n\n    lambda_ : float\n       Estimated precision of the weights.\n\n    sigma_ : array-like of shape (n_features, n_features)\n        Estimated variance-covariance matrix of the weights\n\n    scores_ : array-like of shape (n_iter_+1,)\n        If computed_score is True, value of the log marginal likelihood (to be\n        maximized) at each iteration of the optimization. The array starts\n        with the value of the log marginal likelihood obtained for the initial\n        values of alpha and lambda and ends with the value obtained for the\n        estimated alpha and lambda.\n\n    n_iter_ : int\n        The actual number of iterations to reach the stopping criterion.\n\n    X_offset_ : ndarray of shape (n_features,)\n        If `fit_intercept=True`, offset subtracted for centering data to a\n        zero mean. Set to np.zeros(n_features) otherwise.\n\n    X_scale_ : ndarray of shape (n_features,)\n        Set to np.ones(n_features).\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    ARDRegression : Bayesian ARD regression.\n\n    Notes\n    -----\n    There exist several strategies to perform Bayesian ridge regression. This\n    implementation is based on the algorithm described in Appendix A of\n    (Tipping, 2001) where updates of the regularization parameters are done as\n    suggested in (MacKay, 1992). Note that according to A New\n    View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\n    update rules do not guarantee that the marginal likelihood is increasing\n    between two consecutive iterations of the optimization.\n\n    References\n    ----------\n    D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\n    Vol. 4, No. 3, 1992.\n\n    M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\n    Journal of Machine Learning Research, Vol. 1, 2001.\n\n    Examples\n    --------\n    >>> from sklearn import linear_model\n    >>> clf = linear_model.BayesianRidge()\n    >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n    BayesianRidge()\n    >>> clf.predict([[1, 1]])\n    array([1.])\n    \"\"\"",
                        "filename": "sklearn/linear_model/_bayes.py",
                        "start_index": 2012,
                        "end_index": 7223,
                        "start_line": 71,
                        "end_line": 840,
                        "max_line": 848,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the deprecation of `n_iter` and introduction of `max_iter` in the Bayesian Ridge regression documentation, which is directly related to the issue."
                }
            ]
        },
        "solution_task": "To deprecate `n_iter` in favor of `max_iter` for consistency in `BayesianRidge` and `ARDRegression`, follow these steps:\n\n- Update the constructor of both `BayesianRidge` and `ARDRegression` classes to include `max_iter` instead of `n_iter`.\n- Add a deprecation warning in the constructor if `n_iter` is used, and set `max_iter` to the value of `n_iter`.\n- Update the documentation to reflect the change.\n- Replace all internal uses of `n_iter` with `max_iter`.\n- Ensure that the `n_iter_` attribute (which stores the actual number of iterations run) remains unchanged, as it is not part of the deprecation.\n- Add a test case to check that the deprecation warning is raised when `n_iter` is used and that the correct value is set for `max_iter`.\n- Schedule the removal of `n_iter` in a future release (e.g., 1.5) as per the deprecation cycle.\n\nHere is a code snippet to illustrate the changes in the constructor and the deprecation warning:\n\n```python\nimport warnings\n\nclass BayesianRidge(LinearModel):\n    def __init__(self, max_iter=None, ..., n_iter=\"deprecated\"):\n        # Deprecation warning for n_iter\n        if n_iter != \"deprecated\":\n            warnings.warn(\n                \"'n_iter' was renamed to 'max_iter' in version 1.3 and \"\n                \"will be removed in 1.5. Use 'max_iter' instead.\",\n                FutureWarning\n            )\n            max_iter = n_iter\n        self.max_iter = max_iter if max_iter is not None else 300\n        # Other parameters ...\n\n# Similar changes for ARDRegression\n```\n\nRemember to update the documentation and comments to reflect the deprecation of `n_iter` and the introduction of `max_iter`.\n\nFor the test case, you can add a function in `sklearn/utils/estimator_checks.py` to check for the deprecation warning:\n\n```python\nfrom sklearn.utils._testing import assert_warns_message\n\ndef test_deprecation_n_iter_to_max_iter():\n    # Test that a deprecation warning is issued when using n_iter\n    assert_warns_message(\n        FutureWarning,\n        \"'n_iter' was renamed to 'max_iter' in version 1.3 and will be removed in 1.5. Use 'max_iter' instead.\",\n        BayesianRidge,\n        n_iter=100\n    )\n    # Similar test for ARDRegression\n```\n\nMake sure to run the test suite to ensure that your changes do not break existing functionality and that the deprecation warning is correctly issued.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE must provide precise, actionable guidance and directly address all aspects of the described feature request to facilitate its successful implementation or improvement.",
            "supporting_evidence": "The RESPONSE provides a comprehensive and actionable step-by-step guide to deprecate `n_iter` in favor of `max_iter` in the `BayesianRidge` and `ARDRegression` classes. It includes updating the constructor, adding deprecation warnings, updating documentation, replacing internal uses, ensuring the `n_iter_` attribute remains unchanged, adding test cases, and scheduling the removal of `n_iter`. The RESPONSE also provides relevant code snippets to illustrate the changes and a test case example, which are directly applicable to the feature request. The RESPONSE addresses nearly all aspects of the feature request, providing clear guidance for implementation.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711738148.5321898,
        "result": "To deprecate `n_iter` in favor of `max_iter` for consistency in `BayesianRidge` and `ARDRegression`, follow these steps:\n\n- Update the constructor of both `BayesianRidge` and `ARDRegression` classes to include `max_iter` instead of `n_iter`.\n- Add a deprecation warning in the constructor if `n_iter` is used, and set `max_iter` to the value of `n_iter`.\n- Update the documentation to reflect the change.\n- Replace all internal uses of `n_iter` with `max_iter`.\n- Ensure that the `n_iter_` attribute (which stores the actual number of iterations run) remains unchanged, as it is not part of the deprecation.\n- Add a test case to check that the deprecation warning is raised when `n_iter` is used and that the correct value is set for `max_iter`.\n- Schedule the removal of `n_iter` in a future release (e.g., 1.5) as per the deprecation cycle.\n\nHere is a code snippet to illustrate the changes in the constructor and the deprecation warning:\n\n```python\nimport warnings\n\nclass BayesianRidge(LinearModel):\n    def __init__(self, max_iter=None, ..., n_iter=\"deprecated\"):\n        # Deprecation warning for n_iter\n        if n_iter != \"deprecated\":\n            warnings.warn(\n                \"'n_iter' was renamed to 'max_iter' in version 1.3 and \"\n                \"will be removed in 1.5. Use 'max_iter' instead.\",\n                FutureWarning\n            )\n            max_iter = n_iter\n        self.max_iter = max_iter if max_iter is not None else 300\n        # Other parameters ...\n\n# Similar changes for ARDRegression\n```\n\nRemember to update the documentation and comments to reflect the deprecation of `n_iter` and the introduction of `max_iter`.\n\nFor the test case, you can add a function in `sklearn/utils/estimator_checks.py` to check for the deprecation warning:\n\n```python\nfrom sklearn.utils._testing import assert_warns_message\n\ndef test_deprecation_n_iter_to_max_iter():\n    # Test that a deprecation warning is issued when using n_iter\n    assert_warns_message(\n        FutureWarning,\n        \"'n_iter' was renamed to 'max_iter' in version 1.3 and will be removed in 1.5. Use 'max_iter' instead.\",\n        BayesianRidge,\n        n_iter=100\n    )\n    # Similar test for ARDRegression\n```\n\nMake sure to run the test suite to ensure that your changes do not break existing functionality and that the deprecation warning is correctly issued.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn/linear_model)- [Lines 484 - 840](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/linear_model/_bayes.py#L484-L840)\n\n This snippet shows the deprecation of `n_iter` and introduction of `max_iter` in the Bayesian ARD regression documentation, which is directly related to the issue. \n\n\n   - [Lines 1 - 67](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/linear_model/_bayes.py#L1-L67)\n\n This snippet contains the `_deprecate_n_iter` function which handles the deprecation warning for `n_iter` and is relevant to the issue. \n\n\n   - [Lines 71 - 840](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/linear_model/_bayes.py#L71-L840)\n\n This snippet shows the deprecation of `n_iter` and introduction of `max_iter` in the Bayesian Ridge regression documentation, which is directly related to the issue.",
        "context_relevance": 0.5700000000000001,
        "hypotheses": [],
        "groundedness_score": 1.0,
        "qa_relevance_reasons": {
            "reason": "Criteria: The RESPONSE must be relevant to the entire PROMPT and answer it completely to get a score of 10.\nSupporting Evidence: The RESPONSE provides a detailed step-by-step guide on how to deprecate `n_iter` in favor of `max_iter` for the `BayesianRidge` and `ARDRegression` classes, which is exactly what the PROMPT is requesting. It includes instructions for updating the constructor, adding deprecation warnings, updating documentation, replacing internal uses, ensuring the `n_iter_` attribute remains unchanged, adding test cases, and scheduling the removal of `n_iter`. Additionally, it provides code snippets to illustrate the changes, which directly address the issue raised in the PROMPT. The RESPONSE is fully relevant to the entire PROMPT and answers it completely."
        },
        "qa_relevance": 1.0
    }
}