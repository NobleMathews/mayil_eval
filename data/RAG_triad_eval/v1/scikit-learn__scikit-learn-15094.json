{
    "id": "scikit-learn__scikit-learn-15094",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
    "task_id": 2799426679066375003,
    "title": "MaxAbsScaler Upcasts Pandas to float64",
    "body": "<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn\r\n- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn\r\nFor more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nI am working with the Column transformer, and for memory issues, am trying to produce a float32 sparse matrix. Unfortunately, regardless of pandas input type, the output is always float64.\r\n\r\nI've identified one of the Pipeline scalers, MaxAbsScaler, as being the culprit. Other preprocessing functions, such as OneHotEncoder, have an optional `dtype` argument. This argument does not exist in MaxAbsScaler (among others). It appears that the upcasting happens when `check_array` is executed.\r\n\r\nIs it possible to specify a dtype? Or is there a commonly accepted practice to do so from the Column Transformer?\r\n\r\nThank you!\r\n#### Steps/Code to Reproduce\r\nExample:\r\n```python\r\nimport pandas as pd\r\nfrom sklearn.preprocessing import MaxAbsScaler\r\n\r\ndf = pd.DataFrame({\r\n    'DOW': [0, 1, 2, 3, 4, 5, 6],\r\n    'Month': [3, 2, 4, 3, 2, 6, 7],\r\n    'Value': [3.4, 4., 8, 5, 3, 6, 4]\r\n})\r\ndf = df.astype('float32')\r\nprint(df.dtypes)\r\na = MaxAbsScaler()\r\nscaled = a.fit_transform(df) # providing df.values will produce correct response\r\nprint('Transformed Type: ', scaled.dtype)\r\n```\r\n\r\n#### Expected Results\r\n```\r\nDOW      float32\r\nMonth    float32\r\nValue    float32\r\ndtype: object\r\nTransformed Type: float32\r\n```\r\n\r\n#### Actual Results\r\n```\r\nDOW      float32\r\nMonth    float32\r\nValue    float32\r\ndtype: object\r\nTransformed Type: float64\r\n```\r\n\r\n#### Versions\r\nDarwin-18.7.0-x86_64-i386-64bit\r\nPython 3.6.7 | packaged by conda-forge | (default, Jul  2 2019, 02:07:37) \r\n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\r\nNumPy 1.17.1\r\nSciPy 1.3.1\r\nScikit-Learn 0.20.3\r\nPandas 0.25.1\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.22.git",
    "requested_commit": "",
    "mayil_response": "- Add a `dtype` argument to the `MaxAbsScaler` class constructor and pass it to the `check_array` function within the `fit` and `transform` methods.\n- Modify the `fit` and `transform` methods of `MaxAbsScaler` to include the `dtype` argument when calling `check_array`.\n- Ensure that the `dtype` argument is used to preserve the data type of the input array throughout the scaling process.\n- Update the `ColumnTransformer` to pass the `dtype` argument to the transformers if they accept a `dtype` parameter.\n- Add tests to `check_transformer_preserve_dtypes` to verify that `MaxAbsScaler` preserves the specified `dtype` when transforming data.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [preprocessing](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/preprocessing)- [Lines 1074 - 1074](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/preprocessing/_data.py#L1074-L1074)\n\n This snippet defines the MaxAbsScaler class, which is directly related to the issue as the class lacks a 'dtype' argument that could prevent upcasting.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/utils)- [Lines 181 - 4630](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/estimator_checks.py#L181-L4630)\n\n This snippet contains tests that check if estimators preserve dtypes, which is relevant to the issue since MaxAbsScaler is not preserving the dtype as expected.\n-  \u2192 [examples](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//examples) \u2192 [preprocessing](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//examples/preprocessing)- [Lines 258 - 329](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//examples/preprocessing/plot_all_scaling.py#L258-L329)\n\n This snippet discusses the behavior of MaxAbsScaler and its impact on data, which could provide insight into why it's upcasting the dtype.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/utils)- [Lines 1001 - 2214](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/validation.py#L1001-L2214)\n\n This snippet is about checking large sparse matrices and their dtypes, which might be indirectly related to the issue of dtype handling in MaxAbsScaler.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0003105,
        "snippet_processor": 0.11024,
        "issue_star_creation": 0.03695,
        "issue_star_solver": 0.1084,
        "bouncer": 0.03046
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711738738.6207738,
        "relevant_snippets": [
            {
                "code": "class MaxAbsScaler(OneToOneFeatureMixin, TransformerMixin, BaseEstimator):",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 36804,
                "end_index": 36878,
                "start_line": 1074,
                "end_line": 1074,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "@ignore_warnings\ndef check_estimators_dtypes(name, estimator_orig):\n    rnd = np.random.RandomState(0)\n    X_train_32 = 3 * rnd.uniform(size=(20, 5)).astype(np.float32)\n    X_train_32 = _enforce_estimator_tags_X(estimator_orig, X_train_32)\n    X_train_64 = X_train_32.astype(np.float64)\n    X_train_int_64 = X_train_32.astype(np.int64)\n    X_train_int_32 = X_train_32.astype(np.int32)\n    y = X_train_int_64[:, 0]\n    y = _enforce_estimator_tags_y(estimator_orig, y)\n\n    methods = [\"predict\", \"transform\", \"decision_function\", \"predict_proba\"]\n\n    for X_train in [X_train_32, X_train_64, X_train_int_64, X_train_int_32]:\n        estimator = clone(estimator_orig)\n        set_random_state(estimator, 1)\n        estimator.fit(X_train, y)\n\n        for method in methods:\n            if hasattr(estimator, method):\n                getattr(estimator, method)(X_train)\n\n\ndef check_transformer_preserve_dtypes(name, transformer_orig):\n    # check that dtype are preserved meaning if input X is of some dtype\n    # X_transformed should be from the same dtype.\n    X, y = make_blobs(\n        n_samples=30,\n        centers=[[0, 0, 0], [1, 1, 1]],\n        random_state=0,\n        cluster_std=0.1,\n    )\n    X = StandardScaler().fit_transform(X)\n    X = _enforce_estimator_tags_X(transformer_orig, X)\n\n    for dtype in _safe_tags(transformer_orig, key=\"preserves_dtype\"):\n        X_cast = X.astype(dtype)\n        transformer = clone(transformer_orig)\n        set_random_state(transformer)\n        X_trans1 = transformer.fit_transform(X_cast, y)\n        X_trans2 = transformer.fit(X_cast, y).transform(X_cast)\n\n        for Xt, method in zip([X_trans1, X_trans2], [\"fit_transform\", \"transform\"]):\n            if isinstance(Xt, tuple):\n                # cross-decompostion returns a tuple of (x_scores, y_scores)\n                # when given y with fit_transform; only check the first element\n                Xt = Xt[0]\n\n            # check that the output dtype is preserved\n            assert Xt.dtype == dtype, (\n                f\"{name} (method={method}) does not preserve dtype. \"\n                f\"Original/Expected dtype={dtype.__name__}, got dtype={Xt.dtype}.\"\n            )",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 65098,
                "end_index": 67267,
                "start_line": 181,
                "end_line": 4630,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.9
            },
            {
                "code": "\"\"\"\n=============================\nImportance of Feature Scaling\n=============================\n\nFeature scaling through standardization, also called Z-score normalization, is\nan important preprocessing step for many machine learning algorithms. It\ninvolves rescaling each feature such that it has a standard deviation of 1 and a\nmean of 0.\n\nEven if tree based models are (almost) not affected by scaling, many other\nalgorithms require features to be normalized, often for different reasons: to\nease the convergence (such as a non-penalized logistic regression), to create a\ncompletely different model fit compared to the fit with unscaled data (such as\nKNeighbors models). The latter is demoed on the first part of the present\nexample.\n\nOn the second part of the example we show how Principle Component Analysis (PCA)\nis impacted by normalization of features. To illustrate this, we compare the\nprincipal components found using :class:`~sklearn.decomposition.PCA` on unscaled\ndata with those obatined when using a\n:class:`~sklearn.preprocessing.StandardScaler` to scale data first.\n\nIn the last part of the example we show the effect of the normalization on the\naccuracy of a model trained on PCA-reduced data.\n\n\"\"\"\n\n# Author: Tyler Lanigan <tylerlanigan@gmail.com>\n#         Sebastian Raschka <mail@sebastianraschka.com>\n#         Arturo Amor <david-arturo.amor-quiroz@inria.fr>\n# License: BSD 3 clause\n\n# %%\n# Load and prepare data\n# =====================\n#\n# The dataset used is the :ref:`wine_dataset` available at UCI. This dataset has\n# continuous features that are heterogeneous in scale due to differing\n# properties that they measure (e.g. alcohol content and malic acid).\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX, y = load_wine(return_X_y=True, as_frame=True)\nscaler = StandardScaler().set_output(transform=\"pandas\")\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=42\n)\nscaled_X_train = scaler.fit_transform(X_train)\n\n# %%\n# Effect of rescaling on a k-neighbors models\n# ===========================================\n#\n# For the sake of visualizing the decision boundary of a\n# :class:`~sklearn.neighbors.KNeighborsClassifier`, in this section we select a\n# subset of 2 features that have values with different orders of magnitude.\n#\n# Keep in mind that using a subset of the features to train the model may likely\n# leave out feature with high predictive impact, resulting in a decision\n# boundary that is much worse in comparison to a model trained on the full set\n# of features.\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX_plot = X[[\"proline\", \"hue\"]]\nX_plot_scaled = scaler.fit_transform(X_plot)\nclf = KNeighborsClassifier(n_neighbors=20)",
                "filename": "examples/preprocessing/plot_scaling_importance.py",
                "start_index": 0,
                "end_index": 2897,
                "start_line": 1,
                "end_line": 74,
                "max_line": 256,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"\"\"\n================================\nIntroducing the `set_output` API\n================================\n\n.. currentmodule:: sklearn\n\nThis example will demonstrate the `set_output` API to configure transformers to\noutput pandas DataFrames. `set_output` can be configured per estimator by calling\nthe `set_output` method or globally by setting `set_config(transform_output=\"pandas\")`.\nFor details, see\n`SLEP018 <https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep018/proposal.html>`__.\n\"\"\"  # noqa\n\n# %%\n# First, we load the iris dataset as a DataFrame to demonstrate the `set_output` API.\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_iris(as_frame=True, return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\nX_train.head()\n\n# %%\n# To configure an estimator such as :class:`preprocessing.StandardScaler` to return\n# DataFrames, call `set_output`. This feature requires pandas to be installed.\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().set_output(transform=\"pandas\")\n\nscaler.fit(X_train)\nX_test_scaled = scaler.transform(X_test)\nX_test_scaled.head()\n\n# %%\n# `set_output` can be called after `fit` to configure `transform` after the fact.\nscaler2 = StandardScaler()\n\nscaler2.fit(X_train)\nX_test_np = scaler2.transform(X_test)\nprint(f\"Default output type: {type(X_test_np).__name__}\")\n\nscaler2.set_output(transform=\"pandas\")\nX_test_df = scaler2.transform(X_test)\nprint(f\"Configured pandas output type: {type(X_test_df).__name__}\")\n\n# %%\n# In a :class:`pipeline.Pipeline`, `set_output` configures all steps to output\n# DataFrames.\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\nclf = make_pipeline(\n    StandardScaler(), SelectPercentile(percentile=75), LogisticRegression()\n)\nclf.set_output(transform=\"pandas\")\nclf.fit(X_train, y_train)\n\n# %%\n# Each transformer in the pipeline is configured to return DataFrames. This\n# means that the final logistic regression step contains the feature names of the input.\nclf[-1].feature_names_in_\n\n# %%\n# Next we load the titanic dataset to demonstrate `set_output` with\n# :class:`compose.ColumnTransformer` and heterogeneous data.\nfrom sklearn.datasets import fetch_openml\n\nX, y = fetch_openml(\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n\n# %%\n# The `set_output` API can be configured globally by using :func:`set_config` and\n# setting `transform_output` to `\"pandas\"`.\nfrom sklearn import set_config\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nset_config(transform_output=\"pandas\")\n\nnum_pipe = make_pipeline(SimpleImputer(), StandardScaler())\nnum_cols = [\"age\", \"fare\"]",
                "filename": "examples/miscellaneous/plot_set_output.py",
                "start_index": 0,
                "end_index": 2982,
                "start_line": 1,
                "end_line": 87,
                "max_line": 138,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "# compacted to a specific range, [0, 10] for the median income and [0, 6] for\n# the average house occupancy. Note that there are some marginal outliers (some\n# blocks have average occupancy of more than 1200). Therefore, a specific\n# pre-processing can be very beneficial depending of the application. In the\n# following, we present some insights and behaviors of those pre-processing\n# methods in the presence of marginal outliers.\n\nmake_plot(0)\n\n# %%\n# StandardScaler\n# --------------\n#\n# :class:`~sklearn.preprocessing.StandardScaler` removes the mean and scales\n# the data to unit variance. The scaling shrinks the range of the feature\n# values as shown in the left figure below.\n# However, the outliers have an influence when computing the empirical mean and\n# standard deviation. Note in particular that because the outliers on each\n# feature have different magnitudes, the spread of the transformed data on\n# each feature is very different: most of the data lie in the [-2, 4] range for\n# the transformed median income feature while the same data is squeezed in the\n# smaller [-0.2, 0.2] range for the transformed average house occupancy.\n#\n# :class:`~sklearn.preprocessing.StandardScaler` therefore cannot guarantee\n# balanced feature scales in the\n# presence of outliers.\n\nmake_plot(1)\n\n# %%\n# MinMaxScaler\n# ------------\n#\n# :class:`~sklearn.preprocessing.MinMaxScaler` rescales the data set such that\n# all feature values are in\n# the range [0, 1] as shown in the right panel below. However, this scaling\n# compresses all inliers into the narrow range [0, 0.005] for the transformed\n# average house occupancy.\n#\n# Both :class:`~sklearn.preprocessing.StandardScaler` and\n# :class:`~sklearn.preprocessing.MinMaxScaler` are very sensitive to the\n# presence of outliers.\n\nmake_plot(2)\n\n# %%\n# MaxAbsScaler\n# ------------\n#\n# :class:`~sklearn.preprocessing.MaxAbsScaler` is similar to\n# :class:`~sklearn.preprocessing.MinMaxScaler` except that the\n# values are mapped across several ranges depending on whether negative\n# OR positive values are present. If only positive values are present, the\n# range is [0, 1]. If only negative values are present, the range is [-1, 0].\n# If both negative and positive values are present, the range is [-1, 1].\n# On positive only data, both :class:`~sklearn.preprocessing.MinMaxScaler`\n# and :class:`~sklearn.preprocessing.MaxAbsScaler` behave similarly.\n# :class:`~sklearn.preprocessing.MaxAbsScaler` therefore also suffers from\n# the presence of large outliers.\n\nmake_plot(3)\n\n# %%\n# RobustScaler\n# ------------\n#\n# Unlike the previous scalers, the centering and scaling statistics of\n# :class:`~sklearn.preprocessing.RobustScaler`\n# are based on percentiles and are therefore not influenced by a small\n# number of very large marginal outliers. Consequently, the resulting range of\n# the transformed feature values is larger than for the previous scalers and,\n# more importantly, are approximately similar: for both features most of the",
                "filename": "examples/preprocessing/plot_all_scaling.py",
                "start_index": 8518,
                "end_index": 11498,
                "start_line": 258,
                "end_line": 329,
                "max_line": 400,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "class MinMaxScaler(OneToOneFeatureMixin, TransformerMixin, BaseEstimator):",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 9738,
                "end_index": 9812,
                "start_line": 277,
                "end_line": 277,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"\"\"\n===================================\nColumn Transformer with Mixed Types\n===================================\n\n.. currentmodule:: sklearn\n\nThis example illustrates how to apply different preprocessing and feature\nextraction pipelines to different subsets of features, using\n:class:`~compose.ColumnTransformer`. This is particularly handy for the\ncase of datasets that contain heterogeneous data types, since we may want to\nscale the numeric features and one-hot encode the categorical ones.\n\nIn this example, the numeric data is standard-scaled after mean-imputation. The\ncategorical data is one-hot encoded via ``OneHotEncoder``, which\ncreates a new category for missing values. We further reduce the dimensionality\nby selecting categories using a chi-squared test.\n\nIn addition, we show two different ways to dispatch the columns to the\nparticular pre-processor: by column names and by column data types.\n\nFinally, the preprocessing pipeline is integrated in a full prediction pipeline\nusing :class:`~pipeline.Pipeline`, together with a simple classification\nmodel.\n\n\"\"\"\n\n# Author: Pedro Morales <part.morales@gmail.com>\n#\n# License: BSD 3 clause\n\n# %%\nimport numpy as np\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.feature_selection import SelectPercentile, chi2\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nnp.random.seed(0)\n\n# %%\n# Load data from https://www.openml.org/d/40945\nX, y = fetch_openml(\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n)\n\n# Alternatively X and y can be obtained directly from the frame attribute:\n# X = titanic.frame.drop('survived', axis=1)\n# y = titanic.frame['survived']\n\n# %%\n# Use ``ColumnTransformer`` by selecting column by names\n#\n# We will train our classifier with the following features:\n#\n# Numeric Features:\n#\n# * ``age``: float;\n# * ``fare``: float.\n#\n# Categorical Features:\n#\n# * ``embarked``: categories encoded as strings ``{'C', 'S', 'Q'}``;\n# * ``sex``: categories encoded as strings ``{'female', 'male'}``;\n# * ``pclass``: ordinal integers ``{1, 2, 3}``.\n#\n# We create the preprocessing pipelines for both numeric and categorical data.\n# Note that ``pclass`` could either be treated as a categorical or numeric\n# feature.\n\nnumeric_features = [\"age\", \"fare\"]\nnumeric_transformer = Pipeline(\n    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n)\n\ncategorical_features = [\"embarked\", \"sex\", \"pclass\"]\ncategorical_transformer = Pipeline(\n    steps=[\n        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n        (\"selector\", SelectPercentile(chi2, percentile=50)),\n    ]\n)",
                "filename": "examples/compose/plot_column_transformer_mixed_types.py",
                "start_index": 0,
                "end_index": 2875,
                "start_line": 1,
                "end_line": 234,
                "max_line": 234,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"\"\"\n=============================================================\nCompare the effect of different scalers on data with outliers\n=============================================================\n\nFeature 0 (median income in a block) and feature 5 (average house occupancy) of\nthe :ref:`california_housing_dataset` have very\ndifferent scales and contain some very large outliers. These two\ncharacteristics lead to difficulties to visualize the data and, more\nimportantly, they can degrade the predictive performance of many machine\nlearning algorithms. Unscaled data can also slow down or even prevent the\nconvergence of many gradient-based estimators.\n\nIndeed many estimators are designed with the assumption that each feature takes\nvalues close to zero or more importantly that all features vary on comparable\nscales. In particular, metric-based and gradient-based estimators often assume\napproximately standardized data (centered features with unit variances). A\nnotable exception are decision tree-based estimators that are robust to\narbitrary scaling of the data.\n\nThis example uses different scalers, transformers, and normalizers to bring the\ndata within a pre-defined range.\n\nScalers are linear (or more precisely affine) transformers and differ from each\nother in the way they estimate the parameters used to shift and scale each\nfeature.\n\n:class:`~sklearn.preprocessing.QuantileTransformer` provides non-linear\ntransformations in which distances\nbetween marginal outliers and inliers are shrunk.\n:class:`~sklearn.preprocessing.PowerTransformer` provides\nnon-linear transformations in which data is mapped to a normal distribution to\nstabilize variance and minimize skewness.\n\nUnlike the previous transformations, normalization refers to a per sample\ntransformation instead of a per feature transformation.\n\nThe following code is a bit verbose, feel free to jump directly to the analysis\nof the results_.\n\n\"\"\"\n\n# Author:  Raghav RV <rvraghav93@gmail.com>\n#          Guillaume Lemaitre <g.lemaitre58@gmail.com>\n#          Thomas Unterthiner\n# License: BSD 3 clause\n\nimport matplotlib as mpl\nimport numpy as np\nfrom matplotlib import cm\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import (\n    MaxAbsScaler,\n    MinMaxScaler,\n    Normalizer,\n    PowerTransformer,\n    QuantileTransformer,\n    RobustScaler,\n    StandardScaler,\n    minmax_scale,\n)\n\ndataset = fetch_california_housing()\nX_full, y_full = dataset.data, dataset.target\nfeature_names = dataset.feature_names\n\nfeature_mapping = {\n    \"MedInc\": \"Median income in block\",\n    \"HouseAge\": \"Median house age in block\",\n    \"AveRooms\": \"Average number of rooms\",\n    \"AveBedrms\": \"Average number of bedrooms\",\n    \"Population\": \"Block population\",\n    \"AveOccup\": \"Average house occupancy\",\n    \"Latitude\": \"House block latitude\",\n    \"Longitude\": \"House block longitude\",\n}\n\n# Take only 2 features to make visualization easier",
                "filename": "examples/preprocessing/plot_all_scaling.py",
                "start_index": 0,
                "end_index": 2958,
                "start_line": 1,
                "end_line": 80,
                "max_line": 400,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "def _check_large_sparse(X, accept_large_sparse=False):\n    \"\"\"Raise a ValueError if X has 64bit indices and accept_large_sparse=False\"\"\"\n    if not accept_large_sparse:\n        supported_indices = [\"int32\"]\n        if X.getformat() == \"coo\":\n            index_keys = [\"col\", \"row\"]\n        elif X.getformat() in [\"csr\", \"csc\", \"bsr\"]:\n            index_keys = [\"indices\", \"indptr\"]\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if indices_datatype not in supported_indices:\n                raise ValueError(\n                    \"Only sparse matrices with 32-bit integer indices are accepted.\"\n                    f\" Got {indices_datatype} indices. Please do report a minimal\"\n                    \" reproducer on scikit-learn issue tracker so that support for\"\n                    \" your use-case can be studied by maintainers. See:\"\n                    \" https://scikit-learn.org/dev/developers/minimal_reproducer.html\"\n                )",
                "filename": "sklearn/utils/validation.py",
                "start_index": 35996,
                "end_index": 37016,
                "start_line": 1001,
                "end_line": 2214,
                "max_line": 2282,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "- type: markdown\n  attributes:\n    value: >\n      #### Before submitting a bug, please make sure the issue hasn't been already\n      addressed by searching through [the past issues](https://github.com/scikit-learn/scikit-learn/issues).\n- type: textarea\n  attributes:\n    label: Describe the bug\n    description: >\n      A clear and concise description of what the bug is.\n  validations:\n    required: true\n- type: textarea\n  attributes:\n    label: Steps/Code to Reproduce\n    description: |\n      Please add a [minimal code example](https://scikit-learn.org/dev/developers/minimal_reproducer.html) that can reproduce the error when running it. Be as succinct as possible, **do not depend on external data files**: instead you can generate synthetic data using `numpy.random`, [sklearn.datasets.make_regression](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html), [sklearn.datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) or a few lines of Python code. Example:\n\n      ```python\n      from sklearn.feature_extraction.text import CountVectorizer\n      from sklearn.decomposition import LatentDirichletAllocation\n      docs = [\"Help I have a bug\" for i in range(1000)]\n      vectorizer = CountVectorizer(input=docs, analyzer='word')\n      lda_features = vectorizer.fit_transform(docs)\n      lda_model = LatentDirichletAllocation(\n          n_topics=10,\n          learning_method='online',\n          evaluate_every=10,\n          n_jobs=4,\n      )\n      model = lda_model.fit(lda_features)\n      ```\n\n      If the code is too long, feel free to put it in a public gist and link it in the issue: https://gist.github.com.\n\n      In short, **we are going to copy-paste your code** to run it and we expect to get the same result as you.\n\n      We acknowledge that crafting a [minimal reproducible code example](https://scikit-learn.org/dev/developers/minimal_reproducer.html) requires some effort on your side but it really helps the maintainers quickly reproduce the problem and analyze its cause without any ambiguity. Ambiguous bug reports tend to be slower to fix because they will require more effort and back and forth discussion between the maintainers and the reporter to pin-point the precise conditions necessary to reproduce the problem.\n    placeholder: |\n      ```\n      Sample code to reproduce the problem\n      ```\n  validations:\n    required: true\n- type: textarea\n  attributes:\n    label: Expected Results\n    description: >\n      Please paste or describe the expected results.\n    placeholder: >\n      Example: No error is thrown.\n  validations:\n    required: true",
                "filename": ".github/ISSUE_TEMPLATE/bug_report.yml",
                "start_index": 126,
                "end_index": 2812,
                "start_line": 6,
                "end_line": 91,
                "max_line": 95,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/preprocessing/_data.py": [
                {
                    "chunk": {
                        "code": "class MaxAbsScaler(OneToOneFeatureMixin, TransformerMixin, BaseEstimator):",
                        "filename": "sklearn/preprocessing/_data.py",
                        "start_index": 36804,
                        "end_index": 36878,
                        "start_line": 1074,
                        "end_line": 1074,
                        "max_line": 3519,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet defines the MaxAbsScaler class, which is directly related to the issue as the class lacks a 'dtype' argument that could prevent upcasting."
                }
            ],
            "sklearn/utils/estimator_checks.py": [
                {
                    "chunk": {
                        "code": "@ignore_warnings\ndef check_estimators_dtypes(name, estimator_orig):\n    rnd = np.random.RandomState(0)\n    X_train_32 = 3 * rnd.uniform(size=(20, 5)).astype(np.float32)\n    X_train_32 = _enforce_estimator_tags_X(estimator_orig, X_train_32)\n    X_train_64 = X_train_32.astype(np.float64)\n    X_train_int_64 = X_train_32.astype(np.int64)\n    X_train_int_32 = X_train_32.astype(np.int32)\n    y = X_train_int_64[:, 0]\n    y = _enforce_estimator_tags_y(estimator_orig, y)\n\n    methods = [\"predict\", \"transform\", \"decision_function\", \"predict_proba\"]\n\n    for X_train in [X_train_32, X_train_64, X_train_int_64, X_train_int_32]:\n        estimator = clone(estimator_orig)\n        set_random_state(estimator, 1)\n        estimator.fit(X_train, y)\n\n        for method in methods:\n            if hasattr(estimator, method):\n                getattr(estimator, method)(X_train)\n\n\ndef check_transformer_preserve_dtypes(name, transformer_orig):\n    # check that dtype are preserved meaning if input X is of some dtype\n    # X_transformed should be from the same dtype.\n    X, y = make_blobs(\n        n_samples=30,\n        centers=[[0, 0, 0], [1, 1, 1]],\n        random_state=0,\n        cluster_std=0.1,\n    )\n    X = StandardScaler().fit_transform(X)\n    X = _enforce_estimator_tags_X(transformer_orig, X)\n\n    for dtype in _safe_tags(transformer_orig, key=\"preserves_dtype\"):\n        X_cast = X.astype(dtype)\n        transformer = clone(transformer_orig)\n        set_random_state(transformer)\n        X_trans1 = transformer.fit_transform(X_cast, y)\n        X_trans2 = transformer.fit(X_cast, y).transform(X_cast)\n\n        for Xt, method in zip([X_trans1, X_trans2], [\"fit_transform\", \"transform\"]):\n            if isinstance(Xt, tuple):\n                # cross-decompostion returns a tuple of (x_scores, y_scores)\n                # when given y with fit_transform; only check the first element\n                Xt = Xt[0]\n\n            # check that the output dtype is preserved\n            assert Xt.dtype == dtype, (\n                f\"{name} (method={method}) does not preserve dtype. \"\n                f\"Original/Expected dtype={dtype.__name__}, got dtype={Xt.dtype}.\"\n            )",
                        "filename": "sklearn/utils/estimator_checks.py",
                        "start_index": 65098,
                        "end_index": 67267,
                        "start_line": 181,
                        "end_line": 4630,
                        "max_line": 4639,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains tests that check if estimators preserve dtypes, which is relevant to the issue since MaxAbsScaler is not preserving the dtype as expected."
                }
            ],
            "examples/preprocessing/plot_all_scaling.py": [
                {
                    "chunk": {
                        "code": "# compacted to a specific range, [0, 10] for the median income and [0, 6] for\n# the average house occupancy. Note that there are some marginal outliers (some\n# blocks have average occupancy of more than 1200). Therefore, a specific\n# pre-processing can be very beneficial depending of the application. In the\n# following, we present some insights and behaviors of those pre-processing\n# methods in the presence of marginal outliers.\n\nmake_plot(0)\n\n# %%\n# StandardScaler\n# --------------\n#\n# :class:`~sklearn.preprocessing.StandardScaler` removes the mean and scales\n# the data to unit variance. The scaling shrinks the range of the feature\n# values as shown in the left figure below.\n# However, the outliers have an influence when computing the empirical mean and\n# standard deviation. Note in particular that because the outliers on each\n# feature have different magnitudes, the spread of the transformed data on\n# each feature is very different: most of the data lie in the [-2, 4] range for\n# the transformed median income feature while the same data is squeezed in the\n# smaller [-0.2, 0.2] range for the transformed average house occupancy.\n#\n# :class:`~sklearn.preprocessing.StandardScaler` therefore cannot guarantee\n# balanced feature scales in the\n# presence of outliers.\n\nmake_plot(1)\n\n# %%\n# MinMaxScaler\n# ------------\n#\n# :class:`~sklearn.preprocessing.MinMaxScaler` rescales the data set such that\n# all feature values are in\n# the range [0, 1] as shown in the right panel below. However, this scaling\n# compresses all inliers into the narrow range [0, 0.005] for the transformed\n# average house occupancy.\n#\n# Both :class:`~sklearn.preprocessing.StandardScaler` and\n# :class:`~sklearn.preprocessing.MinMaxScaler` are very sensitive to the\n# presence of outliers.\n\nmake_plot(2)\n\n# %%\n# MaxAbsScaler\n# ------------\n#\n# :class:`~sklearn.preprocessing.MaxAbsScaler` is similar to\n# :class:`~sklearn.preprocessing.MinMaxScaler` except that the\n# values are mapped across several ranges depending on whether negative\n# OR positive values are present. If only positive values are present, the\n# range is [0, 1]. If only negative values are present, the range is [-1, 0].\n# If both negative and positive values are present, the range is [-1, 1].\n# On positive only data, both :class:`~sklearn.preprocessing.MinMaxScaler`\n# and :class:`~sklearn.preprocessing.MaxAbsScaler` behave similarly.\n# :class:`~sklearn.preprocessing.MaxAbsScaler` therefore also suffers from\n# the presence of large outliers.\n\nmake_plot(3)\n\n# %%\n# RobustScaler\n# ------------\n#\n# Unlike the previous scalers, the centering and scaling statistics of\n# :class:`~sklearn.preprocessing.RobustScaler`\n# are based on percentiles and are therefore not influenced by a small\n# number of very large marginal outliers. Consequently, the resulting range of\n# the transformed feature values is larger than for the previous scalers and,\n# more importantly, are approximately similar: for both features most of the",
                        "filename": "examples/preprocessing/plot_all_scaling.py",
                        "start_index": 8518,
                        "end_index": 11498,
                        "start_line": 258,
                        "end_line": 329,
                        "max_line": 400,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet discusses the behavior of MaxAbsScaler and its impact on data, which could provide insight into why it's upcasting the dtype."
                }
            ],
            "sklearn/utils/validation.py": [
                {
                    "chunk": {
                        "code": "def _check_large_sparse(X, accept_large_sparse=False):\n    \"\"\"Raise a ValueError if X has 64bit indices and accept_large_sparse=False\"\"\"\n    if not accept_large_sparse:\n        supported_indices = [\"int32\"]\n        if X.getformat() == \"coo\":\n            index_keys = [\"col\", \"row\"]\n        elif X.getformat() in [\"csr\", \"csc\", \"bsr\"]:\n            index_keys = [\"indices\", \"indptr\"]\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if indices_datatype not in supported_indices:\n                raise ValueError(\n                    \"Only sparse matrices with 32-bit integer indices are accepted.\"\n                    f\" Got {indices_datatype} indices. Please do report a minimal\"\n                    \" reproducer on scikit-learn issue tracker so that support for\"\n                    \" your use-case can be studied by maintainers. See:\"\n                    \" https://scikit-learn.org/dev/developers/minimal_reproducer.html\"\n                )",
                        "filename": "sklearn/utils/validation.py",
                        "start_index": 35996,
                        "end_index": 37016,
                        "start_line": 1001,
                        "end_line": 2214,
                        "max_line": 2282,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet is about checking large sparse matrices and their dtypes, which might be indirectly related to the issue of dtype handling in MaxAbsScaler."
                }
            ]
        },
        "solution_task": "- Add a `dtype` argument to the `MaxAbsScaler` class constructor and pass it to the `check_array` function within the `fit` and `transform` methods.\n- Modify the `fit` and `transform` methods of `MaxAbsScaler` to include the `dtype` argument when calling `check_array`.\n- Ensure that the `dtype` argument is used to preserve the data type of the input array throughout the scaling process.\n- Update the `ColumnTransformer` to pass the `dtype` argument to the transformers if they accept a `dtype` parameter.\n- Add tests to `check_transformer_preserve_dtypes` to verify that `MaxAbsScaler` preserves the specified `dtype` when transforming data.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE provides a detailed and actionable solution to the problem of the MaxAbsScaler upcasting pandas data types to float64. It suggests adding a 'dtype' argument to the MaxAbsScaler class and ensuring that this argument is used throughout the scaling process to preserve the input data type. This directly addresses the issue described in the PROMPT. Additionally, the RESPONSE includes the suggestion to update the ColumnTransformer to pass the 'dtype' argument to transformers, which is relevant to the user's context of working with the ColumnTransformer. The RESPONSE also recommends adding tests to verify that the dtype is preserved, which is a good practice for ensuring the bug is fixed and does not reoccur. The inclusion of code snippets to check provides a starting point for the user to begin implementing the suggested changes.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711738934.318671,
        "result": "- Add a `dtype` argument to the `MaxAbsScaler` class constructor and pass it to the `check_array` function within the `fit` and `transform` methods.\n- Modify the `fit` and `transform` methods of `MaxAbsScaler` to include the `dtype` argument when calling `check_array`.\n- Ensure that the `dtype` argument is used to preserve the data type of the input array throughout the scaling process.\n- Update the `ColumnTransformer` to pass the `dtype` argument to the transformers if they accept a `dtype` parameter.\n- Add tests to `check_transformer_preserve_dtypes` to verify that `MaxAbsScaler` preserves the specified `dtype` when transforming data.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [preprocessing](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/preprocessing)- [Lines 1074 - 1074](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/preprocessing/_data.py#L1074-L1074)\n\n This snippet defines the MaxAbsScaler class, which is directly related to the issue as the class lacks a 'dtype' argument that could prevent upcasting.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/utils)- [Lines 181 - 4630](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/estimator_checks.py#L181-L4630)\n\n This snippet contains tests that check if estimators preserve dtypes, which is relevant to the issue since MaxAbsScaler is not preserving the dtype as expected.\n-  \u2192 [examples](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//examples) \u2192 [preprocessing](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//examples/preprocessing)- [Lines 258 - 329](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//examples/preprocessing/plot_all_scaling.py#L258-L329)\n\n This snippet discusses the behavior of MaxAbsScaler and its impact on data, which could provide insight into why it's upcasting the dtype.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/utils)- [Lines 1001 - 2214](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/validation.py#L1001-L2214)\n\n This snippet is about checking large sparse matrices and their dtypes, which might be indirectly related to the issue of dtype handling in MaxAbsScaler.",
        "context_relevance": 0.27,
        "hypotheses": [],
        "groundedness_score": 1.0,
        "qa_relevance_reasons": {
            "reason": "Criteria: The RESPONSE must be relevant to the entire PROMPT and answer the entire PROMPT completely to get a score of 10.\nSupporting Evidence: The RESPONSE provides a detailed solution to the issue described in the PROMPT, which is the lack of a `dtype` argument in the `MaxAbsScaler` class leading to unwanted upcasting of data types. It suggests adding a `dtype` argument to the `MaxAbsScaler` and ensuring it is used throughout the scaling process, including in the `ColumnTransformer`. The RESPONSE also recommends updating tests to ensure the preservation of data types. Additionally, it provides relevant code snippets from the scikit-learn repository that are directly related to the issue, demonstrating a comprehensive understanding and addressing the entire PROMPT."
        },
        "qa_relevance": 1.0
    }
}