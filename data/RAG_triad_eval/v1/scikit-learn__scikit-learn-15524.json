{
    "id": "scikit-learn__scikit-learn-15524",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
    "task_id": 3980433625269968876,
    "title": "Nested Cross Validation for precomputed KNN",
    "body": "#### Description\r\nA nested cross validation prediction using a knn with precomputed metric raised an error\r\n\r\n\r\n#### Code to Reproduce\r\n```python\r\nfrom sklearn import datasets\r\nfrom sklearn.model_selection import cross_val_predict, GridSearchCV\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.metrics.pairwise import euclidean_distances\r\n\r\n# Open data\r\niris = datasets.load_iris()\r\n\r\n# Compute pairwise metric\r\nmetric = euclidean_distances(iris.data)\r\n\r\n# Create nested cross validation\r\nknn = KNeighborsClassifier(metric = 'precomputed')\r\nknngs = GridSearchCV(knn, param_grid={\"n_neighbors\": [1, 5, 10]})\r\npredicted = cross_val_predict(knngs, metric, iris.target, cv=10)\r\n```\r\n\r\n#### Expected Results\r\nShould return the predictions made by the model as the following code produces:\r\n\r\n```python\r\nfrom sklearn import datasets\r\nfrom sklearn.model_selection import cross_val_predict, GridSearchCV\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\n\r\n# Open data\r\niris = datasets.load_iris()\r\n\r\n# Create nested cross validation\r\nknn = KNeighborsClassifier()\r\nknngs = GridSearchCV(knn, param_grid={\"n_neighbors\": [1, 5, 10]})\r\npredicted = cross_val_predict(knngs, iris.data, iris.target, cv=10)\r\n```\r\n\r\n\r\n#### Actual Results\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-8-97c590e3aa1e> in <module>()\r\n     10 \r\n     11 knngs = GridSearchCV(knn, param_grid={\"n_neighbors\": [1, 5, 10]})\r\n---> 12 predicted = cross_val_predict(knngs, metric, iris.target, cv=10)\r\n\r\n/sklearn/model_selection/_validation.py in cross_val_predict(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\r\n    775     prediction_blocks = parallel(delayed(_fit_and_predict)(\r\n    776         clone(estimator), X, y, train, test, verbose, fit_params, method)\r\n--> 777         for train, test in cv.split(X, y, groups))\r\n    778 \r\n    779     # Concatenate the predictions\r\n\r\n/sklearn/externals/joblib/parallel.py in __call__(self, iterable)\r\n    915             # remaining jobs.\r\n    916             self._iterating = False\r\n--> 917             if self.dispatch_one_batch(iterator):\r\n    918                 self._iterating = self._original_iterator is not None\r\n    919 \r\n\r\n/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self, iterator)\r\n    757                 return False\r\n    758             else:\r\n--> 759                 self._dispatch(tasks)\r\n    760                 return True\r\n    761 \r\n\r\n/sklearn/externals/joblib/parallel.py in _dispatch(self, batch)\r\n    714         with self._lock:\r\n    715             job_idx = len(self._jobs)\r\n--> 716             job = self._backend.apply_async(batch, callback=cb)\r\n    717             # A job can complete so quickly than its callback is\r\n    718             # called before we get here, causing self._jobs to\r\n\r\n/sklearn/externals/joblib/_parallel_backends.py in apply_async(self, func, callback)\r\n    180     def apply_async(self, func, callback=None):\r\n    181         \"\"\"Schedule a func to be run\"\"\"\r\n--> 182         result = ImmediateResult(func)\r\n    183         if callback:\r\n    184             callback(result)\r\n\r\n/sklearn/externals/joblib/_parallel_backends.py in __init__(self, batch)\r\n    547         # Don't delay the application, to avoid keeping the input\r\n    548         # arguments in memory\r\n--> 549         self.results = batch()\r\n    550 \r\n    551     def get(self):\r\n\r\n/sklearn/externals/joblib/parallel.py in __call__(self)\r\n    223         with parallel_backend(self._backend, n_jobs=self._n_jobs):\r\n    224             return [func(*args, **kwargs)\r\n--> 225                     for func, args, kwargs in self.items]\r\n    226 \r\n    227     def __len__(self):\r\n\r\n/sklearn/externals/joblib/parallel.py in <listcomp>(.0)\r\n    223         with parallel_backend(self._backend, n_jobs=self._n_jobs):\r\n    224             return [func(*args, **kwargs)\r\n--> 225                     for func, args, kwargs in self.items]\r\n    226 \r\n    227     def __len__(self):\r\n\r\n/sklearn/model_selection/_validation.py in _fit_and_predict(estimator, X, y, train, test, verbose, fit_params, method)\r\n    848         estimator.fit(X_train, **fit_params)\r\n    849     else:\r\n--> 850         estimator.fit(X_train, y_train, **fit_params)\r\n    851     func = getattr(estimator, method)\r\n    852     predictions = func(X_test)\r\n\r\n/sklearn/model_selection/_search.py in fit(self, X, y, groups, **fit_params)\r\n    720                 return results_container[0]\r\n    721 \r\n--> 722             self._run_search(evaluate_candidates)\r\n    723 \r\n    724         results = results_container[0]\r\n\r\n/sklearn/model_selection/_search.py in _run_search(self, evaluate_candidates)\r\n   1189     def _run_search(self, evaluate_candidates):\r\n   1190         \"\"\"Search all candidates in param_grid\"\"\"\r\n-> 1191         evaluate_candidates(ParameterGrid(self.param_grid))\r\n   1192 \r\n   1193 \r\n\r\n/sklearn/model_selection/_search.py in evaluate_candidates(candidate_params)\r\n    709                                for parameters, (train, test)\r\n    710                                in product(candidate_params,\r\n--> 711                                           cv.split(X, y, groups)))\r\n    712 \r\n    713                 all_candidate_params.extend(candidate_params)\r\n\r\n/sklearn/externals/joblib/parallel.py in __call__(self, iterable)\r\n    915             # remaining jobs.\r\n    916             self._iterating = False\r\n--> 917             if self.dispatch_one_batch(iterator):\r\n    918                 self._iterating = self._original_iterator is not None\r\n    919 \r\n\r\n/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self, iterator)\r\n    757                 return False\r\n    758             else:\r\n--> 759                 self._dispatch(tasks)\r\n    760                 return True\r\n    761 \r\n\r\n/sklearn/externals/joblib/parallel.py in _dispatch(self, batch)\r\n    714         with self._lock:\r\n    715             job_idx = len(self._jobs)\r\n--> 716             job = self._backend.apply_async(batch, callback=cb)\r\n    717             # A job can complete so quickly than its callback is\r\n    718             # called before we get here, causing self._jobs to\r\n\r\n/sklearn/externals/joblib/_parallel_backends.py in apply_async(self, func, callback)\r\n    180     def apply_async(self, func, callback=None):\r\n    181         \"\"\"Schedule a func to be run\"\"\"\r\n--> 182         result = ImmediateResult(func)\r\n    183         if callback:\r\n    184             callback(result)\r\n\r\n/sklearn/externals/joblib/_parallel_backends.py in __init__(self, batch)\r\n    547         # Don't delay the application, to avoid keeping the input\r\n    548         # arguments in memory\r\n--> 549         self.results = batch()\r\n    550 \r\n    551     def get(self):\r\n\r\n/sklearn/externals/joblib/parallel.py in __call__(self)\r\n    223         with parallel_backend(self._backend, n_jobs=self._n_jobs):\r\n    224             return [func(*args, **kwargs)\r\n--> 225                     for func, args, kwargs in self.items]\r\n    226 \r\n    227     def __len__(self):\r\n\r\n/sklearn/externals/joblib/parallel.py in <listcomp>(.0)\r\n    223         with parallel_backend(self._backend, n_jobs=self._n_jobs):\r\n    224             return [func(*args, **kwargs)\r\n--> 225                     for func, args, kwargs in self.items]\r\n    226 \r\n    227     def __len__(self):\r\n\r\n/sklearn/model_selection/_validation.py in _fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\r\n    516     start_time = time.time()\r\n    517 \r\n--> 518     X_train, y_train = _safe_split(estimator, X, y, train)\r\n    519     X_test, y_test = _safe_split(estimator, X, y, test, train)\r\n    520 \r\n\r\n/sklearn/utils/metaestimators.py in _safe_split(estimator, X, y, indices, train_indices)\r\n    195         # X is a precomputed square kernel matrix\r\n    196         if X.shape[0] != X.shape[1]:\r\n--> 197             raise ValueError(\"X should be a square kernel matrix\")\r\n    198         if train_indices is None:\r\n    199             X_subset = X[np.ix_(indices, indices)]\r\n\r\nValueError: X should be a square kernel matrix\r\n```\r\n\r\n#### Versions\r\nsklearn 0.20.2\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.22.git",
    "requested_commit": "",
    "mayil_response": "- The error message indicates that the input `X` to the `KNeighborsClassifier` with a precomputed metric is not a square matrix. This is a requirement for distance/kernel matrices when using the 'precomputed' metric option.\n- The provided code snippet suggests that the `euclidean_distances` function is used to compute the pairwise distances between samples in the `iris` dataset, and this distance matrix is then used as input `X` to `cross_val_predict` with a `KNeighborsClassifier` that has been initialized with `metric='precomputed'`.\n- The `cross_val_predict` function expects the `X` input to be the same shape as the input to the `fit` method of the estimator. However, when using `GridSearchCV` with `cross_val_predict`, the `GridSearchCV` object is passed as the estimator, which expects the original data, not the precomputed distance matrix.\n\nTo resolve the issue, you should modify the code to ensure that `cross_val_predict` is called with the original data, and the precomputed distance matrix is only used within the `GridSearchCV` through a pipeline that computes the distances on the fly for each fold of the cross-validation. Here's how you can adjust the code:\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import cross_val_predict, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier, DistanceMetric\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import euclidean_distances\n\n# Load the iris dataset\niris = load_iris()\n\n# Initialize the KNeighborsClassifier without specifying 'metric' parameter\nknn = KNeighborsClassifier()\n\n# Create a pipeline that computes the distance matrix on the fly\npipe = Pipeline([\n    ('distance', DistanceMetric.get_metric('euclidean').pairwise),\n    ('knn', knn)\n])\n\n# Set up the grid search with specified hyperparameters including the 'precomputed' metric\nparam_grid = {\"knn__n_neighbors\": [1, 5, 10], \"knn__metric\": ['precomputed']}\nknngs = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=10)\n\n# Perform cross-validation and get predictions\npredicted = cross_val_predict(estimator=knngs, X=iris.data, y=iris.target, cv=10)\n```\n\n- In the pipeline, the first step computes the pairwise euclidean distances, and the second step is the `KNeighborsClassifier` with the 'precomputed' metric.\n- The `param_grid` is adjusted to include the `knn__` prefix for the parameters of the `KNeighborsClassifier` within the pipeline.\n- The `cross_val_predict` function is called with the original `iris.data` as `X`, and the `GridSearchCV` object `knngs` handles the precomputation of distances internally for each fold.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [model_selection](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/model_selection)- [Lines 1060 - 1808](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/model_selection/_search.py#L1060-L1808)\n\n This snippet contains the implementation of GridSearchCV which is used in the code to reproduce the issue. It is relevant to understand how the GridSearchCV interacts with the KNeighborsClassifier when the metric is precomputed.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [neighbors](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/neighbors)- [Lines 807 - 1247](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/neighbors/_base.py#L807-L1247)\n\n This snippet from '_base.py' is relevant as it contains the logic for handling the 'precomputed' metric in KNeighborsClassifier, which is directly related to the issue reported.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0003105,
        "snippet_processor": 0.11024,
        "issue_star_creation": 0.03695,
        "issue_star_solver": 0.1084,
        "bouncer": 0.03046
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711738738.474735,
        "relevant_snippets": [
            {
                "code": "\"\"\"\n=========================================\nNested versus non-nested cross-validation\n=========================================\n\nThis example compares non-nested and nested cross-validation strategies on a\nclassifier of the iris data set. Nested cross-validation (CV) is often used to\ntrain a model in which hyperparameters also need to be optimized. Nested CV\nestimates the generalization error of the underlying model and its\n(hyper)parameter search. Choosing the parameters that maximize non-nested CV\nbiases the model to the dataset, yielding an overly-optimistic score.\n\nModel selection without nested CV uses the same data to tune model parameters\nand evaluate model performance. Information may thus \"leak\" into the model\nand overfit the data. The magnitude of this effect is primarily dependent on\nthe size of the dataset and the stability of the model. See Cawley and Talbot\n[1]_ for an analysis of these issues.\n\nTo avoid this problem, nested CV effectively uses a series of\ntrain/validation/test set splits. In the inner loop (here executed by\n:class:`GridSearchCV <sklearn.model_selection.GridSearchCV>`), the score is\napproximately maximized by fitting a model to each training set, and then\ndirectly maximized in selecting (hyper)parameters over the validation set. In\nthe outer loop (here in :func:`cross_val_score\n<sklearn.model_selection.cross_val_score>`), generalization error is estimated\nby averaging test set scores over several dataset splits.\n\nThe example below uses a support vector classifier with a non-linear kernel to\nbuild a model with optimized hyperparameters by grid search. We compare the\nperformance of non-nested and nested CV strategies by taking the difference\nbetween their scores.\n\n.. topic:: See Also:\n\n    - :ref:`cross_validation`\n    - :ref:`grid_search`\n\n.. topic:: References:\n\n    .. [1] `Cawley, G.C.; Talbot, N.L.C. On over-fitting in model selection and\n     subsequent selection bias in performance evaluation.\n     J. Mach. Learn. Res 2010,11, 2079-2107.\n     <http://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf>`_\n\n\"\"\"\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.svm import SVC\n\n# Number of random trials\nNUM_TRIALS = 30\n\n# Load the dataset\niris = load_iris()\nX_iris = iris.data\ny_iris = iris.target\n\n# Set up possible values of parameters to optimize over\np_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}\n\n# We will use a Support Vector Classifier with \"rbf\" kernel\nsvm = SVC(kernel=\"rbf\")\n\n# Arrays to store scores\nnon_nested_scores = np.zeros(NUM_TRIALS)\nnested_scores = np.zeros(NUM_TRIALS)\n\n# Loop for each trial",
                "filename": "examples/model_selection/plot_nested_cross_validation_iris.py",
                "start_index": 0,
                "end_index": 2728,
                "start_line": 1,
                "end_line": 72,
                "max_line": 127,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"\"\"Exhaustive search over specified parameter values for an estimator.\n\n    Important members are fit, predict.\n\n    GridSearchCV implements a \"fit\" and a \"score\" method.\n    It also implements \"score_samples\", \"predict\", \"predict_proba\",\n    \"decision_function\", \"transform\" and \"inverse_transform\" if they are\n    implemented in the estimator used.\n\n    The parameters of the estimator used to apply these methods are optimized\n    by cross-validated grid-search over a parameter grid.\n\n    Read more in the :ref:`User Guide <grid_search>`.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        This is assumed to implement the scikit-learn estimator interface.\n        Either estimator needs to provide a ``score`` function,\n        or ``scoring`` must be passed.\n\n    param_grid : dict or list of dictionaries\n        Dictionary with parameters names (`str`) as keys and lists of\n        parameter settings to try as values, or a list of such\n        dictionaries, in which case the grids spanned by each dictionary\n        in the list are explored. This enables searching over any sequence\n        of parameter settings.\n\n    scoring : str, callable, list, tuple or dict, default=None\n        Strategy to evaluate the performance of the cross-validated model on\n        the test set.\n\n        If `scoring` represents a single score, one can use:\n\n        - a single string (see :ref:`scoring_parameter`);\n        - a callable (see :ref:`scoring`) that returns a single value.\n\n        If `scoring` represents multiple scores, one can use:\n\n        - a list or tuple of unique strings;\n        - a callable returning a dictionary where the keys are the metric\n          names and the values are the metric scores;\n        - a dictionary with metric names as keys and callables a values.\n\n        See :ref:`multimetric_grid_search` for an example.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionchanged:: v0.20\n           `n_jobs` default changed from 1 to None\n\n    refit : bool, str, or callable, default=True\n        Refit an estimator using the best found parameters on the whole\n        dataset.\n\n        For multiple metric evaluation, this needs to be a `str` denoting the\n        scorer that would be used to find the best parameters for refitting\n        the estimator at the end.\n\n        Where there are considerations other than maximum score in\n        choosing a best estimator, ``refit`` can be set to a function which\n        returns the selected ``best_index_`` given ``cv_results_``. In that\n        case, the ``best_estimator_`` and ``best_params_`` will be set\n        according to the returned ``best_index_`` while the ``best_score_``\n        attribute will not be available.\n\n        The refitted estimator is made available at the ``best_estimator_``\n        attribute and permits using ``predict`` directly on this\n        ``GridSearchCV`` instance.\n\n        Also for multiple metric evaluation, the attributes ``best_index_``,\n        ``best_score_`` and ``best_params_`` will only be available if\n        ``refit`` is set and all of them will be determined w.r.t this specific\n        scorer.\n\n        See ``scoring`` parameter to know more about multiple metric\n        evaluation.\n\n        See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`\n        to see how to design a custom selection strategy using a callable\n        via `refit`.\n\n        .. versionchanged:: 0.20\n            Support for callable added.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used. These splitters are instantiated\n        with `shuffle=False` so the splits will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    verbose : int\n        Controls the verbosity: the higher, the more messages.\n\n        - >1 : the computation time for each fold and parameter candidate is\n          displayed;\n        - >2 : the score is also displayed;\n        - >3 : the fold and candidate parameter indexes are also displayed\n          together with the starting time of the computation.\n\n    pre_dispatch : int, or str, default='2*n_jobs'\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A str, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    error_score : 'raise' or numeric, default=np.nan\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised. If a numeric value is given,\n        FitFailedWarning is raised. This parameter does not affect the refit\n        step, which will always raise the error.\n\n    return_train_score : bool, default=False\n        If ``False``, the ``cv_results_`` attribute will not include training\n        scores.\n        Computing training scores is used to get insights on how different\n        parameter settings impact the overfitting/underfitting trade-off.\n        However computing the scores on the training set can be computationally\n        expensive and is not strictly required to select the parameters that\n        yield the best generalization performance.\n\n        .. versionadded:: 0.19\n\n        .. versionchanged:: 0.21\n            Default value was changed from ``True`` to ``False``\n\n    Attributes\n    ----------\n    cv_results_ : dict of numpy (masked) ndarrays\n        A dict with keys as column headers and values as columns, that can be\n        imported into a pandas ``DataFrame``.\n\n        For instance the below given table\n\n        +------------+-----------+------------+-----------------+---+---------+\n        |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n        +============+===========+============+=================+===+=========+\n        |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n        +------------+-----------+------------+-----------------+---+---------+\n\n        will be represented by a ``cv_results_`` dict of::\n\n            {\n            'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n                                         mask = [False False False False]...)\n            'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n                                        mask = [ True  True False False]...),\n            'param_degree': masked_array(data = [2.0 3.0 -- --],\n                                         mask = [False False  True  True]...),\n            'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n            'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n            'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n            'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n            'rank_test_score'    : [2, 4, 3, 1],\n            'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n            'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n            'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n            'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n            'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n            'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n            'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n            'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n            'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n            }\n\n        NOTE\n\n        The key ``'params'`` is used to store a list of parameter\n        settings dicts for all the parameter candidates.\n\n        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n        ``std_score_time`` are all in seconds.\n\n        For multi-metric evaluation, the scores for all the scorers are\n        available in the ``cv_results_`` dict at the keys ending with that\n        scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n        above. ('split0_test_precision', 'mean_train_precision' etc.)\n\n    best_estimator_ : estimator\n        Estimator that was chosen by the search, i.e. estimator\n        which gave highest score (or smallest loss if specified)\n        on the left out data. Not available if ``refit=False``.\n\n        See ``refit`` parameter for more information on allowed values.\n\n    best_score_ : float\n        Mean cross-validated score of the best_estimator\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n        This attribute is not available if ``refit`` is a function.\n\n    best_params_ : dict\n        Parameter setting that gave the best results on the hold out data.\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n    best_index_ : int\n        The index (of the ``cv_results_`` arrays) which corresponds to the best\n        candidate parameter setting.\n\n        The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n        the parameter setting for the best model, that gives the highest\n        mean score (``search.best_score_``).\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n    scorer_ : function or a dict\n        Scorer function used on the held out data to choose the best\n        parameters for the model.\n\n        For multi-metric evaluation, this attribute holds the validated\n        ``scoring`` dict which maps the scorer key to the scorer callable.\n\n    n_splits_ : int\n        The number of cross-validation splits (folds/iterations).\n\n    refit_time_ : float\n        Seconds used for refitting the best model on the whole dataset.\n\n        This is present only if ``refit`` is not False.\n\n        .. versionadded:: 0.20\n\n    multimetric_ : bool\n        Whether or not the scorers compute several metrics.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels. This is present only if ``refit`` is specified and\n        the underlying estimator is a classifier.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if\n        `best_estimator_` is defined (see the documentation for the `refit`\n        parameter for more details) and that `best_estimator_` exposes\n        `n_features_in_` when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if\n        `best_estimator_` is defined (see the documentation for the `refit`\n        parameter for more details) and that `best_estimator_` exposes\n        `feature_names_in_` when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    ParameterGrid : Generates all the combinations of a hyperparameter grid.\n    train_test_split : Utility function to split the data into a development\n        set usable for fitting a GridSearchCV instance and an evaluation set\n        for its final evaluation.\n    sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n        loss function.\n\n    Notes\n    -----\n    The parameters selected are those that maximize the score of the left out\n    data, unless an explicit score is passed in which case it is used instead.\n\n    If `n_jobs` was set to a value higher than one, the data is copied for each\n    point in the grid (and not `n_jobs` times). This is done for efficiency\n    reasons if individual jobs take very little time, but may raise errors if\n    the dataset is large and not enough memory is available.  A workaround in\n    this case is to set `pre_dispatch`. Then, the memory is copied only\n    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n    n_jobs`.\n\n    Examples\n    --------\n    >>> from sklearn import svm, datasets\n    >>> from sklearn.model_selection import GridSearchCV\n    >>> iris = datasets.load_iris()\n    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n    >>> svc = svm.SVC()\n    >>> clf = GridSearchCV(svc, parameters)\n    >>> clf.fit(iris.data, iris.target)\n    GridSearchCV(estimator=SVC(),\n                 param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n    >>> sorted(clf.cv_results_.keys())\n    ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n     'param_C', 'param_kernel', 'params',...\n     'rank_test_score', 'split0_test_score',...\n     'split2_test_score', ...\n     'std_fit_time', 'std_score_time', 'std_test_score']\n    \"\"\"",
                "filename": "sklearn/model_selection/_search.py",
                "start_index": 40693,
                "end_index": 54899,
                "start_line": 1060,
                "end_line": 1808,
                "max_line": 1813,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "for i in range(NUM_TRIALS):\n    # Choose cross-validation techniques for the inner and outer loops,\n    # independently of the dataset.\n    # E.g \"GroupKFold\", \"LeaveOneOut\", \"LeaveOneGroupOut\", etc.\n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n\n    # Non_nested parameter search and scoring\n    clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=outer_cv)\n    clf.fit(X_iris, y_iris)\n    non_nested_scores[i] = clf.best_score_\n\n    # Nested CV with parameter optimization\n    clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=inner_cv)\n    nested_score = cross_val_score(clf, X=X_iris, y=y_iris, cv=outer_cv)\n    nested_scores[i] = nested_score.mean()\n\nscore_difference = non_nested_scores - nested_scores\n\nprint(\n    \"Average difference of {:6f} with std. dev. of {:6f}.\".format(\n        score_difference.mean(), score_difference.std()\n    )\n)\n\n# Plot scores on each trial for nested and non-nested CV\nplt.figure()\nplt.subplot(211)\n(non_nested_scores_line,) = plt.plot(non_nested_scores, color=\"r\")\n(nested_line,) = plt.plot(nested_scores, color=\"b\")\nplt.ylabel(\"score\", fontsize=\"14\")\nplt.legend(\n    [non_nested_scores_line, nested_line],\n    [\"Non-Nested CV\", \"Nested CV\"],\n    bbox_to_anchor=(0, 0.4, 0.5, 0),\n)\nplt.title(\n    \"Non-Nested and Nested Cross Validation on Iris Dataset\",\n    x=0.5,\n    y=1.1,\n    fontsize=\"15\",\n)\n\n# Plot bar chart of the difference.\nplt.subplot(212)\ndifference_plot = plt.bar(range(NUM_TRIALS), score_difference)\nplt.xlabel(\"Individual Trial #\")\nplt.legend(\n    [difference_plot],\n    [\"Non-Nested CV - Nested CV Score\"],\n    bbox_to_anchor=(0, 1, 0.8, 0),\n)\nplt.ylabel(\"score difference\", fontsize=\"14\")\n\nplt.show()",
                "filename": "examples/model_selection/plot_nested_cross_validation_iris.py",
                "start_index": 2729,
                "end_index": 4475,
                "start_line": 73,
                "end_line": 127,
                "max_line": 127,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"\"\"\n=========================\nCaching nearest neighbors\n=========================\n\nThis examples demonstrates how to precompute the k nearest neighbors before\nusing them in KNeighborsClassifier. KNeighborsClassifier can compute the\nnearest neighbors internally, but precomputing them can have several benefits,\nsuch as finer parameter control, caching for multiple use, or custom\nimplementations.\n\nHere we use the caching property of pipelines to cache the nearest neighbors\ngraph between multiple fits of KNeighborsClassifier. The first call is slow\nsince it computes the neighbors graph, while subsequent call are faster as they\ndo not need to recompute the graph. Here the durations are small since the\ndataset is small, but the gain can be more substantial when the dataset grows\nlarger, or when the grid of parameter to search is large.\n\n\"\"\"\n\n# Author: Tom Dupre la Tour\n#\n# License: BSD 3 clause\nfrom tempfile import TemporaryDirectory\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsTransformer\nfrom sklearn.pipeline import Pipeline\n\nX, y = load_digits(return_X_y=True)\nn_neighbors_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n# The transformer computes the nearest neighbors graph using the maximum number\n# of neighbors necessary in the grid search. The classifier model filters the\n# nearest neighbors graph as required by its own n_neighbors parameter.\ngraph_model = KNeighborsTransformer(n_neighbors=max(n_neighbors_list), mode=\"distance\")\nclassifier_model = KNeighborsClassifier(metric=\"precomputed\")\n\n# Note that we give `memory` a directory to cache the graph computation\n# that will be used several times when tuning the hyperparameters of the\n# classifier.\nwith TemporaryDirectory(prefix=\"sklearn_graph_cache_\") as tmpdir:\n    full_model = Pipeline(\n        steps=[(\"graph\", graph_model), (\"classifier\", classifier_model)], memory=tmpdir\n    )\n\n    param_grid = {\"classifier__n_neighbors\": n_neighbors_list}\n    grid_model = GridSearchCV(full_model, param_grid)\n    grid_model.fit(X, y)\n\n# Plot the results of the grid search.\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\naxes[0].errorbar(\n    x=n_neighbors_list,\n    y=grid_model.cv_results_[\"mean_test_score\"],\n    yerr=grid_model.cv_results_[\"std_test_score\"],\n)\naxes[0].set(xlabel=\"n_neighbors\", title=\"Classification accuracy\")\naxes[1].errorbar(\n    x=n_neighbors_list,\n    y=grid_model.cv_results_[\"mean_fit_time\"],\n    yerr=grid_model.cv_results_[\"std_fit_time\"],\n    color=\"r\",\n)\naxes[1].set(xlabel=\"n_neighbors\", title=\"Fit time (with caching)\")\nfig.tight_layout()\nplt.show()",
                "filename": "examples/neighbors/plot_caching_nearest_neighbors.py",
                "start_index": 0,
                "end_index": 2682,
                "start_line": 1,
                "end_line": 70,
                "max_line": 70,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "@validate_params(\n    {\n        \"X\": [\"array-like\", \"sparse matrix\"],\n        \"X_embedded\": [\"array-like\", \"sparse matrix\"],\n        \"n_neighbors\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"metric\": [StrOptions(set(_VALID_METRICS) | {\"precomputed\"}), callable],\n    },\n    prefer_skip_nested_validation=True,\n)",
                "filename": "sklearn/manifold/_t_sne.py",
                "start_index": 14276,
                "end_index": 14599,
                "start_line": 449,
                "end_line": 1157,
                "max_line": 1158,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "if n_neighbors > n_samples_fit:\n            raise ValueError(\n                \"Expected n_neighbors <= n_samples, \"\n                \" but n_samples = %d, n_neighbors = %d\" % (n_samples_fit, n_neighbors)\n            )\n\n        n_jobs = effective_n_jobs(self.n_jobs)\n        chunked_results = None\n        use_pairwise_distances_reductions = (\n            self._fit_method == \"brute\"\n            and ArgKmin.is_usable_for(\n                X if X is not None else self._fit_X, self._fit_X, self.effective_metric_\n            )\n        )\n        if use_pairwise_distances_reductions:\n            results = ArgKmin.compute(\n                X=X,\n                Y=self._fit_X,\n                k=n_neighbors,\n                metric=self.effective_metric_,\n                metric_kwargs=self.effective_metric_params_,\n                strategy=\"auto\",\n                return_distance=return_distance,\n            )\n\n        elif (\n            self._fit_method == \"brute\" and self.metric == \"precomputed\" and issparse(X)\n        ):\n            results = _kneighbors_from_graph(\n                X, n_neighbors=n_neighbors, return_distance=return_distance\n            )\n\n        elif self._fit_method == \"brute\":\n            # Joblib-based backend, which is used when user-defined callable\n            # are passed for metric.\n\n            # This won't be used in the future once PairwiseDistancesReductions\n            # support:\n            #   - DistanceMetrics which work on supposedly binary data\n            #   - CSR-dense and dense-CSR case if 'euclidean' in metric.\n            reduce_func = partial(\n                self._kneighbors_reduce_func,\n                n_neighbors=n_neighbors,\n                return_distance=return_distance,\n            )\n\n            # for efficiency, use squared euclidean distances\n            if self.effective_metric_ == \"euclidean\":\n                kwds = {\"squared\": True}\n            else:\n                kwds = self.effective_metric_params_\n\n            chunked_results = list(\n                pairwise_distances_chunked(\n                    X,\n                    self._fit_X,\n                    reduce_func=reduce_func,\n                    metric=self.effective_metric_,\n                    n_jobs=n_jobs,\n                    **kwds,\n                )\n            )\n\n        elif self._fit_method in [\"ball_tree\", \"kd_tree\"]:\n            if issparse(X):\n                raise ValueError(\n                    \"%s does not work with sparse matrices. Densify the data, \"\n                    \"or set algorithm='brute'\"\n                    % self._fit_method\n                )\n            chunked_results = Parallel(n_jobs, prefer=\"threads\")(\n                delayed(_tree_query_parallel_helper)(\n                    self._tree, X[s], n_neighbors, return_distance\n                )\n                for s in gen_even_slices(X.shape[0], n_jobs)\n            )\n        else:\n            raise ValueError(\"internal: _fit_method not recognized\")",
                "filename": "sklearn/neighbors/_base.py",
                "start_index": 29536,
                "end_index": 32509,
                "start_line": 807,
                "end_line": 1247,
                "max_line": 1359,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"\"\"\n====================================\nPlotting Cross-Validated Predictions\n====================================\n\nThis example shows how to use\n:func:`~sklearn.model_selection.cross_val_predict` together with\n:class:`~sklearn.metrics.PredictionErrorDisplay` to visualize prediction\nerrors.\n\"\"\"\n\n# %%\n# We will load the diabetes dataset and create an instance of a linear\n# regression model.\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\n\nX, y = load_diabetes(return_X_y=True)\nlr = LinearRegression()\n\n# %%\n# :func:`~sklearn.model_selection.cross_val_predict` returns an array of the\n# same size of `y` where each entry is a prediction obtained by cross\n# validation.\nfrom sklearn.model_selection import cross_val_predict\n\ny_pred = cross_val_predict(lr, X, y, cv=10)\n\n# %%\n# Since `cv=10`, it means that we trained 10 models and each model was\n# used to predict on one of the 10 folds. We can now use the\n# :class:`~sklearn.metrics.PredictionErrorDisplay` to visualize the\n# prediction errors.\n#\n# On the left axis, we plot the observed values :math:`y` vs. the predicted\n# values :math:`\\hat{y}` given by the models. On the right axis, we plot the\n# residuals (i.e. the difference between the observed values and the predicted\n# values) vs. the predicted values.\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import PredictionErrorDisplay\n\nfig, axs = plt.subplots(ncols=2, figsize=(8, 4))\nPredictionErrorDisplay.from_predictions(\n    y,\n    y_pred=y_pred,\n    kind=\"actual_vs_predicted\",\n    subsample=100,\n    ax=axs[0],\n    random_state=0,\n)\naxs[0].set_title(\"Actual vs. Predicted values\")\nPredictionErrorDisplay.from_predictions(\n    y,\n    y_pred=y_pred,\n    kind=\"residual_vs_predicted\",\n    subsample=100,\n    ax=axs[1],\n    random_state=0,\n)\naxs[1].set_title(\"Residuals vs. Predicted Values\")\nfig.suptitle(\"Plotting cross-validated predictions\")\nplt.tight_layout()\nplt.show()\n\n# %%\n# It is important to note that we used\n# :func:`~sklearn.model_selection.cross_val_predict` for visualization\n# purpose only in this example.\n#\n# It would be problematic to\n# quantitatively assess the model performance by computing a single\n# performance metric from the concatenated predictions returned by\n# :func:`~sklearn.model_selection.cross_val_predict`\n# when the different CV folds vary by size and distributions.\n#\n# In is recommended to compute per-fold performance metrics using:\n# :func:`~sklearn.model_selection.cross_val_score` or\n# :func:`~sklearn.model_selection.cross_validate` instead.",
                "filename": "examples/model_selection/plot_cv_predict.py",
                "start_index": 0,
                "end_index": 2550,
                "start_line": 1,
                "end_line": 79,
                "max_line": 79,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"\"\"Logistic Regression CV (aka logit, MaxEnt) classifier.\n\n    See glossary entry for :term:`cross-validation estimator`.\n\n    This class implements logistic regression using liblinear, newton-cg, sag\n    of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\n    regularization with primal formulation. The liblinear solver supports both\n    L1 and L2 regularization, with a dual formulation only for the L2 penalty.\n    Elastic-Net penalty is only supported by the saga solver.\n\n    For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\n    is selected by the cross-validator\n    :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\n    using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\n    solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\n\n    Read more in the :ref:`User Guide <logistic_regression>`.\n\n    Parameters\n    ----------\n    Cs : int or list of floats, default=10\n        Each of the values in Cs describes the inverse of regularization\n        strength. If Cs is as an int, then a grid of Cs values are chosen\n        in a logarithmic scale between 1e-4 and 1e4.\n        Like in support vector machines, smaller values specify stronger\n        regularization.\n\n    fit_intercept : bool, default=True\n        Specifies if a constant (a.k.a. bias or intercept) should be\n        added to the decision function.\n\n    cv : int or cross-validation generator, default=None\n        The default cross-validation generator used is Stratified K-Folds.\n        If an integer is provided, then it is the number of folds used.\n        See the module :mod:`sklearn.model_selection` module for the\n        list of possible cross-validation objects.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    dual : bool, default=False\n        Dual or primal formulation. Dual formulation is only implemented for\n        l2 penalty with liblinear solver. Prefer dual=False when\n        n_samples > n_features.\n\n    penalty : {'l1', 'l2', 'elasticnet'}, default='l2'\n        Specify the norm of the penalty:\n\n        - `'l2'`: add a L2 penalty term (used by default);\n        - `'l1'`: add a L1 penalty term;\n        - `'elasticnet'`: both L1 and L2 penalty terms are added.\n\n        .. warning::\n           Some penalties may not work with some solvers. See the parameter\n           `solver` below, to know the compatibility between the penalty and\n           solver.\n\n    scoring : str or callable, default=None\n        A string (see model evaluation documentation) or\n        a scorer callable object / function with signature\n        ``scorer(estimator, X, y)``. For a list of scoring functions\n        that can be used, look at :mod:`sklearn.metrics`. The\n        default scoring option used is 'accuracy'.\n\n    solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'}, \\\n            default='lbfgs'\n\n        Algorithm to use in the optimization problem. Default is 'lbfgs'.\n        To choose a solver, you might want to consider the following aspects:\n\n            - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n              and 'saga' are faster for large ones;\n            - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n              'lbfgs' handle multinomial loss;\n            - 'liblinear' might be slower in :class:`LogisticRegressionCV`\n              because it does not handle warm-starting. 'liblinear' is\n              limited to one-versus-rest schemes.\n            - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n              especially with one-hot encoded categorical features with rare\n              categories. Note that it is limited to binary classification and the\n              one-versus-rest reduction for multiclass classification. Be aware that\n              the memory usage of this solver has a quadratic dependency on\n              `n_features` because it explicitly computes the Hessian matrix.\n\n        .. warning::\n           The choice of the algorithm depends on the penalty chosen.\n           Supported penalties by solver:\n\n           - 'lbfgs'           -   ['l2']\n           - 'liblinear'       -   ['l1', 'l2']\n           - 'newton-cg'       -   ['l2']\n           - 'newton-cholesky' -   ['l2']\n           - 'sag'             -   ['l2']\n           - 'saga'            -   ['elasticnet', 'l1', 'l2']\n\n        .. note::\n           'sag' and 'saga' fast convergence is only guaranteed on features\n           with approximately the same scale. You can preprocess the data with\n           a scaler from :mod:`sklearn.preprocessing`.\n\n        .. versionadded:: 0.17\n           Stochastic Average Gradient descent solver.\n        .. versionadded:: 0.19\n           SAGA solver.\n        .. versionadded:: 1.2\n           newton-cholesky solver.\n\n    tol : float, default=1e-4\n        Tolerance for stopping criteria.\n\n    max_iter : int, default=100\n        Maximum number of iterations of the optimization algorithm.\n\n    class_weight : dict or 'balanced', default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n        Note that these weights will be multiplied with sample_weight (passed\n        through the fit method) if sample_weight is specified.\n\n        .. versionadded:: 0.17\n           class_weight == 'balanced'\n\n    n_jobs : int, default=None\n        Number of CPU cores used during the cross-validation loop.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : int, default=0\n        For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n        positive number for verbosity.\n\n    refit : bool, default=True\n        If set to True, the scores are averaged across all folds, and the\n        coefs and the C that corresponds to the best score is taken, and a\n        final refit is done using these parameters.\n        Otherwise the coefs, intercepts and C that correspond to the\n        best scores across folds are averaged.\n\n    intercept_scaling : float, default=1\n        Useful only when the solver 'liblinear' is used\n        and self.fit_intercept is set to True. In this case, x becomes\n        [x, self.intercept_scaling],\n        i.e. a \"synthetic\" feature with constant value equal to\n        intercept_scaling is appended to the instance vector.\n        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n\n        Note! the synthetic feature weight is subject to l1/l2 regularization\n        as all other features.\n        To lessen the effect of regularization on synthetic feature weight\n        (and therefore on the intercept) intercept_scaling has to be increased.\n\n    multi_class : {'auto, 'ovr', 'multinomial'}, default='auto'\n        If the option chosen is 'ovr', then a binary problem is fit for each\n        label. For 'multinomial' the loss minimised is the multinomial loss fit\n        across the entire probability distribution, *even when the data is\n        binary*. 'multinomial' is unavailable when solver='liblinear'.\n        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n        and otherwise selects 'multinomial'.\n\n        .. versionadded:: 0.18\n           Stochastic Average Gradient descent solver for 'multinomial' case.\n        .. versionchanged:: 0.22\n            Default changed from 'ovr' to 'auto' in 0.22.\n\n    random_state : int, RandomState instance, default=None\n        Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\n        Note that this only applies to the solver and not the cross-validation\n        generator. See :term:`Glossary <random_state>` for details.\n\n    l1_ratios : list of float, default=None\n        The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n        Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n        using ``penalty='l2'``, while 1 is equivalent to using\n        ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n        of L1 and L2.\n\n    Attributes\n    ----------\n    classes_ : ndarray of shape (n_classes, )\n        A list of class labels known to the classifier.\n\n    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n        Coefficient of the features in the decision function.\n\n        `coef_` is of shape (1, n_features) when the given problem\n        is binary.\n\n    intercept_ : ndarray of shape (1,) or (n_classes,)\n        Intercept (a.k.a. bias) added to the decision function.\n\n        If `fit_intercept` is set to False, the intercept is set to zero.\n        `intercept_` is of shape(1,) when the problem is binary.\n\n    Cs_ : ndarray of shape (n_cs)\n        Array of C i.e. inverse of regularization parameter values used\n        for cross-validation.\n\n    l1_ratios_ : ndarray of shape (n_l1_ratios)\n        Array of l1_ratios used for cross-validation. If no l1_ratio is used\n        (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n\n    coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or \\",
                "filename": "sklearn/linear_model/_logistic.py",
                "start_index": 54232,
                "end_index": 63750,
                "start_line": 1435,
                "end_line": 1642,
                "max_line": 2222,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.0
            },
            {
                "code": "\"\"\"Search over specified parameter values with successive halving.\n\n    The search strategy starts evaluating all the candidates with a small\n    amount of resources and iteratively selects the best candidates, using\n    more and more resources.\n\n    Read more in the :ref:`User guide <successive_halving_user_guide>`.\n\n    .. note::\n\n      This estimator is still **experimental** for now: the predictions\n      and the API might change without any deprecation cycle. To use it,\n      you need to explicitly import ``enable_halving_search_cv``::\n\n        >>> # explicitly require this experimental feature\n        >>> from sklearn.experimental import enable_halving_search_cv # noqa\n        >>> # now you can import normally from model_selection\n        >>> from sklearn.model_selection import HalvingGridSearchCV\n\n    Parameters\n    ----------\n    estimator : estimator object\n        This is assumed to implement the scikit-learn estimator interface.\n        Either estimator needs to provide a ``score`` function,\n        or ``scoring`` must be passed.\n\n    param_grid : dict or list of dictionaries\n        Dictionary with parameters names (string) as keys and lists of\n        parameter settings to try as values, or a list of such\n        dictionaries, in which case the grids spanned by each dictionary\n        in the list are explored. This enables searching over any sequence\n        of parameter settings.\n\n    factor : int or float, default=3\n        The 'halving' parameter, which determines the proportion of candidates\n        that are selected for each subsequent iteration. For example,\n        ``factor=3`` means that only one third of the candidates are selected.\n\n    resource : ``'n_samples'`` or str, default='n_samples'\n        Defines the resource that increases with each iteration. By default,\n        the resource is the number of samples. It can also be set to any\n        parameter of the base estimator that accepts positive integer\n        values, e.g. 'n_iterations' or 'n_estimators' for a gradient\n        boosting estimator. In this case ``max_resources`` cannot be 'auto'\n        and must be set explicitly.\n\n    max_resources : int, default='auto'\n        The maximum amount of resource that any candidate is allowed to use\n        for a given iteration. By default, this is set to ``n_samples`` when\n        ``resource='n_samples'`` (default), else an error is raised.\n\n    min_resources : {'exhaust', 'smallest'} or int, default='exhaust'\n        The minimum amount of resource that any candidate is allowed to use\n        for a given iteration. Equivalently, this defines the amount of\n        resources `r0` that are allocated for each candidate at the first\n        iteration.\n\n        - 'smallest' is a heuristic that sets `r0` to a small value:\n\n            - ``n_splits * 2`` when ``resource='n_samples'`` for a regression\n              problem\n            - ``n_classes * n_splits * 2`` when ``resource='n_samples'`` for a\n              classification problem\n            - ``1`` when ``resource != 'n_samples'``\n\n        - 'exhaust' will set `r0` such that the **last** iteration uses as\n          much resources as possible. Namely, the last iteration will use the\n          highest value smaller than ``max_resources`` that is a multiple of\n          both ``min_resources`` and ``factor``. In general, using 'exhaust'\n          leads to a more accurate estimator, but is slightly more time\n          consuming.\n\n        Note that the amount of resources used at each iteration is always a\n        multiple of ``min_resources``.\n\n    aggressive_elimination : bool, default=False\n        This is only relevant in cases where there isn't enough resources to\n        reduce the remaining candidates to at most `factor` after the last\n        iteration. If ``True``, then the search process will 'replay' the\n        first iteration for as long as needed until the number of candidates\n        is small enough. This is ``False`` by default, which means that the\n        last iteration may evaluate more than ``factor`` candidates. See\n        :ref:`aggressive_elimination` for more details.\n\n    cv : int, cross-validation generator or iterable, default=5\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used. These splitters are instantiated\n        with `shuffle=False` so the splits will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. note::\n            Due to implementation details, the folds produced by `cv` must be\n            the same across multiple calls to `cv.split()`. For\n            built-in `scikit-learn` iterators, this can be achieved by\n            deactivating shuffling (`shuffle=False`), or by setting the\n            `cv`'s `random_state` parameter to an integer.\n\n    scoring : str, callable, or None, default=None\n        A single string (see :ref:`scoring_parameter`) or a callable\n        (see :ref:`scoring`) to evaluate the predictions on the test set.\n        If None, the estimator's score method is used.\n\n    refit : bool, default=True\n        If True, refit an estimator using the best found parameters on the\n        whole dataset.\n\n        The refitted estimator is made available at the ``best_estimator_``\n        attribute and permits using ``predict`` directly on this\n        ``HalvingGridSearchCV`` instance.\n\n    error_score : 'raise' or numeric\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised. If a numeric value is given,\n        FitFailedWarning is raised. This parameter does not affect the refit\n        step, which will always raise the error. Default is ``np.nan``.\n\n    return_train_score : bool, default=False\n        If ``False``, the ``cv_results_`` attribute will not include training\n        scores.\n        Computing training scores is used to get insights on how different\n        parameter settings impact the overfitting/underfitting trade-off.\n        However computing the scores on the training set can be computationally\n        expensive and is not strictly required to select the parameters that\n        yield the best generalization performance.\n\n    random_state : int, RandomState instance or None, default=None\n        Pseudo random number generator state used for subsampling the dataset\n        when `resources != 'n_samples'`. Ignored otherwise.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    verbose : int\n        Controls the verbosity: the higher, the more messages.\n\n    Attributes\n    ----------\n    n_resources_ : list of int\n        The amount of resources used at each iteration.\n\n    n_candidates_ : list of int\n        The number of candidate parameters that were evaluated at each\n        iteration.\n\n    n_remaining_candidates_ : int\n        The number of candidate parameters that are left after the last\n        iteration. It corresponds to `ceil(n_candidates[-1] / factor)`\n\n    max_resources_ : int\n        The maximum number of resources that any candidate is allowed to use\n        for a given iteration. Note that since the number of resources used\n        at each iteration must be a multiple of ``min_resources_``, the\n        actual number of resources used at the last iteration may be smaller\n        than ``max_resources_``.\n\n    min_resources_ : int\n        The amount of resources that are allocated for each candidate at the\n        first iteration.\n\n    n_iterations_ : int\n        The actual number of iterations that were run. This is equal to\n        ``n_required_iterations_`` if ``aggressive_elimination`` is ``True``.\n        Else, this is equal to ``min(n_possible_iterations_,\n        n_required_iterations_)``.\n\n    n_possible_iterations_ : int\n        The number of iterations that are possible starting with\n        ``min_resources_`` resources and without exceeding\n        ``max_resources_``.\n\n    n_required_iterations_ : int\n        The number of iterations that are required to end up with less than\n        ``factor`` candidates at the last iteration, starting with\n        ``min_resources_`` resources. This will be smaller than\n        ``n_possible_iterations_`` when there isn't enough resources.\n\n    cv_results_ : dict of numpy (masked) ndarrays\n        A dict with keys as column headers and values as columns, that can be\n        imported into a pandas ``DataFrame``. It contains lots of information\n        for analysing the results of a search.\n        Please refer to the :ref:`User guide<successive_halving_cv_results>`\n        for details.\n\n    best_estimator_ : estimator or dict\n        Estimator that was chosen by the search, i.e. estimator\n        which gave highest score (or smallest loss if specified)\n        on the left out data. Not available if ``refit=False``.\n\n    best_score_ : float\n        Mean cross-validated score of the best_estimator.\n\n    best_params_ : dict\n        Parameter setting that gave the best results on the hold out data.\n\n    best_index_ : int\n        The index (of the ``cv_results_`` arrays) which corresponds to the best\n        candidate parameter setting.\n\n        The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n        the parameter setting for the best model, that gives the highest\n        mean score (``search.best_score_``).\n\n    scorer_ : function or a dict\n        Scorer function used on the held out data to choose the best\n        parameters for the model.\n\n    n_splits_ : int\n        The number of cross-validation splits (folds/iterations).\n\n    refit_time_ : float\n        Seconds used for refitting the best model on the whole dataset.\n\n        This is present only if ``refit`` is not False.\n\n    multimetric_ : bool\n        Whether or not the scorers compute several metrics.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels. This is present only if ``refit`` is specified and\n        the underlying estimator is a classifier.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if\n        `best_estimator_` is defined (see the documentation for the `refit`\n        parameter for more details) and that `best_estimator_` exposes\n        `n_features_in_` when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if\n        `best_estimator_` is defined (see the documentation for the `refit`\n        parameter for more details) and that `best_estimator_` exposes\n        `feature_names_in_` when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    :class:`HalvingRandomSearchCV`:\n        Random search over a set of parameters using successive halving.\n\n    Notes\n    -----\n    The parameters selected are those that maximize the score of the held-out\n    data, according to the scoring parameter.\n\n    All parameter combinations scored with a NaN will share the lowest rank.\n\n    Examples\n    --------\n\n    >>> from sklearn.datasets import load_iris\n    >>> from sklearn.ensemble import RandomForestClassifier\n    >>> from sklearn.experimental import enable_halving_search_cv  # noqa\n    >>> from sklearn.model_selection import HalvingGridSearchCV\n    ...\n    >>> X, y = load_iris(return_X_y=True)\n    >>> clf = RandomForestClassifier(random_state=0)\n    ...\n    >>> param_grid = {\"max_depth\": [3, None],\n    ...               \"min_samples_split\": [5, 10]}\n    >>> search = HalvingGridSearchCV(clf, param_grid, resource='n_estimators',\n    ...                              max_resources=10,\n    ...                              random_state=0).fit(X, y)\n    >>> search.best_params_  # doctest: +SKIP\n    {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 9}\n    \"\"\"",
                "filename": "sklearn/model_selection/_search_successive_halving.py",
                "start_index": 14818,
                "end_index": 27400,
                "start_line": 391,
                "end_line": 1021,
                "max_line": 1083,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.0
            },
            {
                "code": "predictions = []\n    averaged_predictions = []\n\n    # define the prediction_method (predict, predict_proba, decision_function).\n    if is_regressor(est):\n        prediction_method = est.predict\n    else:\n        predict_proba = getattr(est, \"predict_proba\", None)\n        decision_function = getattr(est, \"decision_function\", None)\n        if response_method == \"auto\":\n            # try predict_proba, then decision_function if it doesn't exist\n            prediction_method = predict_proba or decision_function\n        else:\n            prediction_method = (\n                predict_proba\n                if response_method == \"predict_proba\"\n                else decision_function\n            )\n        if prediction_method is None:\n            if response_method == \"auto\":\n                raise ValueError(\n                    \"The estimator has no predict_proba and no \"\n                    \"decision_function method.\"\n                )\n            elif response_method == \"predict_proba\":\n                raise ValueError(\"The estimator has no predict_proba method.\")\n            else:\n                raise ValueError(\"The estimator has no decision_function method.\")\n\n    X_eval = X.copy()\n    for new_values in grid:\n        for i, variable in enumerate(features):\n            _safe_assign(X_eval, new_values[i], column_indexer=variable)\n\n        try:\n            # Note: predictions is of shape\n            # (n_points,) for non-multioutput regressors\n            # (n_points, n_tasks) for multioutput regressors\n            # (n_points, 1) for the regressors in cross_decomposition (I think)\n            # (n_points, 2) for binary classification\n            # (n_points, n_classes) for multiclass classification\n            pred = prediction_method(X_eval)\n\n            predictions.append(pred)\n            # average over samples\n            averaged_predictions.append(np.average(pred, axis=0, weights=sample_weight))\n        except NotFittedError as e:\n            raise ValueError(\"'estimator' parameter must be a fitted estimator\") from e\n\n    n_samples = X.shape[0]\n\n    # reshape to (n_targets, n_instances, n_points) where n_targets is:\n    # - 1 for non-multioutput regression and binary classification (shape is\n    #   already correct in those cases)\n    # - n_tasks for multi-output regression\n    # - n_classes for multiclass classification.\n    predictions = np.array(predictions).T\n    if is_regressor(est) and predictions.ndim == 2:\n        # non-multioutput regression, shape is (n_instances, n_points,)\n        predictions = predictions.reshape(n_samples, -1)\n    elif is_classifier(est) and predictions.shape[0] == 2:\n        # Binary classification, shape is (2, n_instances, n_points).\n        # we output the effect of **positive** class\n        predictions = predictions[1]\n        predictions = predictions.reshape(n_samples, -1)\n\n    # reshape averaged_predictions to (n_targets, n_points) where n_targets is:",
                "filename": "sklearn/inspection/_partial_dependence.py",
                "start_index": 11694,
                "end_index": 14639,
                "start_line": 269,
                "end_line": 335,
                "max_line": 743,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/model_selection/_search.py": [
                {
                    "chunk": {
                        "code": "\"\"\"Exhaustive search over specified parameter values for an estimator.\n\n    Important members are fit, predict.\n\n    GridSearchCV implements a \"fit\" and a \"score\" method.\n    It also implements \"score_samples\", \"predict\", \"predict_proba\",\n    \"decision_function\", \"transform\" and \"inverse_transform\" if they are\n    implemented in the estimator used.\n\n    The parameters of the estimator used to apply these methods are optimized\n    by cross-validated grid-search over a parameter grid.\n\n    Read more in the :ref:`User Guide <grid_search>`.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        This is assumed to implement the scikit-learn estimator interface.\n        Either estimator needs to provide a ``score`` function,\n        or ``scoring`` must be passed.\n\n    param_grid : dict or list of dictionaries\n        Dictionary with parameters names (`str`) as keys and lists of\n        parameter settings to try as values, or a list of such\n        dictionaries, in which case the grids spanned by each dictionary\n        in the list are explored. This enables searching over any sequence\n        of parameter settings.\n\n    scoring : str, callable, list, tuple or dict, default=None\n        Strategy to evaluate the performance of the cross-validated model on\n        the test set.\n\n        If `scoring` represents a single score, one can use:\n\n        - a single string (see :ref:`scoring_parameter`);\n        - a callable (see :ref:`scoring`) that returns a single value.\n\n        If `scoring` represents multiple scores, one can use:\n\n        - a list or tuple of unique strings;\n        - a callable returning a dictionary where the keys are the metric\n          names and the values are the metric scores;\n        - a dictionary with metric names as keys and callables a values.\n\n        See :ref:`multimetric_grid_search` for an example.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n        .. versionchanged:: v0.20\n           `n_jobs` default changed from 1 to None\n\n    refit : bool, str, or callable, default=True\n        Refit an estimator using the best found parameters on the whole\n        dataset.\n\n        For multiple metric evaluation, this needs to be a `str` denoting the\n        scorer that would be used to find the best parameters for refitting\n        the estimator at the end.\n\n        Where there are considerations other than maximum score in\n        choosing a best estimator, ``refit`` can be set to a function which\n        returns the selected ``best_index_`` given ``cv_results_``. In that\n        case, the ``best_estimator_`` and ``best_params_`` will be set\n        according to the returned ``best_index_`` while the ``best_score_``\n        attribute will not be available.\n\n        The refitted estimator is made available at the ``best_estimator_``\n        attribute and permits using ``predict`` directly on this\n        ``GridSearchCV`` instance.\n\n        Also for multiple metric evaluation, the attributes ``best_index_``,\n        ``best_score_`` and ``best_params_`` will only be available if\n        ``refit`` is set and all of them will be determined w.r.t this specific\n        scorer.\n\n        See ``scoring`` parameter to know more about multiple metric\n        evaluation.\n\n        See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`\n        to see how to design a custom selection strategy using a callable\n        via `refit`.\n\n        .. versionchanged:: 0.20\n            Support for callable added.\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n        - None, to use the default 5-fold cross validation,\n        - integer, to specify the number of folds in a `(Stratified)KFold`,\n        - :term:`CV splitter`,\n        - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if the estimator is a classifier and ``y`` is\n        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n        other cases, :class:`KFold` is used. These splitters are instantiated\n        with `shuffle=False` so the splits will be the same across calls.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validation strategies that can be used here.\n\n        .. versionchanged:: 0.22\n            ``cv`` default value if None changed from 3-fold to 5-fold.\n\n    verbose : int\n        Controls the verbosity: the higher, the more messages.\n\n        - >1 : the computation time for each fold and parameter candidate is\n          displayed;\n        - >2 : the score is also displayed;\n        - >3 : the fold and candidate parameter indexes are also displayed\n          together with the starting time of the computation.\n\n    pre_dispatch : int, or str, default='2*n_jobs'\n        Controls the number of jobs that get dispatched during parallel\n        execution. Reducing this number can be useful to avoid an\n        explosion of memory consumption when more jobs get dispatched\n        than CPUs can process. This parameter can be:\n\n            - None, in which case all the jobs are immediately\n              created and spawned. Use this for lightweight and\n              fast-running jobs, to avoid delays due to on-demand\n              spawning of the jobs\n\n            - An int, giving the exact number of total jobs that are\n              spawned\n\n            - A str, giving an expression as a function of n_jobs,\n              as in '2*n_jobs'\n\n    error_score : 'raise' or numeric, default=np.nan\n        Value to assign to the score if an error occurs in estimator fitting.\n        If set to 'raise', the error is raised. If a numeric value is given,\n        FitFailedWarning is raised. This parameter does not affect the refit\n        step, which will always raise the error.\n\n    return_train_score : bool, default=False\n        If ``False``, the ``cv_results_`` attribute will not include training\n        scores.\n        Computing training scores is used to get insights on how different\n        parameter settings impact the overfitting/underfitting trade-off.\n        However computing the scores on the training set can be computationally\n        expensive and is not strictly required to select the parameters that\n        yield the best generalization performance.\n\n        .. versionadded:: 0.19\n\n        .. versionchanged:: 0.21\n            Default value was changed from ``True`` to ``False``\n\n    Attributes\n    ----------\n    cv_results_ : dict of numpy (masked) ndarrays\n        A dict with keys as column headers and values as columns, that can be\n        imported into a pandas ``DataFrame``.\n\n        For instance the below given table\n\n        +------------+-----------+------------+-----------------+---+---------+\n        |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n        +============+===========+============+=================+===+=========+\n        |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n        +------------+-----------+------------+-----------------+---+---------+\n        |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n        +------------+-----------+------------+-----------------+---+---------+\n\n        will be represented by a ``cv_results_`` dict of::\n\n            {\n            'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n                                         mask = [False False False False]...)\n            'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n                                        mask = [ True  True False False]...),\n            'param_degree': masked_array(data = [2.0 3.0 -- --],\n                                         mask = [False False  True  True]...),\n            'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n            'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n            'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n            'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n            'rank_test_score'    : [2, 4, 3, 1],\n            'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n            'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n            'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n            'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n            'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n            'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n            'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n            'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n            'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n            }\n\n        NOTE\n\n        The key ``'params'`` is used to store a list of parameter\n        settings dicts for all the parameter candidates.\n\n        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n        ``std_score_time`` are all in seconds.\n\n        For multi-metric evaluation, the scores for all the scorers are\n        available in the ``cv_results_`` dict at the keys ending with that\n        scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n        above. ('split0_test_precision', 'mean_train_precision' etc.)\n\n    best_estimator_ : estimator\n        Estimator that was chosen by the search, i.e. estimator\n        which gave highest score (or smallest loss if specified)\n        on the left out data. Not available if ``refit=False``.\n\n        See ``refit`` parameter for more information on allowed values.\n\n    best_score_ : float\n        Mean cross-validated score of the best_estimator\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n        This attribute is not available if ``refit`` is a function.\n\n    best_params_ : dict\n        Parameter setting that gave the best results on the hold out data.\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n    best_index_ : int\n        The index (of the ``cv_results_`` arrays) which corresponds to the best\n        candidate parameter setting.\n\n        The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n        the parameter setting for the best model, that gives the highest\n        mean score (``search.best_score_``).\n\n        For multi-metric evaluation, this is present only if ``refit`` is\n        specified.\n\n    scorer_ : function or a dict\n        Scorer function used on the held out data to choose the best\n        parameters for the model.\n\n        For multi-metric evaluation, this attribute holds the validated\n        ``scoring`` dict which maps the scorer key to the scorer callable.\n\n    n_splits_ : int\n        The number of cross-validation splits (folds/iterations).\n\n    refit_time_ : float\n        Seconds used for refitting the best model on the whole dataset.\n\n        This is present only if ``refit`` is not False.\n\n        .. versionadded:: 0.20\n\n    multimetric_ : bool\n        Whether or not the scorers compute several metrics.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels. This is present only if ``refit`` is specified and\n        the underlying estimator is a classifier.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if\n        `best_estimator_` is defined (see the documentation for the `refit`\n        parameter for more details) and that `best_estimator_` exposes\n        `n_features_in_` when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Only defined if\n        `best_estimator_` is defined (see the documentation for the `refit`\n        parameter for more details) and that `best_estimator_` exposes\n        `feature_names_in_` when fit.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    ParameterGrid : Generates all the combinations of a hyperparameter grid.\n    train_test_split : Utility function to split the data into a development\n        set usable for fitting a GridSearchCV instance and an evaluation set\n        for its final evaluation.\n    sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n        loss function.\n\n    Notes\n    -----\n    The parameters selected are those that maximize the score of the left out\n    data, unless an explicit score is passed in which case it is used instead.\n\n    If `n_jobs` was set to a value higher than one, the data is copied for each\n    point in the grid (and not `n_jobs` times). This is done for efficiency\n    reasons if individual jobs take very little time, but may raise errors if\n    the dataset is large and not enough memory is available.  A workaround in\n    this case is to set `pre_dispatch`. Then, the memory is copied only\n    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n    n_jobs`.\n\n    Examples\n    --------\n    >>> from sklearn import svm, datasets\n    >>> from sklearn.model_selection import GridSearchCV\n    >>> iris = datasets.load_iris()\n    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n    >>> svc = svm.SVC()\n    >>> clf = GridSearchCV(svc, parameters)\n    >>> clf.fit(iris.data, iris.target)\n    GridSearchCV(estimator=SVC(),\n                 param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n    >>> sorted(clf.cv_results_.keys())\n    ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n     'param_C', 'param_kernel', 'params',...\n     'rank_test_score', 'split0_test_score',...\n     'split2_test_score', ...\n     'std_fit_time', 'std_score_time', 'std_test_score']\n    \"\"\"",
                        "filename": "sklearn/model_selection/_search.py",
                        "start_index": 40693,
                        "end_index": 54899,
                        "start_line": 1060,
                        "end_line": 1808,
                        "max_line": 1813,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the implementation of GridSearchCV which is used in the code to reproduce the issue. It is relevant to understand how the GridSearchCV interacts with the KNeighborsClassifier when the metric is precomputed."
                }
            ],
            "sklearn/neighbors/_base.py": [
                {
                    "chunk": {
                        "code": "if n_neighbors > n_samples_fit:\n            raise ValueError(\n                \"Expected n_neighbors <= n_samples, \"\n                \" but n_samples = %d, n_neighbors = %d\" % (n_samples_fit, n_neighbors)\n            )\n\n        n_jobs = effective_n_jobs(self.n_jobs)\n        chunked_results = None\n        use_pairwise_distances_reductions = (\n            self._fit_method == \"brute\"\n            and ArgKmin.is_usable_for(\n                X if X is not None else self._fit_X, self._fit_X, self.effective_metric_\n            )\n        )\n        if use_pairwise_distances_reductions:\n            results = ArgKmin.compute(\n                X=X,\n                Y=self._fit_X,\n                k=n_neighbors,\n                metric=self.effective_metric_,\n                metric_kwargs=self.effective_metric_params_,\n                strategy=\"auto\",\n                return_distance=return_distance,\n            )\n\n        elif (\n            self._fit_method == \"brute\" and self.metric == \"precomputed\" and issparse(X)\n        ):\n            results = _kneighbors_from_graph(\n                X, n_neighbors=n_neighbors, return_distance=return_distance\n            )\n\n        elif self._fit_method == \"brute\":\n            # Joblib-based backend, which is used when user-defined callable\n            # are passed for metric.\n\n            # This won't be used in the future once PairwiseDistancesReductions\n            # support:\n            #   - DistanceMetrics which work on supposedly binary data\n            #   - CSR-dense and dense-CSR case if 'euclidean' in metric.\n            reduce_func = partial(\n                self._kneighbors_reduce_func,\n                n_neighbors=n_neighbors,\n                return_distance=return_distance,\n            )\n\n            # for efficiency, use squared euclidean distances\n            if self.effective_metric_ == \"euclidean\":\n                kwds = {\"squared\": True}\n            else:\n                kwds = self.effective_metric_params_\n\n            chunked_results = list(\n                pairwise_distances_chunked(\n                    X,\n                    self._fit_X,\n                    reduce_func=reduce_func,\n                    metric=self.effective_metric_,\n                    n_jobs=n_jobs,\n                    **kwds,\n                )\n            )\n\n        elif self._fit_method in [\"ball_tree\", \"kd_tree\"]:\n            if issparse(X):\n                raise ValueError(\n                    \"%s does not work with sparse matrices. Densify the data, \"\n                    \"or set algorithm='brute'\"\n                    % self._fit_method\n                )\n            chunked_results = Parallel(n_jobs, prefer=\"threads\")(\n                delayed(_tree_query_parallel_helper)(\n                    self._tree, X[s], n_neighbors, return_distance\n                )\n                for s in gen_even_slices(X.shape[0], n_jobs)\n            )\n        else:\n            raise ValueError(\"internal: _fit_method not recognized\")",
                        "filename": "sklearn/neighbors/_base.py",
                        "start_index": 29536,
                        "end_index": 32509,
                        "start_line": 807,
                        "end_line": 1247,
                        "max_line": 1359,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet from '_base.py' is relevant as it contains the logic for handling the 'precomputed' metric in KNeighborsClassifier, which is directly related to the issue reported."
                }
            ]
        },
        "solution_task": "- The error message indicates that the input `X` to the `KNeighborsClassifier` with a precomputed metric is not a square matrix. This is a requirement for distance/kernel matrices when using the 'precomputed' metric option.\n- The provided code snippet suggests that the `euclidean_distances` function is used to compute the pairwise distances between samples in the `iris` dataset, and this distance matrix is then used as input `X` to `cross_val_predict` with a `KNeighborsClassifier` that has been initialized with `metric='precomputed'`.\n- The `cross_val_predict` function expects the `X` input to be the same shape as the input to the `fit` method of the estimator. However, when using `GridSearchCV` with `cross_val_predict`, the `GridSearchCV` object is passed as the estimator, which expects the original data, not the precomputed distance matrix.\n\nTo resolve the issue, you should modify the code to ensure that `cross_val_predict` is called with the original data, and the precomputed distance matrix is only used within the `GridSearchCV` through a pipeline that computes the distances on the fly for each fold of the cross-validation. Here's how you can adjust the code:\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import cross_val_predict, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier, DistanceMetric\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import euclidean_distances\n\n# Load the iris dataset\niris = load_iris()\n\n# Initialize the KNeighborsClassifier without specifying 'metric' parameter\nknn = KNeighborsClassifier()\n\n# Create a pipeline that computes the distance matrix on the fly\npipe = Pipeline([\n    ('distance', DistanceMetric.get_metric('euclidean').pairwise),\n    ('knn', knn)\n])\n\n# Set up the grid search with specified hyperparameters including the 'precomputed' metric\nparam_grid = {\"knn__n_neighbors\": [1, 5, 10], \"knn__metric\": ['precomputed']}\nknngs = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=10)\n\n# Perform cross-validation and get predictions\npredicted = cross_val_predict(estimator=knngs, X=iris.data, y=iris.target, cv=10)\n```\n\n- In the pipeline, the first step computes the pairwise euclidean distances, and the second step is the `KNeighborsClassifier` with the 'precomputed' metric.\n- The `param_grid` is adjusted to include the `knn__` prefix for the parameters of the `KNeighborsClassifier` within the pipeline.\n- The `cross_val_predict` function is called with the original `iris.data` as `X`, and the `GridSearchCV` object `knngs` handles the precomputation of distances internally for each fold.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE accurately identifies the cause of the error, which is the use of a non-square matrix as input to the KNeighborsClassifier with a 'precomputed' metric. It provides a detailed explanation of why the error occurs and offers a comprehensive solution by suggesting the use of a pipeline to compute distances on the fly for each fold of the cross-validation. The RESPONSE also corrects the parameter grid to be used with the pipeline and provides a complete code snippet that should resolve the issue. The RESPONSE is highly relevant and actionable, addressing the specific problem in the PROMPT. The only reason it does not score a 10 is that it includes unnecessary links to code snippets in the sklearn repository, which, while potentially helpful, are not required to resolve the issue.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711738940.2357519,
        "result": "- The error message indicates that the input `X` to the `KNeighborsClassifier` with a precomputed metric is not a square matrix. This is a requirement for distance/kernel matrices when using the 'precomputed' metric option.\n- The provided code snippet suggests that the `euclidean_distances` function is used to compute the pairwise distances between samples in the `iris` dataset, and this distance matrix is then used as input `X` to `cross_val_predict` with a `KNeighborsClassifier` that has been initialized with `metric='precomputed'`.\n- The `cross_val_predict` function expects the `X` input to be the same shape as the input to the `fit` method of the estimator. However, when using `GridSearchCV` with `cross_val_predict`, the `GridSearchCV` object is passed as the estimator, which expects the original data, not the precomputed distance matrix.\n\nTo resolve the issue, you should modify the code to ensure that `cross_val_predict` is called with the original data, and the precomputed distance matrix is only used within the `GridSearchCV` through a pipeline that computes the distances on the fly for each fold of the cross-validation. Here's how you can adjust the code:\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import cross_val_predict, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier, DistanceMetric\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import euclidean_distances\n\n# Load the iris dataset\niris = load_iris()\n\n# Initialize the KNeighborsClassifier without specifying 'metric' parameter\nknn = KNeighborsClassifier()\n\n# Create a pipeline that computes the distance matrix on the fly\npipe = Pipeline([\n    ('distance', DistanceMetric.get_metric('euclidean').pairwise),\n    ('knn', knn)\n])\n\n# Set up the grid search with specified hyperparameters including the 'precomputed' metric\nparam_grid = {\"knn__n_neighbors\": [1, 5, 10], \"knn__metric\": ['precomputed']}\nknngs = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=10)\n\n# Perform cross-validation and get predictions\npredicted = cross_val_predict(estimator=knngs, X=iris.data, y=iris.target, cv=10)\n```\n\n- In the pipeline, the first step computes the pairwise euclidean distances, and the second step is the `KNeighborsClassifier` with the 'precomputed' metric.\n- The `param_grid` is adjusted to include the `knn__` prefix for the parameters of the `KNeighborsClassifier` within the pipeline.\n- The `cross_val_predict` function is called with the original `iris.data` as `X`, and the `GridSearchCV` object `knngs` handles the precomputation of distances internally for each fold.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [model_selection](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/model_selection)- [Lines 1060 - 1808](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/model_selection/_search.py#L1060-L1808)\n\n This snippet contains the implementation of GridSearchCV which is used in the code to reproduce the issue. It is relevant to understand how the GridSearchCV interacts with the KNeighborsClassifier when the metric is precomputed.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [neighbors](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/neighbors)- [Lines 807 - 1247](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/neighbors/_base.py#L807-L1247)\n\n This snippet from '_base.py' is relevant as it contains the logic for handling the 'precomputed' metric in KNeighborsClassifier, which is directly related to the issue reported.",
        "context_relevance": 0.16,
        "hypotheses": [],
        "groundedness_score": 1.0,
        "qa_relevance_reasons": {
            "reason": "Criteria: The RESPONSE is relevant to the entire PROMPT and answers the entire PROMPT completely.\nSupporting Evidence: The RESPONSE provides a detailed explanation of the error encountered in the code provided in the PROMPT, which is due to the use of a precomputed metric in a KNeighborsClassifier within a cross-validation setup. It then offers a solution to the problem by suggesting the use of a pipeline to compute distances on the fly for each fold of the cross-validation, and it adjusts the code accordingly. The RESPONSE also correctly interprets the error message and relates it to the use of the 'precomputed' metric option in the KNeighborsClassifier. Additionally, the RESPONSE provides relevant code snippets from the sklearn library that pertain to the GridSearchCV and KNeighborsClassifier implementations, which are directly related to the issue described in the PROMPT. The RESPONSE is comprehensive, directly addresses the problem, and provides a solution, fulfilling all aspects of the PROMPT."
        },
        "qa_relevance": 1.0
    }
}