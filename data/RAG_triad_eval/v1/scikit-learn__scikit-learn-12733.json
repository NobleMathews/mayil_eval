{
    "id": "scikit-learn__scikit-learn-12733",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
    "task_id": 8303749546727225807,
    "title": "train_test_split excepting negative integers and floats",
    "body": "The following minimal example doesn't fail:\r\n\r\n```python\r\nfrom sklearn.model_selection import train_test_split\r\nl = list(range(100))\r\ntrain_test_split(l, test_size=-2)\r\ntrain_test_split(l, test_size=-2.)\r\n```\r\n\r\nIs it a bug or indented feature? According to the docs neither of the above should make sense. See for reference [this line](https://github.com/scikit-learn/scikit-learn/blob/7f6cb8330a2da1f9810a4f89d4b47ca61a6918b6/sklearn/model_selection/_split.py#L1796) for example.\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.21.git",
    "requested_commit": "",
    "mayil_response": "- Modify the `Interval` for `test_size` in the `@validate_params` decorator to disallow negative values.\n- Update the `Interval` for `test_size` to use `closed='right'` for `numbers.Integral` to exclude negative integers.\n\n```python\n@validate_params(\n    {\n        \"test_size\": [\n            Interval(RealNotInt, 0, 1, closed=\"right\"),\n            Interval(numbers.Integral, 0, None, closed=\"left\"),\n            None,\n        ],\n        ...\n    },\n    prefer_skip_nested_validation=True,\n)\n```\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [model_selection](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/model_selection)- [Lines 2500 - 2751](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/model_selection/_split.py#L2500-L2751)\n\n This snippet contains the validation logic for 'test_size' and 'train_size' parameters in train_test_split, which is directly related to the issue.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0004185,
        "snippet_processor": 0.08203,
        "issue_star_creation": 0.032729999999999995,
        "issue_star_solver": 0.07929000000000001,
        "bouncer": 0.030320000000000003
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711741138.484184,
        "relevant_snippets": [
            {
                "code": "@validate_params(\n    {\n        \"test_size\": [\n            Interval(RealNotInt, 0, 1, closed=\"neither\"),\n            Interval(numbers.Integral, 1, None, closed=\"left\"),\n            None,\n        ],\n        \"train_size\": [\n            Interval(RealNotInt, 0, 1, closed=\"neither\"),\n            Interval(numbers.Integral, 1, None, closed=\"left\"),\n            None,\n        ],\n        \"random_state\": [\"random_state\"],\n        \"shuffle\": [\"boolean\"],\n        \"stratify\": [\"array-like\", None],\n    },\n    prefer_skip_nested_validation=True,\n)",
                "filename": "sklearn/model_selection/_split.py",
                "start_index": 88715,
                "end_index": 89252,
                "start_line": 2500,
                "end_line": 2751,
                "max_line": 2751,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "try:\n        # be robust to the max_iter=0 edge case, see:\n        # https://github.com/scikit-learn/scikit-learn/issues/4134",
                "filename": "sklearn/covariance/_graph_lasso.py",
                "start_index": 3490,
                "end_index": 3615,
                "start_line": 106,
                "end_line": 108,
                "max_line": 1094,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.1
            },
            {
                "code": "def check_outliers_fit_predict(name, estimator_orig):\n    # Check fit_predict for outlier detectors.\n\n    n_samples = 300\n    X, _ = make_blobs(n_samples=n_samples, random_state=0)\n    X = shuffle(X, random_state=7)\n    n_samples, n_features = X.shape\n    estimator = clone(estimator_orig)\n\n    set_random_state(estimator)\n\n    y_pred = estimator.fit_predict(X)\n    assert y_pred.shape == (n_samples,)\n    assert y_pred.dtype.kind == \"i\"\n    assert_array_equal(np.unique(y_pred), np.array([-1, 1]))\n\n    # check fit_predict = fit.predict when the estimator has both a predict and\n    # a fit_predict method. recall that it is already assumed here that the\n    # estimator has a fit_predict method\n    if hasattr(estimator, \"predict\"):\n        y_pred_2 = estimator.fit(X).predict(X)\n        assert_array_equal(y_pred, y_pred_2)\n\n    if hasattr(estimator, \"contamination\"):\n        # proportion of outliers equal to contamination parameter when not\n        # set to 'auto'\n        expected_outliers = 30\n        contamination = float(expected_outliers) / n_samples\n        estimator.set_params(contamination=contamination)\n        y_pred = estimator.fit_predict(X)\n\n        num_outliers = np.sum(y_pred != 1)\n        # num_outliers should be equal to expected_outliers unless\n        # there are ties in the decision_function values. this can\n        # only be tested for estimators with a decision_function\n        # method\n        if num_outliers != expected_outliers and hasattr(\n            estimator, \"decision_function\"\n        ):\n            decision = estimator.decision_function(X)\n            check_outlier_corruption(num_outliers, expected_outliers, decision)\n\n\ndef check_fit_non_negative(name, estimator_orig):\n    # Check that proper warning is raised for non-negative X\n    # when tag requires_positive_X is present\n    X = np.array([[-1.0, 1], [-1.0, 1]])\n    y = np.array([1, 2])\n    estimator = clone(estimator_orig)\n    with raises(ValueError):\n        estimator.fit(X, y)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 134030,
                "end_index": 136018,
                "start_line": 3781,
                "end_line": 4110,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "@validate_params(\n    {\n        \"n_samples\": [Interval(Integral, 0, None, closed=\"left\"), tuple],\n        \"shuffle\": [\"boolean\"],\n        \"noise\": [Interval(Real, 0, None, closed=\"left\"), None],\n        \"random_state\": [\"random_state\"],\n        \"factor\": [Interval(Real, 0, 1, closed=\"left\")],\n    },\n    prefer_skip_nested_validation=True,\n)",
                "filename": "sklearn/datasets/_samples_generator.py",
                "start_index": 24992,
                "end_index": 25334,
                "start_line": 42,
                "end_line": 2124,
                "max_line": 2126,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "@validate_params(\n    {\n        \"n_samples\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"n_features\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"effective_rank\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"tail_strength\": [Interval(Real, 0, 1, closed=\"both\")],\n        \"random_state\": [\"random_state\"],\n    },\n    prefer_skip_nested_validation=True,\n)",
                "filename": "sklearn/datasets/_samples_generator.py",
                "start_index": 43231,
                "end_index": 43616,
                "start_line": 42,
                "end_line": 2124,
                "max_line": 2126,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "@validate_params(\n    {\n        \"n_samples\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"n_features\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"n_informative\": [Interval(Integral, 0, None, closed=\"left\")],\n        \"n_targets\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"bias\": [Interval(Real, None, None, closed=\"neither\")],\n        \"effective_rank\": [Interval(Integral, 1, None, closed=\"left\"), None],\n        \"tail_strength\": [Interval(Real, 0, 1, closed=\"both\")],\n        \"noise\": [Interval(Real, 0, None, closed=\"left\")],\n        \"shuffle\": [\"boolean\"],\n        \"coef\": [\"boolean\"],\n        \"random_state\": [\"random_state\"],\n    },\n    prefer_skip_nested_validation=True,\n)",
                "filename": "sklearn/datasets/_samples_generator.py",
                "start_index": 19405,
                "end_index": 20118,
                "start_line": 42,
                "end_line": 2124,
                "max_line": 2126,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "@validate_params(\n    {\n        \"n_samples\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"n_features\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"n_classes\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"n_labels\": [Interval(Integral, 0, None, closed=\"left\")],\n        \"length\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"allow_unlabeled\": [\"boolean\"],\n        \"sparse\": [\"boolean\"],\n        \"return_indicator\": [StrOptions({\"dense\", \"sparse\"}), \"boolean\"],\n        \"return_distributions\": [\"boolean\"],\n        \"random_state\": [\"random_state\"],\n    },\n    prefer_skip_nested_validation=True,\n)",
                "filename": "sklearn/datasets/_samples_generator.py",
                "start_index": 11685,
                "end_index": 12321,
                "start_line": 42,
                "end_line": 2124,
                "max_line": 2126,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "@validate_params(\n    {\n        \"n_samples\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"n_features\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"n_informative\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"n_redundant\": [Interval(Integral, 0, None, closed=\"left\")],\n        \"n_repeated\": [Interval(Integral, 0, None, closed=\"left\")],\n        \"n_classes\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"n_clusters_per_class\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"weights\": [\"array-like\", None],\n        \"flip_y\": [Interval(Real, 0, 1, closed=\"both\")],\n        \"class_sep\": [Interval(Real, 0, None, closed=\"neither\")],\n        \"hypercube\": [\"boolean\"],\n        \"shift\": [Interval(Real, None, None, closed=\"neither\"), \"array-like\", None],\n        \"scale\": [Interval(Real, 0, None, closed=\"neither\"), \"array-like\", None],\n        \"shuffle\": [\"boolean\"],\n        \"random_state\": [\"random_state\"],\n    },\n    prefer_skip_nested_validation=True,\n)",
                "filename": "sklearn/datasets/_samples_generator.py",
                "start_index": 1193,
                "end_index": 2192,
                "start_line": 42,
                "end_line": 2124,
                "max_line": 2126,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_class_weight_balanced_linear_classifier(name, Classifier):\n    \"\"\"Test class weights with non-contiguous class labels.\"\"\"\n    # this is run on classes, not instances, though this should be changed\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-0.8, -1.0], [1.0, 1.0], [1.0, 0.0]])\n    y = np.array([1, 1, 1, -1, -1])\n\n    classifier = Classifier()\n\n    if hasattr(classifier, \"n_iter\"):\n        # This is a very small dataset, default n_iter are likely to prevent\n        # convergence\n        classifier.set_params(n_iter=1000)\n    if hasattr(classifier, \"max_iter\"):\n        classifier.set_params(max_iter=1000)\n    if hasattr(classifier, \"cv\"):\n        classifier.set_params(cv=3)\n    set_random_state(classifier)\n\n    # Let the model compute the class frequencies\n    classifier.set_params(class_weight=\"balanced\")\n    coef_balanced = classifier.fit(X, y).coef_.copy()\n\n    # Count each label occurrence to reweight manually\n    n_samples = len(y)\n    n_classes = float(len(np.unique(y)))\n\n    class_weight = {\n        1: n_samples / (np.sum(y == 1) * n_classes),\n        -1: n_samples / (np.sum(y == -1) * n_classes),\n    }\n    classifier.set_params(class_weight=class_weight)\n    coef_manual = classifier.fit(X, y).coef_.copy()\n\n    assert_allclose(\n        coef_balanced,\n        coef_manual,\n        err_msg=\"Classifier %s is not computing class_weight=balanced properly.\" % name,\n    )\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_estimators_overwrite_params(name, estimator_orig):\n    X, y = make_blobs(random_state=0, n_samples=21)\n    X = _enforce_estimator_tags_X(estimator_orig, X, kernel=rbf_kernel)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    set_random_state(estimator)\n\n    # Make a physical copy of the original estimator parameters before fitting.\n    params = estimator.get_params()\n    original_params = deepcopy(params)\n\n    # Fit the model\n    estimator.fit(X, y)\n\n    # Compare the state of the model parameters with the original parameters\n    new_params = estimator.get_params()\n    for param_name, original_value in original_params.items():\n        new_value = new_params[param_name]\n\n        # We should never change or mutate the internal state of input\n        # parameters by default. To check this we use the joblib.hash function\n        # that introspects recursively any subobjects to compute a checksum.\n        # The only exception to this rule of immutable constructor parameters\n        # is possible RandomState instance but in this check we explicitly\n        # fixed the random_state params recursively to be integer seeds.\n        assert joblib.hash(new_value) == joblib.hash(original_value), (\n            \"Estimator %s should not change or mutate \"\n            \" the parameter %s from %s to %s during fit.\"\n            % (name, param_name, original_value, new_value)\n        )",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 112710,
                "end_index": 115635,
                "start_line": 181,
                "end_line": 4639,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.0
            },
            {
                "code": "def _check_input_parameters(self, X, y, groups):\n        # We need to enforce that successive calls to cv.split() yield the same\n        # splits: see https://github.com/scikit-learn/scikit-learn/issues/15149\n        if not _yields_constant_splits(self._checked_cv_orig):\n            raise ValueError(\n                \"The cv parameter must yield consistent folds across \"\n                \"calls to split(). Set its random_state to an int, or set \"\n                \"shuffle=False.\"\n            )\n\n        if (\n            self.resource != \"n_samples\"\n            and self.resource not in self.estimator.get_params()\n        ):\n            raise ValueError(\n                f\"Cannot use resource={self.resource} which is not supported \"\n                f\"by estimator {self.estimator.__class__.__name__}\"\n            )\n\n        if isinstance(self, HalvingRandomSearchCV):\n            if self.min_resources == self.n_candidates == \"exhaust\":\n                # for n_candidates=exhaust to work, we need to know what\n                # min_resources is. Similarly min_resources=exhaust needs to\n                # know the actual number of candidates.\n                raise ValueError(\n                    \"n_candidates and min_resources cannot be both set to 'exhaust'.\"\n                )\n\n        self.min_resources_ = self.min_resources\n        if self.min_resources_ in (\"smallest\", \"exhaust\"):\n            if self.resource == \"n_samples\":\n                n_splits = self._checked_cv_orig.get_n_splits(X, y, groups)\n                # please see https://gph.is/1KjihQe for a justification\n                magic_factor = 2\n                self.min_resources_ = n_splits * magic_factor\n                if is_classifier(self.estimator):\n                    y = self._validate_data(X=\"no_validation\", y=y)\n                    check_classification_targets(y)\n                    n_classes = np.unique(y).shape[0]\n                    self.min_resources_ *= n_classes\n            else:\n                self.min_resources_ = 1\n            # if 'exhaust', min_resources_ might be set to a higher value later\n            # in _run_search\n\n        self.max_resources_ = self.max_resources\n        if self.max_resources_ == \"auto\":\n            if not self.resource == \"n_samples\":\n                raise ValueError(\n                    \"resource can only be 'n_samples' when max_resources='auto'\"\n                )\n            self.max_resources_ = _num_samples(X)\n\n        if self.min_resources_ > self.max_resources_:\n            raise ValueError(\n                f\"min_resources_={self.min_resources_} is greater \"\n                f\"than max_resources_={self.max_resources_}.\"\n            )\n\n        if self.min_resources_ == 0:\n            raise ValueError(\n                f\"min_resources_={self.min_resources_}: you might have passed \"\n                \"an empty dataset X.\"\n            )",
                "filename": "sklearn/model_selection/_search_successive_halving.py",
                "start_index": 4152,
                "end_index": 7029,
                "start_line": 126,
                "end_line": 384,
                "max_line": 1083,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.2
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/model_selection/_split.py": [
                {
                    "chunk": {
                        "code": "@validate_params(\n    {\n        \"test_size\": [\n            Interval(RealNotInt, 0, 1, closed=\"neither\"),\n            Interval(numbers.Integral, 1, None, closed=\"left\"),\n            None,\n        ],\n        \"train_size\": [\n            Interval(RealNotInt, 0, 1, closed=\"neither\"),\n            Interval(numbers.Integral, 1, None, closed=\"left\"),\n            None,\n        ],\n        \"random_state\": [\"random_state\"],\n        \"shuffle\": [\"boolean\"],\n        \"stratify\": [\"array-like\", None],\n    },\n    prefer_skip_nested_validation=True,\n)",
                        "filename": "sklearn/model_selection/_split.py",
                        "start_index": 88715,
                        "end_index": 89252,
                        "start_line": 2500,
                        "end_line": 2751,
                        "max_line": 2751,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the validation logic for 'test_size' and 'train_size' parameters in train_test_split, which is directly related to the issue."
                }
            ]
        },
        "solution_task": "- Modify the `Interval` for `test_size` in the `@validate_params` decorator to disallow negative values.\n- Update the `Interval` for `test_size` to use `closed='right'` for `numbers.Integral` to exclude negative integers.\n\n```python\n@validate_params(\n    {\n        \"test_size\": [\n            Interval(RealNotInt, 0, 1, closed=\"right\"),\n            Interval(numbers.Integral, 0, None, closed=\"left\"),\n            None,\n        ],\n        ...\n    },\n    prefer_skip_nested_validation=True,\n)\n```",
        "bouncer_response": {
            "score": 8,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE provides a specific and actionable solution to the issue described in the PROMPT by suggesting modifications to the `@validate_params` decorator to disallow negative values for `test_size`. The RESPONSE also correctly identifies that the `Interval` for `test_size` should use `closed='right'` for `numbers.Integral` to exclude negative integers. The code snippet provided is relevant and points to the section of the code where the validation logic for 'test_size' and 'train_size' parameters in train_test_split is located, which is directly related to the issue. The RESPONSE does not address all aspects of the PROMPT, such as whether the current behavior is a bug or an intended feature, but it does provide a clear path to resolving the issue if it is determined to be a bug.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711741279.322161,
        "result": "- Modify the `Interval` for `test_size` in the `@validate_params` decorator to disallow negative values.\n- Update the `Interval` for `test_size` to use `closed='right'` for `numbers.Integral` to exclude negative integers.\n\n```python\n@validate_params(\n    {\n        \"test_size\": [\n            Interval(RealNotInt, 0, 1, closed=\"right\"),\n            Interval(numbers.Integral, 0, None, closed=\"left\"),\n            None,\n        ],\n        ...\n    },\n    prefer_skip_nested_validation=True,\n)\n```\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [model_selection](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/model_selection)- [Lines 2500 - 2751](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/model_selection/_split.py#L2500-L2751)\n\n This snippet contains the validation logic for 'test_size' and 'train_size' parameters in train_test_split, which is directly related to the issue.",
        "context_relevance": 0.16999999999999998,
        "hypotheses": [],
        "groundedness_score": 1.0,
        "qa_relevance_reasons": {
            "reason": "Criteria: The RESPONSE must be relevant to the entire PROMPT to get a score of 10. A RESPONSE that is relevant to some of the PROMPT should get a score of 2, 3, or 4.\nSupporting Evidence: The RESPONSE provides a potential solution to the issue described in the PROMPT by suggesting code modifications to the `train_test_split` function. However, it does not directly address whether the behavior observed in the PROMPT is a bug or an intended feature, which is the core question. The RESPONSE is partially relevant as it relates to the `train_test_split` function and the validation of the `test_size` parameter but does not fully answer the PROMPT's inquiry."
        },
        "qa_relevance": 0.2
    }
}