{
    "id": "scikit-learn__scikit-learn-13618",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
    "task_id": 7291000626362246049,
    "title": "Allow setting of initial hyperparameters of BayesianRidge",
    "body": "<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn\r\n- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn\r\nFor more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->\r\nI propose to be able to set initial values \u200b\u200bof the hyperparameters (alpha, lambda) in BayesianRidge().fit.\r\n\r\nI tried to fit a sinusoidal curve with polynomials by Bayesian ridge regression, but the default initial values did not work well (left figure).\r\n\r\nSo, I corrected the fit method of the BayesianRidge class so that I could set the initial value, and the regression worked well (right figure).\r\n![Figure_1](https://user-images.githubusercontent.com/40843206/55940024-349d4b80-5c7a-11e9-8390-0c3b800b9d19.png)\r\n\r\n#### Steps/Code to Reproduce\r\n<!--\r\nExample:\r\n```python\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nfrom sklearn.decomposition import LatentDirichletAllocation\r\n\r\ndocs = [\"Help I have a bug\" for i in range(1000)]\r\n\r\nvectorizer = CountVectorizer(input=docs, analyzer='word')\r\nlda_features = vectorizer.fit_transform(docs)\r\n\r\nlda_model = LatentDirichletAllocation(\r\n    n_topics=10,\r\n    learning_method='online',\r\n    evaluate_every=10,\r\n    n_jobs=4,\r\n)\r\nmodel = lda_model.fit(lda_features)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https://gist.github.com\r\n-->\r\nThe code I wrote is Here.\r\n```\r\n#!/usr/bin/env python\r\n# coding: utf-8\r\n\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nfrom math import log\r\nfrom scipy import linalg\r\nfrom sklearn.linear_model import BayesianRidge\r\nfrom sklearn.utils import check_X_y\r\n\r\nclass BayesRidge(BayesianRidge):\r\n    def fit(self, X, y, sample_weight=None, alpha_0=None, lambda_0=None):\r\n        \"\"\"Fit the model\r\n\r\n        Parameters\r\n        ----------\r\n        X : numpy array of shape [n_samples,n_features]\r\n            Training data\r\n        y : numpy array of shape [n_samples]\r\n            Target values. Will be cast to X's dtype if necessary\r\n\r\n        sample_weight : numpy array of shape [n_samples]\r\n            Individual weights for each sample\r\n\r\n            .. versionadded:: 0.20\r\n               parameter *sample_weight* support to BayesianRidge.\r\n\r\n        Returns\r\n        -------\r\n        self : returns an instance of self.\r\n        \"\"\"\r\n        X, y = check_X_y(X, y, dtype=np.float64, y_numeric=True)\r\n        X, y, X_offset_, y_offset_, X_scale_ = self._preprocess_data(\r\n            X, y, self.fit_intercept, self.normalize, self.copy_X,\r\n            sample_weight=sample_weight)\r\n\r\n        if sample_weight is not None:\r\n            # Sample weight can be implemented via a simple rescaling.\r\n            X, y = _rescale_data(X, y, sample_weight)\r\n\r\n        self.X_offset_ = X_offset_\r\n        self.X_scale_ = X_scale_\r\n        n_samples, n_features = X.shape\r\n\r\n        # Initialization of the values of the parameters\r\n        eps = np.finfo(np.float64).eps\r\n        # Add `eps` in the denominator to omit division by zero if `np.var(y)`\r\n        # is zero\r\n        if alpha_0 is None:\r\n            alpha_ = 1. / (np.var(y) + eps)\r\n        else:\r\n            alpha_ = alpha_0\r\n        if lambda_0 is None:\r\n            lambda_ = 1.\r\n        else:\r\n            lambda_ = lambda_0\r\n\r\n        verbose = self.verbose\r\n        lambda_1 = self.lambda_1\r\n        lambda_2 = self.lambda_2\r\n        alpha_1 = self.alpha_1\r\n        alpha_2 = self.alpha_2\r\n\r\n        self.scores_ = list()\r\n        coef_old_ = None\r\n\r\n        XT_y = np.dot(X.T, y)\r\n        U, S, Vh = linalg.svd(X, full_matrices=False)\r\n        eigen_vals_ = S ** 2\r\n\r\n        # Convergence loop of the bayesian ridge regression\r\n        for iter_ in range(self.n_iter):\r\n\r\n            # Compute mu and sigma\r\n            # sigma_ = lambda_ / alpha_ * np.eye(n_features) + np.dot(X.T, X)\r\n            # coef_ = sigma_^-1 * XT * y\r\n            if n_samples > n_features:\r\n                coef_ = np.dot(Vh.T,\r\n                               Vh / (eigen_vals_ +\r\n                                     lambda_ / alpha_)[:, np.newaxis])\r\n                coef_ = np.dot(coef_, XT_y)\r\n                if self.compute_score:\r\n                    logdet_sigma_ = - np.sum(\r\n                        np.log(lambda_ + alpha_ * eigen_vals_))\r\n            else:\r\n                coef_ = np.dot(X.T, np.dot(\r\n                    U / (eigen_vals_ + lambda_ / alpha_)[None, :], U.T))\r\n                coef_ = np.dot(coef_, y)\r\n                if self.compute_score:\r\n                    logdet_sigma_ = np.full(n_features, lambda_,\r\n                                            dtype=np.array(lambda_).dtype)\r\n                    logdet_sigma_[:n_samples] += alpha_ * eigen_vals_\r\n                    logdet_sigma_ = - np.sum(np.log(logdet_sigma_))\r\n\r\n            # Preserve the alpha and lambda values that were used to\r\n            # calculate the final coefficients\r\n            self.alpha_ = alpha_\r\n            self.lambda_ = lambda_\r\n\r\n            # Update alpha and lambda\r\n            rmse_ = np.sum((y - np.dot(X, coef_)) ** 2)\r\n            gamma_ = (np.sum((alpha_ * eigen_vals_) /\r\n                      (lambda_ + alpha_ * eigen_vals_)))\r\n            lambda_ = ((gamma_ + 2 * lambda_1) /\r\n                       (np.sum(coef_ ** 2) + 2 * lambda_2))\r\n            alpha_ = ((n_samples - gamma_ + 2 * alpha_1) /\r\n                      (rmse_ + 2 * alpha_2))\r\n\r\n            # Compute the objective function\r\n            if self.compute_score:\r\n                s = lambda_1 * log(lambda_) - lambda_2 * lambda_\r\n                s += alpha_1 * log(alpha_) - alpha_2 * alpha_\r\n                s += 0.5 * (n_features * log(lambda_) +\r\n                            n_samples * log(alpha_) -\r\n                            alpha_ * rmse_ -\r\n                            (lambda_ * np.sum(coef_ ** 2)) -\r\n                            logdet_sigma_ -\r\n                            n_samples * log(2 * np.pi))\r\n                self.scores_.append(s)\r\n\r\n            # Check for convergence\r\n            if iter_ != 0 and np.sum(np.abs(coef_old_ - coef_)) < self.tol:\r\n                if verbose:\r\n                    print(\"Convergence after \", str(iter_), \" iterations\")\r\n                break\r\n            coef_old_ = np.copy(coef_)\r\n\r\n        self.coef_ = coef_\r\n        sigma_ = np.dot(Vh.T,\r\n                        Vh / (eigen_vals_ + lambda_ / alpha_)[:, np.newaxis])\r\n        self.sigma_ = (1. / alpha_) * sigma_\r\n\r\n        self._set_intercept(X_offset_, y_offset_, X_scale_)\r\n        return self\r\n\r\ndef main():\r\n    def func(x):\r\n        return np.sin(2*np.pi*x)\r\n    size = 25\r\n    np.random.seed(1234)\r\n    xtrain = np.random.uniform(0.,1.,size)\r\n    ytrain = func(xtrain)+np.random.normal(scale=0.1,size=size)\r\n    xtest = np.linspace(0.,1.,100)\r\n\r\n    nOrder = 3\r\n    Xtrain = np.vander(xtrain,nOrder+1,increasing=True)\r\n    Xtest = np.vander(xtest,nOrder+1,increasing=True)\r\n\r\n    fig,ax = plt.subplots(1,2,figsize=(8,4))\r\n    regs = [BayesianRidge(tol=1e-6,fit_intercept=False),\r\n            BayesRidge(tol=1e-6,fit_intercept=False)]\r\n    init = (1.,1.e-3)\r\n\r\n    for i,reg in enumerate(regs):\r\n        if i == 0:\r\n            reg.fit(Xtrain,ytrain)\r\n        elif i == 1:\r\n            reg.fit(Xtrain,ytrain,alpha_0=init[0],lambda_0=init[1])\r\n\r\n        ymean,ystd = reg.predict(Xtest,return_std=True)\r\n        print(reg.alpha_,reg.lambda_)\r\n\r\n        ax[i].scatter(xtrain,ytrain, s=50, alpha=0.5, label=\"observation\")\r\n        ax[i].plot(xtest,func(xtest), color=\"blue\", label=\"sin(2$\u03c0x$)\")\r\n        ax[i].plot(xtest,ymean, color=\"red\", label=\"predict_mean\")\r\n        ax[i].fill_between(xtest,ymean-ystd,ymean+ystd, color=\"pink\", alpha=0.5, label=\"predict_std\")\r\n        ax[i].legend()\r\n        if i == 0:\r\n            ax[i].set_title(\"BayesianRidge\")\r\n        elif i == 1:\r\n            ax[i].set_title(\"$\u03b1\u2080={} ,\u03bb\u2080=${}\".format(init[0],init[1]))\r\n\r\n    plt.tight_layout()\r\n    plt.show()\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n#### Expected Results\r\n<!-- Example: No error is thrown. Please paste or describe the expected results.-->\r\nThe right figure\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback. -->\r\nThe left figure\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nFor scikit-learn >= 0.20:\r\nimport sklearn; sklearn.show_versions()\r\nFor scikit-learn < 0.20:\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\n-->\r\nSystem:\r\n    python: 3.6.6 (default, Mar  8 2019, 18:24:30)  [GCC 7.3.0]\r\nexecutable: /home/*****/.pyenv/versions/3.6.6/bin/python\r\n   machine: Linux-4.15.0-47-generic-x86_64-with-debian-buster-sid\r\n\r\nBLAS:\r\n    macros: NO_ATLAS_INFO=1, HAVE_CBLAS=None\r\n  lib_dirs: /usr/lib/x86_64-linux-gnu\r\ncblas_libs: cblas\r\n\r\nPython deps:\r\n       pip: 19.0.3\r\nsetuptools: 41.0.0\r\n   sklearn: 0.20.3\r\n     numpy: 1.16.2\r\n     scipy: 1.2.1\r\n    Cython: 0.29.6\r\n    pandas: 0.24.2\r\n\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.22.git",
    "requested_commit": "",
    "mayil_response": "Doesn't seem like we have fully grasped all the specifics of this ticket \ud83e\udd14.                   But we've outlined some ideas that might help you get started:\n\nTo implement the feature of setting initial hyperparameters (alpha, lambda) in the `BayesianRidge` model, you can follow these steps:\n\n- Modify the `BayesianRidge` class to accept new parameters `alpha_init` and `lambda_init` in its constructor.\n- Use these initial values in the `fit` method instead of the default values when initializing `alpha_` and `lambda_`.\n\nHere is a high-level overview of the changes needed:\n\n```python\nfrom sklearn.linear_model import BayesianRidge\n\nclass CustomBayesianRidge(BayesianRidge):\n    def __init__(self, alpha_init=None, lambda_init=None, *args, **kwargs):\n        super(CustomBayesianRidge, self).__init__(*args, **kwargs)\n        self.alpha_init = alpha_init\n        self.lambda_init = lambda_init\n\n    def fit(self, X, y, sample_weight=None):\n        # ... existing code ...\n\n        # Use initial values if provided, otherwise use default initialization\n        alpha_ = self.alpha_init if self.alpha_init is not None else 1. / (np.var(y) + eps)\n        lambda_ = self.lambda_init if self.lambda_init is not None else 1.\n\n        # ... rest of the fit method code ...\n\n# Usage\nmodel = CustomBayesianRidge(alpha_init=your_alpha_value, lambda_init=your_lambda_value)\nmodel.fit(X_train, y_train)\n```\n\nMake sure to replace `your_alpha_value` and `your_lambda_value` with the actual initial values you want to use for the hyperparameters.\n\nThis is a simplified version of the changes. You would need to integrate this logic with the existing `BayesianRidge` class in scikit-learn, ensuring that all other functionalities of the class remain intact.\n\n### Code snippets to check\n\n-  \u2192 [examples](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//examples) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//examples/linear_model)- [Lines 64 - 92](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//examples/linear_model/plot_bayesian_ridge_curvefit.py#L64-L92)\n\n This snippet shows an example of how to set initial values for alpha and lambda in BayesianRidge, which is directly related to the issue of allowing the setting of initial hyperparameters. \n\n\n   - [Lines 1 - 63](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//examples/linear_model/plot_bayesian_ridge_curvefit.py#L1-L63)\n\n This snippet includes the example setup and context for the BayesianRidge regression, which is necessary to understand the issue and the proposed changes.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/linear_model)- [Lines 1 - 67](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/linear_model/_bayes.py#L1-L67)\n\n This snippet contains the BayesianRidge regression code from the scikit-learn library, which is likely where the changes need to be implemented to allow setting initial hyperparameters.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0005394999999999999,
        "snippet_processor": 0.08021,
        "issue_star_creation": 0.03338,
        "issue_star_solver": 0.08541,
        "bouncer": 0.023790000000000002
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711739737.0204291,
        "relevant_snippets": [
            {
                "code": "for i, ax in enumerate(axes):\n    # Bayesian ridge regression with different initial value pairs\n    if i == 0:\n        init = [1 / np.var(y_train), 1.0]  # Default values\n    elif i == 1:\n        init = [1.0, 1e-3]\n        reg.set_params(alpha_init=init[0], lambda_init=init[1])\n    reg.fit(X_train, y_train)\n    ymean, ystd = reg.predict(X_test, return_std=True)\n\n    ax.plot(x_test, func(x_test), color=\"blue\", label=\"sin($2\\\\pi x$)\")\n    ax.scatter(x_train, y_train, s=50, alpha=0.5, label=\"observation\")\n    ax.plot(x_test, ymean, color=\"red\", label=\"predict mean\")\n    ax.fill_between(\n        x_test, ymean - ystd, ymean + ystd, color=\"pink\", alpha=0.5, label=\"predict std\"\n    )\n    ax.set_ylim(-1.3, 1.3)\n    ax.legend()\n    title = \"$\\\\alpha$_init$={:.2f},\\\\ \\\\lambda$_init$={}$\".format(init[0], init[1])\n    if i == 0:\n        title += \" (Default)\"\n    ax.set_title(title, fontsize=12)\n    text = \"$\\\\alpha={:.1f}$\\n$\\\\lambda={:.3f}$\\n$L={:.1f}$\".format(\n        reg.alpha_, reg.lambda_, reg.scores_[-1]\n    )\n    ax.text(0.05, -1.0, text, fontsize=12)\n\nplt.tight_layout()\nplt.show()",
                "filename": "examples/linear_model/plot_bayesian_ridge_curvefit.py",
                "start_index": 1996,
                "end_index": 3090,
                "start_line": 64,
                "end_line": 92,
                "max_line": 92,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.8
            },
            {
                "code": "\"\"\"\n============================================\nCurve Fitting with Bayesian Ridge Regression\n============================================\n\nComputes a Bayesian Ridge Regression of Sinusoids.\n\nSee :ref:`bayesian_ridge_regression` for more information on the regressor.\n\nIn general, when fitting a curve with a polynomial by Bayesian ridge\nregression, the selection of initial values of\nthe regularization parameters (alpha, lambda) may be important.\nThis is because the regularization parameters are determined by an iterative\nprocedure that depends on initial values.\n\nIn this example, the sinusoid is approximated by a polynomial using different\npairs of initial values.\n\nWhen starting from the default values (alpha_init = 1.90, lambda_init = 1.),\nthe bias of the resulting curve is large, and the variance is small.\nSo, lambda_init should be relatively small (1.e-3) so as to reduce the bias.\n\nAlso, by evaluating log marginal likelihood (L) of\nthese models, we can determine which one is better.\nIt can be concluded that the model with larger L is more likely.\n\n\"\"\"\n\n# Author: Yoshihiro Uchida <nimbus1after2a1sun7shower@gmail.com>\n\n# %%\n# Generate sinusoidal data with noise\n# -----------------------------------\nimport numpy as np\n\n\ndef func(x):\n    return np.sin(2 * np.pi * x)\n\n\nsize = 25\nrng = np.random.RandomState(1234)\nx_train = rng.uniform(0.0, 1.0, size)\ny_train = func(x_train) + rng.normal(scale=0.1, size=size)\nx_test = np.linspace(0.0, 1.0, 100)\n\n\n# %%\n# Fit by cubic polynomial\n# -----------------------\nfrom sklearn.linear_model import BayesianRidge\n\nn_order = 3\nX_train = np.vander(x_train, n_order + 1, increasing=True)\nX_test = np.vander(x_test, n_order + 1, increasing=True)\nreg = BayesianRidge(tol=1e-6, fit_intercept=False, compute_score=True)\n\n# %%\n# Plot the true and predicted curves with log marginal likelihood (L)\n# -------------------------------------------------------------------\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))",
                "filename": "examples/linear_model/plot_bayesian_ridge_curvefit.py",
                "start_index": 0,
                "end_index": 1995,
                "start_line": 1,
                "end_line": 63,
                "max_line": 92,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 1.0
            },
            {
                "code": "# For the purpose of showing how Ridge regularization works, we will create a\n# non-noisy data set. Then we will train a regularized model on a range of\n# regularization strengths (:math:`\\alpha`) and plot how the trained\n# coefficients and the mean squared error between those and the original values\n# behave as functions of the regularization strength.\n#\n# Creating a non-noisy data set\n# *****************************\n# We make a toy data set with 100 samples and 10 features, that's suitable to\n# detect regression. Out of the 10 features, 8 are informative and contribute to\n# the regression, while the remaining 2 features do not have any effect on the\n# target variable (their true coefficients are 0). Please note that in this\n# example the data is non-noisy, hence we can expect our regression model to\n# recover exactly the true coefficients w.\nfrom sklearn.datasets import make_regression\n\nX, y, w = make_regression(\n    n_samples=100, n_features=10, n_informative=8, coef=True, random_state=1\n)\n\n# Obtain the true coefficients\nprint(f\"The true coefficient of this regression problem are:\\n{w}\")\n\n# %%\n# Training the Ridge Regressor\n# ****************************\n# We use :class:`~sklearn.linear_model.Ridge`, a linear model with L2\n# regularization. We train several models, each with a different value for the\n# model parameter `alpha`, which is a positive constant that multiplies the\n# penalty term, controlling the regularization strength. For each trained model\n# we then compute the error between the true coefficients `w` and the\n# coefficients found by the model `clf`. We store the identified coefficients\n# and the calculated errors for the corresponding coefficients in lists, which\n# makes it convenient for us to plot them.\nimport numpy as np\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\nclf = Ridge()\n\n# Generate values for `alpha` that are evenly distributed on a logarithmic scale\nalphas = np.logspace(-3, 4, 200)\ncoefs = []\nerrors_coefs = []\n\n# Train the model with different regularisation strengths\nfor a in alphas:\n    clf.set_params(alpha=a).fit(X, y)\n    coefs.append(clf.coef_)\n    errors_coefs.append(mean_squared_error(clf.coef_, w))\n\n# %%\n# Plotting trained Coefficients and Mean Squared Errors\n# *****************************************************\n# We now plot the 10 different regularized coefficients as a function of the\n# regularization parameter `alpha` where each color represents a different\n# coefficient.\n#\n# On the right-hand-side, we plot how the errors of the coefficients from the\n# estimator change as a function of regularization.\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nalphas = pd.Index(alphas, name=\"alpha\")\ncoefs = pd.DataFrame(coefs, index=alphas, columns=[f\"Feature {i}\" for i in range(10)])\nerrors = pd.Series(errors_coefs, index=alphas, name=\"Mean squared error\")\n\nfig, axs = plt.subplots(1, 2, figsize=(20, 6))",
                "filename": "examples/linear_model/plot_ridge_coeffs.py",
                "start_index": 2976,
                "end_index": 5910,
                "start_line": 59,
                "end_line": 127,
                "max_line": 180,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"\"\"\nVarious bayesian regression\n\"\"\"\n\n# Authors: V. Michel, F. Pedregosa, A. Gramfort\n# License: BSD 3 clause\n\nimport warnings\nfrom math import log\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy import linalg\nfrom scipy.linalg import pinvh\n\nfrom ..base import RegressorMixin, _fit_context\nfrom ..utils._param_validation import Hidden, Interval, StrOptions\nfrom ..utils.extmath import fast_logdet\nfrom ..utils.validation import _check_sample_weight\nfrom ._base import LinearModel, _preprocess_data, _rescale_data\n\n\n# TODO(1.5) Remove\ndef _deprecate_n_iter(n_iter, max_iter):\n    \"\"\"Deprecates n_iter in favour of max_iter. Checks if the n_iter has been\n    used instead of max_iter and generates a deprecation warning if True.\n\n    Parameters\n    ----------\n    n_iter : int,\n        Value of n_iter attribute passed by the estimator.\n\n    max_iter : int, default=None\n        Value of max_iter attribute passed by the estimator.\n        If `None`, it corresponds to `max_iter=300`.\n\n    Returns\n    -------\n    max_iter : int,\n        Value of max_iter which shall further be used by the estimator.\n\n    Notes\n    -----\n    This function should be completely removed in 1.5.\n    \"\"\"\n    if n_iter != \"deprecated\":\n        if max_iter is not None:\n            raise ValueError(\n                \"Both `n_iter` and `max_iter` attributes were set. Attribute\"\n                \" `n_iter` was deprecated in version 1.3 and will be removed in\"\n                \" 1.5. To avoid this error, only set the `max_iter` attribute.\"\n            )\n        warnings.warn(\n            (\n                \"'n_iter' was renamed to 'max_iter' in version 1.3 and \"\n                \"will be removed in 1.5\"\n            ),\n            FutureWarning,\n        )\n        max_iter = n_iter\n    elif max_iter is None:\n        max_iter = 300\n    return max_iter\n\n\n###############################################################################\n# BayesianRidge regression",
                "filename": "sklearn/linear_model/_bayes.py",
                "start_index": 0,
                "end_index": 1955,
                "start_line": 1,
                "end_line": 67,
                "max_line": 848,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 1.0
            },
            {
                "code": "\"\"\"\n====================================\nComparing Linear Bayesian Regressors\n====================================\n\nThis example compares two different bayesian regressors:\n\n - a :ref:`automatic_relevance_determination`\n - a :ref:`bayesian_ridge_regression`\n\nIn the first part, we use an :ref:`ordinary_least_squares` (OLS) model as a\nbaseline for comparing the models' coefficients with respect to the true\ncoefficients. Thereafter, we show that the estimation of such models is done by\niteratively maximizing the marginal log-likelihood of the observations.\n\nIn the last section we plot predictions and uncertainties for the ARD and the\nBayesian Ridge regressions using a polynomial feature expansion to fit a\nnon-linear relationship between `X` and `y`.\n\n\"\"\"\n\n# Author: Arturo Amor <david-arturo.amor-quiroz@inria.fr>\n\n# %%\n# Models robustness to recover the ground truth weights\n# =====================================================\n#\n# Generate synthetic dataset\n# --------------------------\n#\n# We generate a dataset where `X` and `y` are linearly linked: 10 of the\n# features of `X` will be used to generate `y`. The other features are not\n# useful at predicting `y`. In addition, we generate a dataset where `n_samples\n# == n_features`. Such a setting is challenging for an OLS model and leads\n# potentially to arbitrary large weights. Having a prior on the weights and a\n# penalty alleviates the problem. Finally, gaussian noise is added.\n\nfrom sklearn.datasets import make_regression\n\nX, y, true_weights = make_regression(\n    n_samples=100,\n    n_features=100,\n    n_informative=10,\n    noise=8,\n    coef=True,\n    random_state=42,\n)\n\n# %%\n# Fit the regressors\n# ------------------\n#\n# We now fit both Bayesian models and the OLS to later compare the models'\n# coefficients.\n\nimport pandas as pd\n\nfrom sklearn.linear_model import ARDRegression, BayesianRidge, LinearRegression\n\nolr = LinearRegression().fit(X, y)\nbrr = BayesianRidge(compute_score=True, n_iter=30).fit(X, y)\nard = ARDRegression(compute_score=True, n_iter=30).fit(X, y)\ndf = pd.DataFrame(\n    {\n        \"Weights of true generative process\": true_weights,\n        \"ARDRegression\": ard.coef_,\n        \"BayesianRidge\": brr.coef_,\n        \"LinearRegression\": olr.coef_,\n    }\n)\n\n# %%\n# Plot the true and estimated coefficients\n# ----------------------------------------\n#\n# Now we compare the coefficients of each model with the weights of\n# the true generative model.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import SymLogNorm\n\nplt.figure(figsize=(10, 6))\nax = sns.heatmap(\n    df.T,\n    norm=SymLogNorm(linthresh=10e-4, vmin=-80, vmax=80),\n    cbar_kws={\"label\": \"coefficients' values\"},\n    cmap=\"seismic_r\",\n)\nplt.ylabel(\"linear model\")\nplt.xlabel(\"coefficients\")\nplt.tight_layout(rect=(0, 0, 1, 0.95))\n_ = plt.title(\"Models' coefficients\")\n\n# %%\n# Due to the added noise, none of the models recover the true weights. Indeed,",
                "filename": "examples/linear_model/plot_ard.py",
                "start_index": 0,
                "end_index": 2939,
                "start_line": 1,
                "end_line": 95,
                "max_line": 211,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "ridge = Ridge().fit(training_data, training_noisy_target)\n\nplt.plot(data, target, label=\"True signal\", linewidth=2)\nplt.scatter(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\nplt.plot(data, ridge.predict(data), label=\"Ridge regression\")\nplt.legend()\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\n_ = plt.title(\"Limitation of a linear model such as ridge\")\n\n# %%\n# Such a ridge regressor underfits data since it is not expressive enough.\n#\n# Kernel methods: kernel ridge and Gaussian process\n# -------------------------------------------------\n#\n# Kernel ridge\n# ............\n#\n# We can make the previous linear model more expressive by using a so-called\n# kernel. A kernel is an embedding from the original feature space to another\n# one. Simply put, it is used to map our original data into a newer and more\n# complex feature space. This new space is explicitly defined by the choice of\n# kernel.\n#\n# In our case, we know that the true generative process is a periodic function.\n# We can use a :class:`~sklearn.gaussian_process.kernels.ExpSineSquared` kernel\n# which allows recovering the periodicity. The class\n# :class:`~sklearn.kernel_ridge.KernelRidge` will accept such a kernel.\n#\n# Using this model together with a kernel is equivalent to embed the data\n# using the mapping function of the kernel and then apply a ridge regression.\n# In practice, the data are not mapped explicitly; instead the dot product\n# between samples in the higher dimensional feature space is computed using the\n# \"kernel trick\".\n#\n# Thus, let's use such a :class:`~sklearn.kernel_ridge.KernelRidge`.\nimport time\n\nfrom sklearn.gaussian_process.kernels import ExpSineSquared\nfrom sklearn.kernel_ridge import KernelRidge\n\nkernel_ridge = KernelRidge(kernel=ExpSineSquared())\n\nstart_time = time.time()\nkernel_ridge.fit(training_data, training_noisy_target)\nprint(\n    f\"Fitting KernelRidge with default kernel: {time.time() - start_time:.3f} seconds\"\n)\n\n# %%\nplt.plot(data, target, label=\"True signal\", linewidth=2, linestyle=\"dashed\")\nplt.scatter(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\nplt.plot(\n    data,\n    kernel_ridge.predict(data),\n    label=\"Kernel ridge\",\n    linewidth=2,\n    linestyle=\"dashdot\",\n)\nplt.legend(loc=\"lower right\")\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\n_ = plt.title(\n    \"Kernel ridge regression with an exponential sine squared\\n \"\n    \"kernel using default hyperparameters\"\n)\n\n# %%\n# This fitted model is not accurate. Indeed, we did not set the parameters of\n# the kernel and instead used the default ones. We can inspect them.\nkernel_ridge.kernel\n\n# %%\n# Our kernel has two parameters: the length-scale and the periodicity. For our\n# dataset, we use `sin` as the generative process, implying a\n# :math:`2 \\pi`-periodicity for the signal. The default value of the parameter\n# being :math:`1`, it explains the high frequency observed in the predictions of\n# our model.",
                "filename": "examples/gaussian_process/plot_compare_gpr_krr.py",
                "start_index": 2974,
                "end_index": 5957,
                "start_line": 85,
                "end_line": 274,
                "max_line": 397,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "# improves the conditioning of the problem and reduces the variance of the\n# estimates. :class:`~sklearn.linear_model.RidgeCV` applies cross validation\n# in order to determine which value of the regularization parameter (`alpha`)\n# is best suited for prediction.\n\nfrom sklearn.linear_model import RidgeCV\n\nalphas = np.logspace(-10, 10, 21)  # alpha values to be chosen from by cross-validation\nmodel = make_pipeline(\n    preprocessor,\n    TransformedTargetRegressor(\n        regressor=RidgeCV(alphas=alphas),\n        func=np.log10,\n        inverse_func=sp.special.exp10,\n    ),\n)\nmodel.fit(X_train, y_train)\n\n# %%\n# First we check which value of :math:`\\alpha` has been selected.\n\nmodel[-1].regressor_.alpha_\n\n# %%\n# Then we check the quality of the predictions.\nmae_train = median_absolute_error(y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = median_absolute_error(y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = plt.subplots(figsize=(5, 5))\ndisplay = PredictionErrorDisplay.from_predictions(\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\nplt.tight_layout()\n\n# %%\n# The ability to reproduce the data of the regularized model is similar to\n# the one of the non-regularized model.\n\ncoefs = pd.DataFrame(\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\nplt.title(\"Ridge model, with regularization, normalized variables\")\nplt.xlabel(\"Raw coefficient values\")\nplt.axvline(x=0, color=\".5\")\nplt.subplots_adjust(left=0.3)\n\n# %%\n# The coefficients are significantly different.\n# AGE and EXPERIENCE coefficients are both positive but they now have less\n# influence on the prediction.\n#\n# The regularization reduces the influence of correlated\n# variables on the model because the weight is shared between the two\n# predictive variables, so neither alone would have strong weights.\n#\n# On the other hand, the weights obtained with regularization are more\n# stable (see the :ref:`ridge_regression` User Guide section). This\n# increased stability is visible from the plot, obtained from data\n# perturbations, in a cross-validation. This plot can be compared with\n# the :ref:`previous one<covariation>`.\n\ncv_model = cross_validate(\n    model,\n    X,\n    y,\n    cv=cv,\n    return_estimator=True,\n    n_jobs=2,\n)\ncoefs = pd.DataFrame(\n    [est[-1].regressor_.coef_ for est in cv_model[\"estimator\"]], columns=feature_names\n)\n\n# %%\nplt.ylabel(\"Age coefficient\")\nplt.xlabel(\"Experience coefficient\")\nplt.grid(True)\nplt.xlim(-0.4, 0.5)\nplt.ylim(-0.4, 0.5)\nplt.scatter(coefs[\"AGE\"], coefs[\"EXPERIENCE\"])\n_ = plt.title(\"Co-variations of coefficients for AGE and EXPERIENCE across folds\")\n\n# %%",
                "filename": "examples/inspection/plot_linear_model_coefficient_interpretation.py",
                "start_index": 17829,
                "end_index": 20820,
                "start_line": 518,
                "end_line": 710,
                "max_line": 757,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "# all models always have more than 10 non-zero coefficients. Compared to the OLS\n# estimator, the coefficients using a Bayesian Ridge regression are slightly\n# shifted toward zero, which stabilises them. The ARD regression provides a\n# sparser solution: some of the non-informative coefficients are set exactly to\n# zero, while shifting others closer to zero. Some non-informative coefficients\n# are still present and retain large values.\n\n# %%\n# Plot the marginal log-likelihood\n# --------------------------------\nimport numpy as np\n\nard_scores = -np.array(ard.scores_)\nbrr_scores = -np.array(brr.scores_)\nplt.plot(ard_scores, color=\"navy\", label=\"ARD\")\nplt.plot(brr_scores, color=\"red\", label=\"BayesianRidge\")\nplt.ylabel(\"Log-likelihood\")\nplt.xlabel(\"Iterations\")\nplt.xlim(1, 30)\nplt.legend()\n_ = plt.title(\"Models log-likelihood\")\n\n# %%\n# Indeed, both models minimize the log-likelihood up to an arbitrary cutoff\n# defined by the `n_iter` parameter.\n#\n# Bayesian regressions with polynomial feature expansion\n# ======================================================\n# Generate synthetic dataset\n# --------------------------\n# We create a target that is a non-linear function of the input feature.\n# Noise following a standard uniform distribution is added.\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\n\nrng = np.random.RandomState(0)\nn_samples = 110\n\n# sort the data to make plotting easier later\nX = np.sort(-10 * rng.rand(n_samples) + 10)\nnoise = rng.normal(0, 1, n_samples) * 1.35\ny = np.sqrt(X) * np.sin(X) + noise\nfull_data = pd.DataFrame({\"input_feature\": X, \"target\": y})\nX = X.reshape((-1, 1))\n\n# extrapolation\nX_plot = np.linspace(10, 10.4, 10)\ny_plot = np.sqrt(X_plot) * np.sin(X_plot)\nX_plot = np.concatenate((X, X_plot.reshape((-1, 1))))\ny_plot = np.concatenate((y - noise, y_plot))\n\n# %%\n# Fit the regressors\n# ------------------\n#\n# Here we try a degree 10 polynomial to potentially overfit, though the bayesian\n# linear models regularize the size of the polynomial coefficients. As\n# `fit_intercept=True` by default for\n# :class:`~sklearn.linear_model.ARDRegression` and\n# :class:`~sklearn.linear_model.BayesianRidge`, then\n# :class:`~sklearn.preprocessing.PolynomialFeatures` should not introduce an\n# additional bias feature. By setting `return_std=True`, the bayesian regressors\n# return the standard deviation of the posterior distribution for the model\n# parameters.\n\nard_poly = make_pipeline(\n    PolynomialFeatures(degree=10, include_bias=False),\n    StandardScaler(),\n    ARDRegression(),\n).fit(X, y)\nbrr_poly = make_pipeline(\n    PolynomialFeatures(degree=10, include_bias=False),\n    StandardScaler(),\n    BayesianRidge(),\n).fit(X, y)\n\ny_ard, y_ard_std = ard_poly.predict(X_plot, return_std=True)\ny_brr, y_brr_std = brr_poly.predict(X_plot, return_std=True)\n\n# %%\n# Plotting polynomial regressions with std errors of the scores\n# -------------------------------------------------------------",
                "filename": "examples/linear_model/plot_ard.py",
                "start_index": 2940,
                "end_index": 5927,
                "start_line": 96,
                "end_line": 178,
                "max_line": 211,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"\"\"\n=========================================================\nRidge coefficients as a function of the L2 Regularization\n=========================================================\n\nA model that overfits learns the training data too well, capturing both the\nunderlying patterns and the noise in the data. However, when applied to unseen\ndata, the learned associations may not hold. We normally detect this when we\napply our trained predictions to the test data and see the statistical\nperformance drop significantly compared to the training data.\n\nOne way to overcome overfitting is through regularization, which can be done by\npenalizing large weights (coefficients) in linear models, forcing the model to\nshrink all coefficients. Regularization reduces a model's reliance on specific\ninformation obtained from the training samples.\n\nThis example illustrates how L2 regularization in a\n:class:`~sklearn.linear_model.Ridge` regression affects a model's performance by\nadding a penalty term to the loss that increases with the coefficients\n:math:`\\\\beta`.\n\nThe regularized loss function is given by: :math:`\\\\mathcal{L}(X, y, \\\\beta) =\n\\\\| y - X \\\\beta \\\\|^{2}_{2} + \\\\alpha \\\\| \\\\beta \\\\|^{2}_{2}`\n\nwhere :math:`X` is the input data, :math:`y` is the target variable,\n:math:`\\\\beta` is the vector of coefficients associated with the features, and\n:math:`\\\\alpha` is the regularization strength.\n\nThe regularized loss function aims to balance the trade-off between accurately\npredicting the training set and to prevent overfitting.\n\nIn this regularized loss, the left-hand side (e.g. :math:`\\\\|y -\nX\\\\beta\\\\|^{2}_{2}`) measures the squared difference between the actual target\nvariable, :math:`y`, and the predicted values. Minimizing this term alone could\nlead to overfitting, as the model may become too complex and sensitive to noise\nin the training data.\n\nTo address overfitting, Ridge regularization adds a constraint, called a penalty\nterm, (:math:`\\\\alpha \\\\| \\\\beta\\\\|^{2}_{2}`) to the loss function. This penalty\nterm is the sum of the squares of the model's coefficients, multiplied by the\nregularization strength :math:`\\\\alpha`. By introducing this constraint, Ridge\nregularization discourages any single coefficient :math:`\\\\beta_{i}` from taking\nan excessively large value and encourages smaller and more evenly distributed\ncoefficients. Higher values of :math:`\\\\alpha` force the coefficients towards\nzero. However, an excessively high :math:`\\\\alpha` can result in an underfit\nmodel that fails to capture important patterns in the data.\n\nTherefore, the regularized loss function combines the prediction accuracy term\nand the penalty term. By adjusting the regularization strength, practitioners\ncan fine-tune the degree of constraint imposed on the weights, training a model\ncapable of generalizing well to unseen data while avoiding overfitting.\n\"\"\"\n\n# Author: Kornel Kielczewski -- <kornel.k@plusnet.pl>\n\n# %%\n# Purpose of this example\n# -----------------------",
                "filename": "examples/linear_model/plot_ridge_coeffs.py",
                "start_index": 0,
                "end_index": 2975,
                "start_line": 1,
                "end_line": 58,
                "max_line": 180,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "coefs.plot(\n    ax=axs[0],\n    logx=True,\n    title=\"Ridge coefficients as a function of the regularization strength\",\n)\naxs[0].set_ylabel(\"Ridge coefficient values\")\nerrors.plot(\n    ax=axs[1],\n    logx=True,\n    title=\"Coefficient error as a function of the regularization strength\",\n)\n_ = axs[1].set_ylabel(\"Mean squared error\")\n# %%\n# Interpreting the plots\n# **********************\n# The plot on the left-hand side shows how the regularization strength (`alpha`)\n# affects the Ridge regression coefficients. Smaller values of `alpha` (weak\n# regularization), allow the coefficients to closely resemble the true\n# coefficients (`w`) used to generate the data set. This is because no\n# additional noise was added to our artificial data set. As `alpha` increases,\n# the coefficients shrink towards zero, gradually reducing the impact of the\n# features that were formerly more significant.\n#\n# The right-hand side plot shows the mean squared error (MSE) between the\n# coefficients found by the model and the true coefficients (`w`). It provides a\n# measure that relates to how exact our ridge model is in comparison to the true\n# generative model. A low error means that it found coefficients closer to the\n# ones of the true generative model. In this case, since our toy data set was\n# non-noisy, we can see that the least regularized model retrieves coefficients\n# closest to the true coefficients (`w`) (error is close to 0).\n#\n# When `alpha` is small, the model captures the intricate details of the\n# training data, whether those were caused by noise or by actual information. As\n# `alpha` increases, the highest coefficients shrink more rapidly, rendering\n# their corresponding features less influential in the training process. This\n# can enhance a model's ability to generalize to unseen data (if there was a lot\n# of noise to capture), but it also poses the risk of loosing performance if the\n# regularization becomes too strong compared to the amount of noise the data\n# contained (as in this example).\n#\n# In real-world scenarios where data typically includes noise, selecting an\n# appropriate `alpha` value becomes crucial in striking a balance between an\n# overfitting and an underfitting model.\n#\n# Here, we saw that :class:`~sklearn.linear_model.Ridge` adds a penalty to the\n# coefficients to fight overfitting. Another problem that occurs is linked to\n# the presence of outliers in the training dataset. An outlier is a data point\n# that differs significantly from other observations. Concretely, these outliers\n# impact the left-hand side term of the loss function that we showed earlier.\n# Some other linear models are formulated to be robust to outliers such as the\n# :class:`~sklearn.linear_model.HuberRegressor`. You can learn more about it in\n# the :ref:`sphx_glr_auto_examples_linear_model_plot_huber_vs_ridge.py` example.",
                "filename": "examples/linear_model/plot_ridge_coeffs.py",
                "start_index": 5912,
                "end_index": 8758,
                "start_line": 129,
                "end_line": 180,
                "max_line": 180,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            }
        ],
        "classification": "feature request",
        "snippet_processor_task": {
            "examples/linear_model/plot_bayesian_ridge_curvefit.py": [
                {
                    "chunk": {
                        "code": "for i, ax in enumerate(axes):\n    # Bayesian ridge regression with different initial value pairs\n    if i == 0:\n        init = [1 / np.var(y_train), 1.0]  # Default values\n    elif i == 1:\n        init = [1.0, 1e-3]\n        reg.set_params(alpha_init=init[0], lambda_init=init[1])\n    reg.fit(X_train, y_train)\n    ymean, ystd = reg.predict(X_test, return_std=True)\n\n    ax.plot(x_test, func(x_test), color=\"blue\", label=\"sin($2\\\\pi x$)\")\n    ax.scatter(x_train, y_train, s=50, alpha=0.5, label=\"observation\")\n    ax.plot(x_test, ymean, color=\"red\", label=\"predict mean\")\n    ax.fill_between(\n        x_test, ymean - ystd, ymean + ystd, color=\"pink\", alpha=0.5, label=\"predict std\"\n    )\n    ax.set_ylim(-1.3, 1.3)\n    ax.legend()\n    title = \"$\\\\alpha$_init$={:.2f},\\\\ \\\\lambda$_init$={}$\".format(init[0], init[1])\n    if i == 0:\n        title += \" (Default)\"\n    ax.set_title(title, fontsize=12)\n    text = \"$\\\\alpha={:.1f}$\\n$\\\\lambda={:.3f}$\\n$L={:.1f}$\".format(\n        reg.alpha_, reg.lambda_, reg.scores_[-1]\n    )\n    ax.text(0.05, -1.0, text, fontsize=12)\n\nplt.tight_layout()\nplt.show()",
                        "filename": "examples/linear_model/plot_bayesian_ridge_curvefit.py",
                        "start_index": 1996,
                        "end_index": 3090,
                        "start_line": 64,
                        "end_line": 92,
                        "max_line": 92,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows an example of how to set initial values for alpha and lambda in BayesianRidge, which is directly related to the issue of allowing the setting of initial hyperparameters."
                },
                {
                    "chunk": {
                        "code": "\"\"\"\n============================================\nCurve Fitting with Bayesian Ridge Regression\n============================================\n\nComputes a Bayesian Ridge Regression of Sinusoids.\n\nSee :ref:`bayesian_ridge_regression` for more information on the regressor.\n\nIn general, when fitting a curve with a polynomial by Bayesian ridge\nregression, the selection of initial values of\nthe regularization parameters (alpha, lambda) may be important.\nThis is because the regularization parameters are determined by an iterative\nprocedure that depends on initial values.\n\nIn this example, the sinusoid is approximated by a polynomial using different\npairs of initial values.\n\nWhen starting from the default values (alpha_init = 1.90, lambda_init = 1.),\nthe bias of the resulting curve is large, and the variance is small.\nSo, lambda_init should be relatively small (1.e-3) so as to reduce the bias.\n\nAlso, by evaluating log marginal likelihood (L) of\nthese models, we can determine which one is better.\nIt can be concluded that the model with larger L is more likely.\n\n\"\"\"\n\n# Author: Yoshihiro Uchida <nimbus1after2a1sun7shower@gmail.com>\n\n# %%\n# Generate sinusoidal data with noise\n# -----------------------------------\nimport numpy as np\n\n\ndef func(x):\n    return np.sin(2 * np.pi * x)\n\n\nsize = 25\nrng = np.random.RandomState(1234)\nx_train = rng.uniform(0.0, 1.0, size)\ny_train = func(x_train) + rng.normal(scale=0.1, size=size)\nx_test = np.linspace(0.0, 1.0, 100)\n\n\n# %%\n# Fit by cubic polynomial\n# -----------------------\nfrom sklearn.linear_model import BayesianRidge\n\nn_order = 3\nX_train = np.vander(x_train, n_order + 1, increasing=True)\nX_test = np.vander(x_test, n_order + 1, increasing=True)\nreg = BayesianRidge(tol=1e-6, fit_intercept=False, compute_score=True)\n\n# %%\n# Plot the true and predicted curves with log marginal likelihood (L)\n# -------------------------------------------------------------------\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))",
                        "filename": "examples/linear_model/plot_bayesian_ridge_curvefit.py",
                        "start_index": 0,
                        "end_index": 1995,
                        "start_line": 1,
                        "end_line": 63,
                        "max_line": 92,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes the example setup and context for the BayesianRidge regression, which is necessary to understand the issue and the proposed changes."
                }
            ],
            "sklearn/linear_model/_bayes.py": [
                {
                    "chunk": {
                        "code": "\"\"\"\nVarious bayesian regression\n\"\"\"\n\n# Authors: V. Michel, F. Pedregosa, A. Gramfort\n# License: BSD 3 clause\n\nimport warnings\nfrom math import log\nfrom numbers import Integral, Real\n\nimport numpy as np\nfrom scipy import linalg\nfrom scipy.linalg import pinvh\n\nfrom ..base import RegressorMixin, _fit_context\nfrom ..utils._param_validation import Hidden, Interval, StrOptions\nfrom ..utils.extmath import fast_logdet\nfrom ..utils.validation import _check_sample_weight\nfrom ._base import LinearModel, _preprocess_data, _rescale_data\n\n\n# TODO(1.5) Remove\ndef _deprecate_n_iter(n_iter, max_iter):\n    \"\"\"Deprecates n_iter in favour of max_iter. Checks if the n_iter has been\n    used instead of max_iter and generates a deprecation warning if True.\n\n    Parameters\n    ----------\n    n_iter : int,\n        Value of n_iter attribute passed by the estimator.\n\n    max_iter : int, default=None\n        Value of max_iter attribute passed by the estimator.\n        If `None`, it corresponds to `max_iter=300`.\n\n    Returns\n    -------\n    max_iter : int,\n        Value of max_iter which shall further be used by the estimator.\n\n    Notes\n    -----\n    This function should be completely removed in 1.5.\n    \"\"\"\n    if n_iter != \"deprecated\":\n        if max_iter is not None:\n            raise ValueError(\n                \"Both `n_iter` and `max_iter` attributes were set. Attribute\"\n                \" `n_iter` was deprecated in version 1.3 and will be removed in\"\n                \" 1.5. To avoid this error, only set the `max_iter` attribute.\"\n            )\n        warnings.warn(\n            (\n                \"'n_iter' was renamed to 'max_iter' in version 1.3 and \"\n                \"will be removed in 1.5\"\n            ),\n            FutureWarning,\n        )\n        max_iter = n_iter\n    elif max_iter is None:\n        max_iter = 300\n    return max_iter\n\n\n###############################################################################\n# BayesianRidge regression",
                        "filename": "sklearn/linear_model/_bayes.py",
                        "start_index": 0,
                        "end_index": 1955,
                        "start_line": 1,
                        "end_line": 67,
                        "max_line": 848,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the BayesianRidge regression code from the scikit-learn library, which is likely where the changes need to be implemented to allow setting initial hyperparameters."
                }
            ]
        },
        "solution_task": "To implement the feature of setting initial hyperparameters (alpha, lambda) in the `BayesianRidge` model, you can follow these steps:\n\n- Modify the `BayesianRidge` class to accept new parameters `alpha_init` and `lambda_init` in its constructor.\n- Use these initial values in the `fit` method instead of the default values when initializing `alpha_` and `lambda_`.\n\nHere is a high-level overview of the changes needed:\n\n```python\nfrom sklearn.linear_model import BayesianRidge\n\nclass CustomBayesianRidge(BayesianRidge):\n    def __init__(self, alpha_init=None, lambda_init=None, *args, **kwargs):\n        super(CustomBayesianRidge, self).__init__(*args, **kwargs)\n        self.alpha_init = alpha_init\n        self.lambda_init = lambda_init\n\n    def fit(self, X, y, sample_weight=None):\n        # ... existing code ...\n\n        # Use initial values if provided, otherwise use default initialization\n        alpha_ = self.alpha_init if self.alpha_init is not None else 1. / (np.var(y) + eps)\n        lambda_ = self.lambda_init if self.lambda_init is not None else 1.\n\n        # ... rest of the fit method code ...\n\n# Usage\nmodel = CustomBayesianRidge(alpha_init=your_alpha_value, lambda_init=your_lambda_value)\nmodel.fit(X_train, y_train)\n```\n\nMake sure to replace `your_alpha_value` and `your_lambda_value` with the actual initial values you want to use for the hyperparameters.\n\nThis is a simplified version of the changes. You would need to integrate this logic with the existing `BayesianRidge` class in scikit-learn, ensuring that all other functionalities of the class remain intact.",
        "bouncer_response": {
            "score": 5,
            "criteria": "The RESPONSE provides a partial solution to the feature request by suggesting modifications to the BayesianRidge class to accept new parameters for initial hyperparameters and using these values in the fit method. The RESPONSE includes a code snippet that outlines the necessary changes and provides a usage example. However, the RESPONSE does not address the full context of the PROMPT, such as integrating the changes with the existing scikit-learn codebase and ensuring all functionalities remain intact. Additionally, the RESPONSE includes links to code snippets that are not directly relevant to the implementation of the feature request.",
            "supporting_evidence": "The RESPONSE offers a partial solution by suggesting code modifications to the BayesianRidge class, which is relevant to the feature request. However, it does not provide a comprehensive guide on integrating these changes with the existing scikit-learn codebase. The RESPONSE also includes links to code snippets that are not directly relevant to the implementation of the feature request, which does not add value to the solution.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711739818.271071,
        "result": "Doesn't seem like we have fully grasped all the specifics of this ticket \ud83e\udd14.                   But we've outlined some ideas that might help you get started:\n\nTo implement the feature of setting initial hyperparameters (alpha, lambda) in the `BayesianRidge` model, you can follow these steps:\n\n- Modify the `BayesianRidge` class to accept new parameters `alpha_init` and `lambda_init` in its constructor.\n- Use these initial values in the `fit` method instead of the default values when initializing `alpha_` and `lambda_`.\n\nHere is a high-level overview of the changes needed:\n\n```python\nfrom sklearn.linear_model import BayesianRidge\n\nclass CustomBayesianRidge(BayesianRidge):\n    def __init__(self, alpha_init=None, lambda_init=None, *args, **kwargs):\n        super(CustomBayesianRidge, self).__init__(*args, **kwargs)\n        self.alpha_init = alpha_init\n        self.lambda_init = lambda_init\n\n    def fit(self, X, y, sample_weight=None):\n        # ... existing code ...\n\n        # Use initial values if provided, otherwise use default initialization\n        alpha_ = self.alpha_init if self.alpha_init is not None else 1. / (np.var(y) + eps)\n        lambda_ = self.lambda_init if self.lambda_init is not None else 1.\n\n        # ... rest of the fit method code ...\n\n# Usage\nmodel = CustomBayesianRidge(alpha_init=your_alpha_value, lambda_init=your_lambda_value)\nmodel.fit(X_train, y_train)\n```\n\nMake sure to replace `your_alpha_value` and `your_lambda_value` with the actual initial values you want to use for the hyperparameters.\n\nThis is a simplified version of the changes. You would need to integrate this logic with the existing `BayesianRidge` class in scikit-learn, ensuring that all other functionalities of the class remain intact.\n\n### Code snippets to check\n\n-  \u2192 [examples](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//examples) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//examples/linear_model)- [Lines 64 - 92](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//examples/linear_model/plot_bayesian_ridge_curvefit.py#L64-L92)\n\n This snippet shows an example of how to set initial values for alpha and lambda in BayesianRidge, which is directly related to the issue of allowing the setting of initial hyperparameters. \n\n\n   - [Lines 1 - 63](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//examples/linear_model/plot_bayesian_ridge_curvefit.py#L1-L63)\n\n This snippet includes the example setup and context for the BayesianRidge regression, which is necessary to understand the issue and the proposed changes.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/linear_model)- [Lines 1 - 67](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/linear_model/_bayes.py#L1-L67)\n\n This snippet contains the BayesianRidge regression code from the scikit-learn library, which is likely where the changes need to be implemented to allow setting initial hyperparameters.",
        "context_relevance": 0.42000000000000004,
        "hypotheses": [],
        "groundedness_score": 1.0,
        "qa_relevance_reasons": {
            "reason": "Criteria: The RESPONSE must be relevant to the entire PROMPT to get a score of 10. The RESPONSE that is RELEVANT to some of the PROMPT should get a score of 2, 3, or 4.\nSupporting Evidence: The RESPONSE provides a solution to the issue of setting initial hyperparameters in the BayesianRidge model, which is relevant to the PROMPT's description section. However, it does not address the other sections of the PROMPT, such as the steps/code to reproduce, expected results, actual results, and versions. Therefore, it is only partially relevant to the entire PROMPT."
        },
        "qa_relevance": 0.2
    }
}