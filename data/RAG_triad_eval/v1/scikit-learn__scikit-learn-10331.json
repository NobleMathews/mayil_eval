{
    "id": "scikit-learn__scikit-learn-10331",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
    "task_id": 4823235372057009096,
    "title": "[WIP] gamma=auto in SVC #8361",
    "body": "<!--\r\nThanks for contributing a pull request! Please ensure you have taken a look at\r\nthe contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#Contributing-Pull-Requests\r\n-->\r\n#### Reference Issue\r\nAddresses #8361 \r\n\r\n#### What does this implement/fix? Explain your changes.\r\nDeprecates the default SVC gamma parameter value of \"auto\", which is calculated as 1 / n_features, and introduces \"scale\", which is calculated as 1 / (n_features * X.std()).\r\n\r\n#### Any other comments?\r\nCould not run nosetests due to problems with Conda environent. There are potentially other occurrences of SVC() that need to be updated to SVC(gamma=\"scale\") to avoid Deprecation Warnings associated with SVC(gamma = \"auto\"). Submitting pull request to locate errors.\r\n\r\n<!--\r\nPlease be aware that we are a loose team of volunteers so patience is\r\nnecessary; assistance handling other issues is very welcome. We value\r\nall user contributions, no matter how minor they are. If we are slow to\r\nreview, either the pull request needs some benchmarking, tinkering,\r\nconvincing, etc. or more likely the reviewers are simply busy. In either\r\ncase, we ask for your understanding during the review process.\r\nFor more information, see our FAQ on this topic:\r\nhttp://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.\r\n\r\nThanks for contributing!\r\n-->\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.20.git",
    "requested_commit": "",
    "mayil_response": "- Update the default value of the `gamma` parameter in the `SVC` class from `'auto'` to `'scale'`.\n- Add a deprecation warning when `gamma='auto'` is used, informing users that this option is deprecated and `'scale'` should be used instead.\n- Ensure that the calculation for `gamma='scale'` uses `1 / (n_features * X.var())` as specified in the documentation.\n\n```python\nclass SVC:\n    def __init__(self, ..., gamma='scale', ...):\n        ...\n        if gamma == 'auto':\n            warnings.warn(\n                \"The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\",\n                DeprecationWarning\n            )\n            gamma_value = 1 / n_features\n        elif gamma == 'scale':\n            gamma_value = 1 / (n_features * X.var())\n        else:\n            gamma_value = gamma\n        ...\n```\n\n- Search for occurrences of `SVC()` in the codebase and update the `gamma` parameter to `'scale'` if it is set to `'auto'`.\n- Update the documentation and examples to reflect the change in the default value of the `gamma` parameter.\n- Ensure that the deprecation warning is tested and that the tests pass with the new default value `'scale'`.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [svm](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/svm)- [Lines 607 - 1842](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/svm/_classes.py#L607-L1842)\n\n This snippet contains the definition of the SVC class in scikit-learn, including the 'gamma' parameter which is the focus of the issue. Changes to the default value of 'gamma' and its documentation will likely be made here.\n-  \u2192 [examples](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//examples) \u2192 [svm](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//examples/svm)- [Lines 1 - 76](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//examples/svm/plot_rbf_parameters.py#L1-L76)\n\n This snippet is from an example that demonstrates the effect of the 'gamma' parameter in SVMs. Since the default value of 'gamma' is being changed, this example may need to be updated to reflect the new default or to explain the change.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.000208,
        "snippet_processor": 0.03336,
        "issue_star_creation": 0.03149,
        "issue_star_solver": 0.07856,
        "bouncer": 0.03409
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711742632.4076211,
        "relevant_snippets": [
            {
                "code": "\"\"\"\n==================\nRBF SVM parameters\n==================\n\nThis example illustrates the effect of the parameters ``gamma`` and ``C`` of\nthe Radial Basis Function (RBF) kernel SVM.\n\nIntuitively, the ``gamma`` parameter defines how far the influence of a single\ntraining example reaches, with low values meaning 'far' and high values meaning\n'close'. The ``gamma`` parameters can be seen as the inverse of the radius of\ninfluence of samples selected by the model as support vectors.\n\nThe ``C`` parameter trades off correct classification of training examples\nagainst maximization of the decision function's margin. For larger values of\n``C``, a smaller margin will be accepted if the decision function is better at\nclassifying all training points correctly. A lower ``C`` will encourage a\nlarger margin, therefore a simpler decision function, at the cost of training\naccuracy. In other words ``C`` behaves as a regularization parameter in the\nSVM.\n\nThe first plot is a visualization of the decision function for a variety of\nparameter values on a simplified classification problem involving only 2 input\nfeatures and 2 possible target classes (binary classification). Note that this\nkind of plot is not possible to do for problems with more features or target\nclasses.\n\nThe second plot is a heatmap of the classifier's cross-validation accuracy as a\nfunction of ``C`` and ``gamma``. For this example we explore a relatively large\ngrid for illustration purposes. In practice, a logarithmic grid from\n:math:`10^{-3}` to :math:`10^3` is usually sufficient. If the best parameters\nlie on the boundaries of the grid, it can be extended in that direction in a\nsubsequent search.\n\nNote that the heat map plot has a special colorbar with a midpoint value close\nto the score values of the best performing models so as to make it easy to tell\nthem apart in the blink of an eye.\n\nThe behavior of the model is very sensitive to the ``gamma`` parameter. If\n``gamma`` is too large, the radius of the area of influence of the support\nvectors only includes the support vector itself and no amount of\nregularization with ``C`` will be able to prevent overfitting.\n\nWhen ``gamma`` is very small, the model is too constrained and cannot capture\nthe complexity or \"shape\" of the data. The region of influence of any selected\nsupport vector would include the whole training set. The resulting model will\nbehave similarly to a linear model with a set of hyperplanes that separate the\ncenters of high density of any pair of two classes.\n\nFor intermediate values, we can see on the second plot that good models can\nbe found on a diagonal of ``C`` and ``gamma``. Smooth models (lower ``gamma``\nvalues) can be made more complex by increasing the importance of classifying\neach point correctly (larger ``C`` values) hence the diagonal of good\nperforming models.\n\nFinally, one can also observe that for some intermediate values of ``gamma`` we\nget equally performing models when ``C`` becomes very large. This suggests that\nthe set of support vectors does not change anymore. The radius of the RBF\nkernel alone acts as a good structural regularizer. Increasing ``C`` further\ndoesn't help, likely because there are no more training points in violation\n(inside the margin or wrongly classified), or at least no better solution can\nbe found. Scores being equal, it may make sense to use the smaller ``C``\nvalues, since very high ``C`` values typically increase fitting time.\n\nOn the other hand, lower ``C`` values generally lead to more support vectors,\nwhich may increase prediction time. Therefore, lowering the value of ``C``\ninvolves a trade-off between fitting time and prediction time.\n\nWe should also note that small differences in scores results from the random\nsplits of the cross-validation procedure. Those spurious variations can be\nsmoothed out by increasing the number of CV iterations ``n_splits`` at the\nexpense of compute time. Increasing the value number of ``C_range`` and\n``gamma_range`` steps will increase the resolution of the hyper-parameter heat\nmap.\n\n\"\"\"",
                "filename": "examples/svm/plot_rbf_parameters.py",
                "start_index": 0,
                "end_index": 4050,
                "start_line": 1,
                "end_line": 76,
                "max_line": 218,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "r\"\"\"\n==============================================\nScaling the regularization parameter for SVCs\n==============================================\n\nThe following example illustrates the effect of scaling the\nregularization parameter when using :ref:`svm` for\n:ref:`classification <svm_classification>`.\nFor SVC classification, we are interested in a risk minimization for the\nequation:\n\n\n.. math::\n\n    C \\sum_{i=1, n} \\mathcal{L} (f(x_i), y_i) + \\Omega (w)\n\nwhere\n\n    - :math:`C` is used to set the amount of regularization\n    - :math:`\\mathcal{L}` is a `loss` function of our samples\n      and our model parameters.\n    - :math:`\\Omega` is a `penalty` function of our model parameters\n\nIf we consider the loss function to be the individual error per\nsample, then the data-fit term, or the sum of the error for each sample, will\nincrease as we add more samples. The penalization term, however, will not\nincrease.\n\nWhen using, for example, :ref:`cross validation <cross_validation>`, to\nset the amount of regularization with `C`, there will be a\ndifferent amount of samples between the main problem and the smaller problems\nwithin the folds of the cross validation.\n\nSince our loss function is dependent on the amount of samples, the latter\nwill influence the selected value of `C`.\nThe question that arises is \"How do we optimally adjust C to\naccount for the different amount of training samples?\"\n\nIn the remainder of this example, we will investigate the effect of scaling\nthe value of the regularization parameter `C` in regards to the number of\nsamples for both L1 and L2 penalty. We will generate some synthetic datasets\nthat are appropriate for each type of regularization.\n\"\"\"\n\n# Author: Andreas Mueller <amueller@ais.uni-bonn.de>\n#         Jaques Grobler <jaques.grobler@inria.fr>\n# License: BSD 3 clause\n\n# %%\n# L1-penalty case\n# ---------------\n# In the L1 case, theory says that prediction consistency (i.e. that under\n# given hypothesis, the estimator learned predicts as well as a model knowing\n# the true distribution) is not possible because of the bias of the L1. It\n# does say, however, that model consistency, in terms of finding the right set\n# of non-zero parameters as well as their signs, can be achieved by scaling\n# `C`.\n#\n# We will demonstrate this effect by using a synthetic dataset. This\n# dataset will be sparse, meaning that only a few features will be informative\n# and useful for the model.\nfrom sklearn.datasets import make_classification\n\nn_samples, n_features = 100, 300\nX, y = make_classification(\n    n_samples=n_samples, n_features=n_features, n_informative=5, random_state=1\n)\n\n# %%\n# Now, we can define a linear SVC with the `l1` penalty.\nfrom sklearn.svm import LinearSVC\n\nmodel_l1 = LinearSVC(penalty=\"l1\", loss=\"squared_hinge\", dual=False, tol=1e-3)\n\n# %%\n# We will compute the mean test score for different values of `C`.\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import ShuffleSplit, validation_curve",
                "filename": "examples/svm/plot_svm_scale_c.py",
                "start_index": 0,
                "end_index": 2973,
                "start_line": 1,
                "end_line": 80,
                "max_line": 173,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"\"\"\n==========================\nPlotting Validation Curves\n==========================\n\nIn this plot you can see the training scores and validation scores of an SVM\nfor different values of the kernel parameter gamma. For very low values of\ngamma, you can see that both the training score and the validation score are\nlow. This is called underfitting. Medium values of gamma will result in high\nvalues for both scores, i.e. the classifier is performing fairly well. If gamma\nis too high, the classifier will overfit, which means that the training score\nis good but the validation score is poor.\n\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import ValidationCurveDisplay\nfrom sklearn.svm import SVC\n\nX, y = load_digits(return_X_y=True)\nsubset_mask = np.isin(y, [1, 2])  # binary classification: 1 vs 2\nX, y = X[subset_mask], y[subset_mask]\n\ndisp = ValidationCurveDisplay.from_estimator(\n    SVC(),\n    X,\n    y,\n    param_name=\"gamma\",\n    param_range=np.logspace(-6, -1, 5),\n    score_type=\"both\",\n    n_jobs=2,\n    score_name=\"Accuracy\",\n)\ndisp.ax_.set_title(\"Validation Curve for SVM with an RBF kernel\")\ndisp.ax_.set_xlabel(r\"gamma (inverse radius of the RBF kernel)\")\ndisp.ax_.set_ylim(0.0, 1.1)\nplt.show()",
                "filename": "examples/model_selection/plot_validation_curve.py",
                "start_index": 0,
                "end_index": 1290,
                "start_line": 1,
                "end_line": 40,
                "max_line": 40,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.7
            },
            {
                "code": "\"\"\"C-Support Vector Classification.\n\n    The implementation is based on libsvm. The fit time scales at least\n    quadratically with the number of samples and may be impractical\n    beyond tens of thousands of samples. For large datasets\n    consider using :class:`~sklearn.svm.LinearSVC` or\n    :class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a\n    :class:`~sklearn.kernel_approximation.Nystroem` transformer or\n    other :ref:`kernel_approximation`.\n\n    The multiclass support is handled according to a one-vs-one scheme.\n\n    For details on the precise mathematical formulation of the provided\n    kernel functions and how `gamma`, `coef0` and `degree` affect each\n    other, see the corresponding section in the narrative documentation:\n    :ref:`svm_kernels`.\n\n    Read more in the :ref:`User Guide <svm_classification>`.\n\n    Parameters\n    ----------\n    C : float, default=1.0\n        Regularization parameter. The strength of the regularization is\n        inversely proportional to C. Must be strictly positive. The penalty\n        is a squared l2 penalty.\n\n    kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,  \\\n        default='rbf'\n        Specifies the kernel type to be used in the algorithm.\n        If none is given, 'rbf' will be used. If a callable is given it is\n        used to pre-compute the kernel matrix from data matrices; that matrix\n        should be an array of shape ``(n_samples, n_samples)``.\n\n    degree : int, default=3\n        Degree of the polynomial kernel function ('poly').\n        Must be non-negative. Ignored by all other kernels.\n\n    gamma : {'scale', 'auto'} or float, default='scale'\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n        - if ``gamma='scale'`` (default) is passed then it uses\n          1 / (n_features * X.var()) as value of gamma,\n        - if 'auto', uses 1 / n_features\n        - if float, must be non-negative.\n\n        .. versionchanged:: 0.22\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\n\n    coef0 : float, default=0.0\n        Independent term in kernel function.\n        It is only significant in 'poly' and 'sigmoid'.\n\n    shrinking : bool, default=True\n        Whether to use the shrinking heuristic.\n        See the :ref:`User Guide <shrinking_svm>`.\n\n    probability : bool, default=False\n        Whether to enable probability estimates. This must be enabled prior\n        to calling `fit`, will slow down that method as it internally uses\n        5-fold cross-validation, and `predict_proba` may be inconsistent with\n        `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n\n    tol : float, default=1e-3\n        Tolerance for stopping criterion.\n\n    cache_size : float, default=200\n        Specify the size of the kernel cache (in MB).\n\n    class_weight : dict or 'balanced', default=None\n        Set the parameter C of class i to class_weight[i]*C for\n        SVC. If not given, all classes are supposed to have\n        weight one.\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n    verbose : bool, default=False\n        Enable verbose output. Note that this setting takes advantage of a\n        per-process runtime setting in libsvm that, if enabled, may not work\n        properly in a multithreaded context.\n\n    max_iter : int, default=-1\n        Hard limit on iterations within solver, or -1 for no limit.\n\n    decision_function_shape : {'ovo', 'ovr'}, default='ovr'\n        Whether to return a one-vs-rest ('ovr') decision function of shape\n        (n_samples, n_classes) as all other classifiers, or the original\n        one-vs-one ('ovo') decision function of libsvm which has shape\n        (n_samples, n_classes * (n_classes - 1) / 2). However, note that\n        internally, one-vs-one ('ovo') is always used as a multi-class strategy\n        to train models; an ovr matrix is only constructed from the ovo matrix.\n        The parameter is ignored for binary classification.\n\n        .. versionchanged:: 0.19\n            decision_function_shape is 'ovr' by default.\n\n        .. versionadded:: 0.17\n           *decision_function_shape='ovr'* is recommended.\n\n        .. versionchanged:: 0.17\n           Deprecated *decision_function_shape='ovo' and None*.\n\n    break_ties : bool, default=False\n        If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n        :term:`predict` will break ties according to the confidence values of\n        :term:`decision_function`; otherwise the first class among the tied\n        classes is returned. Please note that breaking ties comes at a\n        relatively high computational cost compared to a simple predict.\n\n        .. versionadded:: 0.22\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the pseudo random number generation for shuffling the data for\n        probability estimates. Ignored when `probability` is False.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    class_weight_ : ndarray of shape (n_classes,)\n        Multipliers of parameter C for each class.\n        Computed based on the ``class_weight`` parameter.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)\n        Weights assigned to the features (coefficients in the primal\n        problem). This is only available in the case of a linear kernel.\n\n        `coef_` is a readonly property derived from `dual_coef_` and\n        `support_vectors_`.\n\n    dual_coef_ : ndarray of shape (n_classes -1, n_SV)\n        Dual coefficients of the support vector in the decision\n        function (see :ref:`sgd_mathematical_formulation`), multiplied by\n        their targets.\n        For multiclass, coefficient for all 1-vs-1 classifiers.\n        The layout of the coefficients in the multiclass case is somewhat\n        non-trivial. See the :ref:`multi-class section of the User Guide\n        <svm_multi_class>` for details.\n\n    fit_status_ : int\n        0 if correctly fitted, 1 otherwise (will raise warning)\n\n    intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n        Constants in decision function.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : ndarray of shape (n_classes * (n_classes - 1) // 2,)\n        Number of iterations run by the optimization routine to fit the model.\n        The shape of this attribute depends on the number of models optimized\n        which in turn depends on the number of classes.\n\n        .. versionadded:: 1.1\n\n    support_ : ndarray of shape (n_SV)\n        Indices of support vectors.\n\n    support_vectors_ : ndarray of shape (n_SV, n_features)\n        Support vectors.\n\n    n_support_ : ndarray of shape (n_classes,), dtype=int32\n        Number of support vectors for each class.\n\n    probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n    probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n        If `probability=True`, it corresponds to the parameters learned in\n        Platt scaling to produce probability estimates from decision values.\n        If `probability=False`, it's an empty array. Platt scaling uses the\n        logistic function\n        ``1 / (1 + exp(decision_value * probA_ + probB_))``\n        where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n        more information on the multiclass case and training procedure see\n        section 8 of [1]_.\n\n    shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n        Array dimensions of training vector ``X``.\n\n    See Also\n    --------\n    SVR : Support Vector Machine for Regression implemented using libsvm.\n\n    LinearSVC : Scalable Linear Support Vector Machine for classification\n        implemented using liblinear. Check the See Also section of\n        LinearSVC for more comparison element.\n\n    References\n    ----------\n    .. [1] `LIBSVM: A Library for Support Vector Machines\n        <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n\n    .. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector\n        Machines and Comparisons to Regularized Likelihood Methods\"\n        <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.pipeline import make_pipeline\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n    >>> y = np.array([1, 1, 2, 2])\n    >>> from sklearn.svm import SVC\n    >>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n    >>> clf.fit(X, y)\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('svc', SVC(gamma='auto'))])\n\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]\n    \"\"\"",
                "filename": "sklearn/svm/_classes.py",
                "start_index": 23041,
                "end_index": 32355,
                "start_line": 607,
                "end_line": 1842,
                "max_line": 1853,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.9
            },
            {
                "code": "Cs = np.logspace(-2.3, -1.3, 10)\ntrain_sizes = np.linspace(0.3, 0.7, 3)\nlabels = [f\"fraction: {train_size}\" for train_size in train_sizes]\n\nresults = {\"C\": Cs}\nfor label, train_size in zip(labels, train_sizes):\n    cv = ShuffleSplit(train_size=train_size, test_size=0.3, n_splits=50, random_state=1)\n    train_scores, test_scores = validation_curve(\n        model_l1, X, y, param_name=\"C\", param_range=Cs, cv=cv\n    )\n    results[label] = test_scores.mean(axis=1)\nresults = pd.DataFrame(results)\n\n# %%\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(12, 6))\n\n# plot results without scaling C\nresults.plot(x=\"C\", ax=axes[0], logx=True)\naxes[0].set_ylabel(\"CV score\")\naxes[0].set_title(\"No scaling\")\n\n# plot results by scaling C\nfor train_size_idx, label in enumerate(labels):\n    results_scaled = results[[label]].assign(\n        C_scaled=Cs * float(n_samples * train_sizes[train_size_idx])\n    )\n    results_scaled.plot(x=\"C_scaled\", ax=axes[1], logx=True, label=label)\naxes[1].set_title(\"Scaling C by 1 / n_samples\")\n\n_ = fig.suptitle(\"Effect of scaling C with L1 penalty\")\n\n# %%\n# Here, we observe that the cross-validation-error correlates best with the\n# test-error, when scaling our `C` with the number of samples, `n`.\n#\n# L2-penalty case\n# ---------------\n# We can repeat a similar experiment with the `l2` penalty. In this case, we\n# don't need to use a sparse dataset.\n#\n# In this case, the theory says that in order to achieve prediction\n# consistency, the penalty parameter should be kept constant as the number of\n# samples grow.\n#\n# So we will repeat the same experiment by creating a linear SVC classifier\n# with the `l2` penalty and check the test score via cross-validation and\n# plot the results with and without scaling the parameter `C`.\nrng = np.random.RandomState(1)\ny = np.sign(0.5 - rng.rand(n_samples))\nX = rng.randn(n_samples, n_features // 5) + y[:, np.newaxis]\nX += 5 * rng.randn(n_samples, n_features // 5)\n\n# %%\nmodel_l2 = LinearSVC(penalty=\"l2\", loss=\"squared_hinge\", dual=True)\nCs = np.logspace(-4.5, -2, 10)\n\nlabels = [f\"fraction: {train_size}\" for train_size in train_sizes]\nresults = {\"C\": Cs}\nfor label, train_size in zip(labels, train_sizes):\n    cv = ShuffleSplit(train_size=train_size, test_size=0.3, n_splits=50, random_state=1)\n    train_scores, test_scores = validation_curve(\n        model_l2, X, y, param_name=\"C\", param_range=Cs, cv=cv\n    )\n    results[label] = test_scores.mean(axis=1)\nresults = pd.DataFrame(results)\n\n# %%\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(12, 6))\n\n# plot results without scaling C\nresults.plot(x=\"C\", ax=axes[0], logx=True)\naxes[0].set_ylabel(\"CV score\")\naxes[0].set_title(\"No scaling\")\n\n# plot results by scaling C",
                "filename": "examples/svm/plot_svm_scale_c.py",
                "start_index": 2975,
                "end_index": 5769,
                "start_line": 82,
                "end_line": 160,
                "max_line": 173,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "_impl = \"epsilon_svr\"\n\n    _parameter_constraints: dict = {**BaseLibSVM._parameter_constraints}\n    for unused_param in [\"class_weight\", \"nu\", \"probability\", \"random_state\"]:\n        _parameter_constraints.pop(unused_param)\n\n    def __init__(\n        self,\n        *,\n        kernel=\"rbf\",\n        degree=3,\n        gamma=\"scale\",\n        coef0=0.0,\n        tol=1e-3,\n        C=1.0,\n        epsilon=0.1,\n        shrinking=True,\n        cache_size=200,\n        verbose=False,\n        max_iter=-1,\n    ):\n        super().__init__(\n            kernel=kernel,\n            degree=degree,\n            gamma=gamma,\n            coef0=coef0,\n            tol=tol,\n            C=C,\n            nu=0.0,\n            epsilon=epsilon,\n            verbose=verbose,\n            shrinking=shrinking,\n            probability=False,\n            cache_size=cache_size,\n            class_weight=None,\n            max_iter=max_iter,\n            random_state=None,\n        )\n\n    # TODO(1.4): Remove\n    @deprecated(  # type: ignore\n        \"Attribute `class_weight_` was deprecated in version 1.2 and will be removed in\"\n        \" 1.4.\"\n    )\n    @property\n    def class_weight_(self):\n        return np.empty(0)\n\n    def _more_tags(self):\n        return {\n            \"_xfail_checks\": {\n                \"check_sample_weights_invariance\": (\n                    \"zero sample_weight is not equivalent to removing samples\"\n                ),\n            }\n        }",
                "filename": "sklearn/svm/_classes.py",
                "start_index": 49551,
                "end_index": 50990,
                "start_line": 1311,
                "end_line": 1853,
                "max_line": 1853,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"\"\"Nu-Support Vector Classification.\n\n    Similar to SVC but uses a parameter to control the number of support\n    vectors.\n\n    The implementation is based on libsvm.\n\n    Read more in the :ref:`User Guide <svm_classification>`.\n\n    Parameters\n    ----------\n    nu : float, default=0.5\n        An upper bound on the fraction of margin errors (see :ref:`User Guide\n        <nu_svc>`) and a lower bound of the fraction of support vectors.\n        Should be in the interval (0, 1].\n\n    kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,  \\\n        default='rbf'\n         Specifies the kernel type to be used in the algorithm.\n         If none is given, 'rbf' will be used. If a callable is given it is\n         used to precompute the kernel matrix.\n\n    degree : int, default=3\n        Degree of the polynomial kernel function ('poly').\n        Must be non-negative. Ignored by all other kernels.\n\n    gamma : {'scale', 'auto'} or float, default='scale'\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n        - if ``gamma='scale'`` (default) is passed then it uses\n          1 / (n_features * X.var()) as value of gamma,\n        - if 'auto', uses 1 / n_features\n        - if float, must be non-negative.\n\n        .. versionchanged:: 0.22\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\n\n    coef0 : float, default=0.0\n        Independent term in kernel function.\n        It is only significant in 'poly' and 'sigmoid'.\n\n    shrinking : bool, default=True\n        Whether to use the shrinking heuristic.\n        See the :ref:`User Guide <shrinking_svm>`.\n\n    probability : bool, default=False\n        Whether to enable probability estimates. This must be enabled prior\n        to calling `fit`, will slow down that method as it internally uses\n        5-fold cross-validation, and `predict_proba` may be inconsistent with\n        `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n\n    tol : float, default=1e-3\n        Tolerance for stopping criterion.\n\n    cache_size : float, default=200\n        Specify the size of the kernel cache (in MB).\n\n    class_weight : {dict, 'balanced'}, default=None\n        Set the parameter C of class i to class_weight[i]*C for\n        SVC. If not given, all classes are supposed to have\n        weight one. The \"balanced\" mode uses the values of y to automatically\n        adjust weights inversely proportional to class frequencies as\n        ``n_samples / (n_classes * np.bincount(y))``.\n\n    verbose : bool, default=False\n        Enable verbose output. Note that this setting takes advantage of a\n        per-process runtime setting in libsvm that, if enabled, may not work\n        properly in a multithreaded context.\n\n    max_iter : int, default=-1\n        Hard limit on iterations within solver, or -1 for no limit.\n\n    decision_function_shape : {'ovo', 'ovr'}, default='ovr'\n        Whether to return a one-vs-rest ('ovr') decision function of shape\n        (n_samples, n_classes) as all other classifiers, or the original\n        one-vs-one ('ovo') decision function of libsvm which has shape\n        (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n        ('ovo') is always used as multi-class strategy. The parameter is\n        ignored for binary classification.\n\n        .. versionchanged:: 0.19\n            decision_function_shape is 'ovr' by default.\n\n        .. versionadded:: 0.17\n           *decision_function_shape='ovr'* is recommended.\n\n        .. versionchanged:: 0.17\n           Deprecated *decision_function_shape='ovo' and None*.\n\n    break_ties : bool, default=False\n        If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n        :term:`predict` will break ties according to the confidence values of\n        :term:`decision_function`; otherwise the first class among the tied\n        classes is returned. Please note that breaking ties comes at a\n        relatively high computational cost compared to a simple predict.\n\n        .. versionadded:: 0.22\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the pseudo random number generation for shuffling the data for\n        probability estimates. Ignored when `probability` is False.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    class_weight_ : ndarray of shape (n_classes,)\n        Multipliers of parameter C of each class.\n        Computed based on the ``class_weight`` parameter.\n\n    classes_ : ndarray of shape (n_classes,)\n        The unique classes labels.\n\n    coef_ : ndarray of shape (n_classes * (n_classes -1) / 2, n_features)\n        Weights assigned to the features (coefficients in the primal\n        problem). This is only available in the case of a linear kernel.\n\n        `coef_` is readonly property derived from `dual_coef_` and\n        `support_vectors_`.\n\n    dual_coef_ : ndarray of shape (n_classes - 1, n_SV)\n        Dual coefficients of the support vector in the decision\n        function (see :ref:`sgd_mathematical_formulation`), multiplied by\n        their targets.\n        For multiclass, coefficient for all 1-vs-1 classifiers.\n        The layout of the coefficients in the multiclass case is somewhat\n        non-trivial. See the :ref:`multi-class section of the User Guide\n        <svm_multi_class>` for details.\n\n    fit_status_ : int\n        0 if correctly fitted, 1 if the algorithm did not converge.\n\n    intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n        Constants in decision function.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : ndarray of shape (n_classes * (n_classes - 1) // 2,)\n        Number of iterations run by the optimization routine to fit the model.\n        The shape of this attribute depends on the number of models optimized\n        which in turn depends on the number of classes.\n\n        .. versionadded:: 1.1\n\n    support_ : ndarray of shape (n_SV,)\n        Indices of support vectors.\n\n    support_vectors_ : ndarray of shape (n_SV, n_features)\n        Support vectors.\n\n    n_support_ : ndarray of shape (n_classes,), dtype=int32\n        Number of support vectors for each class.\n\n    fit_status_ : int\n        0 if correctly fitted, 1 if the algorithm did not converge.\n\n    probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n    probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n        If `probability=True`, it corresponds to the parameters learned in\n        Platt scaling to produce probability estimates from decision values.\n        If `probability=False`, it's an empty array. Platt scaling uses the\n        logistic function\n        ``1 / (1 + exp(decision_value * probA_ + probB_))``\n        where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n        more information on the multiclass case and training procedure see\n        section 8 of [1]_.\n\n    shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n        Array dimensions of training vector ``X``.\n\n    See Also\n    --------\n    SVC : Support Vector Machine for classification using libsvm.\n\n    LinearSVC : Scalable linear Support Vector Machine for classification using\n        liblinear.\n\n    References\n    ----------\n    .. [1] `LIBSVM: A Library for Support Vector Machines\n        <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n\n    .. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector\n        Machines and Comparisons to Regularized Likelihood Methods\"\n        <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n    >>> y = np.array([1, 1, 2, 2])\n    >>> from sklearn.pipeline import make_pipeline\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> from sklearn.svm import NuSVC\n    >>> clf = make_pipeline(StandardScaler(), NuSVC())\n    >>> clf.fit(X, y)\n    Pipeline(steps=[('standardscaler', StandardScaler()), ('nusvc', NuSVC())])\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]\n    \"\"\"",
                "filename": "sklearn/svm/_classes.py",
                "start_index": 33596,
                "end_index": 42051,
                "start_line": 881,
                "end_line": 1842,
                "max_line": 1853,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"\"\"Approximate a RBF kernel feature map using random Fourier features.\n\n    It implements a variant of Random Kitchen Sinks.[1]\n\n    Read more in the :ref:`User Guide <rbf_kernel_approx>`.\n\n    Parameters\n    ----------\n    gamma : 'scale' or float, default=1.0\n        Parameter of RBF kernel: exp(-gamma * x^2).\n        If ``gamma='scale'`` is passed then it uses\n        1 / (n_features * X.var()) as value of gamma.\n\n        .. versionadded:: 1.2\n           The option `\"scale\"` was added in 1.2.\n\n    n_components : int, default=100\n        Number of Monte Carlo samples per original feature.\n        Equals the dimensionality of the computed feature space.\n\n    random_state : int, RandomState instance or None, default=None\n        Pseudo-random number generator to control the generation of the random\n        weights and random offset when fitting the training data.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    random_offset_ : ndarray of shape (n_components,), dtype={np.float64, np.float32}\n        Random offset used to compute the projection in the `n_components`\n        dimensions of the feature space.\n\n    random_weights_ : ndarray of shape (n_features, n_components),\\\n        dtype={np.float64, np.float32}\n        Random projection directions drawn from the Fourier transform\n        of the RBF kernel.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\n    Nystroem : Approximate a kernel map using a subset of the training data.\n    PolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch.\n    SkewedChi2Sampler : Approximate feature map for\n        \"skewed chi-squared\" kernel.\n    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n\n    Notes\n    -----\n    See \"Random Features for Large-Scale Kernel Machines\" by A. Rahimi and\n    Benjamin Recht.\n\n    [1] \"Weighted Sums of Random Kitchen Sinks: Replacing\n    minimization with randomization in learning\" by A. Rahimi and\n    Benjamin Recht.\n    (https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)\n\n    Examples\n    --------\n    >>> from sklearn.kernel_approximation import RBFSampler\n    >>> from sklearn.linear_model import SGDClassifier\n    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n    >>> y = [0, 0, 1, 1]\n    >>> rbf_feature = RBFSampler(gamma=1, random_state=1)\n    >>> X_features = rbf_feature.fit_transform(X)\n    >>> clf = SGDClassifier(max_iter=5, tol=1e-3)\n    >>> clf.fit(X_features, y)\n    SGDClassifier(max_iter=5)\n    >>> clf.score(X_features, y)\n    1.0\n    \"\"\"",
                "filename": "sklearn/kernel_approximation.py",
                "start_index": 8643,
                "end_index": 11636,
                "start_line": 246,
                "end_line": 1089,
                "max_line": 1134,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "_impl = \"nu_svc\"\n\n    _parameter_constraints: dict = {\n        **BaseSVC._parameter_constraints,\n        \"nu\": [Interval(Real, 0.0, 1.0, closed=\"right\")],\n    }\n    _parameter_constraints.pop(\"C\")\n\n    def __init__(\n        self,\n        *,\n        nu=0.5,\n        kernel=\"rbf\",\n        degree=3,\n        gamma=\"scale\",\n        coef0=0.0,\n        shrinking=True,\n        probability=False,\n        tol=1e-3,\n        cache_size=200,\n        class_weight=None,\n        verbose=False,\n        max_iter=-1,\n        decision_function_shape=\"ovr\",\n        break_ties=False,\n        random_state=None,\n    ):\n        super().__init__(\n            kernel=kernel,\n            degree=degree,\n            gamma=gamma,\n            coef0=coef0,\n            tol=tol,\n            C=0.0,\n            nu=nu,\n            shrinking=shrinking,\n            probability=probability,\n            cache_size=cache_size,\n            class_weight=class_weight,\n            verbose=verbose,\n            max_iter=max_iter,\n            decision_function_shape=decision_function_shape,\n            break_ties=break_ties,\n            random_state=random_state,\n        )\n\n    def _more_tags(self):\n        return {\n            \"_xfail_checks\": {\n                \"check_methods_subset_invariance\": (\n                    \"fails for the decision_function method\"\n                ),\n                \"check_class_weight_classifiers\": \"class_weight is ignored.\",\n                \"check_sample_weights_invariance\": (\n                    \"zero sample_weight is not equivalent to removing samples\"\n                ),\n                \"check_classifiers_one_label_sample_weights\": (\n                    \"specified nu is infeasible for the fit.\"\n                ),\n            }\n        }",
                "filename": "sklearn/svm/_classes.py",
                "start_index": 42057,
                "end_index": 43802,
                "start_line": 1091,
                "end_line": 1853,
                "max_line": 1853,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "X = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=[\"a\", \"b\", \"c\"])\nscalar = StandardScaler().fit(X)\nscalar.feature_names_in_\n\n# %%\n# The support of :term:`get_feature_names_out` is available for transformers\n# that already had `get_feature_names` and transformers with a one-to-one\n# correspondence between input and output such as\n# :class:`~preprocessing.StandardScaler`. :term:`get_feature_names_out` support\n# will be added to all other transformers in future releases. Additionally,\n# :meth:`compose.ColumnTransformer.get_feature_names_out` is available to\n# combine feature names of its transformers:\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\n\nX = pd.DataFrame({\"pet\": [\"dog\", \"cat\", \"fish\"], \"age\": [3, 7, 1]})\npreprocessor = ColumnTransformer(\n    [\n        (\"numerical\", StandardScaler(), [\"age\"]),\n        (\"categorical\", OneHotEncoder(), [\"pet\"]),\n    ],\n    verbose_feature_names_out=False,\n).fit(X)\n\npreprocessor.get_feature_names_out()\n\n# %%\n# When this ``preprocessor`` is used with a pipeline, the feature names used\n# by the classifier are obtained by slicing and calling\n# :term:`get_feature_names_out`:\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\ny = [1, 0, 1]\npipe = make_pipeline(preprocessor, LogisticRegression())\npipe.fit(X, y)\npipe[:-1].get_feature_names_out()\n\n\n##############################################################################\n# A more flexible plotting API\n# --------------------------------------------------------------------------\n# :class:`metrics.ConfusionMatrixDisplay`,\n# :class:`metrics.PrecisionRecallDisplay`, :class:`metrics.DetCurveDisplay`,\n# and :class:`inspection.PartialDependenceDisplay` now expose two class\n# methods: `from_estimator` and `from_predictions` which allow users to create\n# a plot given the predictions or an estimator. This means the corresponding\n# `plot_*` functions are deprecated. Please check :ref:`example one\n# <sphx_glr_auto_examples_model_selection_plot_confusion_matrix.py>` and\n# :ref:`example two\n# <sphx_glr_auto_examples_classification_plot_digits_classification.py>` for\n# how to use the new plotting functionalities.\n\n##############################################################################\n# Online One-Class SVM\n# --------------------------------------------------------------------------\n# The new class :class:`~linear_model.SGDOneClassSVM` implements an online\n# linear version of the One-Class SVM using a stochastic gradient descent.\n# Combined with kernel approximation techniques,\n# :class:`~linear_model.SGDOneClassSVM` can be used to approximate the solution\n# of a kernelized One-Class SVM, implemented in :class:`~svm.OneClassSVM`, with\n# a fit time complexity linear in the number of samples. Note that the\n# complexity of a kernelized One-Class SVM is at best quadratic in the number\n# of samples. :class:`~linear_model.SGDOneClassSVM` is thus well suited for",
                "filename": "examples/release_highlights/plot_release_highlights_1_0_0.py",
                "start_index": 5998,
                "end_index": 8992,
                "start_line": 151,
                "end_line": 215,
                "max_line": 241,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.0
            }
        ],
        "classification": "other",
        "snippet_processor_task": {
            "sklearn/svm/_classes.py": [
                {
                    "chunk": {
                        "code": "\"\"\"C-Support Vector Classification.\n\n    The implementation is based on libsvm. The fit time scales at least\n    quadratically with the number of samples and may be impractical\n    beyond tens of thousands of samples. For large datasets\n    consider using :class:`~sklearn.svm.LinearSVC` or\n    :class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a\n    :class:`~sklearn.kernel_approximation.Nystroem` transformer or\n    other :ref:`kernel_approximation`.\n\n    The multiclass support is handled according to a one-vs-one scheme.\n\n    For details on the precise mathematical formulation of the provided\n    kernel functions and how `gamma`, `coef0` and `degree` affect each\n    other, see the corresponding section in the narrative documentation:\n    :ref:`svm_kernels`.\n\n    Read more in the :ref:`User Guide <svm_classification>`.\n\n    Parameters\n    ----------\n    C : float, default=1.0\n        Regularization parameter. The strength of the regularization is\n        inversely proportional to C. Must be strictly positive. The penalty\n        is a squared l2 penalty.\n\n    kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,  \\\n        default='rbf'\n        Specifies the kernel type to be used in the algorithm.\n        If none is given, 'rbf' will be used. If a callable is given it is\n        used to pre-compute the kernel matrix from data matrices; that matrix\n        should be an array of shape ``(n_samples, n_samples)``.\n\n    degree : int, default=3\n        Degree of the polynomial kernel function ('poly').\n        Must be non-negative. Ignored by all other kernels.\n\n    gamma : {'scale', 'auto'} or float, default='scale'\n        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n\n        - if ``gamma='scale'`` (default) is passed then it uses\n          1 / (n_features * X.var()) as value of gamma,\n        - if 'auto', uses 1 / n_features\n        - if float, must be non-negative.\n\n        .. versionchanged:: 0.22\n           The default value of ``gamma`` changed from 'auto' to 'scale'.\n\n    coef0 : float, default=0.0\n        Independent term in kernel function.\n        It is only significant in 'poly' and 'sigmoid'.\n\n    shrinking : bool, default=True\n        Whether to use the shrinking heuristic.\n        See the :ref:`User Guide <shrinking_svm>`.\n\n    probability : bool, default=False\n        Whether to enable probability estimates. This must be enabled prior\n        to calling `fit`, will slow down that method as it internally uses\n        5-fold cross-validation, and `predict_proba` may be inconsistent with\n        `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n\n    tol : float, default=1e-3\n        Tolerance for stopping criterion.\n\n    cache_size : float, default=200\n        Specify the size of the kernel cache (in MB).\n\n    class_weight : dict or 'balanced', default=None\n        Set the parameter C of class i to class_weight[i]*C for\n        SVC. If not given, all classes are supposed to have\n        weight one.\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``.\n\n    verbose : bool, default=False\n        Enable verbose output. Note that this setting takes advantage of a\n        per-process runtime setting in libsvm that, if enabled, may not work\n        properly in a multithreaded context.\n\n    max_iter : int, default=-1\n        Hard limit on iterations within solver, or -1 for no limit.\n\n    decision_function_shape : {'ovo', 'ovr'}, default='ovr'\n        Whether to return a one-vs-rest ('ovr') decision function of shape\n        (n_samples, n_classes) as all other classifiers, or the original\n        one-vs-one ('ovo') decision function of libsvm which has shape\n        (n_samples, n_classes * (n_classes - 1) / 2). However, note that\n        internally, one-vs-one ('ovo') is always used as a multi-class strategy\n        to train models; an ovr matrix is only constructed from the ovo matrix.\n        The parameter is ignored for binary classification.\n\n        .. versionchanged:: 0.19\n            decision_function_shape is 'ovr' by default.\n\n        .. versionadded:: 0.17\n           *decision_function_shape='ovr'* is recommended.\n\n        .. versionchanged:: 0.17\n           Deprecated *decision_function_shape='ovo' and None*.\n\n    break_ties : bool, default=False\n        If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n        :term:`predict` will break ties according to the confidence values of\n        :term:`decision_function`; otherwise the first class among the tied\n        classes is returned. Please note that breaking ties comes at a\n        relatively high computational cost compared to a simple predict.\n\n        .. versionadded:: 0.22\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the pseudo random number generation for shuffling the data for\n        probability estimates. Ignored when `probability` is False.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    Attributes\n    ----------\n    class_weight_ : ndarray of shape (n_classes,)\n        Multipliers of parameter C for each class.\n        Computed based on the ``class_weight`` parameter.\n\n    classes_ : ndarray of shape (n_classes,)\n        The classes labels.\n\n    coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)\n        Weights assigned to the features (coefficients in the primal\n        problem). This is only available in the case of a linear kernel.\n\n        `coef_` is a readonly property derived from `dual_coef_` and\n        `support_vectors_`.\n\n    dual_coef_ : ndarray of shape (n_classes -1, n_SV)\n        Dual coefficients of the support vector in the decision\n        function (see :ref:`sgd_mathematical_formulation`), multiplied by\n        their targets.\n        For multiclass, coefficient for all 1-vs-1 classifiers.\n        The layout of the coefficients in the multiclass case is somewhat\n        non-trivial. See the :ref:`multi-class section of the User Guide\n        <svm_multi_class>` for details.\n\n    fit_status_ : int\n        0 if correctly fitted, 1 otherwise (will raise warning)\n\n    intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n        Constants in decision function.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    n_iter_ : ndarray of shape (n_classes * (n_classes - 1) // 2,)\n        Number of iterations run by the optimization routine to fit the model.\n        The shape of this attribute depends on the number of models optimized\n        which in turn depends on the number of classes.\n\n        .. versionadded:: 1.1\n\n    support_ : ndarray of shape (n_SV)\n        Indices of support vectors.\n\n    support_vectors_ : ndarray of shape (n_SV, n_features)\n        Support vectors.\n\n    n_support_ : ndarray of shape (n_classes,), dtype=int32\n        Number of support vectors for each class.\n\n    probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n    probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n        If `probability=True`, it corresponds to the parameters learned in\n        Platt scaling to produce probability estimates from decision values.\n        If `probability=False`, it's an empty array. Platt scaling uses the\n        logistic function\n        ``1 / (1 + exp(decision_value * probA_ + probB_))``\n        where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n        more information on the multiclass case and training procedure see\n        section 8 of [1]_.\n\n    shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n        Array dimensions of training vector ``X``.\n\n    See Also\n    --------\n    SVR : Support Vector Machine for Regression implemented using libsvm.\n\n    LinearSVC : Scalable Linear Support Vector Machine for classification\n        implemented using liblinear. Check the See Also section of\n        LinearSVC for more comparison element.\n\n    References\n    ----------\n    .. [1] `LIBSVM: A Library for Support Vector Machines\n        <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n\n    .. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector\n        Machines and Comparisons to Regularized Likelihood Methods\"\n        <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.pipeline import make_pipeline\n    >>> from sklearn.preprocessing import StandardScaler\n    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n    >>> y = np.array([1, 1, 2, 2])\n    >>> from sklearn.svm import SVC\n    >>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n    >>> clf.fit(X, y)\n    Pipeline(steps=[('standardscaler', StandardScaler()),\n                    ('svc', SVC(gamma='auto'))])\n\n    >>> print(clf.predict([[-0.8, -1]]))\n    [1]\n    \"\"\"",
                        "filename": "sklearn/svm/_classes.py",
                        "start_index": 23041,
                        "end_index": 32355,
                        "start_line": 607,
                        "end_line": 1842,
                        "max_line": 1853,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the definition of the SVC class in scikit-learn, including the 'gamma' parameter which is the focus of the issue. Changes to the default value of 'gamma' and its documentation will likely be made here."
                }
            ],
            "examples/svm/plot_rbf_parameters.py": [
                {
                    "chunk": {
                        "code": "\"\"\"\n==================\nRBF SVM parameters\n==================\n\nThis example illustrates the effect of the parameters ``gamma`` and ``C`` of\nthe Radial Basis Function (RBF) kernel SVM.\n\nIntuitively, the ``gamma`` parameter defines how far the influence of a single\ntraining example reaches, with low values meaning 'far' and high values meaning\n'close'. The ``gamma`` parameters can be seen as the inverse of the radius of\ninfluence of samples selected by the model as support vectors.\n\nThe ``C`` parameter trades off correct classification of training examples\nagainst maximization of the decision function's margin. For larger values of\n``C``, a smaller margin will be accepted if the decision function is better at\nclassifying all training points correctly. A lower ``C`` will encourage a\nlarger margin, therefore a simpler decision function, at the cost of training\naccuracy. In other words ``C`` behaves as a regularization parameter in the\nSVM.\n\nThe first plot is a visualization of the decision function for a variety of\nparameter values on a simplified classification problem involving only 2 input\nfeatures and 2 possible target classes (binary classification). Note that this\nkind of plot is not possible to do for problems with more features or target\nclasses.\n\nThe second plot is a heatmap of the classifier's cross-validation accuracy as a\nfunction of ``C`` and ``gamma``. For this example we explore a relatively large\ngrid for illustration purposes. In practice, a logarithmic grid from\n:math:`10^{-3}` to :math:`10^3` is usually sufficient. If the best parameters\nlie on the boundaries of the grid, it can be extended in that direction in a\nsubsequent search.\n\nNote that the heat map plot has a special colorbar with a midpoint value close\nto the score values of the best performing models so as to make it easy to tell\nthem apart in the blink of an eye.\n\nThe behavior of the model is very sensitive to the ``gamma`` parameter. If\n``gamma`` is too large, the radius of the area of influence of the support\nvectors only includes the support vector itself and no amount of\nregularization with ``C`` will be able to prevent overfitting.\n\nWhen ``gamma`` is very small, the model is too constrained and cannot capture\nthe complexity or \"shape\" of the data. The region of influence of any selected\nsupport vector would include the whole training set. The resulting model will\nbehave similarly to a linear model with a set of hyperplanes that separate the\ncenters of high density of any pair of two classes.\n\nFor intermediate values, we can see on the second plot that good models can\nbe found on a diagonal of ``C`` and ``gamma``. Smooth models (lower ``gamma``\nvalues) can be made more complex by increasing the importance of classifying\neach point correctly (larger ``C`` values) hence the diagonal of good\nperforming models.\n\nFinally, one can also observe that for some intermediate values of ``gamma`` we\nget equally performing models when ``C`` becomes very large. This suggests that\nthe set of support vectors does not change anymore. The radius of the RBF\nkernel alone acts as a good structural regularizer. Increasing ``C`` further\ndoesn't help, likely because there are no more training points in violation\n(inside the margin or wrongly classified), or at least no better solution can\nbe found. Scores being equal, it may make sense to use the smaller ``C``\nvalues, since very high ``C`` values typically increase fitting time.\n\nOn the other hand, lower ``C`` values generally lead to more support vectors,\nwhich may increase prediction time. Therefore, lowering the value of ``C``\ninvolves a trade-off between fitting time and prediction time.\n\nWe should also note that small differences in scores results from the random\nsplits of the cross-validation procedure. Those spurious variations can be\nsmoothed out by increasing the number of CV iterations ``n_splits`` at the\nexpense of compute time. Increasing the value number of ``C_range`` and\n``gamma_range`` steps will increase the resolution of the hyper-parameter heat\nmap.\n\n\"\"\"",
                        "filename": "examples/svm/plot_rbf_parameters.py",
                        "start_index": 0,
                        "end_index": 4050,
                        "start_line": 1,
                        "end_line": 76,
                        "max_line": 218,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet is from an example that demonstrates the effect of the 'gamma' parameter in SVMs. Since the default value of 'gamma' is being changed, this example may need to be updated to reflect the new default or to explain the change."
                }
            ]
        },
        "solution_task": "- Update the default value of the `gamma` parameter in the `SVC` class from `'auto'` to `'scale'`.\n- Add a deprecation warning when `gamma='auto'` is used, informing users that this option is deprecated and `'scale'` should be used instead.\n- Ensure that the calculation for `gamma='scale'` uses `1 / (n_features * X.var())` as specified in the documentation.\n\n```python\nclass SVC:\n    def __init__(self, ..., gamma='scale', ...):\n        ...\n        if gamma == 'auto':\n            warnings.warn(\n                \"The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\",\n                DeprecationWarning\n            )\n            gamma_value = 1 / n_features\n        elif gamma == 'scale':\n            gamma_value = 1 / (n_features * X.var())\n        else:\n            gamma_value = gamma\n        ...\n```\n\n- Search for occurrences of `SVC()` in the codebase and update the `gamma` parameter to `'scale'` if it is set to `'auto'`.\n- Update the documentation and examples to reflect the change in the default value of the `gamma` parameter.\n- Ensure that the deprecation warning is tested and that the tests pass with the new default value `'scale'`.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE provides a comprehensive and actionable plan to address the issue described in the PROMPT. It suggests updating the default value of the 'gamma' parameter, adding a deprecation warning, and ensuring the correct calculation for the new 'scale' option. It also includes a code snippet that demonstrates how to implement these changes in the SVC class. Furthermore, the RESPONSE advises searching the codebase for occurrences of 'SVC()' where 'gamma' is set to 'auto' and updating the documentation and examples. The RESPONSE also mentions the need to test the deprecation warning and ensure tests pass with the new default value. The only minor discrepancy is the mention of 'X.var()' instead of 'X.std()', which is a slight deviation from the PROMPT's description but does not significantly detract from the overall utility of the RESPONSE.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711742736.070289,
        "result": "- Update the default value of the `gamma` parameter in the `SVC` class from `'auto'` to `'scale'`.\n- Add a deprecation warning when `gamma='auto'` is used, informing users that this option is deprecated and `'scale'` should be used instead.\n- Ensure that the calculation for `gamma='scale'` uses `1 / (n_features * X.var())` as specified in the documentation.\n\n```python\nclass SVC:\n    def __init__(self, ..., gamma='scale', ...):\n        ...\n        if gamma == 'auto':\n            warnings.warn(\n                \"The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\",\n                DeprecationWarning\n            )\n            gamma_value = 1 / n_features\n        elif gamma == 'scale':\n            gamma_value = 1 / (n_features * X.var())\n        else:\n            gamma_value = gamma\n        ...\n```\n\n- Search for occurrences of `SVC()` in the codebase and update the `gamma` parameter to `'scale'` if it is set to `'auto'`.\n- Update the documentation and examples to reflect the change in the default value of the `gamma` parameter.\n- Ensure that the deprecation warning is tested and that the tests pass with the new default value `'scale'`.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [svm](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/svm)- [Lines 607 - 1842](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/svm/_classes.py#L607-L1842)\n\n This snippet contains the definition of the SVC class in scikit-learn, including the 'gamma' parameter which is the focus of the issue. Changes to the default value of 'gamma' and its documentation will likely be made here.\n-  \u2192 [examples](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//examples) \u2192 [svm](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//examples/svm)- [Lines 1 - 76](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//examples/svm/plot_rbf_parameters.py#L1-L76)\n\n This snippet is from an example that demonstrates the effect of the 'gamma' parameter in SVMs. Since the default value of 'gamma' is being changed, this example may need to be updated to reflect the new default or to explain the change.",
        "context_relevance": 0.3,
        "hypotheses": [],
        "groundedness_score": 1.0,
        "qa_relevance_reasons": {
            "reason": "Criteria: The RESPONSE is relevant and answers the entire PROMPT completely.\nSupporting Evidence: The RESPONSE directly addresses the issue described in the PROMPT by providing a detailed explanation of the changes needed to implement and fix the issue with the gamma parameter in the SVC class. It includes code changes, deprecation warnings, documentation updates, and the need to update examples. The RESPONSE also provides a clear plan for locating and updating other occurrences of SVC() in the codebase. The RESPONSE is fully relevant to the entire PROMPT, including the technical details and the context of a pull request on GitHub, thus fulfilling all the requirements for a score of 10."
        },
        "qa_relevance": 1.0
    }
}