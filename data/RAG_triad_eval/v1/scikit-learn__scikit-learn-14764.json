{
    "id": "scikit-learn__scikit-learn-14764",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
    "task_id": 5041286198827502571,
    "title": "datasets :: make_classification() weights parameter should be a sequence (not just a list). ",
    "body": "### `weights` should be passed as list or array (not just list) in `sklearn\\datasets\\samples_generator.py :: make_classification`:\r\n If there is a pertinent reason that `weights` must be a list, while *all other iterable parameters are arrays*, then it should be mentioned in the docstring. Otherwise, the docstring should be amended as in `make_blobs`, e.g.  \"weights : list of floats or None (default=None)\" -> \"weights : sequence of floats or None (default=None)\", along with amended lines 165 and 171 (see Corrections).\r\n\r\n#### Test code to reproduce:\r\n``` \r\nprint('Testing weights type in `datasets.make_classification`:')\r\n# using defaults except for weights (& random_state=1):\r\n\r\nw = [0.25, 0.75]\r\nprint('  Test 1: weights as List {}'.format(w))\r\nX, y = make_classification(weights=w, random_state=1)\r\nprint('  Test 1 result: len(X)={}, len(y)={}'.format(len(X),len(y)))\r\n\r\nw = np.array([0.25, 0.75]) \r\nprint('  Test 2: weights as np.array {}'.format(w))\r\nX, y = make_classification(weights=w, random_state=1)\r\nprint('  Test 2 result: len(X)={}, len(y)={}, '.format(len(X),len(y)))\r\n```\r\n#### Expected Results:\r\nShould not fail: np.array as valid as a list:\r\n```\r\nTesting weights type in `make_classification`:\r\n  Test 1: weights as List [0.25, 0.75]\r\n  Test 1 result: len(X)=100, len(y)=100\r\n  Test 2: weights as np.array [0.25, 0.75]\r\n  Test 2 result: len(X)=100, len(y)=100\r\n```\r\n\r\n#### Actual Results\r\n```\r\nTesting weights type in `make_classification`:\r\n  Test 1: weights as List [0.25, 0.75]\r\n  Test 1 result: len(X)=100, len(y)=100\r\n  Test 2: weights as np.array [0.25, 0.75]\r\n```\r\n```error\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-c297f465db24> in <module>\r\n     13 print('  Test 2: weights as np.array {}'.format(w))\r\n     14 X, y = make_classification(weights=w,\r\n---> 15                            random_state=1)\r\n     16 print('  Test 2 result: len(X)={}, len(y)={}, '.format(len(X),len(y)))\r\n\r\n~\\Anaconda3\\envs\\dsml\\lib\\site-packages\\sklearn\\datasets\\samples_generator.py in make_classification(n_samples, n_features, n_informative, n_redundant, n_repeated, n_classes, n_clusters_per_class, weights, flip_y, class_sep, hypercube, shift, scale, shuffle, random_state)\r\n    163         raise ValueError(\"n_classes * n_clusters_per_class must\"\r\n    164                          \" be smaller or equal 2 ** n_informative\")\r\n--> 165     if weights and len(weights) not in [n_classes, n_classes - 1]:\r\n    166         raise ValueError(\"Weights specified but incompatible with number \"\r\n    167                          \"of classes.\")\r\n\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n```\r\n\r\n#### Corrections needed (fix ready):\r\n```\r\n165     if all(weights) and (len(weights) not in [n_classes, n_classes - 1]):\r\n\r\n171     if all(weights) and len(weights) == (n_classes - 1):\r\n```\r\n\r\n#### Versions:\r\n```  \r\n    System:\r\n    python: 3.6.7 (default, Feb 28 2019, 07:28:18) [MSC v.1900 64 bit (AMD64)]\r\nexecutable: C:\\<conda env path>\\python.exe\r\n   machine: Windows-10-10.0.18362-SP0 [same outcome with Windows-10-10.0.17134-SP0]\r\n\r\nPython deps:\r\n       pip: 19.1.1\r\nsetuptools: 41.0.1\r\n   sklearn: 0.21.3\r\n     numpy: 1.16.4\r\n     scipy: 1.3.0\r\n    Cython: None\r\n    pandas: 0.24.2\r\n```\r\n#wimlds\n[MRG] Added an example to the sklearn.feature_extraction.image.PatchExtractor\n\u2026or class\r\n\r\n<!--\r\nThanks for contributing a pull request! Please ensure you have taken a look at\r\nthe contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist\r\n-->\r\n\r\n#### Reference Issues/PRs\r\n<!--\r\nContributes to #3846.\r\n\r\n-->\r\n\r\n\r\n#### What does this implement/fix? Explain your changes.\r\nI added an example to the sklearn.feature_extraction.image.PatchExtractor (#3846)\r\n\r\n#### Any other comments?\r\n\r\n\r\n<!--\r\nPlease be aware that we are a loose team of volunteers so patience is\r\nnecessary; assistance handling other issues is very welcome. We value\r\nall user contributions, no matter how minor they are. If we are slow to\r\nreview, either the pull request needs some benchmarking, tinkering,\r\nconvincing, etc. or more likely the reviewers are simply busy. In either\r\ncase, we ask for your understanding during the review process.\r\nFor more information, see our FAQ on this topic:\r\nhttp://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.\r\n\r\nThanks for contributing!\r\n-->\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.22.git",
    "requested_commit": "",
    "mayil_response": "Doesn't seem like we have fully grasped all the specifics of this ticket \ud83d\ude10.                   But we've outlined some ideas that might help you get started:\n\n- Check if the `weights` parameter is being properly validated to handle sequences, including numpy arrays, and not just lists.\n- Modify the `make_classification` function to ensure that `weights` is checked for `None` before performing length checks.\n- Ensure that the length of `weights` is compared against `n_classes` and `n_classes - 1` correctly.\n- If `weights` is a numpy array and has a length of `n_classes - 1`, handle the case appropriately by normalizing the weights to sum up to 1, and then appending the weight for the last class.\n- Replace any incorrect usage of `all(weights)` with the appropriate check for `None` and length validation.\n- Ensure that the `random_state` parameter is used consistently to seed the random number generator for reproducibility.\n- Test the modified `make_classification` function with the provided test code to confirm that the expected results match the actual results after the fix.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/utils)- [Lines 181 - 4639](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/estimator_checks.py#L181-L4639)\n\n This snippet contains the check for class weights in classifiers, which is relevant to the issue as it deals with the handling of weights, albeit for a different purpose. \n\n\n   - [Lines 181 - 1181](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/estimator_checks.py#L181-L1181)\n\n This snippet includes tests for handling sample weights as different data types, which is relevant to the issue since it concerns the acceptance of weights as arrays.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0003105,
        "snippet_processor": 0.11024,
        "issue_star_creation": 0.03695,
        "issue_star_solver": 0.1084,
        "bouncer": 0.03046
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711738738.820951,
        "relevant_snippets": [
            {
                "code": "features = [\"MedInc\", \"AveOccup\", \"HouseAge\", \"AveRooms\"]\nest = RandomForestRegressor(n_estimators=10)\nest.fit(X, y)\n\n# plot_partial_dependence has been removed in version 1.2. From 1.2, use\n# PartialDependenceDisplay instead.\n# display = plot_partial_dependence(\ndisplay = PartialDependenceDisplay.from_estimator(\n    est,\n    X,\n    features,\n    kind=\"individual\",\n    subsample=50,\n    n_jobs=3,\n    grid_resolution=20,\n    random_state=0,\n)\ndisplay.figure_.suptitle(\n    \"Partial dependence of house value on non-location features\\n\"\n    \"for the California housing dataset, with BayesianRidge\"\n)\ndisplay.figure_.subplots_adjust(hspace=0.3)\n\n##############################################################################\n# New Poisson splitting criterion for DecisionTreeRegressor\n# ---------------------------------------------------------\n# The integration of Poisson regression estimation continues from version 0.23.\n# :class:`~sklearn.tree.DecisionTreeRegressor` now supports a new `'poisson'`\n# splitting criterion. Setting `criterion=\"poisson\"` might be a good choice\n# if your target is a count or a frequency.\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nn_samples, n_features = 1000, 20\nrng = np.random.RandomState(0)\nX = rng.randn(n_samples, n_features)\n# positive integer target correlated with X[:, 5] with many zeros:\ny = rng.poisson(lam=np.exp(X[:, 5]) / 2)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)\nregressor = DecisionTreeRegressor(criterion=\"poisson\", random_state=0)\nregressor.fit(X_train, y_train)\n\n##############################################################################\n# New documentation improvements\n# ------------------------------\n#\n# New examples and documentation pages have been added, in a continuous effort\n# to improve the understanding of machine learning practices:\n#\n# - a new section about :ref:`common pitfalls and recommended\n#   practices <common_pitfalls>`,\n# - an example illustrating how to :ref:`statistically compare the performance of\n#   models <sphx_glr_auto_examples_model_selection_plot_grid_search_stats.py>`\n#   evaluated using :class:`~sklearn.model_selection.GridSearchCV`,\n# - an example on how to :ref:`interpret coefficients of linear models\n#   <sphx_glr_auto_examples_inspection_plot_linear_model_coefficient_interpretation.py>`,\n# - an :ref:`example\n#   <sphx_glr_auto_examples_cross_decomposition_plot_pcr_vs_pls.py>`\n#   comparing Principal Component Regression and Partial Least Squares.",
                "filename": "examples/release_highlights/plot_release_highlights_0_24_0.py",
                "start_index": 8938,
                "end_index": 11509,
                "start_line": 204,
                "end_line": 264,
                "max_line": 264,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"\"\"\nThe :mod:`sklearn.feature_extraction.image` submodule gathers utilities to\nextract features from images.\n\"\"\"\n\n# Authors: Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>\n#          Gael Varoquaux <gael.varoquaux@normalesup.org>\n#          Olivier Grisel\n#          Vlad Niculae\n# License: BSD 3 clause\n\nfrom itertools import product\nfrom numbers import Integral, Number, Real\n\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\nfrom scipy import sparse\n\nfrom ..base import BaseEstimator, TransformerMixin, _fit_context\nfrom ..utils import check_array, check_random_state\nfrom ..utils._param_validation import Hidden, Interval, RealNotInt, validate_params\n\n__all__ = [\n    \"PatchExtractor\",\n    \"extract_patches_2d\",\n    \"grid_to_graph\",\n    \"img_to_graph\",\n    \"reconstruct_from_patches_2d\",\n]\n\n###############################################################################\n# From an image to a graph\n\n\ndef _make_edges_3d(n_x, n_y, n_z=1):\n    \"\"\"Returns a list of edges for a 3D image.\n\n    Parameters\n    ----------\n    n_x : int\n        The size of the grid in the x direction.\n    n_y : int\n        The size of the grid in the y direction.\n    n_z : integer, default=1\n        The size of the grid in the z direction, defaults to 1\n    \"\"\"\n    vertices = np.arange(n_x * n_y * n_z).reshape((n_x, n_y, n_z))\n    edges_deep = np.vstack((vertices[:, :, :-1].ravel(), vertices[:, :, 1:].ravel()))\n    edges_right = np.vstack((vertices[:, :-1].ravel(), vertices[:, 1:].ravel()))\n    edges_down = np.vstack((vertices[:-1].ravel(), vertices[1:].ravel()))\n    edges = np.hstack((edges_deep, edges_right, edges_down))\n    return edges\n\n\ndef _compute_gradient_3d(edges, img):\n    _, n_y, n_z = img.shape\n    gradient = np.abs(\n        img[\n            edges[0] // (n_y * n_z),\n            (edges[0] % (n_y * n_z)) // n_z,\n            (edges[0] % (n_y * n_z)) % n_z,\n        ]\n        - img[\n            edges[1] // (n_y * n_z),\n            (edges[1] % (n_y * n_z)) // n_z,\n            (edges[1] % (n_y * n_z)) % n_z,\n        ]\n    )\n    return gradient\n\n\n# XXX: Why mask the image after computing the weights?\n\n\ndef _mask_edges_weights(mask, edges, weights=None):\n    \"\"\"Apply a mask to edges (weighted or not)\"\"\"\n    inds = np.arange(mask.size)\n    inds = inds[mask.ravel()]\n    ind_mask = np.logical_and(np.in1d(edges[0], inds), np.in1d(edges[1], inds))\n    edges = edges[:, ind_mask]\n    if weights is not None:\n        weights = weights[ind_mask]\n    if len(edges.ravel()):\n        maxval = edges.max()\n    else:\n        maxval = 0\n    order = np.searchsorted(np.flatnonzero(mask), np.arange(maxval + 1))\n    edges = order[edges]\n    if weights is None:\n        return edges\n    else:\n        return edges, weights",
                "filename": "sklearn/feature_extraction/image.py",
                "start_index": 0,
                "end_index": 2742,
                "start_line": 1,
                "end_line": 92,
                "max_line": 659,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.1
            },
            {
                "code": "\"\"\"Extracts patches from a collection of images.\n\n    Read more in the :ref:`User Guide <image_feature_extraction>`.\n\n    .. versionadded:: 0.9\n\n    Parameters\n    ----------\n    patch_size : tuple of int (patch_height, patch_width), default=None\n        The dimensions of one patch. If set to None, the patch size will be\n        automatically set to `(img_height // 10, img_width // 10)`, where\n        `img_height` and `img_width` are the dimensions of the input images.\n\n    max_patches : int or float, default=None\n        The maximum number of patches per image to extract. If `max_patches` is\n        a float in (0, 1), it is taken to mean a proportion of the total number\n        of patches. If set to None, extract all possible patches.\n\n    random_state : int, RandomState instance, default=None\n        Determines the random number generator used for random sampling when\n        `max_patches is not None`. Use an int to make the randomness\n        deterministic.\n        See :term:`Glossary <random_state>`.\n\n    See Also\n    --------\n    reconstruct_from_patches_2d : Reconstruct image from all of its patches.\n\n    Notes\n    -----\n    This estimator is stateless and does not need to be fitted. However, we\n    recommend to call :meth:`fit_transform` instead of :meth:`transform`, as\n    parameter validation is only performed in :meth:`fit`.\n\n    Examples\n    --------\n    >>> from sklearn.datasets import load_sample_images\n    >>> from sklearn.feature_extraction import image\n    >>> # Use the array data from the second image in this dataset:\n    >>> X = load_sample_images().images[1]\n    >>> X = X[None, ...]\n    >>> print(f\"Image shape: {X.shape}\")\n    Image shape: (1, 427, 640, 3)\n    >>> pe = image.PatchExtractor(patch_size=(10, 10))\n    >>> pe_trans = pe.transform(X)\n    >>> print(f\"Patches shape: {pe_trans.shape}\")\n    Patches shape: (263758, 10, 10, 3)\n    >>> X_reconstructed = image.reconstruct_from_patches_2d(pe_trans, X.shape[1:])\n    >>> print(f\"Reconstructed shape: {X_reconstructed.shape}\")\n    Reconstructed shape: (427, 640, 3)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"patch_size\": [tuple, None],\n        \"max_patches\": [\n            None,\n            Interval(RealNotInt, 0, 1, closed=\"neither\"),\n            Interval(Integral, 1, None, closed=\"left\"),\n        ],\n        \"random_state\": [\"random_state\"],\n    }\n\n    def __init__(self, *, patch_size=None, max_patches=None, random_state=None):\n        self.patch_size = patch_size\n        self.max_patches = max_patches\n        self.random_state = random_state",
                "filename": "sklearn/feature_extraction/image.py",
                "start_index": 16735,
                "end_index": 19300,
                "start_line": 503,
                "end_line": 568,
                "max_line": 659,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "from sklearn.pipeline import make_pipeline\n\nX, y = make_classification(random_state=0)\n\nwith TemporaryDirectory(prefix=\"sklearn_cache_\") as tmpdir:\n    estimator = make_pipeline(\n        KNeighborsTransformer(n_neighbors=10, mode=\"distance\"),\n        Isomap(n_neighbors=10, metric=\"precomputed\"),\n        memory=tmpdir,\n    )\n    estimator.fit(X)\n\n    # We can decrease the number of neighbors and the graph will not be\n    # recomputed.\n    estimator.set_params(isomap__n_neighbors=5)\n    estimator.fit(X)\n\n# %%\n# KNN Based Imputation\n# ------------------------------------\n# We now support imputation for completing missing values using k-Nearest\n# Neighbors.\n#\n# Each sample's missing values are imputed using the mean value from\n# ``n_neighbors`` nearest neighbors found in the training set. Two samples are\n# close if the features that neither is missing are close.\n# By default, a euclidean distance metric\n# that supports missing values,\n# :func:`~sklearn.metrics.pairwise.nan_euclidean_distances`, is used to find the nearest\n# neighbors.\n#\n# Read more in the :ref:`User Guide <knnimpute>`.\n\nfrom sklearn.impute import KNNImputer\n\nX = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\nimputer = KNNImputer(n_neighbors=2)\nprint(imputer.fit_transform(X))\n\n# %%\n# Tree pruning\n# ------------\n#\n# It is now possible to prune most tree-based estimators once the trees are\n# built. The pruning is based on minimal cost-complexity. Read more in the\n# :ref:`User Guide <minimal_cost_complexity_pruning>` for details.\n\nX, y = make_classification(random_state=0)\n\nrf = RandomForestClassifier(random_state=0, ccp_alpha=0).fit(X, y)\nprint(\n    \"Average number of nodes without pruning {:.1f}\".format(\n        np.mean([e.tree_.node_count for e in rf.estimators_])\n    )\n)\n\nrf = RandomForestClassifier(random_state=0, ccp_alpha=0.05).fit(X, y)\nprint(\n    \"Average number of nodes with pruning {:.1f}\".format(\n        np.mean([e.tree_.node_count for e in rf.estimators_])\n    )\n)\n\n# %%\n# Retrieve dataframes from OpenML\n# -------------------------------\n# :func:`datasets.fetch_openml` can now return pandas dataframe and thus\n# properly handle datasets with heterogeneous data:\n\nfrom sklearn.datasets import fetch_openml\n\ntitanic = fetch_openml(\"titanic\", version=1, as_frame=True, parser=\"pandas\")\nprint(titanic.data.head()[[\"pclass\", \"embarked\"]])\n\n# %%\n# Checking scikit-learn compatibility of an estimator\n# ---------------------------------------------------\n# Developers can check the compatibility of their scikit-learn compatible\n# estimators using :func:`~utils.estimator_checks.check_estimator`. For\n# instance, the ``check_estimator(LinearSVC())`` passes.\n#\n# We now provide a ``pytest`` specific decorator which allows ``pytest``\n# to run all checks independently and report the checks that are failing.\n#\n# ..note::\n#   This entry was slightly updated in version 0.24, where passing classes\n#   isn't supported anymore: pass instances instead.",
                "filename": "examples/release_highlights/plot_release_highlights_0_22_0.py",
                "start_index": 5945,
                "end_index": 8900,
                "start_line": 85,
                "end_line": 248,
                "max_line": 282,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "# datasets with a large number of training samples (> 10,000) for which the SGD\n# variant can be several orders of magnitude faster. Please check this\n# :ref:`example\n# <sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py>` to see how\n# it's used, and the :ref:`User Guide <sgd_online_one_class_svm>` for more\n# details.\n#\n# .. figure:: ../miscellaneous/images/sphx_glr_plot_anomaly_comparison_001.png\n#    :target: ../miscellaneous/plot_anomaly_comparison.html\n#    :align: center\n\n##############################################################################\n# Histogram-based Gradient Boosting Models are now stable\n# --------------------------------------------------------------------------\n# :class:`~sklearn.ensemble.HistGradientBoostingRegressor` and\n# :class:`~ensemble.HistGradientBoostingClassifier` are no longer experimental\n# and can simply be imported and used as::\n#\n#     from sklearn.ensemble import HistGradientBoostingClassifier\n\n##############################################################################\n# New documentation improvements\n# ------------------------------\n# This release includes many documentation improvements. Out of over 2100\n# merged pull requests, about 800 of them are improvements to our\n# documentation.",
                "filename": "examples/release_highlights/plot_release_highlights_1_0_0.py",
                "start_index": 8993,
                "end_index": 10257,
                "start_line": 216,
                "end_line": 241,
                "max_line": 241,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_class_weight_balanced_linear_classifier(name, Classifier):\n    \"\"\"Test class weights with non-contiguous class labels.\"\"\"\n    # this is run on classes, not instances, though this should be changed\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-0.8, -1.0], [1.0, 1.0], [1.0, 0.0]])\n    y = np.array([1, 1, 1, -1, -1])\n\n    classifier = Classifier()\n\n    if hasattr(classifier, \"n_iter\"):\n        # This is a very small dataset, default n_iter are likely to prevent\n        # convergence\n        classifier.set_params(n_iter=1000)\n    if hasattr(classifier, \"max_iter\"):\n        classifier.set_params(max_iter=1000)\n    if hasattr(classifier, \"cv\"):\n        classifier.set_params(cv=3)\n    set_random_state(classifier)\n\n    # Let the model compute the class frequencies\n    classifier.set_params(class_weight=\"balanced\")\n    coef_balanced = classifier.fit(X, y).coef_.copy()\n\n    # Count each label occurrence to reweight manually\n    n_samples = len(y)\n    n_classes = float(len(np.unique(y)))\n\n    class_weight = {\n        1: n_samples / (np.sum(y == 1) * n_classes),\n        -1: n_samples / (np.sum(y == -1) * n_classes),\n    }\n    classifier.set_params(class_weight=class_weight)\n    coef_manual = classifier.fit(X, y).coef_.copy()\n\n    assert_allclose(\n        coef_balanced,\n        coef_manual,\n        err_msg=\"Classifier %s is not computing class_weight=balanced properly.\" % name,\n    )\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_estimators_overwrite_params(name, estimator_orig):\n    X, y = make_blobs(random_state=0, n_samples=21)\n    X = _enforce_estimator_tags_X(estimator_orig, X, kernel=rbf_kernel)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    set_random_state(estimator)\n\n    # Make a physical copy of the original estimator parameters before fitting.\n    params = estimator.get_params()\n    original_params = deepcopy(params)\n\n    # Fit the model\n    estimator.fit(X, y)\n\n    # Compare the state of the model parameters with the original parameters\n    new_params = estimator.get_params()\n    for param_name, original_value in original_params.items():\n        new_value = new_params[param_name]\n\n        # We should never change or mutate the internal state of input\n        # parameters by default. To check this we use the joblib.hash function\n        # that introspects recursively any subobjects to compute a checksum.\n        # The only exception to this rule of immutable constructor parameters\n        # is possible RandomState instance but in this check we explicitly\n        # fixed the random_state params recursively to be integer seeds.\n        assert joblib.hash(new_value) == joblib.hash(original_value), (\n            \"Estimator %s should not change or mutate \"\n            \" the parameter %s from %s to %s during fit.\"\n            % (name, param_name, original_value, new_value)\n        )",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 112710,
                "end_index": 115635,
                "start_line": 181,
                "end_line": 4639,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "gbdt_no_cst = HistGradientBoostingRegressor().fit(X, y)\ngbdt_cst = HistGradientBoostingRegressor(monotonic_cst=[1, 0]).fit(X, y)\n\n# plot_partial_dependence has been removed in version 1.2. From 1.2, use\n# PartialDependenceDisplay instead.\n# disp = plot_partial_dependence(\ndisp = PartialDependenceDisplay.from_estimator(\n    gbdt_no_cst,\n    X,\n    features=[0],\n    feature_names=[\"feature 0\"],\n    line_kw={\"linewidth\": 4, \"label\": \"unconstrained\", \"color\": \"tab:blue\"},\n)\n# plot_partial_dependence(\nPartialDependenceDisplay.from_estimator(\n    gbdt_cst,\n    X,\n    features=[0],\n    line_kw={\"linewidth\": 4, \"label\": \"constrained\", \"color\": \"tab:orange\"},\n    ax=disp.axes_,\n)\ndisp.axes_[0, 0].plot(\n    X[:, 0], y, \"o\", alpha=0.5, zorder=-1, label=\"samples\", color=\"tab:green\"\n)\ndisp.axes_[0, 0].set_ylim(-3, 3)\ndisp.axes_[0, 0].set_xlim(-1, 1)\nplt.legend()\nplt.show()\n\n##############################################################################\n# Sample-weight support for Lasso and ElasticNet\n# ----------------------------------------------\n# The two linear regressors :class:`~sklearn.linear_model.Lasso` and\n# :class:`~sklearn.linear_model.ElasticNet` now support sample weights.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import Lasso\nimport numpy as np\n\nn_samples, n_features = 1000, 20\nrng = np.random.RandomState(0)\nX, y = make_regression(n_samples, n_features, random_state=rng)\nsample_weight = rng.rand(n_samples)\nX_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(\n    X, y, sample_weight, random_state=rng\n)\nreg = Lasso()\nreg.fit(X_train, y_train, sample_weight=sw_train)\nprint(reg.score(X_test, y_test, sw_test))",
                "filename": "examples/release_highlights/plot_release_highlights_0_23_0.py",
                "start_index": 5973,
                "end_index": 7708,
                "start_line": 140,
                "end_line": 189,
                "max_line": 189,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "class PatchExtractor(TransformerMixin, BaseEstimator):",
                "filename": "sklearn/feature_extraction/image.py",
                "start_index": 16676,
                "end_index": 16730,
                "start_line": 502,
                "end_line": 502,
                "max_line": 659,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "# flake8: noqa\n\"\"\"\n=======================================\nRelease Highlights for scikit-learn 1.2\n=======================================\n\n.. currentmodule:: sklearn\n\nWe are pleased to announce the release of scikit-learn 1.2! Many bug fixes\nand improvements were added, as well as some new key features. We detail\nbelow a few of the major features of this release. **For an exhaustive list of\nall the changes**, please refer to the :ref:`release notes <changes_1_2>`.\n\nTo install the latest version (with pip)::\n\n    pip install --upgrade scikit-learn\n\nor with conda::\n\n    conda install -c conda-forge scikit-learn\n\n\"\"\"\n\n# %%\n# Pandas output with `set_output` API\n# -----------------------------------\n# scikit-learn's transformers now support pandas output with the `set_output` API.\n# To learn more about the `set_output` API see the example:\n# :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` and\n# # this `video, pandas DataFrame output for scikit-learn transformers\n# (some examples) <https://youtu.be/5bCg8VfX2x8>`__.\n\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler, KBinsDiscretizer\nfrom sklearn.compose import ColumnTransformer\n\nX, y = load_iris(as_frame=True, return_X_y=True)\nsepal_cols = [\"sepal length (cm)\", \"sepal width (cm)\"]\npetal_cols = [\"petal length (cm)\", \"petal width (cm)\"]\n\npreprocessor = ColumnTransformer(\n    [\n        (\"scaler\", StandardScaler(), sepal_cols),\n        (\"kbin\", KBinsDiscretizer(encode=\"ordinal\"), petal_cols),\n    ],\n    verbose_feature_names_out=False,\n).set_output(transform=\"pandas\")\n\nX_out = preprocessor.fit_transform(X)\nX_out.sample(n=5, random_state=0)\n\n# %%\n# Interaction constraints in Histogram-based Gradient Boosting Trees\n# ------------------------------------------------------------------\n# :class:`~ensemble.HistGradientBoostingRegressor` and\n# :class:`~ensemble.HistGradientBoostingClassifier` now supports interaction constraints\n# with the `interaction_cst` parameter. For details, see the\n# :ref:`User Guide <interaction_cst_hgbt>`. In the following example, features are not\n# allowed to interact.\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nX, y = load_diabetes(return_X_y=True, as_frame=True)\n\nhist_no_interact = HistGradientBoostingRegressor(\n    interaction_cst=[[i] for i in range(X.shape[1])], random_state=0\n)\nhist_no_interact.fit(X, y)\n\n# %%\n# New and enhanced displays\n# -------------------------\n# :class:`~metrics.PredictionErrorDisplay` provides a way to analyze regression\n# models in a qualitative manner.\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import PredictionErrorDisplay\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n_ = PredictionErrorDisplay.from_estimator(\n    hist_no_interact, X, y, kind=\"actual_vs_predicted\", ax=axs[0]\n)\n_ = PredictionErrorDisplay.from_estimator(\n    hist_no_interact, X, y, kind=\"residual_vs_predicted\", ax=axs[1]\n)\n\n# %%",
                "filename": "examples/release_highlights/plot_release_highlights_1_2_0.py",
                "start_index": 0,
                "end_index": 2992,
                "start_line": 1,
                "end_line": 153,
                "max_line": 166,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "@ignore_warnings(category=FutureWarning)\ndef check_sample_weights_pandas_series(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type pandas.Series in the 'fit' function.\n    estimator = clone(estimator_orig)\n    try:\n        import pandas as pd\n\n        X = np.array(\n            [\n                [1, 1],\n                [1, 2],\n                [1, 3],\n                [1, 4],\n                [2, 1],\n                [2, 2],\n                [2, 3],\n                [2, 4],\n                [3, 1],\n                [3, 2],\n                [3, 3],\n                [3, 4],\n            ]\n        )\n        X = pd.DataFrame(_enforce_estimator_tags_X(estimator_orig, X), copy=False)\n        y = pd.Series([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2])\n        weights = pd.Series([1] * 12)\n        if _safe_tags(estimator, key=\"multioutput_only\"):\n            y = pd.DataFrame(y, copy=False)\n        try:\n            estimator.fit(X, y, sample_weight=weights)\n        except ValueError:\n            raise ValueError(\n                \"Estimator {0} raises error if \"\n                \"'sample_weight' parameter is of \"\n                \"type pandas.Series\".format(name)\n            )\n    except ImportError:\n        raise SkipTest(\n            \"pandas is not installed: not testing for \"\n            \"input of type pandas.Series to class weight.\"\n        )\n\n\n@ignore_warnings(category=(FutureWarning))\ndef check_sample_weights_not_an_array(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type _NotAnArray in the 'fit' function.\n    estimator = clone(estimator_orig)\n    X = np.array(\n        [\n            [1, 1],\n            [1, 2],\n            [1, 3],\n            [1, 4],\n            [2, 1],\n            [2, 2],\n            [2, 3],\n            [2, 4],\n            [3, 1],\n            [3, 2],\n            [3, 3],\n            [3, 4],\n        ]\n    )\n    X = _NotAnArray(_enforce_estimator_tags_X(estimator_orig, X))\n    y = _NotAnArray([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2])\n    weights = _NotAnArray([1] * 12)\n    if _safe_tags(estimator, key=\"multioutput_only\"):\n        y = _NotAnArray(y.data.reshape(-1, 1))\n    estimator.fit(X, y, sample_weight=weights)\n\n\n@ignore_warnings(category=(FutureWarning))\ndef check_sample_weights_list(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type list in the 'fit' function.\n    estimator = clone(estimator_orig)\n    rnd = np.random.RandomState(0)\n    n_samples = 30\n    X = _enforce_estimator_tags_X(estimator_orig, rnd.uniform(size=(n_samples, 3)))\n    y = np.arange(n_samples) % 3\n    y = _enforce_estimator_tags_y(estimator, y)\n    sample_weight = [3] * n_samples\n    # Test that estimators don't raise any exception\n    estimator.fit(X, y, sample_weight=sample_weight)",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 38580,
                "end_index": 41430,
                "start_line": 181,
                "end_line": 1181,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/utils/estimator_checks.py": [
                {
                    "chunk": {
                        "code": "@ignore_warnings(category=FutureWarning)\ndef check_class_weight_balanced_linear_classifier(name, Classifier):\n    \"\"\"Test class weights with non-contiguous class labels.\"\"\"\n    # this is run on classes, not instances, though this should be changed\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-0.8, -1.0], [1.0, 1.0], [1.0, 0.0]])\n    y = np.array([1, 1, 1, -1, -1])\n\n    classifier = Classifier()\n\n    if hasattr(classifier, \"n_iter\"):\n        # This is a very small dataset, default n_iter are likely to prevent\n        # convergence\n        classifier.set_params(n_iter=1000)\n    if hasattr(classifier, \"max_iter\"):\n        classifier.set_params(max_iter=1000)\n    if hasattr(classifier, \"cv\"):\n        classifier.set_params(cv=3)\n    set_random_state(classifier)\n\n    # Let the model compute the class frequencies\n    classifier.set_params(class_weight=\"balanced\")\n    coef_balanced = classifier.fit(X, y).coef_.copy()\n\n    # Count each label occurrence to reweight manually\n    n_samples = len(y)\n    n_classes = float(len(np.unique(y)))\n\n    class_weight = {\n        1: n_samples / (np.sum(y == 1) * n_classes),\n        -1: n_samples / (np.sum(y == -1) * n_classes),\n    }\n    classifier.set_params(class_weight=class_weight)\n    coef_manual = classifier.fit(X, y).coef_.copy()\n\n    assert_allclose(\n        coef_balanced,\n        coef_manual,\n        err_msg=\"Classifier %s is not computing class_weight=balanced properly.\" % name,\n    )\n\n\n@ignore_warnings(category=FutureWarning)\ndef check_estimators_overwrite_params(name, estimator_orig):\n    X, y = make_blobs(random_state=0, n_samples=21)\n    X = _enforce_estimator_tags_X(estimator_orig, X, kernel=rbf_kernel)\n    estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    set_random_state(estimator)\n\n    # Make a physical copy of the original estimator parameters before fitting.\n    params = estimator.get_params()\n    original_params = deepcopy(params)\n\n    # Fit the model\n    estimator.fit(X, y)\n\n    # Compare the state of the model parameters with the original parameters\n    new_params = estimator.get_params()\n    for param_name, original_value in original_params.items():\n        new_value = new_params[param_name]\n\n        # We should never change or mutate the internal state of input\n        # parameters by default. To check this we use the joblib.hash function\n        # that introspects recursively any subobjects to compute a checksum.\n        # The only exception to this rule of immutable constructor parameters\n        # is possible RandomState instance but in this check we explicitly\n        # fixed the random_state params recursively to be integer seeds.\n        assert joblib.hash(new_value) == joblib.hash(original_value), (\n            \"Estimator %s should not change or mutate \"\n            \" the parameter %s from %s to %s during fit.\"\n            % (name, param_name, original_value, new_value)\n        )",
                        "filename": "sklearn/utils/estimator_checks.py",
                        "start_index": 112710,
                        "end_index": 115635,
                        "start_line": 181,
                        "end_line": 4639,
                        "max_line": 4639,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the check for class weights in classifiers, which is relevant to the issue as it deals with the handling of weights, albeit for a different purpose."
                },
                {
                    "chunk": {
                        "code": "@ignore_warnings(category=FutureWarning)\ndef check_sample_weights_pandas_series(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type pandas.Series in the 'fit' function.\n    estimator = clone(estimator_orig)\n    try:\n        import pandas as pd\n\n        X = np.array(\n            [\n                [1, 1],\n                [1, 2],\n                [1, 3],\n                [1, 4],\n                [2, 1],\n                [2, 2],\n                [2, 3],\n                [2, 4],\n                [3, 1],\n                [3, 2],\n                [3, 3],\n                [3, 4],\n            ]\n        )\n        X = pd.DataFrame(_enforce_estimator_tags_X(estimator_orig, X), copy=False)\n        y = pd.Series([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2])\n        weights = pd.Series([1] * 12)\n        if _safe_tags(estimator, key=\"multioutput_only\"):\n            y = pd.DataFrame(y, copy=False)\n        try:\n            estimator.fit(X, y, sample_weight=weights)\n        except ValueError:\n            raise ValueError(\n                \"Estimator {0} raises error if \"\n                \"'sample_weight' parameter is of \"\n                \"type pandas.Series\".format(name)\n            )\n    except ImportError:\n        raise SkipTest(\n            \"pandas is not installed: not testing for \"\n            \"input of type pandas.Series to class weight.\"\n        )\n\n\n@ignore_warnings(category=(FutureWarning))\ndef check_sample_weights_not_an_array(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type _NotAnArray in the 'fit' function.\n    estimator = clone(estimator_orig)\n    X = np.array(\n        [\n            [1, 1],\n            [1, 2],\n            [1, 3],\n            [1, 4],\n            [2, 1],\n            [2, 2],\n            [2, 3],\n            [2, 4],\n            [3, 1],\n            [3, 2],\n            [3, 3],\n            [3, 4],\n        ]\n    )\n    X = _NotAnArray(_enforce_estimator_tags_X(estimator_orig, X))\n    y = _NotAnArray([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2])\n    weights = _NotAnArray([1] * 12)\n    if _safe_tags(estimator, key=\"multioutput_only\"):\n        y = _NotAnArray(y.data.reshape(-1, 1))\n    estimator.fit(X, y, sample_weight=weights)\n\n\n@ignore_warnings(category=(FutureWarning))\ndef check_sample_weights_list(name, estimator_orig):\n    # check that estimators will accept a 'sample_weight' parameter of\n    # type list in the 'fit' function.\n    estimator = clone(estimator_orig)\n    rnd = np.random.RandomState(0)\n    n_samples = 30\n    X = _enforce_estimator_tags_X(estimator_orig, rnd.uniform(size=(n_samples, 3)))\n    y = np.arange(n_samples) % 3\n    y = _enforce_estimator_tags_y(estimator, y)\n    sample_weight = [3] * n_samples\n    # Test that estimators don't raise any exception\n    estimator.fit(X, y, sample_weight=sample_weight)",
                        "filename": "sklearn/utils/estimator_checks.py",
                        "start_index": 38580,
                        "end_index": 41430,
                        "start_line": 181,
                        "end_line": 1181,
                        "max_line": 4639,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes tests for handling sample weights as different data types, which is relevant to the issue since it concerns the acceptance of weights as arrays."
                }
            ]
        },
        "solution_task": "- Check if the `weights` parameter is being properly validated to handle sequences, including numpy arrays, and not just lists.\n- Modify the `make_classification` function to ensure that `weights` is checked for `None` before performing length checks.\n- Ensure that the length of `weights` is compared against `n_classes` and `n_classes - 1` correctly.\n- If `weights` is a numpy array and has a length of `n_classes - 1`, handle the case appropriately by normalizing the weights to sum up to 1, and then appending the weight for the last class.\n- Replace any incorrect usage of `all(weights)` with the appropriate check for `None` and length validation.\n- Ensure that the `random_state` parameter is used consistently to seed the random number generator for reproducibility.\n- Test the modified `make_classification` function with the provided test code to confirm that the expected results match the actual results after the fix.",
        "bouncer_response": {
            "score": 4,
            "criteria": "RESPONSES that offer some actionable advice but are incomplete or partially relevant should score between 4 and 6.",
            "supporting_evidence": "The RESPONSE provides a list of actions to address the issue with the `weights` parameter in the `make_classification` function. It suggests checking for proper validation of the `weights` parameter to handle sequences, modifying the function to ensure correct length checks, and testing the function with the provided code. However, the RESPONSE does not directly address the specific corrections needed as outlined in the PROMPT, such as the exact lines of code to be changed (lines 165 and 171) and the specific error message encountered. Additionally, the RESPONSE includes a section on 'Code snippets to check' that is not directly relevant to the bug fix and does not provide actionable guidance for the specific issue at hand.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711739000.8829062,
        "result": "Doesn't seem like we have fully grasped all the specifics of this ticket \ud83d\ude10.                   But we've outlined some ideas that might help you get started:\n\n- Check if the `weights` parameter is being properly validated to handle sequences, including numpy arrays, and not just lists.\n- Modify the `make_classification` function to ensure that `weights` is checked for `None` before performing length checks.\n- Ensure that the length of `weights` is compared against `n_classes` and `n_classes - 1` correctly.\n- If `weights` is a numpy array and has a length of `n_classes - 1`, handle the case appropriately by normalizing the weights to sum up to 1, and then appending the weight for the last class.\n- Replace any incorrect usage of `all(weights)` with the appropriate check for `None` and length validation.\n- Ensure that the `random_state` parameter is used consistently to seed the random number generator for reproducibility.\n- Test the modified `make_classification` function with the provided test code to confirm that the expected results match the actual results after the fix.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/utils)- [Lines 181 - 4639](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/estimator_checks.py#L181-L4639)\n\n This snippet contains the check for class weights in classifiers, which is relevant to the issue as it deals with the handling of weights, albeit for a different purpose. \n\n\n   - [Lines 181 - 1181](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/utils/estimator_checks.py#L181-L1181)\n\n This snippet includes tests for handling sample weights as different data types, which is relevant to the issue since it concerns the acceptance of weights as arrays.",
        "context_relevance": 0.19,
        "hypotheses": [],
        "groundedness_score": 1.0,
        "qa_relevance_reasons": {
            "reason": "Criteria: The RESPONSE must be relevant to the entire PROMPT to get a score of 10.\nSupporting Evidence: The RESPONSE does not address the specific issue raised in the PROMPT regarding the `weights` parameter in the `make_classification` function of scikit-learn. Instead, it provides generic advice on how to handle weights in a function, which is not directly related to the problem described in the PROMPT. The RESPONSE also includes irrelevant information about code snippets and a template for answering, which is not pertinent to the issue at hand. Therefore, the RESPONSE is not relevant to the PROMPT."
        },
        "qa_relevance": 0.0
    }
}