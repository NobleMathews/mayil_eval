{
    "id": "scikit-learn__scikit-learn-14067",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
    "task_id": 5214999431176117742,
    "title": "ARD Regressor accuracy degrades when upgrading Scipy 1.2.1 -> 1.3.0",
    "body": "Hi, \r\nbit of a tricky one, I'm hoping someone will have some time and/or suggestions for further investigation!\r\n\r\nThere seems to be an often-occurring worsening of performance (i.e. accuracy, although run-time increases too!) from the ARD regressor when upgrading from Scipy 1.2.1 -> 1.3.0. \r\n\r\n## Description\r\nOn a very simple dataset (see code snippets below) where a near-perfect fit should be achievable, typical error seems to degrade from order 1E-5 to 1E-2. Notably, convergence iterations seem to increase also from ~a few (~5) to around 50-200 iterations.\r\n\r\nHere's the headline plot, plotting absolute co-efficient error when fit across 1000 datasets generated with different random seeds:\r\n![coeff_abs_error_histograms](https://user-images.githubusercontent.com/1352905/59188556-cc7ebf00-8b6f-11e9-9be1-0de44f4beaee.png)\r\n\r\nNote how with Scipy==1.2.1, errors are largely constrained to <0.01, while with Scipy==1.3.0 they range up to 0.05 (and in a few rare cases the algorithm produces garbage results, see later).\r\n\r\nI guess this could be (probably is?) a Scipy rather than Sklearn issue, but probably the only way to confirm / isolate that would be to start here.\r\n\r\nIt's also possible that this worsening of behaviour is a weirdness of my particular toy example, but the difference in behaviour seems large and unexpected enough to warrant further investigation, I'd hope!\r\n\r\n## Steps/Code to Reproduce\r\n### Single Seed:\r\nOK, so here's a short snippet on just a single seed if you're curious to try this yourself. I'm generating three vectors of normally distributed values, 250 samples. Then the target is just a perfect copy of one of those vectors (index=1). We measure the accuracy of the fit by simply checking how close that coefficient is to 1.0 (the other coefficients always shrink to 0., as you'd hope):\r\n\r\n```\r\nimport scipy\r\nimport sklearn\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nfrom sklearn.linear_model import ARDRegression\r\n\r\nsklearn.show_versions()\r\n\r\ndef test_ard_regressor(dataset: np.ndarray) -> float:\r\n    X = dataset\r\n    y = X[:,1]\r\n    regressor = ARDRegression(verbose=True)\r\n    regressor.fit(X, y)\r\n    abs_coef_error = np.abs(1 - regressor.coef_[1])\r\n    print(abs_coef_error)\r\n    return abs_coef_error\r\n\r\nsize=250\r\nX = np.random.RandomState(seed=45).normal(size=(size,3))\r\n\r\ntest_ard_regressor(X)\r\n```\r\n\r\n#### Results\r\nScipy 1.2.1:\r\n```\r\npython single_seed.py \r\n\r\nSystem:\r\n    python: 3.6.7 (default, Oct 22 2018, 11:32:17)  [GCC 8.2.0]\r\nexecutable: /home/staley/.virtualenvs/sklearn-bug-scipy-1.2.1/bin/python\r\n   machine: Linux-4.15.0-47-generic-x86_64-with-Ubuntu-18.04-bionic\r\n\r\nBLAS:\r\n    macros: HAVE_CBLAS=None\r\n  lib_dirs: /usr/lib/x86_64-linux-gnu\r\ncblas_libs: openblas, openblas\r\n\r\nPython deps:\r\n       pip: 19.1.1\r\nsetuptools: 41.0.1\r\n   sklearn: 0.21.2\r\n     numpy: 1.16.4\r\n     scipy: 1.2.1\r\n    Cython: None\r\n    pandas: None\r\nConverged after 4 iterations\r\n9.647701516568574e-07\r\n```\r\n\r\nScipy 1.3.0\r\n```\r\npython single_seed.py \r\n\r\nSystem:\r\n    python: 3.6.7 (default, Oct 22 2018, 11:32:17)  [GCC 8.2.0]\r\nexecutable: /home/staley/.virtualenvs/sklearn-bug-scipy-1.3/bin/python\r\n   machine: Linux-4.15.0-47-generic-x86_64-with-Ubuntu-18.04-bionic\r\n\r\nBLAS:\r\n    macros: HAVE_CBLAS=None\r\n  lib_dirs: /usr/lib/x86_64-linux-gnu\r\ncblas_libs: openblas, openblas\r\n\r\nPython deps:\r\n       pip: 19.1.1\r\nsetuptools: 41.0.1\r\n   sklearn: 0.21.2\r\n     numpy: 1.16.4\r\n     scipy: 1.3.0\r\n    Cython: None\r\n    pandas: None\r\nConverged after 18 iterations\r\n0.16538104739325354\r\n\r\n```\r\n\r\n### Datasets from 1000 different seeds\r\nIt could be that there's some oddity of the random data from a single seed, so I set up some short scripts to first generate a static collection of 1000 of the datasets as seen above, then collate the results from both versions of scipy. The snippets are as follows:\r\n\r\nMake data:\r\n```\r\nimport numpy as np\r\nsize=250\r\nrandom_datasets = {seed: np.random.RandomState(seed).normal(size=(size,3)) \r\n                   for seed in range(1000)}\r\nnp.savez('random_datasets.npz', data=list(random_datasets.values()), seeds=list(random_datasets.keys()))\r\n```\r\n\r\nTest sklearn:\r\n```\r\nimport scipy\r\nimport sklearn\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nfrom sklearn.linear_model import ARDRegression\r\n\r\nrandom_datasets = np.load('random_datasets.npz')\r\nrandom_datasets=dict(zip(random_datasets['seeds'], random_datasets['data']))\r\n\r\ndef test_ard_regressor(dataset: np.ndarray) -> float:\r\n    X = dataset\r\n    y = X[:,1]\r\n    regressor = ARDRegression(verbose=True)\r\n    regressor.fit(X, y)\r\n    abs_coef_error = np.abs(1 - regressor.coef_[1])\r\n    print(abs_coef_error)\r\n    return abs_coef_error\r\n\r\nresults = []\r\nfor seed, data in random_datasets.items():\r\n    print(\"Seed:\",seed)\r\n    results.append(test_ard_regressor(data))\r\n\r\nnp.save(f'scipy_{scipy.__version__}_results', np.array(results))\r\n```\r\n\r\nPlot results:\r\n```\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nresults_1_2_1 = np.load(\"./scipy_1.2.1_results.npy\")\r\nresults_1_3_0 = np.load(\"./scipy_1.3.0_results.npy\")\r\n\r\ncounts, bin_edges = np.histogram(results_1_2_1)\r\n\r\nax = plt.gca()\r\nax.hist(results_1_2_1, label=\"scipy==1.2.1\", alpha=0.5, bins=bin_edges)\r\nax.hist(results_1_3_0, label=\"scipy==1.3.0\", alpha=0.5, bins=bin_edges)\r\n# ax.set_xlim(0, 1.0)\r\nax.legend()\r\nplt.show()\r\n```\r\n\r\nA little investigating summary statistics of those datasets in notebook gives the following points of comparison:\r\n```\r\n> np.median(results_1_2_1)\r\n1.1909624002770514e-05\r\n> np.median(results_1_3_0)\r\n0.008368892887510193\r\n\r\n>np.percentile(results_1_2_1, 99)\r\n0.03166983391537859\r\n>np.percentile(results_1_3_0, 99)\r\n0.16551247976283737\r\n\r\n\r\n> results_1_2_1.max()\r\n0.08478086928684647\r\n>results_1_3_0.max()\r\n46606.5545533851 \r\n\r\n```\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.22.git",
    "requested_commit": "",
    "mayil_response": "To address the issue of ARD regressor performance degradation after upgrading Scipy from 1.2.1 to 1.3.0, consider the following solution:\n\n- Investigate the change log between Scipy versions 1.2.1 and 1.3.0 to identify any changes in optimization algorithms that ARDRegression might rely on.\n- Check if the convergence criteria for the ARDRegression algorithm have been affected by the Scipy upgrade.\n- If the convergence criteria have changed, consider adjusting the `tol` parameter in the ARDRegression constructor to a more suitable value for the new Scipy version.\n- If the optimization algorithm has changed, consider setting the `solver` parameter in ARDRegression to use a specific solver that performs better with the new Scipy version.\n- Ensure that the random state is set consistently across different runs for reproducibility. Use `np.random.RandomState(seed=45)` when generating datasets.\n- If the issue persists, consider reporting it to the maintainers of scikit-learn, as the ARDRegression class is part of this library, and the issue may be related to internal changes in the interaction with Scipy.\n- As a temporary workaround, you may consider downgrading Scipy to version 1.2.1 if the performance degradation is critical and no immediate solution is available.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/linear_model)- [Lines 66 - 480](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/linear_model/_bayes.py#L66-L480)\n\n This snippet is from the file where the ARDRegression class is implemented, which is directly related to the issue reported.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0009725,
        "snippet_processor": 0.08512000000000002,
        "issue_star_creation": 0.049199999999999994,
        "issue_star_solver": 0.054560000000000004,
        "bouncer": 0.02766
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711739001.172,
        "relevant_snippets": [
            {
                "code": "# all models always have more than 10 non-zero coefficients. Compared to the OLS\n# estimator, the coefficients using a Bayesian Ridge regression are slightly\n# shifted toward zero, which stabilises them. The ARD regression provides a\n# sparser solution: some of the non-informative coefficients are set exactly to\n# zero, while shifting others closer to zero. Some non-informative coefficients\n# are still present and retain large values.\n\n# %%\n# Plot the marginal log-likelihood\n# --------------------------------\nimport numpy as np\n\nard_scores = -np.array(ard.scores_)\nbrr_scores = -np.array(brr.scores_)\nplt.plot(ard_scores, color=\"navy\", label=\"ARD\")\nplt.plot(brr_scores, color=\"red\", label=\"BayesianRidge\")\nplt.ylabel(\"Log-likelihood\")\nplt.xlabel(\"Iterations\")\nplt.xlim(1, 30)\nplt.legend()\n_ = plt.title(\"Models log-likelihood\")\n\n# %%\n# Indeed, both models minimize the log-likelihood up to an arbitrary cutoff\n# defined by the `n_iter` parameter.\n#\n# Bayesian regressions with polynomial feature expansion\n# ======================================================\n# Generate synthetic dataset\n# --------------------------\n# We create a target that is a non-linear function of the input feature.\n# Noise following a standard uniform distribution is added.\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\n\nrng = np.random.RandomState(0)\nn_samples = 110\n\n# sort the data to make plotting easier later\nX = np.sort(-10 * rng.rand(n_samples) + 10)\nnoise = rng.normal(0, 1, n_samples) * 1.35\ny = np.sqrt(X) * np.sin(X) + noise\nfull_data = pd.DataFrame({\"input_feature\": X, \"target\": y})\nX = X.reshape((-1, 1))\n\n# extrapolation\nX_plot = np.linspace(10, 10.4, 10)\ny_plot = np.sqrt(X_plot) * np.sin(X_plot)\nX_plot = np.concatenate((X, X_plot.reshape((-1, 1))))\ny_plot = np.concatenate((y - noise, y_plot))\n\n# %%\n# Fit the regressors\n# ------------------\n#\n# Here we try a degree 10 polynomial to potentially overfit, though the bayesian\n# linear models regularize the size of the polynomial coefficients. As\n# `fit_intercept=True` by default for\n# :class:`~sklearn.linear_model.ARDRegression` and\n# :class:`~sklearn.linear_model.BayesianRidge`, then\n# :class:`~sklearn.preprocessing.PolynomialFeatures` should not introduce an\n# additional bias feature. By setting `return_std=True`, the bayesian regressors\n# return the standard deviation of the posterior distribution for the model\n# parameters.\n\nard_poly = make_pipeline(\n    PolynomialFeatures(degree=10, include_bias=False),\n    StandardScaler(),\n    ARDRegression(),\n).fit(X, y)\nbrr_poly = make_pipeline(\n    PolynomialFeatures(degree=10, include_bias=False),\n    StandardScaler(),\n    BayesianRidge(),\n).fit(X, y)\n\ny_ard, y_ard_std = ard_poly.predict(X_plot, return_std=True)\ny_brr, y_brr_std = brr_poly.predict(X_plot, return_std=True)\n\n# %%\n# Plotting polynomial regressions with std errors of the scores\n# -------------------------------------------------------------",
                "filename": "examples/linear_model/plot_ard.py",
                "start_index": 2940,
                "end_index": 5927,
                "start_line": 96,
                "end_line": 178,
                "max_line": 211,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"\"\"\n====================================\nComparing Linear Bayesian Regressors\n====================================\n\nThis example compares two different bayesian regressors:\n\n - a :ref:`automatic_relevance_determination`\n - a :ref:`bayesian_ridge_regression`\n\nIn the first part, we use an :ref:`ordinary_least_squares` (OLS) model as a\nbaseline for comparing the models' coefficients with respect to the true\ncoefficients. Thereafter, we show that the estimation of such models is done by\niteratively maximizing the marginal log-likelihood of the observations.\n\nIn the last section we plot predictions and uncertainties for the ARD and the\nBayesian Ridge regressions using a polynomial feature expansion to fit a\nnon-linear relationship between `X` and `y`.\n\n\"\"\"\n\n# Author: Arturo Amor <david-arturo.amor-quiroz@inria.fr>\n\n# %%\n# Models robustness to recover the ground truth weights\n# =====================================================\n#\n# Generate synthetic dataset\n# --------------------------\n#\n# We generate a dataset where `X` and `y` are linearly linked: 10 of the\n# features of `X` will be used to generate `y`. The other features are not\n# useful at predicting `y`. In addition, we generate a dataset where `n_samples\n# == n_features`. Such a setting is challenging for an OLS model and leads\n# potentially to arbitrary large weights. Having a prior on the weights and a\n# penalty alleviates the problem. Finally, gaussian noise is added.\n\nfrom sklearn.datasets import make_regression\n\nX, y, true_weights = make_regression(\n    n_samples=100,\n    n_features=100,\n    n_informative=10,\n    noise=8,\n    coef=True,\n    random_state=42,\n)\n\n# %%\n# Fit the regressors\n# ------------------\n#\n# We now fit both Bayesian models and the OLS to later compare the models'\n# coefficients.\n\nimport pandas as pd\n\nfrom sklearn.linear_model import ARDRegression, BayesianRidge, LinearRegression\n\nolr = LinearRegression().fit(X, y)\nbrr = BayesianRidge(compute_score=True, n_iter=30).fit(X, y)\nard = ARDRegression(compute_score=True, n_iter=30).fit(X, y)\ndf = pd.DataFrame(\n    {\n        \"Weights of true generative process\": true_weights,\n        \"ARDRegression\": ard.coef_,\n        \"BayesianRidge\": brr.coef_,\n        \"LinearRegression\": olr.coef_,\n    }\n)\n\n# %%\n# Plot the true and estimated coefficients\n# ----------------------------------------\n#\n# Now we compare the coefficients of each model with the weights of\n# the true generative model.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import SymLogNorm\n\nplt.figure(figsize=(10, 6))\nax = sns.heatmap(\n    df.T,\n    norm=SymLogNorm(linthresh=10e-4, vmin=-80, vmax=80),\n    cbar_kws={\"label\": \"coefficients' values\"},\n    cmap=\"seismic_r\",\n)\nplt.ylabel(\"linear model\")\nplt.xlabel(\"coefficients\")\nplt.tight_layout(rect=(0, 0, 1, 0.95))\n_ = plt.title(\"Models' coefficients\")\n\n# %%\n# Due to the added noise, none of the models recover the true weights. Indeed,",
                "filename": "examples/linear_model/plot_ard.py",
                "start_index": 0,
                "end_index": 2939,
                "start_line": 1,
                "end_line": 95,
                "max_line": 211,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "if __name__ == \"__main__\":\n    # Delayed import of matplotlib.pyplot\n    import matplotlib.pyplot as plt\n    from glmnet.elastic_net import Lasso as GlmnetLasso\n\n    from sklearn.linear_model import Lasso as ScikitLasso\n\n    scikit_results = []\n    glmnet_results = []\n    n = 20\n    step = 500\n    n_features = 1000\n    n_informative = n_features / 10\n    n_test_samples = 1000\n    for i in range(1, n + 1):\n        print(\"==================\")\n        print(\"Iteration %s of %s\" % (i, n))\n        print(\"==================\")\n\n        X, Y, coef_ = make_regression(\n            n_samples=(i * step) + n_test_samples,\n            n_features=n_features,\n            noise=0.1,\n            n_informative=n_informative,\n            coef=True,\n        )\n\n        X_test = X[-n_test_samples:]\n        Y_test = Y[-n_test_samples:]\n        X = X[: (i * step)]\n        Y = Y[: (i * step)]\n\n        print(\"benchmarking scikit-learn: \")\n        scikit_results.append(bench(ScikitLasso, X, Y, X_test, Y_test, coef_))\n        print(\"benchmarking glmnet: \")\n        glmnet_results.append(bench(GlmnetLasso, X, Y, X_test, Y_test, coef_))\n\n    plt.clf()\n    xx = range(0, n * step, step)\n    plt.title(\"Lasso regression on sample dataset (%d features)\" % n_features)\n    plt.plot(xx, scikit_results, \"b-\", label=\"scikit-learn\")\n    plt.plot(xx, glmnet_results, \"r-\", label=\"glmnet\")\n    plt.legend()\n    plt.xlabel(\"number of samples to classify\")\n    plt.ylabel(\"Time (s)\")\n    plt.show()\n\n    # now do a benchmark where the number of points is fixed\n    # and the variable is the number of features\n\n    scikit_results = []\n    glmnet_results = []\n    n = 20\n    step = 100\n    n_samples = 500\n\n    for i in range(1, n + 1):\n        print(\"==================\")\n        print(\"Iteration %02d of %02d\" % (i, n))\n        print(\"==================\")\n        n_features = i * step\n        n_informative = n_features / 10\n\n        X, Y, coef_ = make_regression(\n            n_samples=(i * step) + n_test_samples,\n            n_features=n_features,\n            noise=0.1,\n            n_informative=n_informative,\n            coef=True,\n        )\n\n        X_test = X[-n_test_samples:]\n        Y_test = Y[-n_test_samples:]\n        X = X[:n_samples]\n        Y = Y[:n_samples]\n\n        print(\"benchmarking scikit-learn: \")\n        scikit_results.append(bench(ScikitLasso, X, Y, X_test, Y_test, coef_))\n        print(\"benchmarking glmnet: \")\n        glmnet_results.append(bench(GlmnetLasso, X, Y, X_test, Y_test, coef_))\n\n    xx = np.arange(100, 100 + n * step, step)\n    plt.figure(\"scikit-learn vs. glmnet benchmark results\")\n    plt.title(\"Regression in high dimensional spaces (%d samples)\" % n_samples)\n    plt.plot(xx, scikit_results, \"b-\", label=\"scikit-learn\")\n    plt.plot(xx, glmnet_results, \"r-\", label=\"glmnet\")\n    plt.legend()\n    plt.xlabel(\"number of features\")\n    plt.ylabel(\"Time (s)\")\n    plt.axis(\"tight\")\n    plt.show()",
                "filename": "benchmarks/bench_glmnet.py",
                "start_index": 1051,
                "end_index": 3966,
                "start_line": 49,
                "end_line": 139,
                "max_line": 139,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "from sklearn.model_selection import RepeatedKFold, cross_validate\n\ncv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=0)\ncv_model = cross_validate(\n    model,\n    X,\n    y,\n    cv=cv,\n    return_estimator=True,\n    n_jobs=2,\n)\n\ncoefs = pd.DataFrame(\n    [\n        est[-1].regressor_.coef_ * est[:-1].transform(X.iloc[train_idx]).std(axis=0)\n        for est, (train_idx, _) in zip(cv_model[\"estimator\"], cv.split(X, y))\n    ],\n    columns=feature_names,\n)\n\n# %%\nplt.figure(figsize=(9, 7))\nsns.stripplot(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\nsns.boxplot(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5, whis=10)\nplt.axvline(x=0, color=\".5\")\nplt.xlabel(\"Coefficient importance\")\nplt.title(\"Coefficient importance and its variability\")\nplt.suptitle(\"Ridge model, small regularization\")\nplt.subplots_adjust(left=0.3)\n\n# %%\n# The problem of correlated variables\n# -----------------------------------\n#\n# The AGE and EXPERIENCE coefficients are affected by strong variability which\n# might be due to the collinearity between the 2 features: as AGE and\n# EXPERIENCE vary together in the data, their effect is difficult to tease\n# apart.\n#\n# To verify this interpretation we plot the variability of the AGE and\n# EXPERIENCE coefficient.\n#\n# .. _covariation:\n\nplt.ylabel(\"Age coefficient\")\nplt.xlabel(\"Experience coefficient\")\nplt.grid(True)\nplt.xlim(-0.4, 0.5)\nplt.ylim(-0.4, 0.5)\nplt.scatter(coefs[\"AGE\"], coefs[\"EXPERIENCE\"])\n_ = plt.title(\"Co-variations of coefficients for AGE and EXPERIENCE across folds\")\n\n# %%\n# Two regions are populated: when the EXPERIENCE coefficient is\n# positive the AGE one is negative and vice-versa.\n#\n# To go further we remove one of the 2 features and check what is the impact\n# on the model stability.\n\ncolumn_to_drop = [\"AGE\"]\n\ncv_model = cross_validate(\n    model,\n    X.drop(columns=column_to_drop),\n    y,\n    cv=cv,\n    return_estimator=True,\n    n_jobs=2,\n)\n\ncoefs = pd.DataFrame(\n    [\n        est[-1].regressor_.coef_\n        * est[:-1].transform(X.drop(columns=column_to_drop).iloc[train_idx]).std(axis=0)\n        for est, (train_idx, _) in zip(cv_model[\"estimator\"], cv.split(X, y))\n    ],\n    columns=feature_names[:-1],\n)\n\n# %%\nplt.figure(figsize=(9, 7))\nsns.stripplot(data=coefs, orient=\"h\", palette=\"dark:k\", alpha=0.5)\nsns.boxplot(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5)\nplt.axvline(x=0, color=\".5\")\nplt.title(\"Coefficient importance and its variability\")\nplt.xlabel(\"Coefficient importance\")\nplt.suptitle(\"Ridge model, small regularization, AGE dropped\")\nplt.subplots_adjust(left=0.3)\n\n# %%\n# The estimation of the EXPERIENCE coefficient now shows a much reduced\n# variability. EXPERIENCE remains important for all models trained during\n# cross-validation.\n#\n# .. _scaling_num:\n#\n# Preprocessing numerical variables\n# ---------------------------------\n#\n# As said above (see \":ref:`the-pipeline`\"), we could also choose to scale\n# numerical values before training the model.",
                "filename": "examples/inspection/plot_linear_model_coefficient_interpretation.py",
                "start_index": 11920,
                "end_index": 14876,
                "start_line": 320,
                "end_line": 420,
                "max_line": 757,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "###############################################################################\n# ARD (Automatic Relevance Determination) regression",
                "filename": "sklearn/linear_model/_bayes.py",
                "start_index": 16316,
                "end_index": 16448,
                "start_line": 66,
                "end_line": 480,
                "max_line": 848,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "# Gaussian noise), then median estimator and the least squares estimator would\n# have yielded similar predictions.\n#\n# We then do the same on the test set.\nresults = []\nfor name, gbr in sorted(all_models.items()):\n    metrics = {\"model\": name}\n    y_pred = gbr.predict(X_test)\n    for alpha in [0.05, 0.5, 0.95]:\n        metrics[\"pbl=%1.2f\" % alpha] = mean_pinball_loss(y_test, y_pred, alpha=alpha)\n    metrics[\"MSE\"] = mean_squared_error(y_test, y_pred)\n    results.append(metrics)\n\npd.DataFrame(results).set_index(\"model\").style.apply(highlight_min)\n\n\n# %%\n# Errors are higher meaning the models slightly overfitted the data. It still\n# shows that the best test metric is obtained when the model is trained by\n# minimizing this same metric.\n#\n# Note that the conditional median estimator is competitive with the squared\n# error estimator in terms of MSE on the test set: this can be explained by\n# the fact the squared error estimator is very sensitive to large outliers\n# which can cause significant overfitting. This can be seen on the right hand\n# side of the previous plot. The conditional median estimator is biased\n# (underestimation for this asymmetric noise) but is also naturally robust to\n# outliers and overfits less.\n#\n# Calibration of the confidence interval\n# --------------------------------------\n#\n# We can also evaluate the ability of the two extreme quantile estimators at\n# producing a well-calibrated conditional 90%-confidence interval.\n#\n# To do this we can compute the fraction of observations that fall between the\n# predictions:\ndef coverage_fraction(y, y_low, y_high):\n    return np.mean(np.logical_and(y >= y_low, y <= y_high))\n\n\ncoverage_fraction(\n    y_train,\n    all_models[\"q 0.05\"].predict(X_train),\n    all_models[\"q 0.95\"].predict(X_train),\n)\n\n# %%\n# On the training set the calibration is very close to the expected coverage\n# value for a 90% confidence interval.\ncoverage_fraction(\n    y_test, all_models[\"q 0.05\"].predict(X_test), all_models[\"q 0.95\"].predict(X_test)\n)\n\n\n# %%\n# On the test set, the estimated confidence interval is slightly too narrow.\n# Note, however, that we would need to wrap those metrics in a cross-validation\n# loop to assess their variability under data resampling.\n#\n# Tuning the hyper-parameters of the quantile regressors\n# ------------------------------------------------------\n#\n# In the plot above, we observed that the 5th percentile regressor seems to\n# underfit and could not adapt to sinusoidal shape of the signal.\n#\n# The hyper-parameters of the model were approximately hand-tuned for the\n# median regressor and there is no reason that the same hyper-parameters are\n# suitable for the 5th percentile regressor.\n#\n# To confirm this hypothesis, we tune the hyper-parameters of a new regressor\n# of the 5th percentile by selecting the best model parameters by\n# cross-validation on the pinball loss with alpha=0.05:\n\n# %%\nfrom sklearn.experimental import enable_halving_search_cv  # noqa",
                "filename": "examples/ensemble/plot_gradient_boosting_quantile.py",
                "start_index": 5919,
                "end_index": 8882,
                "start_line": 164,
                "end_line": 239,
                "max_line": 334,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "# capture non-predictive features as well.\n#\n# :class:`~sklearn.linear_model.ARDRegression` is better when handling gaussian\n# noise, but is still unable to handle correlated features and requires a larger\n# amount of time due to fitting a prior.\n#\n# References\n# ----------\n#\n#   .. [1] :doi:`\"Lasso-type recovery of sparse representations for\n#    high-dimensional data\" N. Meinshausen, B. Yu - The Annals of Statistics\n#    2009, Vol. 37, No. 1, 246\u2013270 <10.1214/07-AOS582>`",
                "filename": "examples/linear_model/plot_lasso_and_elasticnet.py",
                "start_index": 8921,
                "end_index": 9398,
                "start_line": 237,
                "end_line": 248,
                "max_line": 248,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"\"\"\n==================================\nL1-based models for Sparse Signals\n==================================\n\nThe present example compares three l1-based regression models on a synthetic\nsignal obtained from sparse and correlated features that are further corrupted\nwith additive gaussian noise:\n\n - a :ref:`lasso`;\n - an :ref:`automatic_relevance_determination`;\n - an :ref:`elastic_net`.\n\nIt is known that the Lasso estimates turn to be close to the model selection\nestimates when the data dimensions grow, given that the irrelevant variables are\nnot too correlated with the relevant ones. In the presence of correlated\nfeatures, Lasso itself cannot select the correct sparsity pattern [1]_.\n\nHere we compare the performance of the three models in terms of the :math:`R^2`\nscore, the fitting time and the sparsity of the estimated coefficients when\ncompared with the ground-truth.\n\"\"\"\n\n# Author: Arturo Amor <david-arturo.amor-quiroz@inria.fr>\n\n# %%\n# Generate synthetic dataset\n# --------------------------\n#\n# We generate a dataset where the number of samples is lower than the total\n# number of features. This leads to an underdetermined system, i.e. the solution\n# is not unique, and thus we cannot apply an :ref:`ordinary_least_squares` by\n# itself. Regularization introduces a penalty term to the objective function,\n# which modifies the optimization problem and can help alleviate the\n# underdetermined nature of the system.\n#\n# The target `y` is a linear combination with alternating signs of sinusoidal\n# signals. Only the 10 lowest out of the 100 frequencies in `X` are used to\n# generate `y`, while the rest of the features are not informative. This results\n# in a high dimensional sparse feature space, where some degree of\n# l1-penalization is necessary.\n\nimport numpy as np\n\nrng = np.random.RandomState(0)\nn_samples, n_features, n_informative = 50, 100, 10\ntime_step = np.linspace(-2, 2, n_samples)\nfreqs = 2 * np.pi * np.sort(rng.rand(n_features)) / 0.01\nX = np.zeros((n_samples, n_features))\n\nfor i in range(n_features):\n    X[:, i] = np.sin(freqs[i] * time_step)\n\nidx = np.arange(n_features)\ntrue_coef = (-1) ** idx * np.exp(-idx / 10)\ntrue_coef[n_informative:] = 0  # sparsify coef\ny = np.dot(X, true_coef)\n\n# %%\n# Some of the informative features have close frequencies to induce\n# (anti-)correlations.\n\nfreqs[:n_informative]\n\n# %%\n# A random phase is introduced using :func:`numpy.random.random_sample`\n# and some gaussian noise (implemented by :func:`numpy.random.normal`)\n# is added to both the features and the target.\n\nfor i in range(n_features):\n    X[:, i] = np.sin(freqs[i] * time_step + 2 * (rng.random_sample() - 0.5))\n    X[:, i] += 0.2 * rng.normal(0, 1, n_samples)\n\ny += 0.2 * rng.normal(0, 1, n_samples)\n\n# %%\n# Such sparse, noisy and correlated features can be obtained, for instance, from\n# sensor nodes monitoring some environmental variables, as they typically register\n# similar values depending on their positions (spatial correlations).",
                "filename": "examples/linear_model/plot_lasso_and_elasticnet.py",
                "start_index": 0,
                "end_index": 2978,
                "start_line": 1,
                "end_line": 79,
                "max_line": 248,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "# For the purpose of showing how Ridge regularization works, we will create a\n# non-noisy data set. Then we will train a regularized model on a range of\n# regularization strengths (:math:`\\alpha`) and plot how the trained\n# coefficients and the mean squared error between those and the original values\n# behave as functions of the regularization strength.\n#\n# Creating a non-noisy data set\n# *****************************\n# We make a toy data set with 100 samples and 10 features, that's suitable to\n# detect regression. Out of the 10 features, 8 are informative and contribute to\n# the regression, while the remaining 2 features do not have any effect on the\n# target variable (their true coefficients are 0). Please note that in this\n# example the data is non-noisy, hence we can expect our regression model to\n# recover exactly the true coefficients w.\nfrom sklearn.datasets import make_regression\n\nX, y, w = make_regression(\n    n_samples=100, n_features=10, n_informative=8, coef=True, random_state=1\n)\n\n# Obtain the true coefficients\nprint(f\"The true coefficient of this regression problem are:\\n{w}\")\n\n# %%\n# Training the Ridge Regressor\n# ****************************\n# We use :class:`~sklearn.linear_model.Ridge`, a linear model with L2\n# regularization. We train several models, each with a different value for the\n# model parameter `alpha`, which is a positive constant that multiplies the\n# penalty term, controlling the regularization strength. For each trained model\n# we then compute the error between the true coefficients `w` and the\n# coefficients found by the model `clf`. We store the identified coefficients\n# and the calculated errors for the corresponding coefficients in lists, which\n# makes it convenient for us to plot them.\nimport numpy as np\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\nclf = Ridge()\n\n# Generate values for `alpha` that are evenly distributed on a logarithmic scale\nalphas = np.logspace(-3, 4, 200)\ncoefs = []\nerrors_coefs = []\n\n# Train the model with different regularisation strengths\nfor a in alphas:\n    clf.set_params(alpha=a).fit(X, y)\n    coefs.append(clf.coef_)\n    errors_coefs.append(mean_squared_error(clf.coef_, w))\n\n# %%\n# Plotting trained Coefficients and Mean Squared Errors\n# *****************************************************\n# We now plot the 10 different regularized coefficients as a function of the\n# regularization parameter `alpha` where each color represents a different\n# coefficient.\n#\n# On the right-hand-side, we plot how the errors of the coefficients from the\n# estimator change as a function of regularization.\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nalphas = pd.Index(alphas, name=\"alpha\")\ncoefs = pd.DataFrame(coefs, index=alphas, columns=[f\"Feature {i}\" for i in range(10)])\nerrors = pd.Series(errors_coefs, index=alphas, name=\"Mean squared error\")\n\nfig, axs = plt.subplots(1, 2, figsize=(20, 6))",
                "filename": "examples/linear_model/plot_ridge_coeffs.py",
                "start_index": 2976,
                "end_index": 5910,
                "start_line": 59,
                "end_line": 127,
                "max_line": 180,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "if __name__ == \"__main__\":\n    print(\"============================================\")\n    print(\"Warning: this is going to take a looong time\")\n    print(\"============================================\")\n\n    n = 10\n    step = 10000\n    n_samples = 10000\n    dim = 10\n    n_classes = 10\n    for i in range(n):\n        print(\"============================================\")\n        print(\"Entering iteration %s of %s\" % (i, n))\n        print(\"============================================\")\n        n_samples += step\n        X = np.random.randn(n_samples, dim)\n        Y = np.random.randint(0, n_classes, (n_samples,))\n        bench_scikit_tree_classifier(X, Y)\n        Y = np.random.randn(n_samples)\n        bench_scikit_tree_regressor(X, Y)\n\n    xx = range(0, n * step, step)\n    plt.figure(\"scikit-learn tree benchmark results\")\n    plt.subplot(211)\n    plt.title(\"Learning with varying number of samples\")\n    plt.plot(xx, scikit_classifier_results, \"g-\", label=\"classification\")\n    plt.plot(xx, scikit_regressor_results, \"r-\", label=\"regression\")\n    plt.legend(loc=\"upper left\")\n    plt.xlabel(\"number of samples\")\n    plt.ylabel(\"Time (s)\")\n\n    scikit_classifier_results = []\n    scikit_regressor_results = []\n    n = 10\n    step = 500\n    start_dim = 500\n    n_classes = 10\n\n    dim = start_dim\n    for i in range(0, n):\n        print(\"============================================\")\n        print(\"Entering iteration %s of %s\" % (i, n))\n        print(\"============================================\")\n        dim += step\n        X = np.random.randn(100, dim)\n        Y = np.random.randint(0, n_classes, (100,))\n        bench_scikit_tree_classifier(X, Y)\n        Y = np.random.randn(100)\n        bench_scikit_tree_regressor(X, Y)\n\n    xx = np.arange(start_dim, start_dim + n * step, step)\n    plt.subplot(212)\n    plt.title(\"Learning in high dimensional spaces\")\n    plt.plot(xx, scikit_classifier_results, \"g-\", label=\"classification\")\n    plt.plot(xx, scikit_regressor_results, \"r-\", label=\"regression\")\n    plt.legend(loc=\"upper left\")\n    plt.xlabel(\"number of dimensions\")\n    plt.ylabel(\"Time (s)\")\n    plt.axis(\"tight\")\n    plt.show()",
                "filename": "benchmarks/bench_tree.py",
                "start_index": 1480,
                "end_index": 3622,
                "start_line": 63,
                "end_line": 122,
                "max_line": 122,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/linear_model/_bayes.py": [
                {
                    "chunk": {
                        "code": "###############################################################################\n# ARD (Automatic Relevance Determination) regression",
                        "filename": "sklearn/linear_model/_bayes.py",
                        "start_index": 16316,
                        "end_index": 16448,
                        "start_line": 66,
                        "end_line": 480,
                        "max_line": 848,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet is from the file where the ARDRegression class is implemented, which is directly related to the issue reported."
                }
            ]
        },
        "solution_task": "To address the issue of ARD regressor performance degradation after upgrading Scipy from 1.2.1 to 1.3.0, consider the following solution:\n\n- Investigate the change log between Scipy versions 1.2.1 and 1.3.0 to identify any changes in optimization algorithms that ARDRegression might rely on.\n- Check if the convergence criteria for the ARDRegression algorithm have been affected by the Scipy upgrade.\n- If the convergence criteria have changed, consider adjusting the `tol` parameter in the ARDRegression constructor to a more suitable value for the new Scipy version.\n- If the optimization algorithm has changed, consider setting the `solver` parameter in ARDRegression to use a specific solver that performs better with the new Scipy version.\n- Ensure that the random state is set consistently across different runs for reproducibility. Use `np.random.RandomState(seed=45)` when generating datasets.\n- If the issue persists, consider reporting it to the maintainers of scikit-learn, as the ARDRegression class is part of this library, and the issue may be related to internal changes in the interaction with Scipy.\n- As a temporary workaround, you may consider downgrading Scipy to version 1.2.1 if the performance degradation is critical and no immediate solution is available.",
        "bouncer_response": {
            "score": 7,
            "criteria": "RESPONSE that are highly relevant, offering clear and comprehensive guidance or solutions for the described bug, should score between 7 and 9.",
            "supporting_evidence": "The RESPONSE provides a series of actionable steps that are relevant to the issue of the ARD regressor's performance degradation after upgrading Scipy. It suggests checking the change log for differences between the versions, adjusting the convergence criteria, setting the solver parameter, ensuring consistent random states, and reporting the issue to scikit-learn maintainers. These are all logical and relevant steps that could lead to resolving the issue. The RESPONSE also provides a link to the ARDRegression class implementation, which is directly related to the problem. However, it does not address all aspects of the bug, such as the increase in run-time or the rare cases where the algorithm produces garbage results, which prevents it from scoring a 10.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711739177.2240288,
        "result": "To address the issue of ARD regressor performance degradation after upgrading Scipy from 1.2.1 to 1.3.0, consider the following solution:\n\n- Investigate the change log between Scipy versions 1.2.1 and 1.3.0 to identify any changes in optimization algorithms that ARDRegression might rely on.\n- Check if the convergence criteria for the ARDRegression algorithm have been affected by the Scipy upgrade.\n- If the convergence criteria have changed, consider adjusting the `tol` parameter in the ARDRegression constructor to a more suitable value for the new Scipy version.\n- If the optimization algorithm has changed, consider setting the `solver` parameter in ARDRegression to use a specific solver that performs better with the new Scipy version.\n- Ensure that the random state is set consistently across different runs for reproducibility. Use `np.random.RandomState(seed=45)` when generating datasets.\n- If the issue persists, consider reporting it to the maintainers of scikit-learn, as the ARDRegression class is part of this library, and the issue may be related to internal changes in the interaction with Scipy.\n- As a temporary workaround, you may consider downgrading Scipy to version 1.2.1 if the performance degradation is critical and no immediate solution is available.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [linear_model](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/linear_model)- [Lines 66 - 480](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/linear_model/_bayes.py#L66-L480)\n\n This snippet is from the file where the ARDRegression class is implemented, which is directly related to the issue reported.",
        "context_relevance": 0.2,
        "hypotheses": [],
        "groundedness_score": 1.0,
        "qa_relevance_reasons": {
            "reason": "Criteria: The RESPONSE must be relevant to the entire PROMPT and answer it completely to get a score of 10.\nSupporting Evidence: The RESPONSE provides a comprehensive solution to the issue of ARD regressor performance degradation after upgrading Scipy from 1.2.1 to 1.3.0. It suggests checking the change log for differences between the versions, adjusting parameters, ensuring consistent random states, and reporting the issue to scikit-learn maintainers if necessary. It also provides a workaround by downgrading Scipy. Additionally, it includes a link to the relevant section of the scikit-learn codebase, which is directly related to the ARDRegression class mentioned in the PROMPT. The RESPONSE addresses the entire PROMPT, including the observed issues, steps to reproduce, and potential solutions, thus earning a score of 10."
        },
        "qa_relevance": 1.0
    }
}