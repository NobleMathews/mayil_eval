{
    "id": "scikit-learn__scikit-learn-24769",
    "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
    "task_id": 2071200840233323702,
    "title": "Add mean_average_precision",
    "body": "Mean average precision (mAP) is a standard multi-class extension of average precision using OVR: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision\r\n\r\nRecently I prefer AP over AUC so I think it would be cool to add this.\r\nMaybe @gbolmier is interested? @thomasjpfan probably knows how to do this ;)\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__1.3.git",
    "requested_commit": "",
    "mayil_response": "To implement the mean average precision (mAP) feature, follow these steps:\n\n- Import necessary modules:\n  ```python\n  from sklearn.metrics import precision_recall_curve, average_precision_score\n  from sklearn.preprocessing import label_binarize\n  import numpy as np\n  ```\n\n- Define a function to calculate mAP for multi-class classification:\n  ```python\n  def mean_average_precision(y_true, y_scores, n_classes):\n      # Binarize the output\n      y_true = label_binarize(y_true, classes=[*range(n_classes)])\n      \n      # Calculate the average precision for each class\n      average_precision = dict()\n      for i in range(n_classes):\n          average_precision[i] = average_precision_score(y_true[:, i], y_scores[:, i])\n      \n      # Calculate the mean average precision across all classes\n      mAP = np.mean(list(average_precision.values()))\n      return mAP\n  ```\n\n- Use the function in the context where you have `y_test` and `y_score` available, and you know the number of classes `n_classes`:\n  ```python\n  mAP = mean_average_precision(y_test, y_score, n_classes)\n  print(f\"Mean Average Precision (mAP): {mAP}\")\n  ```\n\n- If you need to integrate this into an existing codebase, find the appropriate place where the model's predictions (`y_score`) and the true labels (`y_test`) are available, and where the number of classes (`n_classes`) is known.\n\n- If the feature needs to be exposed to users (e.g., via a command-line interface or a web service), ensure that the function is called with the correct parameters and that the result is outputted or logged appropriately.\n\n- Add unit tests to verify the correctness of the mAP calculation.\n\n- Update documentation to include the new feature.\n\nFor further research, you can look into the `sklearn.metrics` module documentation to understand more about the functions used and other potential metrics that can be calculated. Additionally, you may want to explore different strategies for handling multi-label classification if that is relevant to your project.\n\n### Code snippets to check\n\n-  \u2192 [examples](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//examples) \u2192 [model_selection](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//examples/model_selection)- [Lines 68 - 92](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//examples/model_selection/plot_precision_recall.py#L68-L92)\n\n This snippet explains the concept of Average Precision (AP) and how it is used in binary classification, which is relevant to the implementation of mean average precision (mAP) for multi-class classification. \n\n\n   - [Lines 94 - 275](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//examples/model_selection/plot_precision_recall.py#L94-L275)\n\n This snippet demonstrates the calculation of average precision for multi-label classification and plotting of precision-recall curves, which is closely related to the implementation of mAP for multi-class classification.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn) \u2192 [metrics](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn/metrics)- [Lines 226 - 1975](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/metrics/_ranking.py#L226-L1975)\n\n This snippet contains the implementation of the binary uninterpolated average precision, which is a part of the calculation for mAP. It may need to be extended or modified to support multi-class classification.",
    "ai_cost": {
        "search_embedding": 2.6e-07,
        "issue_classifier": 0.004554,
        "snippet_processor": 0.29709,
        "issue_star_creation": 0.02362,
        "issue_star_solver": 0.06302,
        "bouncer": 0.02704
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711738633.901396,
        "relevant_snippets": [
            {
                "code": "text{AP} = \\\\sum_n (R_n - R_{n-1}) P_n`\n\nwhere :math:`P_n` and :math:`R_n` are the precision and recall at the\nnth threshold. A pair :math:`(R_k, P_k)` is referred to as an\n*operating point*.\n\nAP and the trapezoidal area under the operating points\n(:func:`sklearn.metrics.auc`) are common ways to summarize a precision-recall\ncurve that lead to different results. Read more in the\n:ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nPrecision-recall curves are typically used in binary classification to study\nthe output of a classifier. In order to extend the precision-recall curve and\naverage precision to multi-class or multi-label classification, it is necessary\nto binarize the output. One curve can be drawn per label, but one can also draw\na precision-recall curve by considering each element of the label indicator\nmatrix as a binary prediction (micro-averaging).\n\n.. note::\n\n    See also :func:`sklearn.metrics.average_precision_score`,\n             :func:`sklearn.metrics.recall_score`,\n             :func:`sklearn.metrics.precision_score`,\n             :func:`sklearn.metrics.f1_score`\n\"\"\"",
                "filename": "examples/model_selection/plot_precision_recall.py",
                "start_index": 3110,
                "end_index": 4216,
                "start_line": 68,
                "end_line": 92,
                "max_line": 277,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 0.7
            },
            {
                "code": "\"\"\"\n================\nPrecision-Recall\n================\n\nExample of Precision-Recall metric to evaluate classifier output quality.\n\nPrecision-Recall is a useful measure of success of prediction when the\nclasses are very imbalanced. In information retrieval, precision is a\nmeasure of result relevancy, while recall is a measure of how many truly\nrelevant results are returned.\n\nThe precision-recall curve shows the tradeoff between precision and\nrecall for different threshold. A high area under the curve represents\nboth high recall and high precision, where high precision relates to a\nlow false positive rate, and high recall relates to a low false negative\nrate. High scores for both show that the classifier is returning accurate\nresults (high precision), as well as returning a majority of all positive\nresults (high recall).\n\nA system with high recall but low precision returns many results, but most of\nits predicted labels are incorrect when compared to the training labels. A\nsystem with high precision but low recall is just the opposite, returning very\nfew results, but most of its predicted labels are correct when compared to the\ntraining labels. An ideal system with high precision and high recall will\nreturn many results, with all results labeled correctly.\n\nPrecision (:math:`P`) is defined as the number of true positives (:math:`T_p`)\nover the number of true positives plus the number of false positives\n(:math:`F_p`).\n\n:math:`P = \\\\frac{T_p}{T_p+F_p}`\n\nRecall (:math:`R`) is defined as the number of true positives (:math:`T_p`)\nover the number of true positives plus the number of false negatives\n(:math:`F_n`).\n\n:math:`R = \\\\frac{T_p}{T_p + F_n}`\n\nThese quantities are also related to the (:math:`F_1`) score, which is defined\nas the harmonic mean of precision and recall.\n\n:math:`F1 = 2\\\\frac{P \\\\times R}{P+R}`\n\nNote that the precision may not decrease with recall. The\ndefinition of precision (:math:`\\\\frac{T_p}{T_p + F_p}`) shows that lowering\nthe threshold of a classifier may increase the denominator, by increasing the\nnumber of results returned. If the threshold was previously set too high, the\nnew results may all be true positives, which will increase precision. If the\nprevious threshold was about right or too low, further lowering the threshold\nwill introduce false positives, decreasing precision.\n\nRecall is defined as :math:`\\\\frac{T_p}{T_p+F_n}`, where :math:`T_p+F_n` does\nnot depend on the classifier threshold. This means that lowering the classifier\nthreshold may increase recall, by increasing the number of true positive\nresults. It is also possible that lowering the threshold may leave recall\nunchanged, while the precision fluctuates.\n\nThe relationship between recall and precision can be observed in the\nstairstep area of the plot - at the edges of these steps a small change\nin the threshold considerably reduces precision, with only a minor gain in\nrecall.\n\n**Average precision** (AP) summarizes such a plot as the weighted mean of\nprecisions achieved at each threshold, with the increase in recall from the\nprevious threshold used as the weight:\n\n:math:`\\\\",
                "filename": "examples/model_selection/plot_precision_recall.py",
                "start_index": 0,
                "end_index": 3110,
                "start_line": 1,
                "end_line": 68,
                "max_line": 277,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 0.8
            },
            {
                "code": "# ROC-AUC score (0.77) is between the OvO ROC-AUC scores for \"versicolor\" vs\n# \"virginica\" (0.64) and \"setosa\" vs \"virginica\" (0.90). Indeed, the OvO\n# strategy gives additional information on the confusion between a pair of\n# classes, at the expense of computational cost when the number of classes\n# is large.\n#\n# The OvO strategy is recommended if the user is mainly interested in correctly\n# identifying a particular class or subset of classes, whereas evaluating the\n# global performance of a classifier can still be summarized via a given\n# averaging strategy.\n#\n# Micro-averaged OvR ROC is dominated by the more frequent class, since the\n# counts are pooled. The macro-averaged alternative better reflects the\n# statistics of the less frequent classes, and then is more appropriate when\n# performance on all the classes is deemed equally important.",
                "filename": "examples/model_selection/plot_roc.py",
                "start_index": 13690,
                "end_index": 14545,
                "start_line": 428,
                "end_line": 442,
                "max_line": 442,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "def _binary_uninterpolated_average_precision(\n        y_true, y_score, pos_label=1, sample_weight=None\n    ):\n        precision, recall, _ = precision_recall_curve(\n            y_true, y_score, pos_label=pos_label, sample_weight=sample_weight\n        )\n        # Return the step function integral\n        # The following works because the last entry of precision is\n        # guaranteed to be 1, as returned by precision_recall_curve\n        return -np.sum(np.diff(recall) * np.array(precision)[:-1])\n\n    y_type = type_of_target(y_true, input_name=\"y_true\")\n\n    # Convert to Python primitive type to avoid NumPy type / Python str\n    # comparison. See https://github.com/numpy/numpy/issues/6784\n    present_labels = np.unique(y_true).tolist()\n\n    if y_type == \"binary\":\n        if len(present_labels) == 2 and pos_label not in present_labels:\n            raise ValueError(\n                f\"pos_label={pos_label} is not a valid label. It should be \"\n                f\"one of {present_labels}\"\n            )\n\n    elif y_type == \"multilabel-indicator\" and pos_label != 1:\n        raise ValueError(\n            \"Parameter pos_label is fixed to 1 for multilabel-indicator y_true. \"\n            \"Do not set pos_label or set pos_label to 1.\"\n        )\n\n    elif y_type == \"multiclass\":\n        if pos_label != 1:\n            raise ValueError(\n                \"Parameter pos_label is fixed to 1 for multiclass y_true. \"\n                \"Do not set pos_label or set pos_label to 1.\"\n            )\n        y_true = label_binarize(y_true, classes=present_labels)\n\n    average_precision = partial(\n        _binary_uninterpolated_average_precision, pos_label=pos_label\n    )\n    return _average_binary_score(\n        average_precision, y_true, y_score, average, sample_weight=sample_weight\n    )",
                "filename": "sklearn/metrics/_ranking.py",
                "start_index": 7615,
                "end_index": 9401,
                "start_line": 226,
                "end_line": 1975,
                "max_line": 1995,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "# %%\n# We use :class:`~sklearn.multiclass.OneVsRestClassifier` for multi-label\n# prediction.\nfrom sklearn.multiclass import OneVsRestClassifier\n\nclassifier = OneVsRestClassifier(\n    make_pipeline(StandardScaler(), LinearSVC(random_state=random_state, dual=\"auto\"))\n)\nclassifier.fit(X_train, Y_train)\ny_score = classifier.decision_function(X_test)\n\n\n# %%\n# The average precision score in multi-label settings\n# ...................................................\nfrom sklearn.metrics import average_precision_score, precision_recall_curve\n\n# For each class\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(n_classes):\n    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i], y_score[:, i])\n    average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])\n\n# A \"micro-average\": quantifying score on all classes jointly\nprecision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(\n    Y_test.ravel(), y_score.ravel()\n)\naverage_precision[\"micro\"] = average_precision_score(Y_test, y_score, average=\"micro\")\n\n# %%\n# Plot the micro-averaged Precision-Recall curve\n# ..............................................\nfrom collections import Counter\n\ndisplay = PrecisionRecallDisplay(\n    recall=recall[\"micro\"],\n    precision=precision[\"micro\"],\n    average_precision=average_precision[\"micro\"],\n    prevalence_pos_label=Counter(Y_test.ravel())[1] / Y_test.size,\n)\ndisplay.plot(plot_chance_level=True)\n_ = display.ax_.set_title(\"Micro-averaged over all classes\")\n\n# %%\n# Plot Precision-Recall curve for each class and iso-f1 curves\n# ............................................................\nfrom itertools import cycle\n\nimport matplotlib.pyplot as plt\n\n# setup plot details\ncolors = cycle([\"navy\", \"turquoise\", \"darkorange\", \"cornflowerblue\", \"teal\"])\n\n_, ax = plt.subplots(figsize=(7, 8))\n\nf_scores = np.linspace(0.2, 0.8, num=4)\nlines, labels = [], []\nfor f_score in f_scores:\n    x = np.linspace(0.01, 1)\n    y = f_score * x / (2 * x - f_score)\n    (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n    plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n\ndisplay = PrecisionRecallDisplay(\n    recall=recall[\"micro\"],\n    precision=precision[\"micro\"],\n    average_precision=average_precision[\"micro\"],\n)\ndisplay.plot(ax=ax, name=\"Micro-average precision-recall\", color=\"gold\")\n\nfor i, color in zip(range(n_classes), colors):\n    display = PrecisionRecallDisplay(\n        recall=recall[i],\n        precision=precision[i],\n        average_precision=average_precision[i],\n    )\n    display.plot(ax=ax, name=f\"Precision-recall for class {i}\", color=color)\n\n# add the legend for the iso-f1 curves\nhandles, labels = display.ax_.get_legend_handles_labels()\nhandles.extend([l])\nlabels.extend([\"iso-f1 curves\"])\n# set the legend and the axes\nax.set_xlim([0.0, 1.0])\nax.set_ylim([0.0, 1.05])\nax.legend(handles=handles, labels=labels, loc=\"best\")\nax.set_title(\"Extension of Precision-Recall curve to multi-class\")",
                "filename": "examples/model_selection/plot_precision_recall.py",
                "start_index": 7217,
                "end_index": 10206,
                "start_line": 94,
                "end_line": 275,
                "max_line": 277,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 0.8
            },
            {
                "code": "for ix, (label_a, label_b) in enumerate(pair_list):\n    a_mask = y_test == label_a\n    b_mask = y_test == label_b\n    ab_mask = np.logical_or(a_mask, b_mask)\n\n    a_true = a_mask[ab_mask]\n    b_true = b_mask[ab_mask]\n\n    idx_a = np.flatnonzero(label_binarizer.classes_ == label_a)[0]\n    idx_b = np.flatnonzero(label_binarizer.classes_ == label_b)[0]\n\n    fpr_a, tpr_a, _ = roc_curve(a_true, y_score[ab_mask, idx_a])\n    fpr_b, tpr_b, _ = roc_curve(b_true, y_score[ab_mask, idx_b])\n\n    mean_tpr[ix] = np.zeros_like(fpr_grid)\n    mean_tpr[ix] += np.interp(fpr_grid, fpr_a, tpr_a)\n    mean_tpr[ix] += np.interp(fpr_grid, fpr_b, tpr_b)\n    mean_tpr[ix] /= 2\n    mean_score = auc(fpr_grid, mean_tpr[ix])\n    pair_scores.append(mean_score)\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n    plt.plot(\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\n        linestyle=\":\",\n        linewidth=4,\n    )\n    RocCurveDisplay.from_predictions(\n        a_true,\n        y_score[ab_mask, idx_a],\n        ax=ax,\n        name=f\"{label_a} as positive class\",\n    )\n    RocCurveDisplay.from_predictions(\n        b_true,\n        y_score[ab_mask, idx_b],\n        ax=ax,\n        name=f\"{label_b} as positive class\",\n        plot_chance_level=True,\n    )\n    plt.axis(\"square\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(f\"{target_names[idx_a]} vs {label_b} ROC curves\")\n    plt.legend()\n    plt.show()\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{np.average(pair_scores):.2f}\")\n\n# %%\n# One can also assert that the macro-average we computed \"by hand\" is equivalent\n# to the implemented `average=\"macro\"` option of the\n# :class:`~sklearn.metrics.roc_auc_score` function.\n\nmacro_roc_auc_ovo = roc_auc_score(\n    y_test,\n    y_score,\n    multi_class=\"ovo\",\n    average=\"macro\",\n)\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{macro_roc_auc_ovo:.2f}\")\n\n# %%\n# Plot all OvO ROC curves together\n# --------------------------------\n\novo_tpr = np.zeros_like(fpr_grid)\n\nfig, ax = plt.subplots(figsize=(6, 6))\nfor ix, (label_a, label_b) in enumerate(pair_list):\n    ovo_tpr += mean_tpr[ix]\n    plt.plot(\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {pair_scores[ix]:.2f})\",\n    )\n\novo_tpr /= sum(1 for pair in enumerate(pair_list))\n\nplt.plot(\n    fpr_grid,\n    ovo_tpr,\n    label=f\"One-vs-One macro-average (AUC = {macro_roc_auc_ovo:.2f})\",\n    linestyle=\":\",\n    linewidth=4,\n)\nplt.plot([0, 1], [0, 1], \"k--\", label=\"Chance level (AUC = 0.5)\")\nplt.axis(\"square\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Extension of Receiver Operating Characteristic\\nto One-vs-One multiclass\")\nplt.legend()\nplt.show()\n\n# %%\n# We confirm that the classes \"versicolor\" and \"virginica\" are not well\n# identified by a linear classifier. Notice that the \"virginica\"-vs-the-rest",
                "filename": "examples/model_selection/plot_roc.py",
                "start_index": 10759,
                "end_index": 13689,
                "start_line": 328,
                "end_line": 427,
                "max_line": 442,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "plt.title(\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\")\nplt.legend()\nplt.show()\n\n# %%\n# In the case where the main interest is not the plot but the ROC-AUC score\n# itself, we can reproduce the value shown in the plot using\n# :class:`~sklearn.metrics.roc_auc_score`.\n\nfrom sklearn.metrics import roc_auc_score\n\nmicro_roc_auc_ovr = roc_auc_score(\n    y_test,\n    y_score,\n    multi_class=\"ovr\",\n    average=\"micro\",\n)\n\nprint(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{micro_roc_auc_ovr:.2f}\")\n\n# %%\n# This is equivalent to computing the ROC curve with\n# :class:`~sklearn.metrics.roc_curve` and then the area under the curve with\n# :class:`~sklearn.metrics.auc` for the raveled true and predicted classes.\n\nfrom sklearn.metrics import auc, roc_curve\n\n# store the fpr, tpr, and roc_auc for all averaging strategies\nfpr, tpr, roc_auc = dict(), dict(), dict()\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_onehot_test.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\nprint(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['micro']:.2f}\")\n\n# %%\n# .. note:: By default, the computation of the ROC curve adds a single point at\n#     the maximal false positive rate by using linear interpolation and the\n#     McClish correction [:doi:`Analyzing a portion of the ROC curve Med Decis\n#     Making. 1989 Jul-Sep; 9(3):190-5.<10.1177/0272989x8900900307>`].\n#\n# ROC curve using the OvR macro-average\n# -------------------------------------\n#\n# Obtaining the macro-average requires computing the metric independently for\n# each class and then taking the average over them, hence treating all classes\n# equally a priori. We first aggregate the true/false positive rates per class:\n\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nfpr_grid = np.linspace(0.0, 1.0, 1000)\n\n# Interpolate all ROC curves at these points\nmean_tpr = np.zeros_like(fpr_grid)\n\nfor i in range(n_classes):\n    mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])  # linear interpolation\n\n# Average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = fpr_grid\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")\n\n# %%\n# This computation is equivalent to simply calling\n\nmacro_roc_auc_ovr = roc_auc_score(\n    y_test,\n    y_score,\n    multi_class=\"ovr\",\n    average=\"macro\",\n)\n\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{macro_roc_auc_ovr:.2f}\")\n\n# %%\n# Plot all OvR ROC curves together\n# --------------------------------\n\nfrom itertools import cycle\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\nplt.plot(\n    fpr[\"micro\"],\n    tpr[\"micro\"],\n    label=f\"micro-average ROC curve (AUC = {roc_auc['micro']:.2f})\",\n    color=\"deeppink\",\n    linestyle=\":\",\n    linewidth=4,\n)",
                "filename": "examples/model_selection/plot_roc.py",
                "start_index": 5927,
                "end_index": 8844,
                "start_line": 171,
                "end_line": 429,
                "max_line": 442,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "mean_fpr = np.linspace(0, 1, 100)\n\nfig, ax = plt.subplots(figsize=(6, 6))\nfor fold, (train, test) in enumerate(cv.split(X, y)):\n    classifier.fit(X[train], y[train])\n    viz = RocCurveDisplay.from_estimator(\n        classifier,\n        X[test],\n        y[test],\n        name=f\"ROC fold {fold}\",\n        alpha=0.3,\n        lw=1,\n        ax=ax,\n        plot_chance_level=(fold == n_splits - 1),\n    )\n    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n    interp_tpr[0] = 0.0\n    tprs.append(interp_tpr)\n    aucs.append(viz.roc_auc)\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nstd_auc = np.std(aucs)\nax.plot(\n    mean_fpr,\n    mean_tpr,\n    color=\"b\",\n    label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n    lw=2,\n    alpha=0.8,\n)\n\nstd_tpr = np.std(tprs, axis=0)\ntprs_upper = np.minimum(mean_tpr + std_tpr, 1)\ntprs_lower = np.maximum(mean_tpr - std_tpr, 0)\nax.fill_between(\n    mean_fpr,\n    tprs_lower,\n    tprs_upper,\n    color=\"grey\",\n    alpha=0.2,\n    label=r\"$\\pm$ 1 std. dev.\",\n)\n\nax.set(\n    xlim=[-0.05, 1.05],\n    ylim=[-0.05, 1.05],\n    xlabel=\"False Positive Rate\",\n    ylabel=\"True Positive Rate\",\n    title=f\"Mean ROC curve with variability\\n(Positive label '{target_names[1]}')\",\n)\nax.axis(\"square\")\nax.legend(loc=\"lower right\")\nplt.show()",
                "filename": "examples/model_selection/plot_roc_crossval.py",
                "start_index": 2979,
                "end_index": 4299,
                "start_line": 79,
                "end_line": 133,
                "max_line": 133,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "pos_label, name = cls._validate_from_predictions_params(\n            y_true, y_pred, sample_weight=sample_weight, pos_label=pos_label, name=name\n        )\n\n        precision, recall, _ = precision_recall_curve(\n            y_true,\n            y_pred,\n            pos_label=pos_label,\n            sample_weight=sample_weight,\n            drop_intermediate=drop_intermediate,\n        )\n        average_precision = average_precision_score(\n            y_true, y_pred, pos_label=pos_label, sample_weight=sample_weight\n        )\n\n        class_count = Counter(y_true)\n        prevalence_pos_label = class_count[pos_label] / sum(class_count.values())\n\n        viz = PrecisionRecallDisplay(\n            precision=precision,\n            recall=recall,\n            average_precision=average_precision,\n            estimator_name=name,\n            pos_label=pos_label,\n            prevalence_pos_label=prevalence_pos_label,\n        )\n\n        return viz.plot(\n            ax=ax,\n            name=name,\n            plot_chance_level=plot_chance_level,\n            chance_level_kw=chance_level_kw,\n            **kwargs,\n        )",
                "filename": "sklearn/metrics/_plot/precision_recall_curve.py",
                "start_index": 16462,
                "end_index": 17579,
                "start_line": 465,
                "end_line": 498,
                "max_line": 498,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "for i, species in enumerate([BV_bunch, MM_bunch]):\n        print(\"_\" * 80)\n        print(\"Modeling distribution of species '%s'\" % species.name)\n\n        # Standardize features\n        mean = species.cov_train.mean(axis=0)\n        std = species.cov_train.std(axis=0)\n        train_cover_std = (species.cov_train - mean) / std\n\n        # Fit OneClassSVM\n        print(\" - fit OneClassSVM ... \", end=\"\")\n        clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.5)\n        clf.fit(train_cover_std)\n        print(\"done.\")\n\n        # Plot map of South America\n        plt.subplot(1, 2, i + 1)\n        if basemap:\n            print(\" - plot coastlines using basemap\")\n            m = Basemap(\n                projection=\"cyl\",\n                llcrnrlat=Y.min(),\n                urcrnrlat=Y.max(),\n                llcrnrlon=X.min(),\n                urcrnrlon=X.max(),\n                resolution=\"c\",\n            )\n            m.drawcoastlines()\n            m.drawcountries()\n        else:\n            print(\" - plot coastlines from coverage\")\n            plt.contour(\n                X, Y, land_reference, levels=[-9998], colors=\"k\", linestyles=\"solid\"\n            )\n            plt.xticks([])\n            plt.yticks([])\n\n        print(\" - predict species distribution\")\n\n        # Predict species distribution using the training data\n        Z = np.ones((data.Ny, data.Nx), dtype=np.float64)\n\n        # We'll predict only for the land points.\n        idx = np.where(land_reference > -9999)\n        coverages_land = data.coverages[:, idx[0], idx[1]].T\n\n        pred = clf.decision_function((coverages_land - mean) / std)\n        Z *= pred.min()\n        Z[idx[0], idx[1]] = pred\n\n        levels = np.linspace(Z.min(), Z.max(), 25)\n        Z[land_reference == -9999] = -9999\n\n        # plot contours of the prediction\n        plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)\n        plt.colorbar(format=\"%.2f\")\n\n        # scatter training/testing points\n        plt.scatter(\n            species.pts_train[\"dd long\"],\n            species.pts_train[\"dd lat\"],\n            s=2**2,\n            c=\"black\",\n            marker=\"^\",\n            label=\"train\",\n        )\n        plt.scatter(\n            species.pts_test[\"dd long\"],\n            species.pts_test[\"dd lat\"],\n            s=2**2,\n            c=\"black\",\n            marker=\"x\",\n            label=\"test\",\n        )\n        plt.legend()\n        plt.title(species.name)\n        plt.axis(\"equal\")\n\n        # Compute AUC with regards to background points\n        pred_background = Z[background_points[0], background_points[1]]\n        pred_test = clf.decision_function((species.cov_test - mean) / std)\n        scores = np.r_[pred_test, pred_background]\n        y = np.r_[np.ones(pred_test.shape), np.zeros(pred_background.shape)]\n        fpr, tpr, thresholds = metrics.roc_curve(y, scores)\n        roc_auc = metrics.auc(fpr, tpr)\n        plt.text(-35, -70, \"AUC: %.3f\" % roc_auc, ha=\"right\")\n        print(\"\\n Area under the ROC curve : %f\" % roc_auc)",
                "filename": "examples/applications/plot_species_distribution_modeling.py",
                "start_index": 4760,
                "end_index": 7759,
                "start_line": 156,
                "end_line": 242,
                "max_line": 248,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                "sha": "",
                "context_relevance": 0.2
            }
        ],
        "classification": "feature request",
        "snippet_processor_task": {
            "examples/model_selection/plot_precision_recall.py": [
                {
                    "chunk": {
                        "code": "text{AP} = \\\\sum_n (R_n - R_{n-1}) P_n`\n\nwhere :math:`P_n` and :math:`R_n` are the precision and recall at the\nnth threshold. A pair :math:`(R_k, P_k)` is referred to as an\n*operating point*.\n\nAP and the trapezoidal area under the operating points\n(:func:`sklearn.metrics.auc`) are common ways to summarize a precision-recall\ncurve that lead to different results. Read more in the\n:ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nPrecision-recall curves are typically used in binary classification to study\nthe output of a classifier. In order to extend the precision-recall curve and\naverage precision to multi-class or multi-label classification, it is necessary\nto binarize the output. One curve can be drawn per label, but one can also draw\na precision-recall curve by considering each element of the label indicator\nmatrix as a binary prediction (micro-averaging).\n\n.. note::\n\n    See also :func:`sklearn.metrics.average_precision_score`,\n             :func:`sklearn.metrics.recall_score`,\n             :func:`sklearn.metrics.precision_score`,\n             :func:`sklearn.metrics.f1_score`\n\"\"\"",
                        "filename": "examples/model_selection/plot_precision_recall.py",
                        "start_index": 3110,
                        "end_index": 4216,
                        "start_line": 68,
                        "end_line": 92,
                        "max_line": 277,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet explains the concept of Average Precision (AP) and how it is used in binary classification, which is relevant to the implementation of mean average precision (mAP) for multi-class classification."
                },
                {
                    "chunk": {
                        "code": "# %%\n# We use :class:`~sklearn.multiclass.OneVsRestClassifier` for multi-label\n# prediction.\nfrom sklearn.multiclass import OneVsRestClassifier\n\nclassifier = OneVsRestClassifier(\n    make_pipeline(StandardScaler(), LinearSVC(random_state=random_state, dual=\"auto\"))\n)\nclassifier.fit(X_train, Y_train)\ny_score = classifier.decision_function(X_test)\n\n\n# %%\n# The average precision score in multi-label settings\n# ...................................................\nfrom sklearn.metrics import average_precision_score, precision_recall_curve\n\n# For each class\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(n_classes):\n    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i], y_score[:, i])\n    average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])\n\n# A \"micro-average\": quantifying score on all classes jointly\nprecision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(\n    Y_test.ravel(), y_score.ravel()\n)\naverage_precision[\"micro\"] = average_precision_score(Y_test, y_score, average=\"micro\")\n\n# %%\n# Plot the micro-averaged Precision-Recall curve\n# ..............................................\nfrom collections import Counter\n\ndisplay = PrecisionRecallDisplay(\n    recall=recall[\"micro\"],\n    precision=precision[\"micro\"],\n    average_precision=average_precision[\"micro\"],\n    prevalence_pos_label=Counter(Y_test.ravel())[1] / Y_test.size,\n)\ndisplay.plot(plot_chance_level=True)\n_ = display.ax_.set_title(\"Micro-averaged over all classes\")\n\n# %%\n# Plot Precision-Recall curve for each class and iso-f1 curves\n# ............................................................\nfrom itertools import cycle\n\nimport matplotlib.pyplot as plt\n\n# setup plot details\ncolors = cycle([\"navy\", \"turquoise\", \"darkorange\", \"cornflowerblue\", \"teal\"])\n\n_, ax = plt.subplots(figsize=(7, 8))\n\nf_scores = np.linspace(0.2, 0.8, num=4)\nlines, labels = [], []\nfor f_score in f_scores:\n    x = np.linspace(0.01, 1)\n    y = f_score * x / (2 * x - f_score)\n    (l,) = plt.plot(x[y >= 0], y[y >= 0], color=\"gray\", alpha=0.2)\n    plt.annotate(\"f1={0:0.1f}\".format(f_score), xy=(0.9, y[45] + 0.02))\n\ndisplay = PrecisionRecallDisplay(\n    recall=recall[\"micro\"],\n    precision=precision[\"micro\"],\n    average_precision=average_precision[\"micro\"],\n)\ndisplay.plot(ax=ax, name=\"Micro-average precision-recall\", color=\"gold\")\n\nfor i, color in zip(range(n_classes), colors):\n    display = PrecisionRecallDisplay(\n        recall=recall[i],\n        precision=precision[i],\n        average_precision=average_precision[i],\n    )\n    display.plot(ax=ax, name=f\"Precision-recall for class {i}\", color=color)\n\n# add the legend for the iso-f1 curves\nhandles, labels = display.ax_.get_legend_handles_labels()\nhandles.extend([l])\nlabels.extend([\"iso-f1 curves\"])\n# set the legend and the axes\nax.set_xlim([0.0, 1.0])\nax.set_ylim([0.0, 1.05])\nax.legend(handles=handles, labels=labels, loc=\"best\")\nax.set_title(\"Extension of Precision-Recall curve to multi-class\")",
                        "filename": "examples/model_selection/plot_precision_recall.py",
                        "start_index": 7217,
                        "end_index": 10206,
                        "start_line": 94,
                        "end_line": 275,
                        "max_line": 277,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet demonstrates the calculation of average precision for multi-label classification and plotting of precision-recall curves, which is closely related to the implementation of mAP for multi-class classification."
                }
            ],
            "sklearn/metrics/_ranking.py": [
                {
                    "chunk": {
                        "code": "def _binary_uninterpolated_average_precision(\n        y_true, y_score, pos_label=1, sample_weight=None\n    ):\n        precision, recall, _ = precision_recall_curve(\n            y_true, y_score, pos_label=pos_label, sample_weight=sample_weight\n        )\n        # Return the step function integral\n        # The following works because the last entry of precision is\n        # guaranteed to be 1, as returned by precision_recall_curve\n        return -np.sum(np.diff(recall) * np.array(precision)[:-1])\n\n    y_type = type_of_target(y_true, input_name=\"y_true\")\n\n    # Convert to Python primitive type to avoid NumPy type / Python str\n    # comparison. See https://github.com/numpy/numpy/issues/6784\n    present_labels = np.unique(y_true).tolist()\n\n    if y_type == \"binary\":\n        if len(present_labels) == 2 and pos_label not in present_labels:\n            raise ValueError(\n                f\"pos_label={pos_label} is not a valid label. It should be \"\n                f\"one of {present_labels}\"\n            )\n\n    elif y_type == \"multilabel-indicator\" and pos_label != 1:\n        raise ValueError(\n            \"Parameter pos_label is fixed to 1 for multilabel-indicator y_true. \"\n            \"Do not set pos_label or set pos_label to 1.\"\n        )\n\n    elif y_type == \"multiclass\":\n        if pos_label != 1:\n            raise ValueError(\n                \"Parameter pos_label is fixed to 1 for multiclass y_true. \"\n                \"Do not set pos_label or set pos_label to 1.\"\n            )\n        y_true = label_binarize(y_true, classes=present_labels)\n\n    average_precision = partial(\n        _binary_uninterpolated_average_precision, pos_label=pos_label\n    )\n    return _average_binary_score(\n        average_precision, y_true, y_score, average, sample_weight=sample_weight\n    )",
                        "filename": "sklearn/metrics/_ranking.py",
                        "start_index": 7615,
                        "end_index": 9401,
                        "start_line": 226,
                        "end_line": 1975,
                        "max_line": 1995,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__1.3",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the implementation of the binary uninterpolated average precision, which is a part of the calculation for mAP. It may need to be extended or modified to support multi-class classification."
                }
            ]
        },
        "solution_task": "To implement the mean average precision (mAP) feature, follow these steps:\n\n- Import necessary modules:\n  ```python\n  from sklearn.metrics import precision_recall_curve, average_precision_score\n  from sklearn.preprocessing import label_binarize\n  import numpy as np\n  ```\n\n- Define a function to calculate mAP for multi-class classification:\n  ```python\n  def mean_average_precision(y_true, y_scores, n_classes):\n      # Binarize the output\n      y_true = label_binarize(y_true, classes=[*range(n_classes)])\n      \n      # Calculate the average precision for each class\n      average_precision = dict()\n      for i in range(n_classes):\n          average_precision[i] = average_precision_score(y_true[:, i], y_scores[:, i])\n      \n      # Calculate the mean average precision across all classes\n      mAP = np.mean(list(average_precision.values()))\n      return mAP\n  ```\n\n- Use the function in the context where you have `y_test` and `y_score` available, and you know the number of classes `n_classes`:\n  ```python\n  mAP = mean_average_precision(y_test, y_score, n_classes)\n  print(f\"Mean Average Precision (mAP): {mAP}\")\n  ```\n\n- If you need to integrate this into an existing codebase, find the appropriate place where the model's predictions (`y_score`) and the true labels (`y_test`) are available, and where the number of classes (`n_classes`) is known.\n\n- If the feature needs to be exposed to users (e.g., via a command-line interface or a web service), ensure that the function is called with the correct parameters and that the result is outputted or logged appropriately.\n\n- Add unit tests to verify the correctness of the mAP calculation.\n\n- Update documentation to include the new feature.\n\nFor further research, you can look into the `sklearn.metrics` module documentation to understand more about the functions used and other potential metrics that can be calculated. Additionally, you may want to explore different strategies for handling multi-label classification if that is relevant to your project.",
        "bouncer_response": {
            "score": 9,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant suggestions that can directly lead to the successful implementation or improvement of the requested feature.",
            "supporting_evidence": "The RESPONSE provides a clear and detailed step-by-step guide on how to implement the mean average precision (mAP) feature, including code snippets and explanations of each step. It suggests importing necessary modules, defining a function to calculate mAP, and integrating the function into an existing codebase. The RESPONSE also advises on exposing the feature to users, adding unit tests, and updating documentation. Furthermore, it includes links to relevant code examples and sections of the sklearn.metrics module that could be useful for further research and understanding. The RESPONSE is comprehensive and addresses the feature request effectively, with only minor aspects such as specific integration details or user interface design left to the discretion of the implementer.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711738708.3820071,
        "result": "To implement the mean average precision (mAP) feature, follow these steps:\n\n- Import necessary modules:\n  ```python\n  from sklearn.metrics import precision_recall_curve, average_precision_score\n  from sklearn.preprocessing import label_binarize\n  import numpy as np\n  ```\n\n- Define a function to calculate mAP for multi-class classification:\n  ```python\n  def mean_average_precision(y_true, y_scores, n_classes):\n      # Binarize the output\n      y_true = label_binarize(y_true, classes=[*range(n_classes)])\n      \n      # Calculate the average precision for each class\n      average_precision = dict()\n      for i in range(n_classes):\n          average_precision[i] = average_precision_score(y_true[:, i], y_scores[:, i])\n      \n      # Calculate the mean average precision across all classes\n      mAP = np.mean(list(average_precision.values()))\n      return mAP\n  ```\n\n- Use the function in the context where you have `y_test` and `y_score` available, and you know the number of classes `n_classes`:\n  ```python\n  mAP = mean_average_precision(y_test, y_score, n_classes)\n  print(f\"Mean Average Precision (mAP): {mAP}\")\n  ```\n\n- If you need to integrate this into an existing codebase, find the appropriate place where the model's predictions (`y_score`) and the true labels (`y_test`) are available, and where the number of classes (`n_classes`) is known.\n\n- If the feature needs to be exposed to users (e.g., via a command-line interface or a web service), ensure that the function is called with the correct parameters and that the result is outputted or logged appropriately.\n\n- Add unit tests to verify the correctness of the mAP calculation.\n\n- Update documentation to include the new feature.\n\nFor further research, you can look into the `sklearn.metrics` module documentation to understand more about the functions used and other potential metrics that can be calculated. Additionally, you may want to explore different strategies for handling multi-label classification if that is relevant to your project.\n\n### Code snippets to check\n\n-  \u2192 [examples](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//examples) \u2192 [model_selection](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//examples/model_selection)- [Lines 68 - 92](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//examples/model_selection/plot_precision_recall.py#L68-L92)\n\n This snippet explains the concept of Average Precision (AP) and how it is used in binary classification, which is relevant to the implementation of mean average precision (mAP) for multi-class classification. \n\n\n   - [Lines 94 - 275](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//examples/model_selection/plot_precision_recall.py#L94-L275)\n\n This snippet demonstrates the calculation of average precision for multi-label classification and plotting of precision-recall curves, which is closely related to the implementation of mAP for multi-class classification.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn) \u2192 [metrics](https://github.com/testbed/scikit-learn__scikit-learn__1.3/tree//sklearn/metrics)- [Lines 226 - 1975](https://github.com/testbed/scikit-learn__scikit-learn__1.3/blob//sklearn/metrics/_ranking.py#L226-L1975)\n\n This snippet contains the implementation of the binary uninterpolated average precision, which is a part of the calculation for mAP. It may need to be extended or modified to support multi-class classification.",
        "context_relevance": 0.37,
        "hypotheses": [],
        "groundedness_score": 1.0,
        "qa_relevance_reasons": {
            "reason": "Criteria: The RESPONSE is relevant to the entire PROMPT and answers the PROMPT completely.\nSupporting Evidence: The RESPONSE provides a detailed explanation and steps on how to implement mean average precision (mAP) for multi-class classification, which is exactly what the PROMPT is requesting. It includes code snippets, a function definition, and instructions on how to integrate the feature into an existing codebase, which directly addresses the user's interest in adding mAP. The RESPONSE also suggests looking into the `sklearn.metrics` module for further research and provides links to code snippets that are relevant to the implementation of mAP, showing a thorough understanding of the topic and directly responding to the user's mention of preferring AP over AUC and the potential interest of other users mentioned in the PROMPT."
        },
        "qa_relevance": 1.0
    }
}