{
    "id": "scikit-learn__scikit-learn-10452",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
    "task_id": 4563020319794376963,
    "title": "Polynomial Features for sparse data",
    "body": "I'm not sure if that came up before but PolynomialFeatures doesn't support sparse data, which is not great. Should be easy but I haven't checked ;)\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.20.git",
    "requested_commit": "",
    "mayil_response": "- Add a parameter to the `PolynomialFeatures` class to support sparse input.\n- Modify the `transform` method to handle sparse matrices efficiently when the new parameter is set to `True`.\n- Ensure that the `fit`, `transform`, and `fit_transform` methods of `PolynomialFeatures` work with sparse matrices without converting them to dense arrays.\n\n```python\nclass PolynomialFeatures(TransformerMixin, BaseEstimator):\n    def __init__(self, degree=2, interaction_only=False, include_bias=True, order='C', support_sparse=False):\n        self.degree = degree\n        self.interaction_only = interaction_only\n        self.include_bias = include_bias\n        self.order = order\n        self.support_sparse = support_sparse\n\n    def fit(self, X, y=None):\n        # Add support for fitting on sparse data if support_sparse is True\n        if self.support_sparse and sparse.issparse(X):\n            # Perform necessary operations for sparse data\n            # ...\n        else:\n            # Existing fitting logic for dense data\n            # ...\n        return self\n\n    def transform(self, X):\n        # Add support for transforming sparse data if support_sparse is True\n        if self.support_sparse and sparse.issparse(X):\n            # Perform necessary operations for sparse data\n            # ...\n        else:\n            # Existing transforming logic for dense data\n            # ...\n        return XP\n\n    def fit_transform(self, X, y=None):\n        # Efficiently handle fit_transform for sparse data if support_sparse is True\n        if self.support_sparse and sparse.issparse(X):\n            return self.fit(X, y).transform(X)\n        else:\n            # Existing fit_transform logic for dense data\n            # ...\n```\n\n- Update the `_create_expansion` function to handle sparse data efficiently when `support_sparse` is `True`.\n- Ensure that the `_create_expansion` function is called with the correct parameters when `support_sparse` is `True`.\n- Test the updated `PolynomialFeatures` with sparse input to ensure that it works as expected and that the performance is acceptable.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [preprocessing](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/preprocessing)- [Lines 37 - 1167](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/preprocessing/_polynomial.py#L37-L1167)\n\n This snippet contains the implementation of the polynomial expansion for sparse matrices, which is directly related to the issue of supporting sparse data. \n\n\n   - [Lines 438 - 484](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/preprocessing/_polynomial.py#L438-L484)\n\n This snippet is part of the transform method in PolynomialFeatures that handles sparse matrices and decides when to convert them, which is relevant to the issue. \n\n\n   - [Lines 98 - 98](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/preprocessing/_polynomial.py#L98-L98)\n\n This snippet shows the class definition of PolynomialFeatures, which is the subject of the issue.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.000641,
        "snippet_processor": 0.08109000000000001,
        "issue_star_creation": 0.027979999999999998,
        "issue_star_solver": 0.06982000000000001,
        "bouncer": 0.03111
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711742308.5359108,
        "relevant_snippets": [
            {
                "code": "from time import time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.sparse as sparse\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndegree = 2\ntrials = 3\nnum_rows = 1000\ndimensionalities = np.array([1, 2, 8, 16, 32, 64])\ndensities = np.array([0.01, 0.1, 1.0])\ncsr_times = {d: np.zeros(len(dimensionalities)) for d in densities}\ndense_times = {d: np.zeros(len(dimensionalities)) for d in densities}\ntransform = PolynomialFeatures(\n    degree=degree, include_bias=False, interaction_only=False\n)\n\nfor trial in range(trials):\n    for density in densities:\n        for dim_index, dim in enumerate(dimensionalities):\n            print(trial, density, dim)\n            X_csr = sparse.random(num_rows, dim, density).tocsr()\n            X_dense = X_csr.toarray()\n            # CSR\n            t0 = time()\n            transform.fit_transform(X_csr)\n            csr_times[density][dim_index] += time() - t0\n            # Dense\n            t0 = time()\n            transform.fit_transform(X_dense)\n            dense_times[density][dim_index] += time() - t0\n\ncsr_linestyle = (0, (3, 1, 1, 1, 1, 1))  # densely dashdotdotted\ndense_linestyle = (0, ())  # solid\n\nfig, axes = plt.subplots(nrows=len(densities), ncols=1, figsize=(8, 10))\nfor density, ax in zip(densities, axes):\n    ax.plot(\n        dimensionalities,\n        csr_times[density] / trials,\n        label=\"csr\",\n        linestyle=csr_linestyle,\n    )\n    ax.plot(\n        dimensionalities,\n        dense_times[density] / trials,\n        label=\"dense\",\n        linestyle=dense_linestyle,\n    )\n    ax.set_title(\"density %0.2f, degree=%d, n_samples=%d\" % (density, degree, num_rows))\n    ax.legend()\n    ax.set_xlabel(\"Dimensionality\")\n    ax.set_ylabel(\"Time (seconds)\")\n\nplt.tight_layout()\nplt.show()",
                "filename": "benchmarks/bench_feature_expansions.py",
                "start_index": 0,
                "end_index": 1773,
                "start_line": 1,
                "end_line": 58,
                "max_line": 58,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.7
            },
            {
                "code": "def _create_expansion(X, interaction_only, deg, n_features, cumulative_size=0):\n    \"\"\"Helper function for creating and appending sparse expansion matrices\"\"\"\n\n    total_nnz = _calc_total_nnz(X.indptr, interaction_only, deg)\n    expanded_col = _calc_expanded_nnz(n_features, interaction_only, deg)\n\n    if expanded_col == 0:\n        return None\n    # This only checks whether each block needs 64bit integers upon\n    # expansion. We prefer to keep int32 indexing where we can,\n    # since currently SciPy's CSR construction downcasts when possible,\n    # so we prefer to avoid an unnecessary cast. The dtype may still\n    # change in the concatenation process if needed.\n    # See: https://github.com/scipy/scipy/issues/16569\n    max_indices = expanded_col - 1\n    max_indptr = total_nnz\n    max_int32 = np.iinfo(np.int32).max\n    needs_int64 = max(max_indices, max_indptr) > max_int32\n    index_dtype = np.int64 if needs_int64 else np.int32\n\n    # This is a pretty specific bug that is hard to work around by a user,\n    # hence we do not detail the entire bug and all possible avoidance\n    # mechnasisms. Instead we recommend upgrading scipy or shrinking their data.\n    cumulative_size += expanded_col\n    if (\n        sp_version < parse_version(\"1.8.0\")\n        and cumulative_size - 1 > max_int32\n        and not needs_int64\n    ):\n        raise ValueError(\n            \"In scipy versions `<1.8.0`, the function `scipy.sparse.hstack`\"\n            \" sometimes produces negative columns when the output shape contains\"\n            \" `n_cols` too large to be represented by a 32bit signed\"\n            \" integer. To avoid this error, either use a version\"\n            \" of scipy `>=1.8.0` or alter the `PolynomialFeatures`\"\n            \" transformer to produce fewer than 2^31 output features.\"\n        )\n\n    # Result of the expansion, modified in place by the\n    # `_csr_polynomial_expansion` routine.\n    expanded_data = np.empty(shape=total_nnz, dtype=X.data.dtype)\n    expanded_indices = np.empty(shape=total_nnz, dtype=index_dtype)\n    expanded_indptr = np.empty(shape=X.indptr.shape[0], dtype=index_dtype)\n    _csr_polynomial_expansion(\n        X.data,\n        X.indices,\n        X.indptr,\n        X.shape[1],\n        expanded_data,\n        expanded_indices,\n        expanded_indptr,\n        interaction_only,\n        deg,\n    )\n    return sparse.csr_matrix(\n        (expanded_data, expanded_indices, expanded_indptr),\n        shape=(X.indptr.shape[0] - 1, expanded_col),\n        dtype=X.dtype,\n    )",
                "filename": "sklearn/preprocessing/_polynomial.py",
                "start_index": 917,
                "end_index": 3428,
                "start_line": 37,
                "end_line": 1167,
                "max_line": 1169,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.8
            },
            {
                "code": "def _sparse_fit(self, X, strategy, missing_values, fill_value):\n        \"\"\"Fit the transformer on sparse data.\"\"\"\n        missing_mask = _get_mask(X, missing_values)\n        mask_data = missing_mask.data\n        n_implicit_zeros = X.shape[0] - np.diff(X.indptr)\n\n        statistics = np.empty(X.shape[1])\n\n        if strategy == \"constant\":\n            # for constant strategy, self.statistics_ is used to store\n            # fill_value in each column\n            statistics.fill(fill_value)\n        else:\n            for i in range(X.shape[1]):\n                column = X.data[X.indptr[i] : X.indptr[i + 1]]\n                mask_column = mask_data[X.indptr[i] : X.indptr[i + 1]]\n                column = column[~mask_column]\n\n                # combine explicit and implicit zeros\n                mask_zeros = _get_mask(column, 0)\n                column = column[~mask_zeros]\n                n_explicit_zeros = mask_zeros.sum()\n                n_zeros = n_implicit_zeros[i] + n_explicit_zeros\n\n                if len(column) == 0 and self.keep_empty_features:\n                    # in case we want to keep columns with only missing values.\n                    statistics[i] = 0\n                else:\n                    if strategy == \"mean\":\n                        s = column.size + n_zeros\n                        statistics[i] = np.nan if s == 0 else column.sum() / s\n\n                    elif strategy == \"median\":\n                        statistics[i] = _get_median(column, n_zeros)\n\n                    elif strategy == \"most_frequent\":\n                        statistics[i] = _most_frequent(column, 0, n_zeros)\n\n        super()._fit_indicator(missing_mask)\n\n        return statistics",
                "filename": "sklearn/impute/_base.py",
                "start_index": 15066,
                "end_index": 16757,
                "start_line": 411,
                "end_line": 451,
                "max_line": 1053,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "def check_array_api_input_and_values(\n    name,\n    estimator_orig,\n    array_namespace,\n    device=None,\n    dtype=\"float64\",\n):\n    return check_array_api_input(\n        name,\n        estimator_orig,\n        array_namespace=array_namespace,\n        device=device,\n        dtype=dtype,\n        check_values=True,\n    )\n\n\ndef check_estimator_sparse_data(name, estimator_orig):\n    rng = np.random.RandomState(0)\n    X = rng.uniform(size=(40, 3))\n    X[X < 0.8] = 0\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    X_csr = sparse.csr_matrix(X)\n    y = (4 * rng.uniform(size=40)).astype(int)\n    # catch deprecation warnings\n    with ignore_warnings(category=FutureWarning):\n        estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n    tags = _safe_tags(estimator_orig)\n    for matrix_format, X in _generate_sparse_matrix(X_csr):\n        # catch deprecation warnings\n        with ignore_warnings(category=FutureWarning):\n            estimator = clone(estimator_orig)\n            if name in [\"Scaler\", \"StandardScaler\"]:\n                estimator.set_params(with_mean=False)\n        # fit and predict\n        if \"64\" in matrix_format:\n            err_msg = (\n                f\"Estimator {name} doesn't seem to support {matrix_format} \"\n                \"matrix, and is not failing gracefully, e.g. by using \"\n                \"check_array(X, accept_large_sparse=False)\"\n            )\n        else:\n            err_msg = (\n                f\"Estimator {name} doesn't seem to fail gracefully on sparse \"\n                \"data: error message should state explicitly that sparse \"\n                \"input is not supported if this is not the case.\"\n            )\n        with raises(\n            (TypeError, ValueError),\n            match=[\"sparse\", \"Sparse\"],\n            may_pass=True,\n            err_msg=err_msg,\n        ):\n            with ignore_warnings(category=FutureWarning):\n                estimator.fit(X, y)\n            if hasattr(estimator, \"predict\"):\n                pred = estimator.predict(X)\n                if tags[\"multioutput_only\"]:\n                    assert pred.shape == (X.shape[0], 1)\n                else:\n                    assert pred.shape == (X.shape[0],)\n            if hasattr(estimator, \"predict_proba\"):\n                probs = estimator.predict_proba(X)\n                if tags[\"binary_only\"]:\n                    expected_probs_shape = (X.shape[0], 2)\n                else:\n                    expected_probs_shape = (X.shape[0], 4)\n                assert probs.shape == expected_probs_shape",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 36016,
                "end_index": 38577,
                "start_line": 1025,
                "end_line": 1093,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "if sparse.issparse(X) and X.format == \"csr\":\n            if self._max_degree > 3:\n                return self.transform(X.tocsc()).tocsr()\n            to_stack = []\n            if self.include_bias:\n                to_stack.append(\n                    sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n                )\n            if self._min_degree <= 1 and self._max_degree > 0:\n                to_stack.append(X)\n\n            cumulative_size = sum(mat.shape[1] for mat in to_stack)\n            for deg in range(max(2, self._min_degree), self._max_degree + 1):\n                expanded = _create_expansion(\n                    X=X,\n                    interaction_only=self.interaction_only,\n                    deg=deg,\n                    n_features=n_features,\n                    cumulative_size=cumulative_size,\n                )\n                if expanded is not None:\n                    to_stack.append(expanded)\n                    cumulative_size += expanded.shape[1]\n            if len(to_stack) == 0:\n                # edge case: deal with empty matrix\n                XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n            else:\n                # `scipy.sparse.hstack` breaks in scipy<1.9.2\n                # when `n_output_features_ > max_int32`\n                all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n                if (\n                    sp_version < parse_version(\"1.9.2\")\n                    and self.n_output_features_ > max_int32\n                    and all_int32\n                ):\n                    raise ValueError(  # pragma: no cover\n                        \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n                        \" produces negative columns when:\\n1. The output shape contains\"\n                        \" `n_cols` too large to be represented by a 32bit signed\"\n                        \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n                        \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n                        \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n                        \" transformer to produce fewer than 2^31 output features\"\n                    )\n                XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n        elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n            return self.transform(X.tocsr()).tocsc()",
                "filename": "sklearn/preprocessing/_polynomial.py",
                "start_index": 16495,
                "end_index": 18941,
                "start_line": 438,
                "end_line": 484,
                "max_line": 1169,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 1.0
            },
            {
                "code": "\"\"\"\nThis file contains preprocessing tools based on polynomials.\n\"\"\"\nimport collections\nfrom itertools import chain, combinations\nfrom itertools import combinations_with_replacement as combinations_w_r\nfrom numbers import Integral\n\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.interpolate import BSpline\nfrom scipy.special import comb\n\nfrom ..base import BaseEstimator, TransformerMixin, _fit_context\nfrom ..utils import check_array\nfrom ..utils._param_validation import Interval, StrOptions\nfrom ..utils.fixes import parse_version, sp_version\nfrom ..utils.stats import _weighted_percentile\nfrom ..utils.validation import (\n    FLOAT_DTYPES,\n    _check_feature_names_in,\n    _check_sample_weight,\n    check_is_fitted,\n)\nfrom ._csr_polynomial_expansion import (\n    _calc_expanded_nnz,\n    _calc_total_nnz,\n    _csr_polynomial_expansion,\n)\n\n__all__ = [\n    \"PolynomialFeatures\",\n    \"SplineTransformer\",\n]",
                "filename": "sklearn/preprocessing/_polynomial.py",
                "start_index": 0,
                "end_index": 914,
                "start_line": 1,
                "end_line": 1159,
                "max_line": 1169,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.7
            },
            {
                "code": "elif sparse.issparse(X):\n            combinations = self._combinations(\n                n_features=n_features,\n                min_degree=self._min_degree,\n                max_degree=self._max_degree,\n                interaction_only=self.interaction_only,\n                include_bias=self.include_bias,\n            )\n            columns = []\n            for combi in combinations:\n                if combi:\n                    out_col = 1\n                    for col_idx in combi:\n                        out_col = X[:, col_idx].multiply(out_col)\n                    columns.append(out_col)\n                else:\n                    bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n                    columns.append(bias)\n            XP = sparse.hstack(columns, dtype=X.dtype).tocsc()",
                "filename": "sklearn/preprocessing/_polynomial.py",
                "start_index": 18950,
                "end_index": 19739,
                "start_line": 485,
                "end_line": 503,
                "max_line": 1169,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 1.0
            },
            {
                "code": "class PolynomialFeatures(TransformerMixin, BaseEstimator):",
                "filename": "sklearn/preprocessing/_polynomial.py",
                "start_index": 3431,
                "end_index": 3489,
                "start_line": 98,
                "end_line": 98,
                "max_line": 1169,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.1
            },
            {
                "code": "assert array(\"i\").itemsize == 4, (\n            \"sizeof(int) != 4 on your platform; please report this at\"\n            \" https://github.com/scikit-learn/scikit-learn/issues and\"\n            \" include the output from platform.platform() in your bug report\"\n        )\n\n        dtype = self.dtype\n        if fitting:\n            feature_names = []\n            vocab = {}\n        else:\n            feature_names = self.feature_names_\n            vocab = self.vocabulary_\n\n        transforming = True\n\n        # Process everything as sparse regardless of setting\n        X = [X] if isinstance(X, Mapping) else X\n\n        indices = array(\"i\")\n        indptr = [0]\n        # XXX we could change values to an array.array as well, but it\n        # would require (heuristic) conversion of dtype to typecode...\n        values = []\n\n        # collect all the possible feature names and build sparse matrix at\n        # same time\n        for x in X:\n            for f, v in x.items():\n                if isinstance(v, str):\n                    feature_name = \"%s%s%s\" % (f, self.separator, v)\n                    v = 1\n                elif isinstance(v, Number) or (v is None):\n                    feature_name = f\n                elif not isinstance(v, Mapping) and isinstance(v, Iterable):\n                    feature_name = None\n                    self._add_iterable_element(\n                        f,\n                        v,\n                        feature_names,\n                        vocab,\n                        fitting=fitting,\n                        transforming=transforming,\n                        indices=indices,\n                        values=values,\n                    )\n                else:\n                    raise TypeError(\n                        f\"Unsupported value Type {type(v)} \"\n                        f\"for {f}: {v}.\\n\"\n                        f\"{type(v)} objects are not supported.\"\n                    )\n\n                if feature_name is not None:\n                    if fitting and feature_name not in vocab:\n                        vocab[feature_name] = len(feature_names)\n                        feature_names.append(feature_name)\n\n                    if feature_name in vocab:\n                        indices.append(vocab[feature_name])\n                        values.append(self.dtype(v))\n\n            indptr.append(len(indices))\n\n        if len(indptr) == 1:\n            raise ValueError(\"Sample sequence X is empty.\")\n\n        indices = np.frombuffer(indices, dtype=np.intc)\n        shape = (len(indptr) - 1, len(vocab))\n\n        result_matrix = sp.csr_matrix(\n            (values, indices, indptr), shape=shape, dtype=dtype\n        )\n\n        # Sort everything if asked",
                "filename": "sklearn/feature_extraction/_dict_vectorizer.py",
                "start_index": 6855,
                "end_index": 9563,
                "start_line": 195,
                "end_line": 269,
                "max_line": 444,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "for degree in [3, 4, 5]:\n    model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = make_pipeline(SplineTransformer(n_knots=4, degree=3), Ridge(alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\nplt.show()\n\n# %%\n# This shows nicely that higher degree polynomials can fit the data better. But\n# at the same time, too high powers can show unwanted oscillatory behaviour\n# and are particularly dangerous for extrapolation beyond the range of fitted\n# data. This is an advantage of B-splines. They usually fit the data as well as\n# polynomials and show very nice and smooth behaviour. They have also good\n# options to control the extrapolation, which defaults to continue with a\n# constant. Note that most often, you would rather increase the number of knots\n# but keep ``degree=3``.\n#\n# In order to give more insights into the generated feature bases, we plot all\n# columns of both transformers separately.\n\nfig, axes = plt.subplots(ncols=2, figsize=(16, 5))\npft = PolynomialFeatures(degree=3).fit(X_train)\naxes[0].plot(x_plot, pft.transform(X_plot))\naxes[0].legend(axes[0].lines, [f\"degree {n}\" for n in range(4)])\naxes[0].set_title(\"PolynomialFeatures\")\n\nsplt = SplineTransformer(n_knots=4, degree=3).fit(X_train)\naxes[1].plot(x_plot, splt.transform(X_plot))\naxes[1].legend(axes[1].lines, [f\"spline {n}\" for n in range(6)])\naxes[1].set_title(\"SplineTransformer\")\n\n# plot knots of spline\nknots = splt.bsplines_[0].t\naxes[1].vlines(knots[3:-3], ymin=0, ymax=0.8, linestyles=\"dashed\")\nplt.show()\n\n# %%\n# In the left plot, we recognize the lines corresponding to simple monomials\n# from ``x**0`` to ``x**3``. In the right figure, we see the six B-spline\n# basis functions of ``degree=3`` and also the four knot positions that were\n# chosen during ``fit``. Note that there are ``degree`` number of additional\n# knots each to the left and to the right of the fitted interval. These are\n# there for technical reasons, so we refrain from showing them. Every basis\n# function has local support and is continued as a constant beyond the fitted\n# range. This extrapolating behaviour could be changed by the argument\n# ``extrapolation``.\n\n# %%\n# Periodic Splines\n# ----------------\n# In the previous example we saw the limitations of polynomials and splines for\n# extrapolation beyond the range of the training observations. In some\n# settings, e.g. with seasonal effects, we expect a periodic continuation of\n# the underlying signal. Such effects can be modelled using periodic splines,\n# which have equal function value and equal derivatives at the first and last\n# knot. In the following case we show how periodic splines provide a better fit",
                "filename": "examples/linear_model/plot_polynomial_interpolation.py",
                "start_index": 2966,
                "end_index": 5903,
                "start_line": 93,
                "end_line": 157,
                "max_line": 215,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                "sha": "",
                "context_relevance": 0.2
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/preprocessing/_polynomial.py": [
                {
                    "chunk": {
                        "code": "def _create_expansion(X, interaction_only, deg, n_features, cumulative_size=0):\n    \"\"\"Helper function for creating and appending sparse expansion matrices\"\"\"\n\n    total_nnz = _calc_total_nnz(X.indptr, interaction_only, deg)\n    expanded_col = _calc_expanded_nnz(n_features, interaction_only, deg)\n\n    if expanded_col == 0:\n        return None\n    # This only checks whether each block needs 64bit integers upon\n    # expansion. We prefer to keep int32 indexing where we can,\n    # since currently SciPy's CSR construction downcasts when possible,\n    # so we prefer to avoid an unnecessary cast. The dtype may still\n    # change in the concatenation process if needed.\n    # See: https://github.com/scipy/scipy/issues/16569\n    max_indices = expanded_col - 1\n    max_indptr = total_nnz\n    max_int32 = np.iinfo(np.int32).max\n    needs_int64 = max(max_indices, max_indptr) > max_int32\n    index_dtype = np.int64 if needs_int64 else np.int32\n\n    # This is a pretty specific bug that is hard to work around by a user,\n    # hence we do not detail the entire bug and all possible avoidance\n    # mechnasisms. Instead we recommend upgrading scipy or shrinking their data.\n    cumulative_size += expanded_col\n    if (\n        sp_version < parse_version(\"1.8.0\")\n        and cumulative_size - 1 > max_int32\n        and not needs_int64\n    ):\n        raise ValueError(\n            \"In scipy versions `<1.8.0`, the function `scipy.sparse.hstack`\"\n            \" sometimes produces negative columns when the output shape contains\"\n            \" `n_cols` too large to be represented by a 32bit signed\"\n            \" integer. To avoid this error, either use a version\"\n            \" of scipy `>=1.8.0` or alter the `PolynomialFeatures`\"\n            \" transformer to produce fewer than 2^31 output features.\"\n        )\n\n    # Result of the expansion, modified in place by the\n    # `_csr_polynomial_expansion` routine.\n    expanded_data = np.empty(shape=total_nnz, dtype=X.data.dtype)\n    expanded_indices = np.empty(shape=total_nnz, dtype=index_dtype)\n    expanded_indptr = np.empty(shape=X.indptr.shape[0], dtype=index_dtype)\n    _csr_polynomial_expansion(\n        X.data,\n        X.indices,\n        X.indptr,\n        X.shape[1],\n        expanded_data,\n        expanded_indices,\n        expanded_indptr,\n        interaction_only,\n        deg,\n    )\n    return sparse.csr_matrix(\n        (expanded_data, expanded_indices, expanded_indptr),\n        shape=(X.indptr.shape[0] - 1, expanded_col),\n        dtype=X.dtype,\n    )",
                        "filename": "sklearn/preprocessing/_polynomial.py",
                        "start_index": 917,
                        "end_index": 3428,
                        "start_line": 37,
                        "end_line": 1167,
                        "max_line": 1169,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the implementation of the polynomial expansion for sparse matrices, which is directly related to the issue of supporting sparse data."
                },
                {
                    "chunk": {
                        "code": "if sparse.issparse(X) and X.format == \"csr\":\n            if self._max_degree > 3:\n                return self.transform(X.tocsc()).tocsr()\n            to_stack = []\n            if self.include_bias:\n                to_stack.append(\n                    sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n                )\n            if self._min_degree <= 1 and self._max_degree > 0:\n                to_stack.append(X)\n\n            cumulative_size = sum(mat.shape[1] for mat in to_stack)\n            for deg in range(max(2, self._min_degree), self._max_degree + 1):\n                expanded = _create_expansion(\n                    X=X,\n                    interaction_only=self.interaction_only,\n                    deg=deg,\n                    n_features=n_features,\n                    cumulative_size=cumulative_size,\n                )\n                if expanded is not None:\n                    to_stack.append(expanded)\n                    cumulative_size += expanded.shape[1]\n            if len(to_stack) == 0:\n                # edge case: deal with empty matrix\n                XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n            else:\n                # `scipy.sparse.hstack` breaks in scipy<1.9.2\n                # when `n_output_features_ > max_int32`\n                all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n                if (\n                    sp_version < parse_version(\"1.9.2\")\n                    and self.n_output_features_ > max_int32\n                    and all_int32\n                ):\n                    raise ValueError(  # pragma: no cover\n                        \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n                        \" produces negative columns when:\\n1. The output shape contains\"\n                        \" `n_cols` too large to be represented by a 32bit signed\"\n                        \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n                        \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n                        \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n                        \" transformer to produce fewer than 2^31 output features\"\n                    )\n                XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n        elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n            return self.transform(X.tocsr()).tocsc()",
                        "filename": "sklearn/preprocessing/_polynomial.py",
                        "start_index": 16495,
                        "end_index": 18941,
                        "start_line": 438,
                        "end_line": 484,
                        "max_line": 1169,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet is part of the transform method in PolynomialFeatures that handles sparse matrices and decides when to convert them, which is relevant to the issue."
                },
                {
                    "chunk": {
                        "code": "class PolynomialFeatures(TransformerMixin, BaseEstimator):",
                        "filename": "sklearn/preprocessing/_polynomial.py",
                        "start_index": 3431,
                        "end_index": 3489,
                        "start_line": 98,
                        "end_line": 98,
                        "max_line": 1169,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.20",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet shows the class definition of PolynomialFeatures, which is the subject of the issue."
                }
            ]
        },
        "solution_task": "- Add a parameter to the `PolynomialFeatures` class to support sparse input.\n- Modify the `transform` method to handle sparse matrices efficiently when the new parameter is set to `True`.\n- Ensure that the `fit`, `transform`, and `fit_transform` methods of `PolynomialFeatures` work with sparse matrices without converting them to dense arrays.\n\n```python\nclass PolynomialFeatures(TransformerMixin, BaseEstimator):\n    def __init__(self, degree=2, interaction_only=False, include_bias=True, order='C', support_sparse=False):\n        self.degree = degree\n        self.interaction_only = interaction_only\n        self.include_bias = include_bias\n        self.order = order\n        self.support_sparse = support_sparse\n\n    def fit(self, X, y=None):\n        # Add support for fitting on sparse data if support_sparse is True\n        if self.support_sparse and sparse.issparse(X):\n            # Perform necessary operations for sparse data\n            # ...\n        else:\n            # Existing fitting logic for dense data\n            # ...\n        return self\n\n    def transform(self, X):\n        # Add support for transforming sparse data if support_sparse is True\n        if self.support_sparse and sparse.issparse(X):\n            # Perform necessary operations for sparse data\n            # ...\n        else:\n            # Existing transforming logic for dense data\n            # ...\n        return XP\n\n    def fit_transform(self, X, y=None):\n        # Efficiently handle fit_transform for sparse data if support_sparse is True\n        if self.support_sparse and sparse.issparse(X):\n            return self.fit(X, y).transform(X)\n        else:\n            # Existing fit_transform logic for dense data\n            # ...\n```\n\n- Update the `_create_expansion` function to handle sparse data efficiently when `support_sparse` is `True`.\n- Ensure that the `_create_expansion` function is called with the correct parameters when `support_sparse` is `True`.\n- Test the updated `PolynomialFeatures` with sparse input to ensure that it works as expected and that the performance is acceptable.",
        "bouncer_response": {
            "score": 9,
            "criteria": "RESPONSE provides clear, comprehensive guidance or solutions for the described bug.",
            "supporting_evidence": "The RESPONSE offers a detailed and actionable plan to modify the `PolynomialFeatures` class to support sparse data. It suggests adding a new parameter to handle sparse matrices and outlines changes to the `fit`, `transform`, and `fit_transform` methods to accommodate sparse input without converting them to dense arrays. The RESPONSE also advises updating the `_create_expansion` function and testing the changes to ensure they work as expected. The RESPONSE is highly relevant and provides a clear path to resolving the issue, although it does not include the actual implementation details for the operations on sparse data, which is why it does not score a perfect 10.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711742502.509435,
        "result": "- Add a parameter to the `PolynomialFeatures` class to support sparse input.\n- Modify the `transform` method to handle sparse matrices efficiently when the new parameter is set to `True`.\n- Ensure that the `fit`, `transform`, and `fit_transform` methods of `PolynomialFeatures` work with sparse matrices without converting them to dense arrays.\n\n```python\nclass PolynomialFeatures(TransformerMixin, BaseEstimator):\n    def __init__(self, degree=2, interaction_only=False, include_bias=True, order='C', support_sparse=False):\n        self.degree = degree\n        self.interaction_only = interaction_only\n        self.include_bias = include_bias\n        self.order = order\n        self.support_sparse = support_sparse\n\n    def fit(self, X, y=None):\n        # Add support for fitting on sparse data if support_sparse is True\n        if self.support_sparse and sparse.issparse(X):\n            # Perform necessary operations for sparse data\n            # ...\n        else:\n            # Existing fitting logic for dense data\n            # ...\n        return self\n\n    def transform(self, X):\n        # Add support for transforming sparse data if support_sparse is True\n        if self.support_sparse and sparse.issparse(X):\n            # Perform necessary operations for sparse data\n            # ...\n        else:\n            # Existing transforming logic for dense data\n            # ...\n        return XP\n\n    def fit_transform(self, X, y=None):\n        # Efficiently handle fit_transform for sparse data if support_sparse is True\n        if self.support_sparse and sparse.issparse(X):\n            return self.fit(X, y).transform(X)\n        else:\n            # Existing fit_transform logic for dense data\n            # ...\n```\n\n- Update the `_create_expansion` function to handle sparse data efficiently when `support_sparse` is `True`.\n- Ensure that the `_create_expansion` function is called with the correct parameters when `support_sparse` is `True`.\n- Test the updated `PolynomialFeatures` with sparse input to ensure that it works as expected and that the performance is acceptable.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn) \u2192 [preprocessing](https://github.com/testbed/scikit-learn__scikit-learn__0.20/tree//sklearn/preprocessing)- [Lines 37 - 1167](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/preprocessing/_polynomial.py#L37-L1167)\n\n This snippet contains the implementation of the polynomial expansion for sparse matrices, which is directly related to the issue of supporting sparse data. \n\n\n   - [Lines 438 - 484](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/preprocessing/_polynomial.py#L438-L484)\n\n This snippet is part of the transform method in PolynomialFeatures that handles sparse matrices and decides when to convert them, which is relevant to the issue. \n\n\n   - [Lines 98 - 98](https://github.com/testbed/scikit-learn__scikit-learn__0.20/blob//sklearn/preprocessing/_polynomial.py#L98-L98)\n\n This snippet shows the class definition of PolynomialFeatures, which is the subject of the issue.",
        "context_relevance": 0.51,
        "hypotheses": [],
        "groundedness_score": 1.0,
        "qa_relevance_reasons": {
            "reason": "Criteria: The response is relevant to the entire prompt and answers the prompt completely.\nSupporting Evidence: The response directly addresses the concern about `PolynomialFeatures` not supporting sparse data by suggesting modifications to the class to handle sparse matrices efficiently. It provides a detailed code example showing how to implement these changes, including the addition of a new parameter and modifications to the `fit`, `transform`, and `fit_transform` methods. The response also includes references to specific lines of code in the sklearn repository that are relevant to the issue, demonstrating a thorough understanding of the problem and how to address it. The response is fully relevant to the prompt and provides a complete answer to the issue raised."
        },
        "qa_relevance": 1.0
    }
}