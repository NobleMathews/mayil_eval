{
    "id": "scikit-learn__scikit-learn-13910",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
    "task_id": 1222244228599237938,
    "title": "Untreated overflow (?) for float32 in euclidean_distances new in sklearn 21.1",
    "body": "#### Description\r\nI am using euclidean distances in a project and after updating, the result is wrong for just one of several datasets. When comparing it to scipy.spatial.distance.cdist one can see that in version 21.1 it behaves substantially different to 20.3.\r\n\r\nThe matrix is an ndarray with size (100,10000) with float32.\r\n\r\n#### Steps/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.metrics.pairwise import euclidean_distances\r\nimport sklearn\r\nfrom scipy.spatial.distance import cdist\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\nX = np.load('wont.npy')\r\n\r\ned = euclidean_distances(X)\r\ned_ = cdist(X, X, metric='euclidean')\r\n\r\nplt.plot(np.sort(ed.flatten()), label='euclidean_distances sklearn {}'.format(sklearn.__version__))\r\nplt.plot(np.sort(ed_.flatten()), label='cdist')\r\nplt.yscale('symlog', linthreshy=1E3)\r\nplt.legend()\r\nplt.show()\r\n\r\n```\r\nThe data are in this zip\r\n[wont.zip](https://github.com/scikit-learn/scikit-learn/files/3194196/wont.zip)\r\n\r\n\r\n\r\n#### Expected Results\r\nCan be found when using sklearn 20.3, both behave identical.\r\n[sklearn20.pdf](https://github.com/scikit-learn/scikit-learn/files/3194197/sklearn20.pdf)\r\n\r\n\r\n#### Actual Results\r\nWhen using version 21.1 has many 0 entries and some unreasonably high entries \r\n[sklearn_v21.pdf](https://github.com/scikit-learn/scikit-learn/files/3194198/sklearn_v21.pdf)\r\n\r\n\r\n#### Versions\r\nSklearn 21\r\nSystem:\r\n    python: 3.6.7 (default, Oct 22 2018, 11:32:17)  [GCC 8.2.0]\r\nexecutable: /home/lenz/PycharmProjects/pyrolmm/venv_sklearn21/bin/python3\r\n   machine: Linux-4.15.0-50-generic-x86_64-with-Ubuntu-18.04-bionic\r\n\r\nBLAS:\r\n    macros: HAVE_CBLAS=None, NO_ATLAS_INFO=-1\r\n  lib_dirs: /usr/lib/x86_64-linux-gnu\r\ncblas_libs: cblas\r\n\r\nPython deps:\r\n       pip: 9.0.1\r\nsetuptools: 39.0.1\r\n   sklearn: 0.21.1\r\n     numpy: 1.16.3\r\n     scipy: 1.3.0\r\n    Cython: None\r\n    pandas: None\r\n\r\nFor sklearn 20.3 the versions are:\r\nSystem:\r\n    python: 3.6.7 (default, Oct 22 2018, 11:32:17)  [GCC 8.2.0]\r\nexecutable: /home/lenz/PycharmProjects/pyrolmm/venv_sklearn20/bin/python3\r\n   machine: Linux-4.15.0-50-generic-x86_64-with-Ubuntu-18.04-bionic\r\n\r\nBLAS:\r\n    macros: HAVE_CBLAS=None, NO_ATLAS_INFO=-1\r\n  lib_dirs: /usr/lib/x86_64-linux-gnu\r\ncblas_libs: cblas\r\n\r\nPython deps:\r\n       pip: 9.0.1\r\nsetuptools: 39.0.1\r\n   sklearn: 0.20.3\r\n     numpy: 1.16.3\r\n     scipy: 1.3.0\r\n    Cython: None\r\n    pandas: None\r\n\r\n\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.22.git",
    "requested_commit": "",
    "mayil_response": "- The issue is likely related to the changes in the `euclidean_distances` function between `sklearn 0.20.3` and `sklearn 0.21.1`.\n- The problem is specific to `float32` data, which suggests numerical precision issues.\n- The `_euclidean_distances_upcast` function in `sklearn/metrics/pairwise.py` is a potential source of the issue, as it handles the upcasting of `float32` data to `float64` to minimize precision issues.\n- The `_euclidean_distances` function in `sklearn/metrics/pairwise.py` is also relevant, as it decides whether to use the upcasting function based on the data type.\n\nTo resolve the issue:\n- Verify that the upcasting from `float32` to `float64` in `_euclidean_distances_upcast` is performed correctly without introducing errors in the distance calculations.\n- Check if the batch size calculation in `_euclidean_distances_upcast` is appropriate and does not lead to memory issues or incorrect distance calculations.\n- Ensure that the addition of `XX_chunk` and `YY_chunk` to the distance matrix `d` in `_euclidean_distances_upcast` does not introduce overflow or underflow issues.\n- Confirm that the final conversion of the distance matrix back to `float32` does not lead to loss of significant digits or precision issues.\n- If the issue persists, consider using `float64` data type for the entire computation if memory and performance constraints allow.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [metrics](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/metrics)- [Lines 533 - 1320](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/metrics/pairwise.py#L533-L1320)\n\n This snippet contains the implementation of '_euclidean_distances_upcast' which is directly related to the computation of Euclidean distances on float32 data, which is the core of the issue. \n\n\n   - [Lines 350 - 398](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/metrics/pairwise.py#L350-L398)\n\n This snippet includes the '_euclidean_distances' function which calls the upcast function and is part of the Euclidean distance calculation process, potentially relevant to the issue.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0005394999999999999,
        "snippet_processor": 0.08021,
        "issue_star_creation": 0.03338,
        "issue_star_solver": 0.08541,
        "bouncer": 0.023790000000000002
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711739736.870741,
        "relevant_snippets": [
            {
                "code": "# combinations of dense and sparse inputs on float32 and float64 datasets, except\n# the sparse-dense and dense-sparse combinations for the Euclidean and Squared\n# Euclidean Distance metrics.\n# A detailed list of the impacted estimators can be found in the\n# :ref:`changelog <changes_1_2>`.",
                "filename": "examples/release_highlights/plot_release_highlights_1_2_0.py",
                "start_index": 5985,
                "end_index": 6274,
                "start_line": 162,
                "end_line": 166,
                "max_line": 166,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"\"\"\nAgglomerative clustering with different metrics\n===============================================\n\nDemonstrates the effect of different metrics on the hierarchical clustering.\n\nThe example is engineered to show the effect of the choice of different\nmetrics. It is applied to waveforms, which can be seen as\nhigh-dimensional vector. Indeed, the difference between metrics is\nusually more pronounced in high dimension (in particular for euclidean\nand cityblock).\n\nWe generate data from three groups of waveforms. Two of the waveforms\n(waveform 1 and waveform 2) are proportional one to the other. The cosine\ndistance is invariant to a scaling of the data, as a result, it cannot\ndistinguish these two waveforms. Thus even with no noise, clustering\nusing this distance will not separate out waveform 1 and 2.\n\nWe add observation noise to these waveforms. We generate very sparse\nnoise: only 6% of the time points contain noise. As a result, the\nl1 norm of this noise (ie \"cityblock\" distance) is much smaller than it's\nl2 norm (\"euclidean\" distance). This can be seen on the inter-class\ndistance matrices: the values on the diagonal, that characterize the\nspread of the class, are much bigger for the Euclidean distance than for\nthe cityblock distance.\n\nWhen we apply clustering to the data, we find that the clustering\nreflects what was in the distance matrices. Indeed, for the Euclidean\ndistance, the classes are ill-separated because of the noise, and thus\nthe clustering does not separate the waveforms. For the cityblock\ndistance, the separation is good and the waveform classes are recovered.\nFinally, the cosine distance does not separate at all waveform 1 and 2,\nthus the clustering puts them in the same cluster.\n\n\"\"\"\n\n# Author: Gael Varoquaux\n# License: BSD 3-Clause or CC-0\n\nimport matplotlib.patheffects as PathEffects\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import pairwise_distances\n\nnp.random.seed(0)\n\n# Generate waveform data\nn_features = 2000\nt = np.pi * np.linspace(0, 1, n_features)\n\n\ndef sqr(x):\n    return np.sign(np.cos(x))\n\n\nX = list()\ny = list()\nfor i, (phi, a) in enumerate([(0.5, 0.15), (0.5, 0.6), (0.3, 0.2)]):\n    for _ in range(30):\n        phase_noise = 0.01 * np.random.normal()\n        amplitude_noise = 0.04 * np.random.normal()\n        additional_noise = 1 - 2 * np.random.rand(n_features)\n        # Make the noise sparse\n        additional_noise[np.abs(additional_noise) < 0.997] = 0\n\n        X.append(\n            12\n            * (\n                (a + amplitude_noise) * (sqr(6 * (t + phi + phase_noise)))\n                + additional_noise\n            )\n        )\n        y.append(i)\n\nX = np.array(X)\ny = np.array(y)\n\nn_clusters = 3\n\nlabels = (\"Waveform 1\", \"Waveform 2\", \"Waveform 3\")\n\ncolors = [\"#f7bd01\", \"#377eb8\", \"#f781bf\"]\n\n# Plot the ground-truth labelling\nplt.figure()\nplt.axes([0, 0, 1, 1])",
                "filename": "examples/cluster/plot_agglomerative_clustering_metrics.py",
                "start_index": 0,
                "end_index": 2925,
                "start_line": 1,
                "end_line": 138,
                "max_line": 146,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "# are Gaussian distributed with mean of 0 but feature 1 has a standard\n# deviation equal to 2 and feature 2 has a standard deviation equal to 1. Next,\n# 25 samples are replaced with Gaussian outlier samples where feature 1 has\n# a standard deviation equal to 1 and feature 2 has a standard deviation equal\n# to 7.\n\nimport numpy as np\n\n# for consistent results\nnp.random.seed(7)\n\nn_samples = 125\nn_outliers = 25\nn_features = 2\n\n# generate Gaussian data of shape (125, 2)\ngen_cov = np.eye(n_features)\ngen_cov[0, 0] = 2.0\nX = np.dot(np.random.randn(n_samples, n_features), gen_cov)\n# add some outliers\noutliers_cov = np.eye(n_features)\noutliers_cov[np.arange(1, n_features), np.arange(1, n_features)] = 7.0\nX[-n_outliers:] = np.dot(np.random.randn(n_outliers, n_features), outliers_cov)\n\n# %%\n# Comparison of results\n# ---------------------\n#\n# Below, we fit MCD and MLE based covariance estimators to our data and print\n# the estimated covariance matrices. Note that the estimated variance of\n# feature 2 is much higher with the MLE based estimator (7.5) than\n# that of the MCD robust estimator (1.2). This shows that the MCD based\n# robust estimator is much more resistant to the outlier samples, which were\n# designed to have a much larger variance in feature 2.\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.covariance import EmpiricalCovariance, MinCovDet\n\n# fit a MCD robust estimator to data\nrobust_cov = MinCovDet().fit(X)\n# fit a MLE estimator to data\nemp_cov = EmpiricalCovariance().fit(X)\nprint(\n    \"Estimated covariance matrix:\\nMCD (Robust):\\n{}\\nMLE:\\n{}\".format(\n        robust_cov.covariance_, emp_cov.covariance_\n    )\n)\n\n# %%\n# To better visualize the difference, we plot contours of the\n# Mahalanobis distances calculated by both methods. Notice that the robust\n# MCD based Mahalanobis distances fit the inlier black points much better,\n# whereas the MLE based distances are more influenced by the outlier\n# red points.\n\nfig, ax = plt.subplots(figsize=(10, 5))\n# Plot data set\ninlier_plot = ax.scatter(X[:, 0], X[:, 1], color=\"black\", label=\"inliers\")\noutlier_plot = ax.scatter(\n    X[:, 0][-n_outliers:], X[:, 1][-n_outliers:], color=\"red\", label=\"outliers\"\n)\nax.set_xlim(ax.get_xlim()[0], 10.0)\nax.set_title(\"Mahalanobis distances of a contaminated data set\")\n\n# Create meshgrid of feature 1 and feature 2 values\nxx, yy = np.meshgrid(\n    np.linspace(plt.xlim()[0], plt.xlim()[1], 100),\n    np.linspace(plt.ylim()[0], plt.ylim()[1], 100),\n)\nzz = np.c_[xx.ravel(), yy.ravel()]\n# Calculate the MLE based Mahalanobis distances of the meshgrid\nmahal_emp_cov = emp_cov.mahalanobis(zz)\nmahal_emp_cov = mahal_emp_cov.reshape(xx.shape)\nemp_cov_contour = plt.contour(\n    xx, yy, np.sqrt(mahal_emp_cov), cmap=plt.cm.PuBu_r, linestyles=\"dashed\"\n)\n# Calculate the MCD based Mahalanobis distances\nmahal_robust_cov = robust_cov.mahalanobis(zz)\nmahal_robust_cov = mahal_robust_cov.reshape(xx.shape)",
                "filename": "examples/covariance/plot_mahalanobis_distances.py",
                "start_index": 2950,
                "end_index": 5854,
                "start_line": 70,
                "end_line": 149,
                "max_line": 212,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "def _euclidean_distances_upcast(X, XX=None, Y=None, YY=None, batch_size=None):\n    \"\"\"Euclidean distances between X and Y.\n\n    Assumes X and Y have float32 dtype.\n    Assumes XX and YY have float64 dtype or are None.\n\n    X and Y are upcast to float64 by chunks, which size is chosen to limit\n    memory increase by approximately 10% (at least 10MiB).\n    \"\"\"\n    n_samples_X = X.shape[0]\n    n_samples_Y = Y.shape[0]\n    n_features = X.shape[1]\n\n    distances = np.empty((n_samples_X, n_samples_Y), dtype=np.float32)\n\n    if batch_size is None:\n        x_density = X.nnz / np.prod(X.shape) if issparse(X) else 1\n        y_density = Y.nnz / np.prod(Y.shape) if issparse(Y) else 1\n\n        # Allow 10% more memory than X, Y and the distance matrix take (at\n        # least 10MiB)\n        maxmem = max(\n            (\n                (x_density * n_samples_X + y_density * n_samples_Y) * n_features\n                + (x_density * n_samples_X * y_density * n_samples_Y)\n            )\n            / 10,\n            10 * 2**17,\n        )\n\n        # The increase amount of memory in 8-byte blocks is:\n        # - x_density * batch_size * n_features (copy of chunk of X)\n        # - y_density * batch_size * n_features (copy of chunk of Y)\n        # - batch_size * batch_size (chunk of distance matrix)\n        # Hence x\u00b2 + (xd+yd)kx = M, where x=batch_size, k=n_features, M=maxmem\n        #                                 xd=x_density and yd=y_density\n        tmp = (x_density + y_density) * n_features\n        batch_size = (-tmp + np.sqrt(tmp**2 + 4 * maxmem)) / 2\n        batch_size = max(int(batch_size), 1)\n\n    x_batches = gen_batches(n_samples_X, batch_size)\n\n    for i, x_slice in enumerate(x_batches):\n        X_chunk = X[x_slice].astype(np.float64)\n        if XX is None:\n            XX_chunk = row_norms(X_chunk, squared=True)[:, np.newaxis]\n        else:\n            XX_chunk = XX[x_slice]\n\n        y_batches = gen_batches(n_samples_Y, batch_size)\n\n        for j, y_slice in enumerate(y_batches):\n            if X is Y and j < i:\n                # when X is Y the distance matrix is symmetric so we only need\n                # to compute half of it.\n                d = distances[y_slice, x_slice].T\n\n            else:\n                Y_chunk = Y[y_slice].astype(np.float64)\n                if YY is None:\n                    YY_chunk = row_norms(Y_chunk, squared=True)[np.newaxis, :]\n                else:\n                    YY_chunk = YY[:, y_slice]\n\n                d = -2 * safe_sparse_dot(X_chunk, Y_chunk.T, dense_output=True)\n                d += XX_chunk\n                d += YY_chunk\n\n            distances[x_slice, y_slice] = d.astype(np.float32, copy=False)\n\n    return distances",
                "filename": "sklearn/metrics/pairwise.py",
                "start_index": 17860,
                "end_index": 20557,
                "start_line": 533,
                "end_line": 1320,
                "max_line": 2425,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 1.0
            },
            {
                "code": "def _euclidean_distances(X, Y, X_norm_squared=None, Y_norm_squared=None, squared=False):\n    \"\"\"Computational part of euclidean_distances\n\n    Assumes inputs are already checked.\n\n    If norms are passed as float32, they are unused. If arrays are passed as\n    float32, norms needs to be recomputed on upcast chunks.\n    TODO: use a float64 accumulator in row_norms to avoid the latter.\n    \"\"\"\n    if X_norm_squared is not None:\n        if X_norm_squared.dtype == np.float32:\n            XX = None\n        else:\n            XX = X_norm_squared.reshape(-1, 1)\n    elif X.dtype == np.float32:\n        XX = None\n    else:\n        XX = row_norms(X, squared=True)[:, np.newaxis]\n\n    if Y is X:\n        YY = None if XX is None else XX.T\n    else:\n        if Y_norm_squared is not None:\n            if Y_norm_squared.dtype == np.float32:\n                YY = None\n            else:\n                YY = Y_norm_squared.reshape(1, -1)\n        elif Y.dtype == np.float32:\n            YY = None\n        else:\n            YY = row_norms(Y, squared=True)[np.newaxis, :]\n\n    if X.dtype == np.float32:\n        # To minimize precision issues with float32, we compute the distance\n        # matrix on chunks of X and Y upcast to float64\n        distances = _euclidean_distances_upcast(X, XX, Y, YY)\n    else:\n        # if dtype is already float64, no need to chunk and upcast\n        distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)\n        distances += XX\n        distances += YY\n    np.maximum(distances, 0, out=distances)\n\n    # Ensure that distances between vectors and themselves are set to 0.0.\n    # This may not be the case due to floating point rounding errors.\n    if X is Y:\n        np.fill_diagonal(distances, 0)\n\n    return distances if squared else np.sqrt(distances, out=distances)",
                "filename": "sklearn/metrics/pairwise.py",
                "start_index": 11874,
                "end_index": 13667,
                "start_line": 350,
                "end_line": 398,
                "max_line": 2425,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 1.0
            },
            {
                "code": "\"\"\"\n=========================\nMulti-dimensional scaling\n=========================\n\nAn illustration of the metric and non-metric MDS on generated noisy data.\n\nThe reconstructed points using the metric MDS and non metric MDS are slightly\nshifted to avoid overlapping.\n\n\"\"\"\n\n# Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>\n# License: BSD\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn import manifold\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import euclidean_distances\n\nEPSILON = np.finfo(np.float32).eps\nn_samples = 20\nseed = np.random.RandomState(seed=3)\nX_true = seed.randint(0, 20, 2 * n_samples).astype(float)\nX_true = X_true.reshape((n_samples, 2))\n# Center the data\nX_true -= X_true.mean()\n\nsimilarities = euclidean_distances(X_true)\n\n# Add noise to the similarities\nnoise = np.random.rand(n_samples, n_samples)\nnoise = noise + noise.T\nnoise[np.arange(noise.shape[0]), np.arange(noise.shape[0])] = 0\nsimilarities += noise\n\nmds = manifold.MDS(\n    n_components=2,\n    max_iter=3000,\n    eps=1e-9,\n    random_state=seed,\n    dissimilarity=\"precomputed\",\n    n_jobs=1,\n    normalized_stress=\"auto\",\n)\npos = mds.fit(similarities).embedding_\n\nnmds = manifold.MDS(\n    n_components=2,\n    metric=False,\n    max_iter=3000,\n    eps=1e-12,\n    dissimilarity=\"precomputed\",\n    random_state=seed,\n    n_jobs=1,\n    n_init=1,\n    normalized_stress=\"auto\",\n)\nnpos = nmds.fit_transform(similarities, init=pos)\n\n# Rescale the data\npos *= np.sqrt((X_true**2).sum()) / np.sqrt((pos**2).sum())\nnpos *= np.sqrt((X_true**2).sum()) / np.sqrt((npos**2).sum())\n\n# Rotate the data\nclf = PCA(n_components=2)\nX_true = clf.fit_transform(X_true)\n\npos = clf.fit_transform(pos)\n\nnpos = clf.fit_transform(npos)\n\nfig = plt.figure(1)\nax = plt.axes([0.0, 0.0, 1.0, 1.0])\n\ns = 100\nplt.scatter(X_true[:, 0], X_true[:, 1], color=\"navy\", s=s, lw=0, label=\"True Position\")\nplt.scatter(pos[:, 0], pos[:, 1], color=\"turquoise\", s=s, lw=0, label=\"MDS\")\nplt.scatter(npos[:, 0], npos[:, 1], color=\"darkorange\", s=s, lw=0, label=\"NMDS\")\nplt.legend(scatterpoints=1, loc=\"best\", shadow=False)\n\nsimilarities = similarities.max() / (similarities + EPSILON) * 100\nnp.fill_diagonal(similarities, 0)\n# Plot the edges\nstart_idx, end_idx = np.where(pos)\n# a sequence of (*line0*, *line1*, *line2*), where::\n#            linen = (x0, y0), (x1, y1), ... (xm, ym)\nsegments = [\n    [X_true[i, :], X_true[j, :]] for i in range(len(pos)) for j in range(len(pos))\n]\nvalues = np.abs(similarities)\nlc = LineCollection(\n    segments, zorder=0, cmap=plt.cm.Blues, norm=plt.Normalize(0, values.max())\n)\nlc.set_array(similarities.flatten())\nlc.set_linewidths(np.full(len(segments), 0.5))\nax.add_collection(lc)\n\nplt.show()",
                "filename": "examples/manifold/plot_mds.py",
                "start_index": 0,
                "end_index": 2757,
                "start_line": 1,
                "end_line": 102,
                "max_line": 102,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "\"metrics._pairwise_distances_reduction\": [\n        {\n            \"sources\": [\"_datasets_pair.pyx.tp\", \"_datasets_pair.pxd.tp\"],\n            \"language\": \"c++\",\n            \"include_np\": True,\n            \"extra_compile_args\": [\"-std=c++11\"],\n        },\n        {\n            \"sources\": [\"_middle_term_computer.pyx.tp\", \"_middle_term_computer.pxd.tp\"],\n            \"language\": \"c++\",\n            \"extra_compile_args\": [\"-std=c++11\"],\n        },\n        {\n            \"sources\": [\"_base.pyx.tp\", \"_base.pxd.tp\"],\n            \"language\": \"c++\",\n            \"include_np\": True,\n            \"extra_compile_args\": [\"-std=c++11\"],\n        },\n        {\n            \"sources\": [\"_argkmin.pyx.tp\", \"_argkmin.pxd.tp\"],\n            \"language\": \"c++\",\n            \"include_np\": True,\n            \"extra_compile_args\": [\"-std=c++11\"],\n        },\n        {\n            \"sources\": [\"_argkmin_classmode.pyx.tp\"],\n            \"language\": \"c++\",\n            \"include_np\": True,\n            \"extra_compile_args\": [\"-std=c++11\"],\n        },\n        {\n            \"sources\": [\"_radius_neighbors.pyx.tp\", \"_radius_neighbors.pxd.tp\"],\n            \"language\": \"c++\",\n            \"include_np\": True,\n            \"extra_compile_args\": [\"-std=c++11\"],\n        },\n    ],\n    \"preprocessing\": [\n        {\"sources\": [\"_csr_polynomial_expansion.pyx\"]},\n        {\n            \"sources\": [\"_target_encoder_fast.pyx\"],\n            \"include_np\": True,\n            \"language\": \"c++\",\n            \"extra_compile_args\": [\"-std=c++11\"],\n        },\n    ],\n    \"neighbors\": [\n        {\"sources\": [\"_ball_tree.pyx\"], \"include_np\": True},\n        {\"sources\": [\"_kd_tree.pyx\"], \"include_np\": True},\n        {\"sources\": [\"_partition_nodes.pyx\"], \"language\": \"c++\", \"include_np\": True},\n        {\"sources\": [\"_quad_tree.pyx\"], \"include_np\": True},\n    ],",
                "filename": "setup.py",
                "start_index": 9151,
                "end_index": 10957,
                "start_line": 262,
                "end_line": 592,
                "max_line": 626,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "def fit_and_plot_model(X_plot, y, clf, ax):\n    clf.fit(X_plot, y)\n    disp = DecisionBoundaryDisplay.from_estimator(\n        clf,\n        X_plot,\n        response_method=\"predict\",\n        alpha=0.5,\n        ax=ax,\n    )\n    disp.ax_.scatter(X_plot[\"proline\"], X_plot[\"hue\"], c=y, s=20, edgecolor=\"k\")\n    disp.ax_.set_xlim((X_plot[\"proline\"].min(), X_plot[\"proline\"].max()))\n    disp.ax_.set_ylim((X_plot[\"hue\"].min(), X_plot[\"hue\"].max()))\n    return disp.ax_\n\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 6))\n\nfit_and_plot_model(X_plot, y, clf, ax1)\nax1.set_title(\"KNN without scaling\")\n\nfit_and_plot_model(X_plot_scaled, y, clf, ax2)\nax2.set_xlabel(\"scaled proline\")\nax2.set_ylabel(\"scaled hue\")\n_ = ax2.set_title(\"KNN with scaling\")\n\n# %%\n# Here the decision boundary shows that fitting scaled or non-scaled data lead\n# to completely different models. The reason is that the variable \"proline\" has\n# values which vary between 0 and 1,000; whereas the variable \"hue\" varies\n# between 1 and 10. Because of this, distances between samples are mostly\n# impacted by differences in values of \"proline\", while values of the \"hue\" will\n# be comparatively ignored. If one uses\n# :class:`~sklearn.preprocessing.StandardScaler` to normalize this database,\n# both scaled values lay approximately between -3 and 3 and the neighbors\n# structure will be impacted more or less equivalently by both variables.\n#\n# Effect of rescaling on a PCA dimensional reduction\n# ==================================================\n#\n# Dimensional reduction using :class:`~sklearn.decomposition.PCA` consists of\n# finding the features that maximize the variance. If one feature varies more\n# than the others only because of their respective scales,\n# :class:`~sklearn.decomposition.PCA` would determine that such feature\n# dominates the direction of the principal components.\n#\n# We can inspect the first principal components using all the original features:\n\nimport pandas as pd\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2).fit(X_train)\nscaled_pca = PCA(n_components=2).fit(scaled_X_train)\nX_train_transformed = pca.transform(X_train)\nX_train_std_transformed = scaled_pca.transform(scaled_X_train)\n\nfirst_pca_component = pd.DataFrame(\n    pca.components_[0], index=X.columns, columns=[\"without scaling\"]\n)\nfirst_pca_component[\"with scaling\"] = scaled_pca.components_[0]\nfirst_pca_component.plot.bar(\n    title=\"Weights of the first principal component\", figsize=(6, 8)\n)\n\n_ = plt.tight_layout()\n\n# %%\n# Indeed we find that the \"proline\" feature dominates the direction of the first\n# principal component without scaling, being about two orders of magnitude above\n# the other features. This is contrasted when observing the first principal\n# component for the scaled version of the data, where the orders of magnitude\n# are roughly the same across all the features.\n#\n# We can visualize the distribution of the principal components in both cases:",
                "filename": "examples/preprocessing/plot_scaling_importance.py",
                "start_index": 2900,
                "end_index": 5851,
                "start_line": 77,
                "end_line": 150,
                "max_line": 256,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.0
            },
            {
                "code": "\"\"\"\n=================================================\nOutlier detection with Local Outlier Factor (LOF)\n=================================================\n\nThe Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection\nmethod which computes the local density deviation of a given data point with\nrespect to its neighbors. It considers as outliers the samples that have a\nsubstantially lower density than their neighbors. This example shows how to use\nLOF for outlier detection which is the default use case of this estimator in\nscikit-learn. Note that when LOF is used for outlier detection it has no\n`predict`, `decision_function` and `score_samples` methods. See the :ref:`User\nGuide <outlier_detection>` for details on the difference between outlier\ndetection and novelty detection and how to use LOF for novelty detection.\n\nThe number of neighbors considered (parameter `n_neighbors`) is typically set 1)\ngreater than the minimum number of samples a cluster has to contain, so that\nother samples can be local outliers relative to this cluster, and 2) smaller\nthan the maximum number of close by samples that can potentially be local\noutliers. In practice, such information is generally not available, and taking\n`n_neighbors=20` appears to work well in general.\n\n\"\"\"\n\n# %%\n# Generate data with outliers\n# ---------------------------\n\n# %%\nimport numpy as np\n\nnp.random.seed(42)\n\nX_inliers = 0.3 * np.random.randn(100, 2)\nX_inliers = np.r_[X_inliers + 2, X_inliers - 2]\nX_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\nX = np.r_[X_inliers, X_outliers]\n\nn_outliers = len(X_outliers)\nground_truth = np.ones(len(X), dtype=int)\nground_truth[-n_outliers:] = -1\n\n# %%\n# Fit the model for outlier detection (default)\n# ---------------------------------------------\n#\n# Use `fit_predict` to compute the predicted labels of the training samples\n# (when LOF is used for outlier detection, the estimator has no `predict`,\n# `decision_function` and `score_samples` methods).\n\nfrom sklearn.neighbors import LocalOutlierFactor\n\nclf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\ny_pred = clf.fit_predict(X)\nn_errors = (y_pred != ground_truth).sum()\nX_scores = clf.negative_outlier_factor_\n\n# %%\n# Plot results\n# ------------\n\n# %%\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerPathCollection\n\n\ndef update_legend_marker_size(handle, orig):\n    \"Customize size of the legend marker\"\n    handle.update_from(orig)\n    handle.set_sizes([20])\n\n\nplt.scatter(X[:, 0], X[:, 1], color=\"k\", s=3.0, label=\"Data points\")\n# plot circles with radius proportional to the outlier scores\nradius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())\nscatter = plt.scatter(\n    X[:, 0],\n    X[:, 1],\n    s=1000 * radius,\n    edgecolors=\"r\",\n    facecolors=\"none\",\n    label=\"Outlier scores\",\n)\nplt.axis(\"tight\")\nplt.xlim((-5, 5))\nplt.ylim((-5, 5))\nplt.xlabel(\"prediction errors: %d\" % (n_errors))",
                "filename": "examples/neighbors/plot_lof_outlier_detection.py",
                "start_index": 0,
                "end_index": 2943,
                "start_line": 1,
                "end_line": 87,
                "max_line": 92,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "def _argmin_min_reduce(dist, start):\n    # `start` is specified in the signature but not used. This is because the higher\n    # order `pairwise_distances_chunked` function needs reduction functions that are\n    # passed as argument to have a two arguments signature.\n    indices = dist.argmin(axis=1)\n    values = dist[np.arange(dist.shape[0]), indices]\n    return indices, values\n\n\ndef _argmin_reduce(dist, start):\n    # `start` is specified in the signature but not used. This is because the higher\n    # order `pairwise_distances_chunked` function needs reduction functions that are\n    # passed as argument to have a two arguments signature.\n    return dist.argmin(axis=1)\n\n\n_VALID_METRICS = [\n    \"euclidean\",\n    \"l2\",\n    \"l1\",\n    \"manhattan\",\n    \"cityblock\",\n    \"braycurtis\",\n    \"canberra\",\n    \"chebyshev\",\n    \"correlation\",\n    \"cosine\",\n    \"dice\",\n    \"hamming\",\n    \"jaccard\",\n    \"mahalanobis\",\n    \"matching\",\n    \"minkowski\",\n    \"rogerstanimoto\",\n    \"russellrao\",\n    \"seuclidean\",\n    \"sokalmichener\",\n    \"sokalsneath\",\n    \"sqeuclidean\",\n    \"yule\",\n    \"wminkowski\",\n    \"nan_euclidean\",\n    \"haversine\",\n]\nif sp_base_version < parse_version(\"1.11\"):  # pragma: no cover\n    # Deprecated in SciPy 1.9 and removed in SciPy 1.11\n    _VALID_METRICS += [\"kulsinski\"]\nif sp_base_version < parse_version(\"1.9\"):\n    # Deprecated in SciPy 1.0 and removed in SciPy 1.9\n    _VALID_METRICS += [\"matching\"]\n\n_NAN_METRICS = [\"nan_euclidean\"]",
                "filename": "sklearn/metrics/pairwise.py",
                "start_index": 20560,
                "end_index": 22016,
                "start_line": 606,
                "end_line": 657,
                "max_line": 2425,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                "sha": "",
                "context_relevance": 0.2
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/metrics/pairwise.py": [
                {
                    "chunk": {
                        "code": "def _euclidean_distances_upcast(X, XX=None, Y=None, YY=None, batch_size=None):\n    \"\"\"Euclidean distances between X and Y.\n\n    Assumes X and Y have float32 dtype.\n    Assumes XX and YY have float64 dtype or are None.\n\n    X and Y are upcast to float64 by chunks, which size is chosen to limit\n    memory increase by approximately 10% (at least 10MiB).\n    \"\"\"\n    n_samples_X = X.shape[0]\n    n_samples_Y = Y.shape[0]\n    n_features = X.shape[1]\n\n    distances = np.empty((n_samples_X, n_samples_Y), dtype=np.float32)\n\n    if batch_size is None:\n        x_density = X.nnz / np.prod(X.shape) if issparse(X) else 1\n        y_density = Y.nnz / np.prod(Y.shape) if issparse(Y) else 1\n\n        # Allow 10% more memory than X, Y and the distance matrix take (at\n        # least 10MiB)\n        maxmem = max(\n            (\n                (x_density * n_samples_X + y_density * n_samples_Y) * n_features\n                + (x_density * n_samples_X * y_density * n_samples_Y)\n            )\n            / 10,\n            10 * 2**17,\n        )\n\n        # The increase amount of memory in 8-byte blocks is:\n        # - x_density * batch_size * n_features (copy of chunk of X)\n        # - y_density * batch_size * n_features (copy of chunk of Y)\n        # - batch_size * batch_size (chunk of distance matrix)\n        # Hence x\u00b2 + (xd+yd)kx = M, where x=batch_size, k=n_features, M=maxmem\n        #                                 xd=x_density and yd=y_density\n        tmp = (x_density + y_density) * n_features\n        batch_size = (-tmp + np.sqrt(tmp**2 + 4 * maxmem)) / 2\n        batch_size = max(int(batch_size), 1)\n\n    x_batches = gen_batches(n_samples_X, batch_size)\n\n    for i, x_slice in enumerate(x_batches):\n        X_chunk = X[x_slice].astype(np.float64)\n        if XX is None:\n            XX_chunk = row_norms(X_chunk, squared=True)[:, np.newaxis]\n        else:\n            XX_chunk = XX[x_slice]\n\n        y_batches = gen_batches(n_samples_Y, batch_size)\n\n        for j, y_slice in enumerate(y_batches):\n            if X is Y and j < i:\n                # when X is Y the distance matrix is symmetric so we only need\n                # to compute half of it.\n                d = distances[y_slice, x_slice].T\n\n            else:\n                Y_chunk = Y[y_slice].astype(np.float64)\n                if YY is None:\n                    YY_chunk = row_norms(Y_chunk, squared=True)[np.newaxis, :]\n                else:\n                    YY_chunk = YY[:, y_slice]\n\n                d = -2 * safe_sparse_dot(X_chunk, Y_chunk.T, dense_output=True)\n                d += XX_chunk\n                d += YY_chunk\n\n            distances[x_slice, y_slice] = d.astype(np.float32, copy=False)\n\n    return distances",
                        "filename": "sklearn/metrics/pairwise.py",
                        "start_index": 17860,
                        "end_index": 20557,
                        "start_line": 533,
                        "end_line": 1320,
                        "max_line": 2425,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the implementation of '_euclidean_distances_upcast' which is directly related to the computation of Euclidean distances on float32 data, which is the core of the issue."
                },
                {
                    "chunk": {
                        "code": "def _euclidean_distances(X, Y, X_norm_squared=None, Y_norm_squared=None, squared=False):\n    \"\"\"Computational part of euclidean_distances\n\n    Assumes inputs are already checked.\n\n    If norms are passed as float32, they are unused. If arrays are passed as\n    float32, norms needs to be recomputed on upcast chunks.\n    TODO: use a float64 accumulator in row_norms to avoid the latter.\n    \"\"\"\n    if X_norm_squared is not None:\n        if X_norm_squared.dtype == np.float32:\n            XX = None\n        else:\n            XX = X_norm_squared.reshape(-1, 1)\n    elif X.dtype == np.float32:\n        XX = None\n    else:\n        XX = row_norms(X, squared=True)[:, np.newaxis]\n\n    if Y is X:\n        YY = None if XX is None else XX.T\n    else:\n        if Y_norm_squared is not None:\n            if Y_norm_squared.dtype == np.float32:\n                YY = None\n            else:\n                YY = Y_norm_squared.reshape(1, -1)\n        elif Y.dtype == np.float32:\n            YY = None\n        else:\n            YY = row_norms(Y, squared=True)[np.newaxis, :]\n\n    if X.dtype == np.float32:\n        # To minimize precision issues with float32, we compute the distance\n        # matrix on chunks of X and Y upcast to float64\n        distances = _euclidean_distances_upcast(X, XX, Y, YY)\n    else:\n        # if dtype is already float64, no need to chunk and upcast\n        distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)\n        distances += XX\n        distances += YY\n    np.maximum(distances, 0, out=distances)\n\n    # Ensure that distances between vectors and themselves are set to 0.0.\n    # This may not be the case due to floating point rounding errors.\n    if X is Y:\n        np.fill_diagonal(distances, 0)\n\n    return distances if squared else np.sqrt(distances, out=distances)",
                        "filename": "sklearn/metrics/pairwise.py",
                        "start_index": 11874,
                        "end_index": 13667,
                        "start_line": 350,
                        "end_line": 398,
                        "max_line": 2425,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.22",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes the '_euclidean_distances' function which calls the upcast function and is part of the Euclidean distance calculation process, potentially relevant to the issue."
                }
            ]
        },
        "solution_task": "- The issue is likely related to the changes in the `euclidean_distances` function between `sklearn 0.20.3` and `sklearn 0.21.1`.\n- The problem is specific to `float32` data, which suggests numerical precision issues.\n- The `_euclidean_distances_upcast` function in `sklearn/metrics/pairwise.py` is a potential source of the issue, as it handles the upcasting of `float32` data to `float64` to minimize precision issues.\n- The `_euclidean_distances` function in `sklearn/metrics/pairwise.py` is also relevant, as it decides whether to use the upcasting function based on the data type.\n\nTo resolve the issue:\n- Verify that the upcasting from `float32` to `float64` in `_euclidean_distances_upcast` is performed correctly without introducing errors in the distance calculations.\n- Check if the batch size calculation in `_euclidean_distances_upcast` is appropriate and does not lead to memory issues or incorrect distance calculations.\n- Ensure that the addition of `XX_chunk` and `YY_chunk` to the distance matrix `d` in `_euclidean_distances_upcast` does not introduce overflow or underflow issues.\n- Confirm that the final conversion of the distance matrix back to `float32` does not lead to loss of significant digits or precision issues.\n- If the issue persists, consider using `float64` data type for the entire computation if memory and performance constraints allow.",
        "bouncer_response": {
            "score": 8,
            "criteria": "The RESPONSE delivers precise, actionable, and highly relevant hints and solutions that can directly lead to resolving the issues outlined in the PROMPT.",
            "supporting_evidence": "The RESPONSE accurately identifies the potential source of the bug as being related to changes in the `euclidean_distances` function between versions of scikit-learn and suggests that the issue may be due to numerical precision problems with `float32` data. It provides specific and actionable advice on verifying the upcasting process, checking batch size calculations, and ensuring that no overflow or underflow issues are introduced. The RESPONSE also suggests confirming the final conversion back to `float32` and considering the use of `float64` if the problem persists. Additionally, the RESPONSE provides direct links to the relevant code snippets, which is highly valuable for developers looking to investigate and fix the issue. The RESPONSE does not address all aspects of the bug, such as the presence of many zero entries and some unreasonably high entries in the results, which prevents it from scoring a 10.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711739942.632672,
        "result": "- The issue is likely related to the changes in the `euclidean_distances` function between `sklearn 0.20.3` and `sklearn 0.21.1`.\n- The problem is specific to `float32` data, which suggests numerical precision issues.\n- The `_euclidean_distances_upcast` function in `sklearn/metrics/pairwise.py` is a potential source of the issue, as it handles the upcasting of `float32` data to `float64` to minimize precision issues.\n- The `_euclidean_distances` function in `sklearn/metrics/pairwise.py` is also relevant, as it decides whether to use the upcasting function based on the data type.\n\nTo resolve the issue:\n- Verify that the upcasting from `float32` to `float64` in `_euclidean_distances_upcast` is performed correctly without introducing errors in the distance calculations.\n- Check if the batch size calculation in `_euclidean_distances_upcast` is appropriate and does not lead to memory issues or incorrect distance calculations.\n- Ensure that the addition of `XX_chunk` and `YY_chunk` to the distance matrix `d` in `_euclidean_distances_upcast` does not introduce overflow or underflow issues.\n- Confirm that the final conversion of the distance matrix back to `float32` does not lead to loss of significant digits or precision issues.\n- If the issue persists, consider using `float64` data type for the entire computation if memory and performance constraints allow.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn) \u2192 [metrics](https://github.com/testbed/scikit-learn__scikit-learn__0.22/tree//sklearn/metrics)- [Lines 533 - 1320](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/metrics/pairwise.py#L533-L1320)\n\n This snippet contains the implementation of '_euclidean_distances_upcast' which is directly related to the computation of Euclidean distances on float32 data, which is the core of the issue. \n\n\n   - [Lines 350 - 398](https://github.com/testbed/scikit-learn__scikit-learn__0.22/blob//sklearn/metrics/pairwise.py#L350-L398)\n\n This snippet includes the '_euclidean_distances' function which calls the upcast function and is part of the Euclidean distance calculation process, potentially relevant to the issue.",
        "context_relevance": 0.34,
        "hypotheses": [],
        "groundedness_score": 1.0,
        "qa_relevance_reasons": {
            "reason": "Criteria: The RESPONSE is relevant to the entire PROMPT and provides a detailed analysis of the potential source of the issue, including specific functions and lines of code that may be responsible for the problem. It also offers steps to resolve the issue, which are directly related to the problem described in the PROMPT.\nSupporting Evidence: The RESPONSE addresses the issue of numerical precision with `float32` data in the `euclidean_distances` function after an update from `sklearn 0.20.3` to `sklearn 0.21.1`. It identifies specific functions (`_euclidean_distances_upcast` and `_euclidean_distances`) that could be the source of the problem and provides a thorough explanation of what to check within these functions. The RESPONSE also includes a link to the relevant code snippets, which is directly related to the user's issue with the `euclidean_distances` function. The RESPONSE does not receive a 10 because it does not explicitly mention the observed symptoms of the issue (0 entries and unreasonably high entries), although it implies them by discussing precision issues."
        },
        "qa_relevance": 0.9
    }
}