{
    "id": "scikit-learn__scikit-learn-13915",
    "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
    "task_id": 7291326653071784238,
    "title": "utils.sparsefuncs.min_max_axis gives TypeError when input is large csc matrix when OS is 32 bit Windows",
    "body": "#### Description\r\nOn 32 bit versions of Windows, when `min_max_axis` is called on a csc matrix where `indptr.dtype` is int64, an error is produced. This prevents [this](https://github.com/scikit-learn/scikit-learn/pull/13704/) pull request passing tests (see [here](https://github.com/scikit-learn/scikit-learn/pull/13704/checks?check_run_id=109958355)).\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nimport scipy.sparse as sp\r\nfrom sklearn.utils.sparsefuncs import min_max_axis\r\n\r\nX = sp.csc_matrix([[1,2],[3,4]])\r\nX.indptr = X.indptr.astype('int64')\r\n\r\nY = sp.csr_matrix([[1,2],[3,4]])\r\nY.indptr = Y.indptr.astype('int64')\r\n\r\nprint(min_max_axis(Y, 0))\r\nprint(min_max_axis(X, 0))\r\n```\r\n\r\n#### Expected Results\r\n```\r\n(array([1, 2], dtype=int32), array([3, 4], dtype=int32))\r\n(array([1, 2], dtype=int32), array([3, 4], dtype=int32))\r\n```\r\n\r\n#### Actual Results\r\n```\r\n(array([1, 2], dtype=int32), array([3, 4], dtype=int32))\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\rod\\bug.py\", line 12, in <module>\r\n    print(min_max_axis(X, 0))\r\n  File \"C:\\Users\\rod\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\sklearn\\utils\\sparsefuncs.py\", line 434, in min_max_axis\r\n    return _sparse_min_max(X, axis=axis)\r\n  File \"C:\\Users\\rod\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\sklearn\\utils\\sparsefuncs.py\", line 395, in _sparse_min_max\r\n    return (_sparse_min_or_max(X, axis, np.minimum),\r\n  File \"C:\\Users\\rod\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\sklearn\\utils\\sparsefuncs.py\", line 389, in _sparse_min_or_max\r\n    return _min_or_max_axis(X, axis, min_or_max)\r\n  File \"C:\\Users\\rod\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\sklearn\\utils\\sparsefuncs.py\", line 359, in _min_or_max_axis\r\n    major_index, value = _minor_reduce(mat, min_or_max)\r\n  File \"C:\\Users\\rod\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\sklearn\\utils\\sparsefuncs.py\", line 344, in _minor_reduce\r\n    value = ufunc.reduceat(X.data, X.indptr[major_index])\r\nTypeError: Cannot cast array data from dtype('int64') to dtype('int32') according to the rule 'safe'\r\n```\r\n\r\n#### Versions\r\nSystem:\r\n    python: 3.5.4 (v3.5.4:3f56838, Aug  8 2017, 02:07:06) [MSC v.1900 32 bit (Intel)]\r\n   machine: Windows-10-10.0.17763-SP0\r\nexecutable: C:\\Users\\rod\\AppData\\Local\\Programs\\Python\\Python35-32\\pythonw.exe\r\n\r\nBLAS:\r\n    macros: \r\ncblas_libs: cblas\r\n  lib_dirs: \r\n\r\nPython deps:\r\n    Cython: 0.29.7\r\n     scipy: 1.2.1\r\nsetuptools: 28.8.0\r\n     numpy: 1.16.3\r\n       pip: 19.1\r\n    pandas: None\r\n   sklearn: 0.20.3\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/scikit-learn__scikit-learn__0.21.git",
    "requested_commit": "",
    "mayil_response": "- Change the `astype` call to use `'int32'` instead of `'int64'` to match the system's integer size:\n  ```python\n  X.indptr = X.indptr.astype('int32')\n  ```\n- Ensure that all other parts of the code that might be casting `indptr` to `'int64'` are also casting to `'int32'` instead on a 32-bit system.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/utils)- [Lines 415 - 478](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/utils/sparsefuncs.py#L415-L478)\n\n This snippet contains the '_minor_reduce' function where the TypeError occurs due to the casting issue on 32-bit systems. The reinitialization of 'X' in this function is directly related to the issue. \n\n\n   - [Lines 481 - 568](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/utils/sparsefuncs.py#L481-L568)\n\n This snippet contains the 'min_max_axis' function which is the entry point for the error. Understanding how this function invokes '_sparse_min_max' and subsequently '_minor_reduce' is necessary for solving the issue.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/utils)- [Lines 1001 - 2214](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/utils/validation.py#L1001-L2214)\n\n This snippet includes the '_check_large_sparse' function which raises a ValueError if a sparse matrix with 64-bit indices is used and 'accept_large_sparse' is False. This is relevant as the issue occurs with 64-bit indices on a 32-bit system.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0008225,
        "snippet_processor": 0.051160000000000004,
        "issue_star_creation": 0.035809999999999995,
        "issue_star_solver": 0.07194,
        "bouncer": 0.02488
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711740446.503223,
        "relevant_snippets": [
            {
                "code": "def _minor_reduce(X, ufunc):\n    major_index = np.flatnonzero(np.diff(X.indptr))\n\n    # reduceat tries casts X.indptr to intp, which errors\n    # if it is int64 on a 32 bit system.\n    # Reinitializing prevents this where possible, see #13737\n    X = type(X)((X.data, X.indices, X.indptr), shape=X.shape)\n    value = ufunc.reduceat(X.data, X.indptr[major_index])\n    return major_index, value\n\n\ndef _min_or_max_axis(X, axis, min_or_max):\n    N = X.shape[axis]\n    if N == 0:\n        raise ValueError(\"zero-size array to reduction operation\")\n    M = X.shape[1 - axis]\n    mat = X.tocsc() if axis == 0 else X.tocsr()\n    mat.sum_duplicates()\n    major_index, value = _minor_reduce(mat, min_or_max)\n    not_full = np.diff(mat.indptr)[major_index] < N\n    value[not_full] = min_or_max(value[not_full], 0)\n    mask = value != 0\n    major_index = np.compress(mask, major_index)\n    value = np.compress(mask, value)\n\n    if axis == 0:\n        res = sp.coo_matrix(\n            (value, (np.zeros(len(value)), major_index)), dtype=X.dtype, shape=(1, M)\n        )\n    else:\n        res = sp.coo_matrix(\n            (value, (major_index, np.zeros(len(value)))), dtype=X.dtype, shape=(M, 1)\n        )\n    return res.A.ravel()\n\n\ndef _sparse_min_or_max(X, axis, min_or_max):\n    if axis is None:\n        if 0 in X.shape:\n            raise ValueError(\"zero-size array to reduction operation\")\n        zero = X.dtype.type(0)\n        if X.nnz == 0:\n            return zero\n        m = min_or_max.reduce(X.data.ravel())\n        if X.nnz != np.prod(X.shape):\n            m = min_or_max(zero, m)\n        return m\n    if axis < 0:\n        axis += 2\n    if (axis == 0) or (axis == 1):\n        return _min_or_max_axis(X, axis, min_or_max)\n    else:\n        raise ValueError(\"invalid axis, use 0 for rows, or 1 for columns\")\n\n\ndef _sparse_min_max(X, axis):\n    return (\n        _sparse_min_or_max(X, axis, np.minimum),\n        _sparse_min_or_max(X, axis, np.maximum),\n    )\n\n\ndef _sparse_nan_min_max(X, axis):\n    return (_sparse_min_or_max(X, axis, np.fmin), _sparse_min_or_max(X, axis, np.fmax))",
                "filename": "sklearn/utils/sparsefuncs.py",
                "start_index": 12700,
                "end_index": 14773,
                "start_line": 415,
                "end_line": 478,
                "max_line": 630,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 1.0
            },
            {
                "code": "def _check_large_sparse(X, accept_large_sparse=False):\n    \"\"\"Raise a ValueError if X has 64bit indices and accept_large_sparse=False\"\"\"\n    if not accept_large_sparse:\n        supported_indices = [\"int32\"]\n        if X.getformat() == \"coo\":\n            index_keys = [\"col\", \"row\"]\n        elif X.getformat() in [\"csr\", \"csc\", \"bsr\"]:\n            index_keys = [\"indices\", \"indptr\"]\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if indices_datatype not in supported_indices:\n                raise ValueError(\n                    \"Only sparse matrices with 32-bit integer indices are accepted.\"\n                    f\" Got {indices_datatype} indices. Please do report a minimal\"\n                    \" reproducer on scikit-learn issue tracker so that support for\"\n                    \" your use-case can be studied by maintainers. See:\"\n                    \" https://scikit-learn.org/dev/developers/minimal_reproducer.html\"\n                )",
                "filename": "sklearn/utils/validation.py",
                "start_index": 35996,
                "end_index": 37016,
                "start_line": 1001,
                "end_line": 2214,
                "max_line": 2282,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 1.0
            },
            {
                "code": "def min_max_axis(X, axis, ignore_nan=False):\n    \"\"\"Compute minimum and maximum along an axis on a CSR or CSC matrix.\n\n     Optionally ignore NaN values.\n\n    Parameters\n    ----------\n    X : sparse matrix of shape (n_samples, n_features)\n        Input data. It should be of CSR or CSC format.\n\n    axis : {0, 1}\n        Axis along which the axis should be computed.\n\n    ignore_nan : bool, default=False\n        Ignore or passing through NaN values.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n\n    mins : ndarray of shape (n_features,), dtype={np.float32, np.float64}\n        Feature-wise minima.\n\n    maxs : ndarray of shape (n_features,), dtype={np.float32, np.float64}\n        Feature-wise maxima.\n    \"\"\"\n    if sp.issparse(X) and X.format in (\"csr\", \"csc\"):\n        if ignore_nan:\n            return _sparse_nan_min_max(X, axis=axis)\n        else:\n            return _sparse_min_max(X, axis=axis)\n    else:\n        _raise_typeerror(X)\n\n\ndef count_nonzero(X, axis=None, sample_weight=None):\n    \"\"\"A variant of X.getnnz() with extension to weighting on axis 0.\n\n    Useful in efficiently calculating multilabel metrics.\n\n    Parameters\n    ----------\n    X : sparse matrix of shape (n_samples, n_labels)\n        Input data. It should be of CSR format.\n\n    axis : {0, 1}, default=None\n        The axis on which the data is aggregated.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weight for each row of X.\n\n    Returns\n    -------\n    nnz : int, float, ndarray of shape (n_samples,) or ndarray of shape (n_features,)\n        Number of non-zero values in the array along a given axis. Otherwise,\n        the total number of non-zero values in the array is returned.\n    \"\"\"\n    if axis == -1:\n        axis = 1\n    elif axis == -2:\n        axis = 0\n    elif X.format != \"csr\":\n        raise TypeError(\"Expected CSR sparse format, got {0}\".format(X.format))\n\n    # We rely here on the fact that np.diff(Y.indptr) for a CSR\n    # will return the number of nonzero entries in each row.\n    # A bincount over Y.indices will return the number of nonzeros\n    # in each column. See ``csr_matrix.getnnz`` in scipy >= 0.14.\n    if axis is None:\n        if sample_weight is None:\n            return X.nnz\n        else:\n            return np.dot(np.diff(X.indptr), sample_weight)\n    elif axis == 1:\n        out = np.diff(X.indptr)\n        if sample_weight is None:\n            # astype here is for consistency with axis=0 dtype\n            return out.astype(\"intp\")\n        return out * sample_weight\n    elif axis == 0:\n        if sample_weight is None:\n            return np.bincount(X.indices, minlength=X.shape[1])\n        else:\n            weights = np.repeat(sample_weight, np.diff(X.indptr))\n            return np.bincount(X.indices, minlength=X.shape[1], weights=weights)\n    else:\n        raise ValueError(\"Unsupported axis: {0}\".format(axis))",
                "filename": "sklearn/utils/sparsefuncs.py",
                "start_index": 14776,
                "end_index": 17668,
                "start_line": 481,
                "end_line": 568,
                "max_line": 630,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 1.0
            },
            {
                "code": "assert array(\"i\").itemsize == 4, (\n            \"sizeof(int) != 4 on your platform; please report this at\"\n            \" https://github.com/scikit-learn/scikit-learn/issues and\"\n            \" include the output from platform.platform() in your bug report\"\n        )\n\n        dtype = self.dtype\n        if fitting:\n            feature_names = []\n            vocab = {}\n        else:\n            feature_names = self.feature_names_\n            vocab = self.vocabulary_\n\n        transforming = True\n\n        # Process everything as sparse regardless of setting\n        X = [X] if isinstance(X, Mapping) else X\n\n        indices = array(\"i\")\n        indptr = [0]\n        # XXX we could change values to an array.array as well, but it\n        # would require (heuristic) conversion of dtype to typecode...\n        values = []\n\n        # collect all the possible feature names and build sparse matrix at\n        # same time\n        for x in X:\n            for f, v in x.items():\n                if isinstance(v, str):\n                    feature_name = \"%s%s%s\" % (f, self.separator, v)\n                    v = 1\n                elif isinstance(v, Number) or (v is None):\n                    feature_name = f\n                elif not isinstance(v, Mapping) and isinstance(v, Iterable):\n                    feature_name = None\n                    self._add_iterable_element(\n                        f,\n                        v,\n                        feature_names,\n                        vocab,\n                        fitting=fitting,\n                        transforming=transforming,\n                        indices=indices,\n                        values=values,\n                    )\n                else:\n                    raise TypeError(\n                        f\"Unsupported value Type {type(v)} \"\n                        f\"for {f}: {v}.\\n\"\n                        f\"{type(v)} objects are not supported.\"\n                    )\n\n                if feature_name is not None:\n                    if fitting and feature_name not in vocab:\n                        vocab[feature_name] = len(feature_names)\n                        feature_names.append(feature_name)\n\n                    if feature_name in vocab:\n                        indices.append(vocab[feature_name])\n                        values.append(self.dtype(v))\n\n            indptr.append(len(indices))\n\n        if len(indptr) == 1:\n            raise ValueError(\"Sample sequence X is empty.\")\n\n        indices = np.frombuffer(indices, dtype=np.intc)\n        shape = (len(indptr) - 1, len(vocab))\n\n        result_matrix = sp.csr_matrix(\n            (values, indices, indptr), shape=shape, dtype=dtype\n        )\n\n        # Sort everything if asked",
                "filename": "sklearn/feature_extraction/_dict_vectorizer.py",
                "start_index": 6855,
                "end_index": 9563,
                "start_line": 195,
                "end_line": 269,
                "max_line": 444,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "# Authors: Manoj Kumar\n#          Thomas Unterthiner\n#          Giorgio Patrini\n#\n# License: BSD 3 clause\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom ..utils.validation import _check_sample_weight\nfrom .sparsefuncs_fast import (\n    csc_mean_variance_axis0 as _csc_mean_var_axis0,\n)\nfrom .sparsefuncs_fast import (\n    csr_mean_variance_axis0 as _csr_mean_var_axis0,\n)\nfrom .sparsefuncs_fast import (\n    incr_mean_variance_axis0 as _incr_mean_var_axis0,\n)\n\n\ndef _raise_typeerror(X):\n    \"\"\"Raises a TypeError if X is not a CSR or CSC matrix\"\"\"\n    input_type = X.format if sp.issparse(X) else type(X)\n    err = \"Expected a CSR or CSC sparse matrix, got %s.\" % input_type\n    raise TypeError(err)\n\n\ndef _raise_error_wrong_axis(axis):\n    if axis not in (0, 1):\n        raise ValueError(\n            \"Unknown axis value: %d. Use 0 for rows, or 1 for columns\" % axis\n        )\n\n\ndef inplace_csr_column_scale(X, scale):\n    \"\"\"Inplace column scaling of a CSR matrix.\n\n    Scale each feature of the data matrix by multiplying with specific scale\n    provided by the caller assuming a (n_samples, n_features) shape.\n\n    Parameters\n    ----------\n    X : sparse matrix of shape (n_samples, n_features)\n        Matrix to normalize using the variance of the features.\n        It should be of CSR format.\n\n    scale : ndarray of shape (n_features,), dtype={np.float32, np.float64}\n        Array of precomputed feature-wise values to use for scaling.\n    \"\"\"\n    assert scale.shape[0] == X.shape[1]\n    X.data *= scale.take(X.indices, mode=\"clip\")\n\n\ndef inplace_csr_row_scale(X, scale):\n    \"\"\"Inplace row scaling of a CSR matrix.\n\n    Scale each sample of the data matrix by multiplying with specific scale\n    provided by the caller assuming a (n_samples, n_features) shape.\n\n    Parameters\n    ----------\n    X : sparse matrix of shape (n_samples, n_features)\n        Matrix to be scaled. It should be of CSR format.\n\n    scale : ndarray of float of shape (n_samples,)\n        Array of precomputed sample-wise values to use for scaling.\n    \"\"\"\n    assert scale.shape[0] == X.shape[0]\n    X.data *= np.repeat(scale, np.diff(X.indptr))",
                "filename": "sklearn/utils/sparsefuncs.py",
                "start_index": 0,
                "end_index": 2135,
                "start_line": 1,
                "end_line": 69,
                "max_line": 630,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "def axis0_safe_slice(X, mask, len_mask):\n    \"\"\"Return a mask which is safer to use on X than safe_mask.\n\n    This mask is safer than safe_mask since it returns an\n    empty array, when a sparse matrix is sliced with a boolean mask\n    with all False, instead of raising an unhelpful error in older\n    versions of SciPy.\n\n    See: https://github.com/scipy/scipy/issues/5361\n\n    Also note that we can avoid doing the dot product by checking if\n    the len_mask is not zero in _huber_loss_and_gradient but this\n    is not going to be the bottleneck, since the number of outliers\n    and non_outliers are typically non-zero and it makes the code\n    tougher to follow.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix}\n        Data on which to apply mask.\n\n    mask : ndarray\n        Mask to be used on X.\n\n    len_mask : int\n        The length of the mask.\n\n    Returns\n    -------\n    mask : ndarray\n        Array that is safe to use on X.\n    \"\"\"\n    if len_mask != 0:\n        return X[safe_mask(X, mask), :]\n    return np.zeros(shape=(0, X.shape[1]))\n\n\ndef _array_indexing(array, key, key_dtype, axis):\n    \"\"\"Index an array or scipy.sparse consistently across NumPy version.\"\"\"\n    if issparse(array) and key_dtype == \"bool\":\n        key = np.asarray(key)\n    if isinstance(key, tuple):\n        key = list(key)\n    return array[key] if axis == 0 else array[:, key]\n\n\ndef _pandas_indexing(X, key, key_dtype, axis):\n    \"\"\"Index a pandas dataframe or a series.\"\"\"\n    if _is_arraylike_not_scalar(key):\n        key = np.asarray(key)\n\n    if key_dtype == \"int\" and not (isinstance(key, slice) or np.isscalar(key)):\n        # using take() instead of iloc[] ensures the return value is a \"proper\"\n        # copy that will not raise SettingWithCopyWarning\n        return X.take(key, axis=axis)\n    else:\n        # check whether we should index with loc or iloc\n        indexer = X.iloc if key_dtype == \"int\" else X.loc\n        return indexer[:, key] if axis else indexer[key]\n\n\ndef _list_indexing(X, key, key_dtype):\n    \"\"\"Index a Python list.\"\"\"\n    if np.isscalar(key) or isinstance(key, slice):\n        # key is a slice or a scalar\n        return X[key]\n    if key_dtype == \"bool\":\n        # key is a boolean array-like\n        return list(compress(X, key))\n    # key is a integer array-like of key\n    return [X[idx] for idx in key]",
                "filename": "sklearn/utils/__init__.py",
                "start_index": 4158,
                "end_index": 6509,
                "start_line": 148,
                "end_line": 218,
                "max_line": 1210,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "def check_array_api_input_and_values(\n    name,\n    estimator_orig,\n    array_namespace,\n    device=None,\n    dtype=\"float64\",\n):\n    return check_array_api_input(\n        name,\n        estimator_orig,\n        array_namespace=array_namespace,\n        device=device,\n        dtype=dtype,\n        check_values=True,\n    )\n\n\ndef check_estimator_sparse_data(name, estimator_orig):\n    rng = np.random.RandomState(0)\n    X = rng.uniform(size=(40, 3))\n    X[X < 0.8] = 0\n    X = _enforce_estimator_tags_X(estimator_orig, X)\n    X_csr = sparse.csr_matrix(X)\n    y = (4 * rng.uniform(size=40)).astype(int)\n    # catch deprecation warnings\n    with ignore_warnings(category=FutureWarning):\n        estimator = clone(estimator_orig)\n    y = _enforce_estimator_tags_y(estimator, y)\n    tags = _safe_tags(estimator_orig)\n    for matrix_format, X in _generate_sparse_matrix(X_csr):\n        # catch deprecation warnings\n        with ignore_warnings(category=FutureWarning):\n            estimator = clone(estimator_orig)\n            if name in [\"Scaler\", \"StandardScaler\"]:\n                estimator.set_params(with_mean=False)\n        # fit and predict\n        if \"64\" in matrix_format:\n            err_msg = (\n                f\"Estimator {name} doesn't seem to support {matrix_format} \"\n                \"matrix, and is not failing gracefully, e.g. by using \"\n                \"check_array(X, accept_large_sparse=False)\"\n            )\n        else:\n            err_msg = (\n                f\"Estimator {name} doesn't seem to fail gracefully on sparse \"\n                \"data: error message should state explicitly that sparse \"\n                \"input is not supported if this is not the case.\"\n            )\n        with raises(\n            (TypeError, ValueError),\n            match=[\"sparse\", \"Sparse\"],\n            may_pass=True,\n            err_msg=err_msg,\n        ):\n            with ignore_warnings(category=FutureWarning):\n                estimator.fit(X, y)\n            if hasattr(estimator, \"predict\"):\n                pred = estimator.predict(X)\n                if tags[\"multioutput_only\"]:\n                    assert pred.shape == (X.shape[0], 1)\n                else:\n                    assert pred.shape == (X.shape[0],)\n            if hasattr(estimator, \"predict_proba\"):\n                probs = estimator.predict_proba(X)\n                if tags[\"binary_only\"]:\n                    expected_probs_shape = (X.shape[0], 2)\n                else:\n                    expected_probs_shape = (X.shape[0], 4)\n                assert probs.shape == expected_probs_shape",
                "filename": "sklearn/utils/estimator_checks.py",
                "start_index": 36016,
                "end_index": 38577,
                "start_line": 1025,
                "end_line": 1093,
                "max_line": 4639,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "def _get_median(data, n_zeros):\n    \"\"\"Compute the median of data with n_zeros additional zeros.\n\n    This function is used to support sparse matrices; it modifies data\n    in-place.\n    \"\"\"\n    n_elems = len(data) + n_zeros\n    if not n_elems:\n        return np.nan\n    n_negative = np.count_nonzero(data < 0)\n    middle, is_odd = divmod(n_elems, 2)\n    data.sort()\n\n    if is_odd:\n        return _get_elem_at_rank(middle, data, n_negative, n_zeros)\n\n    return (\n        _get_elem_at_rank(middle - 1, data, n_negative, n_zeros)\n        + _get_elem_at_rank(middle, data, n_negative, n_zeros)\n    ) / 2.0\n\n\ndef _get_elem_at_rank(rank, data, n_negative, n_zeros):\n    \"\"\"Find the value in data augmented with n_zeros for the given rank\"\"\"\n    if rank < n_negative:\n        return data[rank]\n    if rank - n_negative < n_zeros:\n        return 0\n    return data[rank - n_zeros]\n\n\ndef csc_median_axis_0(X):\n    \"\"\"Find the median across axis 0 of a CSC matrix.\n\n    It is equivalent to doing np.median(X, axis=0).\n\n    Parameters\n    ----------\n    X : sparse matrix of shape (n_samples, n_features)\n        Input data. It should be of CSC format.\n\n    Returns\n    -------\n    median : ndarray of shape (n_features,)\n        Median.\n    \"\"\"\n    if not (sp.issparse(X) and X.format == \"csc\"):\n        raise TypeError(\"Expected matrix of CSC format, got %s\" % X.format)\n\n    indptr = X.indptr\n    n_samples, n_features = X.shape\n    median = np.zeros(n_features)\n\n    for f_ind, (start, end) in enumerate(zip(indptr[:-1], indptr[1:])):\n        # Prevent modifying X in place\n        data = np.copy(X.data[start:end])\n        nz = n_samples - data.size\n        median[f_ind] = _get_median(data, nz)\n\n    return median",
                "filename": "sklearn/utils/sparsefuncs.py",
                "start_index": 17671,
                "end_index": 19381,
                "start_line": 571,
                "end_line": 630,
                "max_line": 630,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "def _transform(self, X, fitting):\n        # Sanity check: Python's array has no way of explicitly requesting the\n        # signed 32-bit integers that scipy.sparse needs, so we use the next\n        # best thing: typecode \"i\" (int). However, if that gives larger or\n        # smaller integers than 32-bit ones, np.frombuffer screws up.",
                "filename": "sklearn/feature_extraction/_dict_vectorizer.py",
                "start_index": 6512,
                "end_index": 6846,
                "start_line": 190,
                "end_line": 194,
                "max_line": 444,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "@validate_params(\n    {\"X\": [\"array-like\", \"sparse matrix\"], \"axis\": [Options(Integral, {0, 1})]},\n    prefer_skip_nested_validation=False,\n)",
                "filename": "sklearn/preprocessing/_data.py",
                "start_index": 55131,
                "end_index": 55272,
                "start_line": 120,
                "end_line": 3519,
                "max_line": 3519,
                "git_instance": "github",
                "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                "sha": "",
                "context_relevance": 0.0
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "sklearn/utils/sparsefuncs.py": [
                {
                    "chunk": {
                        "code": "def _minor_reduce(X, ufunc):\n    major_index = np.flatnonzero(np.diff(X.indptr))\n\n    # reduceat tries casts X.indptr to intp, which errors\n    # if it is int64 on a 32 bit system.\n    # Reinitializing prevents this where possible, see #13737\n    X = type(X)((X.data, X.indices, X.indptr), shape=X.shape)\n    value = ufunc.reduceat(X.data, X.indptr[major_index])\n    return major_index, value\n\n\ndef _min_or_max_axis(X, axis, min_or_max):\n    N = X.shape[axis]\n    if N == 0:\n        raise ValueError(\"zero-size array to reduction operation\")\n    M = X.shape[1 - axis]\n    mat = X.tocsc() if axis == 0 else X.tocsr()\n    mat.sum_duplicates()\n    major_index, value = _minor_reduce(mat, min_or_max)\n    not_full = np.diff(mat.indptr)[major_index] < N\n    value[not_full] = min_or_max(value[not_full], 0)\n    mask = value != 0\n    major_index = np.compress(mask, major_index)\n    value = np.compress(mask, value)\n\n    if axis == 0:\n        res = sp.coo_matrix(\n            (value, (np.zeros(len(value)), major_index)), dtype=X.dtype, shape=(1, M)\n        )\n    else:\n        res = sp.coo_matrix(\n            (value, (major_index, np.zeros(len(value)))), dtype=X.dtype, shape=(M, 1)\n        )\n    return res.A.ravel()\n\n\ndef _sparse_min_or_max(X, axis, min_or_max):\n    if axis is None:\n        if 0 in X.shape:\n            raise ValueError(\"zero-size array to reduction operation\")\n        zero = X.dtype.type(0)\n        if X.nnz == 0:\n            return zero\n        m = min_or_max.reduce(X.data.ravel())\n        if X.nnz != np.prod(X.shape):\n            m = min_or_max(zero, m)\n        return m\n    if axis < 0:\n        axis += 2\n    if (axis == 0) or (axis == 1):\n        return _min_or_max_axis(X, axis, min_or_max)\n    else:\n        raise ValueError(\"invalid axis, use 0 for rows, or 1 for columns\")\n\n\ndef _sparse_min_max(X, axis):\n    return (\n        _sparse_min_or_max(X, axis, np.minimum),\n        _sparse_min_or_max(X, axis, np.maximum),\n    )\n\n\ndef _sparse_nan_min_max(X, axis):\n    return (_sparse_min_or_max(X, axis, np.fmin), _sparse_min_or_max(X, axis, np.fmax))",
                        "filename": "sklearn/utils/sparsefuncs.py",
                        "start_index": 12700,
                        "end_index": 14773,
                        "start_line": 415,
                        "end_line": 478,
                        "max_line": 630,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the '_minor_reduce' function where the TypeError occurs due to the casting issue on 32-bit systems. The reinitialization of 'X' in this function is directly related to the issue."
                },
                {
                    "chunk": {
                        "code": "def min_max_axis(X, axis, ignore_nan=False):\n    \"\"\"Compute minimum and maximum along an axis on a CSR or CSC matrix.\n\n     Optionally ignore NaN values.\n\n    Parameters\n    ----------\n    X : sparse matrix of shape (n_samples, n_features)\n        Input data. It should be of CSR or CSC format.\n\n    axis : {0, 1}\n        Axis along which the axis should be computed.\n\n    ignore_nan : bool, default=False\n        Ignore or passing through NaN values.\n\n        .. versionadded:: 0.20\n\n    Returns\n    -------\n\n    mins : ndarray of shape (n_features,), dtype={np.float32, np.float64}\n        Feature-wise minima.\n\n    maxs : ndarray of shape (n_features,), dtype={np.float32, np.float64}\n        Feature-wise maxima.\n    \"\"\"\n    if sp.issparse(X) and X.format in (\"csr\", \"csc\"):\n        if ignore_nan:\n            return _sparse_nan_min_max(X, axis=axis)\n        else:\n            return _sparse_min_max(X, axis=axis)\n    else:\n        _raise_typeerror(X)\n\n\ndef count_nonzero(X, axis=None, sample_weight=None):\n    \"\"\"A variant of X.getnnz() with extension to weighting on axis 0.\n\n    Useful in efficiently calculating multilabel metrics.\n\n    Parameters\n    ----------\n    X : sparse matrix of shape (n_samples, n_labels)\n        Input data. It should be of CSR format.\n\n    axis : {0, 1}, default=None\n        The axis on which the data is aggregated.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weight for each row of X.\n\n    Returns\n    -------\n    nnz : int, float, ndarray of shape (n_samples,) or ndarray of shape (n_features,)\n        Number of non-zero values in the array along a given axis. Otherwise,\n        the total number of non-zero values in the array is returned.\n    \"\"\"\n    if axis == -1:\n        axis = 1\n    elif axis == -2:\n        axis = 0\n    elif X.format != \"csr\":\n        raise TypeError(\"Expected CSR sparse format, got {0}\".format(X.format))\n\n    # We rely here on the fact that np.diff(Y.indptr) for a CSR\n    # will return the number of nonzero entries in each row.\n    # A bincount over Y.indices will return the number of nonzeros\n    # in each column. See ``csr_matrix.getnnz`` in scipy >= 0.14.\n    if axis is None:\n        if sample_weight is None:\n            return X.nnz\n        else:\n            return np.dot(np.diff(X.indptr), sample_weight)\n    elif axis == 1:\n        out = np.diff(X.indptr)\n        if sample_weight is None:\n            # astype here is for consistency with axis=0 dtype\n            return out.astype(\"intp\")\n        return out * sample_weight\n    elif axis == 0:\n        if sample_weight is None:\n            return np.bincount(X.indices, minlength=X.shape[1])\n        else:\n            weights = np.repeat(sample_weight, np.diff(X.indptr))\n            return np.bincount(X.indices, minlength=X.shape[1], weights=weights)\n    else:\n        raise ValueError(\"Unsupported axis: {0}\".format(axis))",
                        "filename": "sklearn/utils/sparsefuncs.py",
                        "start_index": 14776,
                        "end_index": 17668,
                        "start_line": 481,
                        "end_line": 568,
                        "max_line": 630,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the 'min_max_axis' function which is the entry point for the error. Understanding how this function invokes '_sparse_min_max' and subsequently '_minor_reduce' is necessary for solving the issue."
                }
            ],
            "sklearn/utils/validation.py": [
                {
                    "chunk": {
                        "code": "def _check_large_sparse(X, accept_large_sparse=False):\n    \"\"\"Raise a ValueError if X has 64bit indices and accept_large_sparse=False\"\"\"\n    if not accept_large_sparse:\n        supported_indices = [\"int32\"]\n        if X.getformat() == \"coo\":\n            index_keys = [\"col\", \"row\"]\n        elif X.getformat() in [\"csr\", \"csc\", \"bsr\"]:\n            index_keys = [\"indices\", \"indptr\"]\n        else:\n            return\n        for key in index_keys:\n            indices_datatype = getattr(X, key).dtype\n            if indices_datatype not in supported_indices:\n                raise ValueError(\n                    \"Only sparse matrices with 32-bit integer indices are accepted.\"\n                    f\" Got {indices_datatype} indices. Please do report a minimal\"\n                    \" reproducer on scikit-learn issue tracker so that support for\"\n                    \" your use-case can be studied by maintainers. See:\"\n                    \" https://scikit-learn.org/dev/developers/minimal_reproducer.html\"\n                )",
                        "filename": "sklearn/utils/validation.py",
                        "start_index": 35996,
                        "end_index": 37016,
                        "start_line": 1001,
                        "end_line": 2214,
                        "max_line": 2282,
                        "git_instance": "github",
                        "repo_name": "testbed/scikit-learn__scikit-learn__0.21",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet includes the '_check_large_sparse' function which raises a ValueError if a sparse matrix with 64-bit indices is used and 'accept_large_sparse' is False. This is relevant as the issue occurs with 64-bit indices on a 32-bit system."
                }
            ]
        },
        "solution_task": "- Change the `astype` call to use `'int32'` instead of `'int64'` to match the system's integer size:\n  ```python\n  X.indptr = X.indptr.astype('int32')\n  ```\n- Ensure that all other parts of the code that might be casting `indptr` to `'int64'` are also casting to `'int32'` instead on a 32-bit system.",
        "bouncer_response": {
            "score": 7,
            "criteria": "The RESPONSE must provide actionable, relevant guidance to more parts of the PROMPT's described issue to score higher.",
            "supporting_evidence": "The RESPONSE provides a direct and actionable solution to the TypeError issue by suggesting to change the 'astype' call to use 'int32' instead of 'int64', which is relevant to the error message indicating a casting problem from 'int64' to 'int32'. Additionally, the RESPONSE advises ensuring that all parts of the code that might cast 'indptr' to 'int64' are also casting to 'int32' on a 32-bit system, which is a comprehensive approach to the problem. The RESPONSE also includes links to specific code snippets in the sklearn repository that are relevant to the issue, which could help in diagnosing and fixing the bug. However, the RESPONSE does not provide a complete solution to the problem, as it does not address potential side effects of changing the data type or how to handle the situation programmatically for different system architectures. Therefore, it scores a 7 for providing clear and actionable guidance that is highly relevant but not entirely comprehensive.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711740584.981202,
        "result": "- Change the `astype` call to use `'int32'` instead of `'int64'` to match the system's integer size:\n  ```python\n  X.indptr = X.indptr.astype('int32')\n  ```\n- Ensure that all other parts of the code that might be casting `indptr` to `'int64'` are also casting to `'int32'` instead on a 32-bit system.\n\n### Code snippets to check\n\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/utils)- [Lines 415 - 478](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/utils/sparsefuncs.py#L415-L478)\n\n This snippet contains the '_minor_reduce' function where the TypeError occurs due to the casting issue on 32-bit systems. The reinitialization of 'X' in this function is directly related to the issue. \n\n\n   - [Lines 481 - 568](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/utils/sparsefuncs.py#L481-L568)\n\n This snippet contains the 'min_max_axis' function which is the entry point for the error. Understanding how this function invokes '_sparse_min_max' and subsequently '_minor_reduce' is necessary for solving the issue.\n-  \u2192 [sklearn](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn) \u2192 [utils](https://github.com/testbed/scikit-learn__scikit-learn__0.21/tree//sklearn/utils)- [Lines 1001 - 2214](https://github.com/testbed/scikit-learn__scikit-learn__0.21/blob//sklearn/utils/validation.py#L1001-L2214)\n\n This snippet includes the '_check_large_sparse' function which raises a ValueError if a sparse matrix with 64-bit indices is used and 'accept_large_sparse' is False. This is relevant as the issue occurs with 64-bit indices on a 32-bit system.",
        "context_relevance": 0.42000000000000004,
        "hypotheses": [],
        "groundedness_score": 1.0,
        "qa_relevance_reasons": {
            "reason": "Criteria: The RESPONSE is relevant and answers the entire PROMPT completely.\nSupporting Evidence: The RESPONSE provides a direct solution to the TypeError issue described in the PROMPT by suggesting a change in the code to use 'int32' instead of 'int64', which is the root cause of the problem on a 32-bit system. It also advises checking other parts of the code for similar issues, which is a comprehensive approach to fixing the bug. Additionally, the RESPONSE includes relevant code snippets from the sklearn repository that are related to the issue, thus providing context and resources for further investigation and resolution of the problem. The RESPONSE is fully relevant to the PROMPT, addressing the described issue, the steps to reproduce it, the expected and actual results, and the system versions involved."
        },
        "qa_relevance": 1.0
    }
}