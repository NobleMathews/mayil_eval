{
    "id": "pydata__xarray-4684",
    "repo_name": "testbed/pydata__xarray__0.12",
    "task_id": 2232855310585060801,
    "title": "Millisecond precision is lost on datetime64 during IO roundtrip",
    "body": "<!-- A short summary of the issue, if appropriate -->\r\nI have millisecond-resolution time data as a coordinate on a DataArray. That data loses precision when round-tripping through disk.\r\n\r\n#### MCVE Code Sample\r\n<!-- In order for the maintainers to efficiently understand and prioritize issues, we ask you post a \"Minimal, Complete and Verifiable Example\" (MCVE): http://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports -->\r\n\r\n[bug_data.p.zip](https://github.com/pydata/xarray/files/4595145/bug_data.p.zip)\r\n\r\nUnzip the data. It will result in a pickle file.\r\n\r\n```python\r\nbug_data_path = '/path/to/unzipped/bug_data.p'\r\ntmp_path = '~/Desktop/test.nc'\r\n\r\nwith open(bug_data_path, 'rb') as f:\r\n    data = pickle.load(f)\r\n\r\nselector = dict(animal=0, timepoint=0, wavelength='410', pair=0)\r\n\r\nbefore_disk_ts = data.time.sel(**selector).values[()]\r\n\r\ndata.time.encoding = {'units': 'microseconds since 1900-01-01', 'calendar': 'proleptic_gregorian'}\r\n\r\ndata.to_netcdf(tmp_path)\r\nafter_disk_ts = xr.load_dataarray(tmp_path).time.sel(**selector).values[()]\r\n\r\nprint(f'before roundtrip: {before_disk_ts}')\r\nprint(f' after roundtrip: {after_disk_ts}')\r\n```\r\noutput:\r\n```\r\nbefore roundtrip: 2017-02-22T16:24:10.586000000\r\nafter roundtrip:  2017-02-22T16:24:10.585999872\r\n```\r\n\r\n#### Expected Output\r\n```\r\nBefore: 2017-02-22T16:24:10.586000000\r\nAfter:  2017-02-22T16:24:10.586000000\r\n```\r\n\r\n#### Problem Description\r\n<!-- this should explain why the current behavior is a problem and why the expected output is a better solution -->\r\n\r\nAs you can see, I lose millisecond precision in this data. (The same happens when I use millisecond in the encoding).\r\n\r\n#### Versions\r\n\r\n<details><summary>Output of <tt>xr.show_versions()</tt></summary>\r\n\r\n<!-- Paste the output here xr.show_versions() here -->\r\nINSTALLED VERSIONS\r\n------------------\r\ncommit: None\r\npython: 3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:05:27) \r\n[Clang 9.0.1 ]\r\npython-bits: 64\r\nOS: Darwin\r\nOS-release: 19.4.0\r\nmachine: x86_64\r\nprocessor: i386\r\nbyteorder: little\r\nLC_ALL: None\r\nLANG: en_US.UTF-8\r\nLOCALE: None.UTF-8\r\nlibhdf5: 1.10.5\r\nlibnetcdf: 4.7.3\r\n\r\nxarray: 0.15.1\r\npandas: 1.0.1\r\nnumpy: 1.18.1\r\nscipy: 1.4.1\r\nnetCDF4: 1.5.3\r\npydap: None\r\nh5netcdf: 0.8.0\r\nh5py: 2.10.0\r\nNio: None\r\nzarr: None\r\ncftime: 1.0.4.2\r\nnc_time_axis: None\r\nPseudoNetCDF: None\r\nrasterio: None\r\ncfgrib: None\r\niris: None\r\nbottleneck: None\r\ndask: 2.11.0\r\ndistributed: 2.14.0\r\nmatplotlib: 3.1.3\r\ncartopy: None\r\nseaborn: 0.10.0\r\nnumbagg: None\r\nsetuptools: 45.2.0.post20200209\r\npip: 20.0.2\r\nconda: None\r\npytest: 5.3.5\r\nIPython: 7.12.0\r\nsphinx: 2.4.3\r\n\r\n</details>\r\n\n",
    "state": "closed",
    "repo_link": "https://github.com/testbed/pydata__xarray__0.12.git",
    "requested_commit": "",
    "mayil_response": "- Ensure that the `units` and `calendar` attributes are properly set in the encoding for the `time` variable before saving to NetCDF.\n- Use `cftime.num2date` and `cftime.date2num` to ensure that the conversion between time values and numeric time values is consistent with the specified calendar and units.\n- When saving the DataArray to NetCDF, explicitly specify the encoding for the `time` variable, including the `units` as \"microseconds since 1900-01-01\" and the `calendar` as \"proleptic_gregorian\".\n- When loading the DataArray from the NetCDF file, ensure that the `decode_times` parameter is set to `True` to properly decode the time values.\n- If the issue persists, consider updating the libraries to the latest versions that may contain bug fixes related to time precision.\n- Verify that the `dtype` of the time variable is appropriate for the precision required (e.g., `np.datetime64[us]` for microsecond precision).\n- After loading the DataArray from the NetCDF file, compare the time values before and after the IO roundtrip using `np.allclose` with an appropriate `atol` to account for any acceptable minor differences in precision.\n\n### Code snippets to check\n\n-  \u2192 [xarray](https://github.com/testbed/pydata__xarray__0.12/tree//xarray) \u2192 [coding](https://github.com/testbed/pydata__xarray__0.12/tree//xarray/coding)- [Lines 1 - 122](https://github.com/testbed/pydata__xarray__0.12/blob//xarray/coding/times.py#L1-L122)\n\n This snippet contains the time encoding and decoding logic, which is likely where the precision loss issue during IO roundtrip is occurring.",
    "ai_cost": {
        "search_embedding": 1.3e-07,
        "issue_classifier": 0.0005495,
        "snippet_processor": 0.06112,
        "issue_star_creation": 0.025959999999999997,
        "issue_star_solver": 0.08023,
        "bouncer": 0.027180000000000003
    },
    "mayil_collected_data": {
        "status": "completed",
        "start_time": 1711759330.9577181,
        "relevant_snippets": [
            {
                "code": "class IOMultipleNetCDF:\n    \"\"\"\n    A few examples that benchmark reading/writing multiple netCDF files with\n    xarray\n    \"\"\"\n\n    timeout = 300.0\n    repeat = 1\n    number = 5\n\n    def make_ds(self, nfiles=10):\n        # multiple Dataset\n        self.ds = xr.Dataset()\n        self.nt = 1000\n        self.nx = 90\n        self.ny = 45\n        self.nfiles = nfiles\n\n        self.block_chunks = {\n            \"time\": self.nt / 4,\n            \"lon\": self.nx / 3,\n            \"lat\": self.ny / 3,\n        }\n\n        self.time_chunks = {\"time\": int(self.nt / 36)}\n\n        self.time_vars = np.split(\n            pd.date_range(\"1970-01-01\", periods=self.nt, freq=\"D\"), self.nfiles\n        )\n\n        self.ds_list = []\n        self.filenames_list = []\n        for i, times in enumerate(self.time_vars):\n            ds = xr.Dataset()\n            nt = len(times)\n            lons = xr.DataArray(\n                np.linspace(0, 360, self.nx),\n                dims=(\"lon\",),\n                attrs={\"units\": \"degrees east\", \"long_name\": \"longitude\"},\n            )\n            lats = xr.DataArray(\n                np.linspace(-90, 90, self.ny),\n                dims=(\"lat\",),\n                attrs={\"units\": \"degrees north\", \"long_name\": \"latitude\"},\n            )\n            ds[\"foo\"] = xr.DataArray(\n                randn((nt, self.nx, self.ny), frac_nan=0.2),\n                coords={\"lon\": lons, \"lat\": lats, \"time\": times},\n                dims=(\"time\", \"lon\", \"lat\"),\n                name=\"foo\",\n                attrs={\"units\": \"foo units\", \"description\": \"a description\"},\n            )\n            ds[\"bar\"] = xr.DataArray(\n                randn((nt, self.nx, self.ny), frac_nan=0.2),\n                coords={\"lon\": lons, \"lat\": lats, \"time\": times},\n                dims=(\"time\", \"lon\", \"lat\"),\n                name=\"bar\",\n                attrs={\"units\": \"bar units\", \"description\": \"a description\"},\n            )\n            ds[\"baz\"] = xr.DataArray(\n                randn((self.nx, self.ny), frac_nan=0.2).astype(np.float32),\n                coords={\"lon\": lons, \"lat\": lats},\n                dims=(\"lon\", \"lat\"),\n                name=\"baz\",\n                attrs={\"units\": \"baz units\", \"description\": \"a description\"},\n            )\n\n            ds.attrs = {\"history\": \"created for xarray benchmarking\"}\n\n            self.ds_list.append(ds)\n            self.filenames_list.append(\"test_netcdf_%i.nc\" % i)\n\n\nclass IOWriteMultipleNetCDF3(IOMultipleNetCDF):\n    def setup(self):\n        # TODO: Lazily skipped in CI as it is very demanding and slow.\n        # Improve times and remove errors.\n        _skip_slow()\n\n        self.make_ds()\n        self.format = \"NETCDF3_64BIT\"\n\n    def time_write_dataset_netcdf4(self):\n        xr.save_mfdataset(\n            self.ds_list, self.filenames_list, engine=\"netcdf4\", format=self.format\n        )\n\n    def time_write_dataset_scipy(self):\n        xr.save_mfdataset(\n            self.ds_list, self.filenames_list, engine=\"scipy\", format=self.format\n        )",
                "filename": "asv_bench/benchmarks/dataset_io.py",
                "start_index": 7697,
                "end_index": 10696,
                "start_line": 239,
                "end_line": 641,
                "max_line": 652,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "name: \ud83d\udca1 Feature Request\ndescription: Suggest an idea for xarray\nlabels: [enhancement]\nbody:\n  - type: textarea\n    id: description\n    attributes:\n      label: Is your feature request related to a problem?\n      description: |\n        Please do a quick search of existing issues to make sure that this has not been asked before.\n        Please provide a clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n    validations:\n      required: true\n  - type: textarea\n    id: solution\n    attributes:\n      label: Describe the solution you'd like\n      description: |\n        A clear and concise description of what you want to happen.\n  - type: textarea\n    id: alternatives\n    attributes:\n      label: Describe alternatives you've considered\n      description: |\n        A clear and concise description of any alternative solutions or features you've considered.\n    validations:\n      required: false\n  - type: textarea\n    id: additional-context\n    attributes:\n      label: Additional context\n      description: |\n        Add any other context about the feature request here.\n    validations:\n      required: false",
                "filename": ".github/ISSUE_TEMPLATE/newfeature.yml",
                "start_index": 0,
                "end_index": 1154,
                "start_line": 1,
                "end_line": 35,
                "max_line": 35,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "from __future__ import annotations\n\nimport re\nimport warnings\nfrom collections.abc import Hashable\nfrom datetime import datetime, timedelta\nfrom functools import partial\nfrom typing import TYPE_CHECKING, Callable, Union\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.errors import OutOfBoundsDatetime, OutOfBoundsTimedelta\n\nfrom xarray.coding.variables import (\n    SerializationWarning,\n    VariableCoder,\n    lazy_elemwise_func,\n    pop_to,\n    safe_setitem,\n    unpack_for_decoding,\n    unpack_for_encoding,\n)\nfrom xarray.core import indexing\nfrom xarray.core.common import contains_cftime_datetimes, is_np_datetime_like\nfrom xarray.core.formatting import first_n_items, format_timestamp, last_item\nfrom xarray.core.pdcompat import nanosecond_precision_timestamp\nfrom xarray.core.pycompat import is_duck_dask_array\nfrom xarray.core.variable import Variable\n\ntry:\n    import cftime\nexcept ImportError:\n    cftime = None\n\nif TYPE_CHECKING:\n    from xarray.core.types import CFCalendar\n\n    T_Name = Union[Hashable, None]\n\n# standard calendars recognized by cftime\n_STANDARD_CALENDARS = {\"standard\", \"gregorian\", \"proleptic_gregorian\"}\n\n_NS_PER_TIME_DELTA = {\n    \"ns\": 1,\n    \"us\": int(1e3),\n    \"ms\": int(1e6),\n    \"s\": int(1e9),\n    \"m\": int(1e9) * 60,\n    \"h\": int(1e9) * 60 * 60,\n    \"D\": int(1e9) * 60 * 60 * 24,\n}\n\n_US_PER_TIME_DELTA = {\n    \"microseconds\": 1,\n    \"milliseconds\": 1_000,\n    \"seconds\": 1_000_000,\n    \"minutes\": 60 * 1_000_000,\n    \"hours\": 60 * 60 * 1_000_000,\n    \"days\": 24 * 60 * 60 * 1_000_000,\n}\n\n_NETCDF_TIME_UNITS_CFTIME = [\n    \"days\",\n    \"hours\",\n    \"minutes\",\n    \"seconds\",\n    \"milliseconds\",\n    \"microseconds\",\n]\n\n_NETCDF_TIME_UNITS_NUMPY = _NETCDF_TIME_UNITS_CFTIME + [\"nanoseconds\"]\n\nTIME_UNITS = frozenset(\n    [\n        \"days\",\n        \"hours\",\n        \"minutes\",\n        \"seconds\",\n        \"milliseconds\",\n        \"microseconds\",\n        \"nanoseconds\",\n    ]\n)\n\n\ndef _is_standard_calendar(calendar: str) -> bool:\n    return calendar.lower() in _STANDARD_CALENDARS\n\n\ndef _is_numpy_compatible_time_range(times):\n    if is_np_datetime_like(times.dtype):\n        return True\n    # times array contains cftime objects\n    times = np.asarray(times)\n    tmin = times.min()\n    tmax = times.max()\n    try:\n        convert_time_or_go_back(tmin, pd.Timestamp)\n        convert_time_or_go_back(tmax, pd.Timestamp)\n    except pd.errors.OutOfBoundsDatetime:\n        return False\n    except ValueError as err:\n        if err.args[0] == \"year 0 is out of range\":\n            return False\n        raise\n    else:\n        return True\n\n\ndef _netcdf_to_numpy_timeunit(units: str) -> str:\n    units = units.lower()\n    if not units.endswith(\"s\"):\n        units = f\"{units}s\"\n    return {\n        \"nanoseconds\": \"ns\",\n        \"microseconds\": \"us\",\n        \"milliseconds\": \"ms\",\n        \"seconds\": \"s\",\n        \"minutes\": \"m\",\n        \"hours\": \"h\",\n        \"days\": \"D\",\n    }[units]",
                "filename": "xarray/coding/times.py",
                "start_index": 0,
                "end_index": 2910,
                "start_line": 1,
                "end_line": 122,
                "max_line": 762,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "from __future__ import annotations\n\nimport os\nfrom dataclasses import dataclass\n\nimport numpy as np\nimport pandas as pd\n\nimport xarray as xr\n\nfrom . import _skip_slow, parameterized, randint, randn, requires_dask\n\ntry:\n    import dask\n    import dask.multiprocessing\nexcept ImportError:\n    pass\n\n\nos.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n\n_ENGINES = tuple(xr.backends.list_engines().keys() - {\"store\"})\n\n\nclass IOSingleNetCDF:\n    \"\"\"\n    A few examples that benchmark reading/writing a single netCDF file with\n    xarray\n    \"\"\"\n\n    timeout = 300.0\n    repeat = 1\n    number = 5\n\n    def make_ds(self):\n        # single Dataset\n        self.ds = xr.Dataset()\n        self.nt = 1000\n        self.nx = 90\n        self.ny = 45\n\n        self.block_chunks = {\n            \"time\": self.nt / 4,\n            \"lon\": self.nx / 3,\n            \"lat\": self.ny / 3,\n        }\n\n        self.time_chunks = {\"time\": int(self.nt / 36)}\n\n        times = pd.date_range(\"1970-01-01\", periods=self.nt, freq=\"D\")\n        lons = xr.DataArray(\n            np.linspace(0, 360, self.nx),\n            dims=(\"lon\",),\n            attrs={\"units\": \"degrees east\", \"long_name\": \"longitude\"},\n        )\n        lats = xr.DataArray(\n            np.linspace(-90, 90, self.ny),\n            dims=(\"lat\",),\n            attrs={\"units\": \"degrees north\", \"long_name\": \"latitude\"},\n        )\n        self.ds[\"foo\"] = xr.DataArray(\n            randn((self.nt, self.nx, self.ny), frac_nan=0.2),\n            coords={\"lon\": lons, \"lat\": lats, \"time\": times},\n            dims=(\"time\", \"lon\", \"lat\"),\n            name=\"foo\",\n            attrs={\"units\": \"foo units\", \"description\": \"a description\"},\n        )\n        self.ds[\"bar\"] = xr.DataArray(\n            randn((self.nt, self.nx, self.ny), frac_nan=0.2),\n            coords={\"lon\": lons, \"lat\": lats, \"time\": times},\n            dims=(\"time\", \"lon\", \"lat\"),\n            name=\"bar\",\n            attrs={\"units\": \"bar units\", \"description\": \"a description\"},\n        )\n        self.ds[\"baz\"] = xr.DataArray(\n            randn((self.nx, self.ny), frac_nan=0.2).astype(np.float32),\n            coords={\"lon\": lons, \"lat\": lats},\n            dims=(\"lon\", \"lat\"),\n            name=\"baz\",\n            attrs={\"units\": \"baz units\", \"description\": \"a description\"},\n        )\n\n        self.ds.attrs = {\"history\": \"created for xarray benchmarking\"}\n\n        self.oinds = {\n            \"time\": randint(0, self.nt, 120),\n            \"lon\": randint(0, self.nx, 20),\n            \"lat\": randint(0, self.ny, 10),\n        }\n        self.vinds = {\n            \"time\": xr.DataArray(randint(0, self.nt, 120), dims=\"x\"),\n            \"lon\": xr.DataArray(randint(0, self.nx, 120), dims=\"x\"),\n            \"lat\": slice(3, 20),\n        }",
                "filename": "asv_bench/benchmarks/dataset_io.py",
                "start_index": 0,
                "end_index": 2724,
                "start_line": 1,
                "end_line": 607,
                "max_line": 652,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": "",
                "context_relevance": 0.0
            },
            {
                "code": "import numpy as np\nimport pandas as pd\n\nimport xarray as xr\n\n\nclass Repr:\n    def setup(self):\n        a = np.arange(0, 100)\n        data_vars = dict()\n        for i in a:\n            data_vars[f\"long_variable_name_{i}\"] = xr.DataArray(\n                name=f\"long_variable_name_{i}\",\n                data=np.arange(0, 20),\n                dims=[f\"long_coord_name_{i}_x\"],\n                coords={f\"long_coord_name_{i}_x\": np.arange(0, 20) * 2},\n            )\n        self.ds = xr.Dataset(data_vars)\n        self.ds.attrs = {f\"attr_{k}\": 2 for k in a}\n\n    def time_repr(self):\n        repr(self.ds)\n\n    def time_repr_html(self):\n        self.ds._repr_html_()\n\n\nclass ReprMultiIndex:\n    def setup(self):\n        index = pd.MultiIndex.from_product(\n            [range(1000), range(1000)], names=(\"level_0\", \"level_1\")\n        )\n        series = pd.Series(range(1000 * 1000), index=index)\n        self.da = xr.DataArray(series)\n\n    def time_repr(self):\n        repr(self.da)\n\n    def time_repr_html(self):\n        self.da._repr_html_()",
                "filename": "asv_bench/benchmarks/repr.py",
                "start_index": 0,
                "end_index": 1036,
                "start_line": 1,
                "end_line": 40,
                "max_line": 40,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": "",
                "context_relevance": 0.1
            },
            {
                "code": "from __future__ import annotations\n\nimport datetime\nimport sys\nfrom collections.abc import Hashable, Iterable, Iterator, Mapping, Sequence\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Literal,\n    Protocol,\n    SupportsIndex,\n    TypeVar,\n    Union,\n)\n\nimport numpy as np\nimport pandas as pd\n\ntry:\n    if sys.version_info >= (3, 11):\n        from typing import Self\n    else:\n        from typing_extensions import Self\nexcept ImportError:\n    if TYPE_CHECKING:\n        raise\n    else:\n        Self: Any = None\n\nif TYPE_CHECKING:\n    from numpy._typing import _SupportsDType\n    from numpy.typing import ArrayLike\n\n    from xarray.backends.common import BackendEntrypoint\n    from xarray.core.alignment import Aligner\n    from xarray.core.common import AbstractArray, DataWithCoords\n    from xarray.core.coordinates import Coordinates\n    from xarray.core.dataarray import DataArray\n    from xarray.core.dataset import Dataset\n    from xarray.core.groupby import DataArrayGroupBy, GroupBy\n    from xarray.core.indexes import Index, Indexes\n    from xarray.core.utils import Frozen\n    from xarray.core.variable import Variable\n\n    try:\n        from dask.array import Array as DaskArray\n    except ImportError:\n        DaskArray = np.ndarray  # type: ignore\n\n    try:\n        from cubed import Array as CubedArray\n    except ImportError:\n        CubedArray = np.ndarray\n\n    try:\n        from zarr.core import Array as ZarrArray\n    except ImportError:\n        ZarrArray = np.ndarray\n\n    # Anything that can be coerced to a shape tuple\n    _ShapeLike = Union[SupportsIndex, Sequence[SupportsIndex]]\n    _DTypeLikeNested = Any  # TODO: wait for support for recursive types\n\n    # Xarray requires a Mapping[Hashable, dtype] in many places which\n    # conflics with numpys own DTypeLike (with dtypes for fields).\n    # https://numpy.org/devdocs/reference/typing.html#numpy.typing.DTypeLike\n    # This is a copy of this DTypeLike that allows only non-Mapping dtypes.\n    DTypeLikeSave = Union[\n        np.dtype[Any],\n        # default data type (float64)\n        None,\n        # array-scalar types and generic types\n        type[Any],\n        # character codes, type strings or comma-separated fields, e.g., 'float64'\n        str,\n        # (flexible_dtype, itemsize)\n        tuple[_DTypeLikeNested, int],\n        # (fixed_dtype, shape)\n        tuple[_DTypeLikeNested, _ShapeLike],\n        # (base_dtype, new_dtype)\n        tuple[_DTypeLikeNested, _DTypeLikeNested],\n        # because numpy does the same?\n        list[Any],\n        # anything with a dtype attribute\n        _SupportsDType[np.dtype[Any]],\n    ]\n    try:\n        from cftime import datetime as CFTimeDatetime\n    except ImportError:\n        CFTimeDatetime = Any\n    DatetimeLike = Union[pd.Timestamp, datetime.datetime, np.datetime64, CFTimeDatetime]\nelse:\n    DTypeLikeSave: Any = None",
                "filename": "xarray/core/types.py",
                "start_index": 0,
                "end_index": 2870,
                "start_line": 1,
                "end_line": 94,
                "max_line": 264,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "def _convert_base_to_offset(base, freq, index):\n    \"\"\"Required until we officially deprecate the base argument to resample.  This\n    translates a provided `base` argument to an `offset` argument, following logic\n    from pandas.\n    \"\"\"\n    from xarray.coding.cftimeindex import CFTimeIndex\n\n    if isinstance(index, pd.DatetimeIndex):\n        freq = pd.tseries.frequencies.to_offset(freq)\n        if isinstance(freq, pd.offsets.Tick):\n            return pd.Timedelta(base * freq.nanos // freq.n)\n    elif isinstance(index, CFTimeIndex):\n        freq = cftime_offsets.to_offset(freq)\n        if isinstance(freq, cftime_offsets.Tick):\n            return base * freq.as_timedelta() // freq.n\n    else:\n        raise ValueError(\"Can only resample using a DatetimeIndex or CFTimeIndex.\")\n\n\ndef nanosecond_precision_timestamp(*args, **kwargs) -> pd.Timestamp:\n    \"\"\"Return a nanosecond-precision Timestamp object.\n\n    Note this function should no longer be needed after addressing GitHub issue\n    #7493.\n    \"\"\"\n    if Version(pd.__version__) >= Version(\"2.0.0\"):\n        return pd.Timestamp(*args, **kwargs).as_unit(\"ns\")\n    else:\n        return pd.Timestamp(*args, **kwargs)",
                "filename": "xarray/core/pdcompat.py",
                "start_index": 2725,
                "end_index": 3902,
                "start_line": 78,
                "end_line": 106,
                "max_line": 106,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "from __future__ import annotations\n\nimport gzip\nimport io\nimport os\nfrom collections.abc import Iterable\nfrom typing import TYPE_CHECKING, Any\n\nimport numpy as np\n\nfrom xarray.backends.common import (\n    BACKEND_ENTRYPOINTS,\n    BackendArray,\n    BackendEntrypoint,\n    WritableCFDataStore,\n    _normalize_path,\n)\nfrom xarray.backends.file_manager import CachingFileManager, DummyFileManager\nfrom xarray.backends.locks import ensure_lock, get_write_lock\nfrom xarray.backends.netcdf3 import (\n    encode_nc3_attr_value,\n    encode_nc3_variable,\n    is_valid_nc3_name,\n)\nfrom xarray.backends.store import StoreBackendEntrypoint\nfrom xarray.core.indexing import NumpyIndexingAdapter\nfrom xarray.core.utils import (\n    Frozen,\n    FrozenDict,\n    close_on_error,\n    try_read_magic_number_from_file_or_path,\n)\nfrom xarray.core.variable import Variable\n\nif TYPE_CHECKING:\n    from io import BufferedIOBase\n\n    from xarray.backends.common import AbstractDataStore\n    from xarray.core.dataset import Dataset\n\n\ndef _decode_string(s):\n    if isinstance(s, bytes):\n        return s.decode(\"utf-8\", \"replace\")\n    return s\n\n\ndef _decode_attrs(d):\n    # don't decode _FillValue from bytes -> unicode, because we want to ensure\n    # that its type matches the data exactly\n    return {k: v if k == \"_FillValue\" else _decode_string(v) for (k, v) in d.items()}\n\n\nclass ScipyArrayWrapper(BackendArray):\n    def __init__(self, variable_name, datastore):\n        self.datastore = datastore\n        self.variable_name = variable_name\n        array = self.get_variable().data\n        self.shape = array.shape\n        self.dtype = np.dtype(array.dtype.kind + str(array.dtype.itemsize))\n\n    def get_variable(self, needs_lock=True):\n        ds = self.datastore._manager.acquire(needs_lock)\n        return ds.variables[self.variable_name]\n\n    def __getitem__(self, key):\n        data = NumpyIndexingAdapter(self.get_variable().data)[key]\n        # Copy data if the source file is mmapped. This makes things consistent\n        # with the netCDF4 library by ensuring we can safely read arrays even\n        # after closing associated files.\n        copy = self.datastore.ds.use_mmap\n        return np.array(data, dtype=self.dtype, copy=copy)\n\n    def __setitem__(self, key, value):\n        with self.datastore.lock:\n            data = self.get_variable(needs_lock=False)\n            try:\n                data[key] = value\n            except TypeError:\n                if key is Ellipsis:\n                    # workaround for GH: scipy/scipy#6880\n                    data[:] = value\n                else:\n                    raise",
                "filename": "xarray/backends/scipy_.py",
                "start_index": 0,
                "end_index": 2608,
                "start_line": 1,
                "end_line": 84,
                "max_line": 328,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "from __future__ import annotations\n\nimport warnings\nfrom typing import TYPE_CHECKING, Generic\n\nimport numpy as np\nimport pandas as pd\n\nfrom xarray.coding.times import infer_calendar_name\nfrom xarray.core.common import (\n    _contains_datetime_like_objects,\n    is_np_datetime_like,\n    is_np_timedelta_like,\n)\nfrom xarray.core.pycompat import is_duck_dask_array\nfrom xarray.core.types import T_DataArray\nfrom xarray.core.variable import IndexVariable\n\nif TYPE_CHECKING:\n    from numpy.typing import DTypeLike\n\n    from xarray.core.dataarray import DataArray\n    from xarray.core.dataset import Dataset\n    from xarray.core.types import CFCalendar\n\n\ndef _season_from_months(months):\n    \"\"\"Compute season (DJF, MAM, JJA, SON) from month ordinal\"\"\"\n    # TODO: Move \"season\" accessor upstream into pandas\n    seasons = np.array([\"DJF\", \"MAM\", \"JJA\", \"SON\", \"nan\"])\n    months = np.asarray(months)\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\", message=\"invalid value encountered in floor_divide\"\n        )\n        warnings.filterwarnings(\n            \"ignore\", message=\"invalid value encountered in remainder\"\n        )\n        idx = (months // 3) % 4\n\n    idx[np.isnan(idx)] = 4\n    return seasons[idx.astype(int)]\n\n\ndef _access_through_cftimeindex(values, name):\n    \"\"\"Coerce an array of datetime-like values to a CFTimeIndex\n    and access requested datetime component\n    \"\"\"\n    from xarray.coding.cftimeindex import CFTimeIndex\n\n    if not isinstance(values, CFTimeIndex):\n        values_as_cftimeindex = CFTimeIndex(values.ravel())\n    else:\n        values_as_cftimeindex = values\n    if name == \"season\":\n        months = values_as_cftimeindex.month\n        field_values = _season_from_months(months)\n    elif name == \"date\":\n        raise AttributeError(\n            \"'CFTimeIndex' object has no attribute `date`. Consider using the floor method instead, for instance: `.time.dt.floor('D')`.\"\n        )\n    else:\n        field_values = getattr(values_as_cftimeindex, name)\n    return field_values.reshape(values.shape)\n\n\ndef _access_through_series(values, name):\n    \"\"\"Coerce an array of datetime-like values to a pandas Series and\n    access requested datetime component\n    \"\"\"\n    values_as_series = pd.Series(values.ravel(), copy=False)\n    if name == \"season\":\n        months = values_as_series.dt.month.values\n        field_values = _season_from_months(months)\n    elif name == \"isocalendar\":\n        # isocalendar returns iso- year, week, and weekday -> reshape\n        field_values = np.array(values_as_series.dt.isocalendar(), dtype=np.int64)\n        return field_values.T.reshape(3, *values.shape)\n    else:\n        field_values = getattr(values_as_series.dt, name).values\n    return field_values.reshape(values.shape)",
                "filename": "xarray/core/accessor_dt.py",
                "start_index": 0,
                "end_index": 2782,
                "start_line": 1,
                "end_line": 140,
                "max_line": 599,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": "",
                "context_relevance": 0.2
            },
            {
                "code": "from __future__ import annotations\n\nimport functools\nimport operator\nimport os\nfrom collections.abc import Iterable\nfrom contextlib import suppress\nfrom typing import TYPE_CHECKING, Any\n\nimport numpy as np\n\nfrom xarray import coding\nfrom xarray.backends.common import (\n    BACKEND_ENTRYPOINTS,\n    BackendArray,\n    BackendEntrypoint,\n    WritableCFDataStore,\n    _normalize_path,\n    find_root_and_group,\n    robust_getitem,\n)\nfrom xarray.backends.file_manager import CachingFileManager, DummyFileManager\nfrom xarray.backends.locks import (\n    HDF5_LOCK,\n    NETCDFC_LOCK,\n    combine_locks,\n    ensure_lock,\n    get_write_lock,\n)\nfrom xarray.backends.netcdf3 import encode_nc3_attr_value, encode_nc3_variable\nfrom xarray.backends.store import StoreBackendEntrypoint\nfrom xarray.coding.variables import pop_to\nfrom xarray.core import indexing\nfrom xarray.core.utils import (\n    FrozenDict,\n    close_on_error,\n    is_remote_uri,\n    try_read_magic_number_from_path,\n)\nfrom xarray.core.variable import Variable\n\nif TYPE_CHECKING:\n    from io import BufferedIOBase\n\n    from xarray.backends.common import AbstractDataStore\n    from xarray.core.dataset import Dataset\n\n# This lookup table maps from dtype.byteorder to a readable endian\n# string used by netCDF4.\n_endian_lookup = {\"=\": \"native\", \">\": \"big\", \"<\": \"little\", \"|\": \"native\"}\n\n\nNETCDF4_PYTHON_LOCK = combine_locks([NETCDFC_LOCK, HDF5_LOCK])\n\n\nclass BaseNetCDF4Array(BackendArray):\n    __slots__ = (\"datastore\", \"dtype\", \"shape\", \"variable_name\")\n\n    def __init__(self, variable_name, datastore):\n        self.datastore = datastore\n        self.variable_name = variable_name\n\n        array = self.get_array()\n        self.shape = array.shape\n\n        dtype = array.dtype\n        if dtype is str:\n            # use object dtype (with additional vlen string metadata) because that's\n            # the only way in numpy to represent variable length strings and to\n            # check vlen string dtype in further steps\n            # it also prevents automatic string concatenation via\n            # conventions.decode_cf_variable\n            dtype = coding.strings.create_vlen_dtype(str)\n        self.dtype = dtype\n\n    def __setitem__(self, key, value):\n        with self.datastore.lock:\n            data = self.get_array(needs_lock=False)\n            data[key] = value\n            if self.datastore.autoclose:\n                self.datastore.close(needs_lock=False)\n\n    def get_array(self, needs_lock=True):\n        raise NotImplementedError(\"Virtual Method\")",
                "filename": "xarray/backends/netCDF4_.py",
                "start_index": 0,
                "end_index": 2520,
                "start_line": 1,
                "end_line": 84,
                "max_line": 629,
                "git_instance": "github",
                "repo_name": "testbed/pydata__xarray__0.12",
                "sha": "",
                "context_relevance": 0.2
            }
        ],
        "classification": "bug",
        "snippet_processor_task": {
            "xarray/coding/times.py": [
                {
                    "chunk": {
                        "code": "from __future__ import annotations\n\nimport re\nimport warnings\nfrom collections.abc import Hashable\nfrom datetime import datetime, timedelta\nfrom functools import partial\nfrom typing import TYPE_CHECKING, Callable, Union\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.errors import OutOfBoundsDatetime, OutOfBoundsTimedelta\n\nfrom xarray.coding.variables import (\n    SerializationWarning,\n    VariableCoder,\n    lazy_elemwise_func,\n    pop_to,\n    safe_setitem,\n    unpack_for_decoding,\n    unpack_for_encoding,\n)\nfrom xarray.core import indexing\nfrom xarray.core.common import contains_cftime_datetimes, is_np_datetime_like\nfrom xarray.core.formatting import first_n_items, format_timestamp, last_item\nfrom xarray.core.pdcompat import nanosecond_precision_timestamp\nfrom xarray.core.pycompat import is_duck_dask_array\nfrom xarray.core.variable import Variable\n\ntry:\n    import cftime\nexcept ImportError:\n    cftime = None\n\nif TYPE_CHECKING:\n    from xarray.core.types import CFCalendar\n\n    T_Name = Union[Hashable, None]\n\n# standard calendars recognized by cftime\n_STANDARD_CALENDARS = {\"standard\", \"gregorian\", \"proleptic_gregorian\"}\n\n_NS_PER_TIME_DELTA = {\n    \"ns\": 1,\n    \"us\": int(1e3),\n    \"ms\": int(1e6),\n    \"s\": int(1e9),\n    \"m\": int(1e9) * 60,\n    \"h\": int(1e9) * 60 * 60,\n    \"D\": int(1e9) * 60 * 60 * 24,\n}\n\n_US_PER_TIME_DELTA = {\n    \"microseconds\": 1,\n    \"milliseconds\": 1_000,\n    \"seconds\": 1_000_000,\n    \"minutes\": 60 * 1_000_000,\n    \"hours\": 60 * 60 * 1_000_000,\n    \"days\": 24 * 60 * 60 * 1_000_000,\n}\n\n_NETCDF_TIME_UNITS_CFTIME = [\n    \"days\",\n    \"hours\",\n    \"minutes\",\n    \"seconds\",\n    \"milliseconds\",\n    \"microseconds\",\n]\n\n_NETCDF_TIME_UNITS_NUMPY = _NETCDF_TIME_UNITS_CFTIME + [\"nanoseconds\"]\n\nTIME_UNITS = frozenset(\n    [\n        \"days\",\n        \"hours\",\n        \"minutes\",\n        \"seconds\",\n        \"milliseconds\",\n        \"microseconds\",\n        \"nanoseconds\",\n    ]\n)\n\n\ndef _is_standard_calendar(calendar: str) -> bool:\n    return calendar.lower() in _STANDARD_CALENDARS\n\n\ndef _is_numpy_compatible_time_range(times):\n    if is_np_datetime_like(times.dtype):\n        return True\n    # times array contains cftime objects\n    times = np.asarray(times)\n    tmin = times.min()\n    tmax = times.max()\n    try:\n        convert_time_or_go_back(tmin, pd.Timestamp)\n        convert_time_or_go_back(tmax, pd.Timestamp)\n    except pd.errors.OutOfBoundsDatetime:\n        return False\n    except ValueError as err:\n        if err.args[0] == \"year 0 is out of range\":\n            return False\n        raise\n    else:\n        return True\n\n\ndef _netcdf_to_numpy_timeunit(units: str) -> str:\n    units = units.lower()\n    if not units.endswith(\"s\"):\n        units = f\"{units}s\"\n    return {\n        \"nanoseconds\": \"ns\",\n        \"microseconds\": \"us\",\n        \"milliseconds\": \"ms\",\n        \"seconds\": \"s\",\n        \"minutes\": \"m\",\n        \"hours\": \"h\",\n        \"days\": \"D\",\n    }[units]",
                        "filename": "xarray/coding/times.py",
                        "start_index": 0,
                        "end_index": 2910,
                        "start_line": 1,
                        "end_line": 122,
                        "max_line": 762,
                        "git_instance": "github",
                        "repo_name": "testbed/pydata__xarray__0.12",
                        "sha": ""
                    },
                    "reason_for_relevance": "This snippet contains the time encoding and decoding logic, which is likely where the precision loss issue during IO roundtrip is occurring."
                }
            ]
        },
        "solution_task": "- Ensure that the `units` and `calendar` attributes are properly set in the encoding for the `time` variable before saving to NetCDF.\n- Use `cftime.num2date` and `cftime.date2num` to ensure that the conversion between time values and numeric time values is consistent with the specified calendar and units.\n- When saving the DataArray to NetCDF, explicitly specify the encoding for the `time` variable, including the `units` as \"microseconds since 1900-01-01\" and the `calendar` as \"proleptic_gregorian\".\n- When loading the DataArray from the NetCDF file, ensure that the `decode_times` parameter is set to `True` to properly decode the time values.\n- If the issue persists, consider updating the libraries to the latest versions that may contain bug fixes related to time precision.\n- Verify that the `dtype` of the time variable is appropriate for the precision required (e.g., `np.datetime64[us]` for microsecond precision).\n- After loading the DataArray from the NetCDF file, compare the time values before and after the IO roundtrip using `np.allclose` with an appropriate `atol` to account for any acceptable minor differences in precision.",
        "bouncer_response": {
            "score": 7,
            "criteria": "The RESPONSE must provide actionable, relevant guidance to more parts of the PROMPT's described issue to score higher.",
            "supporting_evidence": "The RESPONSE provides several actionable suggestions that are relevant to the issue of losing millisecond precision during IO roundtrip. It advises on ensuring proper setting of `units` and `calendar` attributes, using `cftime.num2date` and `cftime.date2num` for consistent conversion, specifying encoding explicitly, setting `decode_times` parameter to `True`, updating libraries, verifying `dtype`, and using `np.allclose` to compare time values. These are all relevant to the problem of precision loss in datetime64 data. However, the RESPONSE includes a link to a code snippet that does not exist (the provided URL is not valid), which could lead to confusion and does not add value to the solution. This prevents the RESPONSE from scoring higher as it contains a potentially misleading element.",
            "clarification_questions": "NONE"
        },
        "end_time": 1711759524.051253,
        "result": "- Ensure that the `units` and `calendar` attributes are properly set in the encoding for the `time` variable before saving to NetCDF.\n- Use `cftime.num2date` and `cftime.date2num` to ensure that the conversion between time values and numeric time values is consistent with the specified calendar and units.\n- When saving the DataArray to NetCDF, explicitly specify the encoding for the `time` variable, including the `units` as \"microseconds since 1900-01-01\" and the `calendar` as \"proleptic_gregorian\".\n- When loading the DataArray from the NetCDF file, ensure that the `decode_times` parameter is set to `True` to properly decode the time values.\n- If the issue persists, consider updating the libraries to the latest versions that may contain bug fixes related to time precision.\n- Verify that the `dtype` of the time variable is appropriate for the precision required (e.g., `np.datetime64[us]` for microsecond precision).\n- After loading the DataArray from the NetCDF file, compare the time values before and after the IO roundtrip using `np.allclose` with an appropriate `atol` to account for any acceptable minor differences in precision.\n\n### Code snippets to check\n\n-  \u2192 [xarray](https://github.com/testbed/pydata__xarray__0.12/tree//xarray) \u2192 [coding](https://github.com/testbed/pydata__xarray__0.12/tree//xarray/coding)- [Lines 1 - 122](https://github.com/testbed/pydata__xarray__0.12/blob//xarray/coding/times.py#L1-L122)\n\n This snippet contains the time encoding and decoding logic, which is likely where the precision loss issue during IO roundtrip is occurring.",
        "context_relevance": 0.17,
        "hypotheses": [],
        "groundedness_score": 1.0,
        "qa_relevance_reasons": {
            "reason": "Criteria: The RESPONSE provides a detailed and relevant set of instructions and considerations that address the entire PROMPT, including the issue of precision loss during IO roundtrip, the expected output, and the problem description. It also provides a link to code snippets that could be related to the issue.\nSupporting Evidence: The RESPONSE directly addresses the problem of precision loss by suggesting checks and adjustments to the encoding attributes, the use of specific functions for time conversion, and the verification of data types and library versions. It also suggests a method for comparing time values before and after the IO roundtrip, which is directly related to the user's concern. The RESPONSE is relevant to the entire PROMPT and answers it completely, but it does not directly address the specific versions provided in the PROMPT, which might be necessary for a full resolution, hence the score of 9 instead of 10."
        },
        "qa_relevance": 0.9
    }
}