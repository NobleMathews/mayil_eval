{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load env\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package llama-index is installed but has a version conflict:\n",
      "\t(llama-index 0.9.21 (/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages), Requirement.parse('llama-index>=0.9.26'))\n",
      "\n",
      "This package is optional for trulens_eval so this may not be a problem but if\n",
      "you need to use the related optional features and find there are errors, you\n",
      "will need to resolve the conflict:\n",
      "\n",
      "    ```bash\n",
      "    pip install 'llama-index>=0.9.26'\n",
      "    ```\n",
      "\n",
      "If you are running trulens_eval in a notebook, you may need to restart the\n",
      "kernel after resolving the conflict. If your distribution is in a bad place\n",
      "beyond this package, you may need to reinstall trulens_eval so that all of the\n",
      "dependencies get installed and hopefully corrected:\n",
      "    \n",
      "    ```bash\n",
      "    pip uninstall -y trulens_eval\n",
      "    pip install trulens_eval\n",
      "    ```\n",
      "\n",
      "Package chromadb is installed but has a version conflict:\n",
      "\t(chromadb 0.4.14 (/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages), Requirement.parse('chromadb>=0.4.18'))\n",
      "\n",
      "This package is optional for trulens_eval so this may not be a problem but if\n",
      "you need to use the related optional features and find there are errors, you\n",
      "will need to resolve the conflict:\n",
      "\n",
      "    ```bash\n",
      "    pip install 'chromadb>=0.4.18'\n",
      "    ```\n",
      "\n",
      "If you are running trulens_eval in a notebook, you may need to restart the\n",
      "kernel after resolving the conflict. If your distribution is in a bad place\n",
      "beyond this package, you may need to reinstall trulens_eval so that all of the\n",
      "dependencies get installed and hopefully corrected:\n",
      "    \n",
      "    ```bash\n",
      "    pip uninstall -y trulens_eval\n",
      "    pip install trulens_eval\n",
      "    ```\n",
      "\n",
      "Using legacy llama_index version 0.9.21. Consider upgrading to 0.10.0 or later.\n"
     ]
    }
   ],
   "source": [
    "virtual_app = dict(\n",
    "    llm=dict(\n",
    "        modelname=\"some llm component model name\"\n",
    "    ),\n",
    "    template=\"information about the template I used in my app\",\n",
    "    debug=\"all of these fields are completely optional\"\n",
    ")\n",
    "from trulens_eval import Select\n",
    "from trulens_eval.tru_virtual import VirtualApp\n",
    "\n",
    "virtual_app = VirtualApp(virtual_app) # can start with the prior dictionary\n",
    "virtual_app[Select.RecordCalls.llm.maxtokens] = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Select\n",
    "retriever = Select.RecordCalls.retriever\n",
    "synthesizer = Select.RecordCalls.synthesizer\n",
    "\n",
    "virtual_app[retriever] = \"retriever\"\n",
    "virtual_app[synthesizer] = \"synthesizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.tru_virtual import VirtualRecord\n",
    "\n",
    "# The selector for a presumed context retrieval component's call to\n",
    "# `get_context`. The names are arbitrary but may be useful for readability on\n",
    "# your end.\n",
    "context_call = retriever.get_context\n",
    "generation = synthesizer.generate\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"./data/test.json\", \"r\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "data = []\n",
    "task_instances = []\n",
    "for issue_details in ground_truth:\n",
    "    with open(f\"./data/v1/{issue_details['instance_id']}.json\", \"r\") as f:\n",
    "        generated_details = json.load(f)\n",
    "    problem_statement = issue_details[\"problem_statement\"]\n",
    "    mayil_response = generated_details[\"mayil_response\"]\n",
    "    rets = [f\"Filename: {c['filename']} | (Lines: {c['start_line']} to {c['end_line']})\\nCode Snippet:\\n{c['code']}\" for c in generated_details[\"mayil_collected_data\"][\"relevant_snippets\"]]\n",
    "    if not mayil_response:\n",
    "        continue\n",
    "\n",
    "    rec = VirtualRecord(\n",
    "        main_input=problem_statement,\n",
    "        main_output=mayil_response,\n",
    "        calls=\n",
    "            {\n",
    "                context_call: dict(\n",
    "                    args=[problem_statement],\n",
    "                    rets=rets\n",
    "                ),\n",
    "                generation: dict(\n",
    "                    args=[\"\"\"\n",
    "                        We have provided the below context: \\n\n",
    "                        ---------------------\\n\n",
    "                        \"\"\"+ \"\\n\".join(rets) +\n",
    "                        \"\"\"\n",
    "                        ---------------------\\n\n",
    "                        Given this information, please create a response to guide the developer in solving the issue:\n",
    "                        \"\"\"+ problem_statement],\n",
    "                    rets=[mayil_response]\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "    data.append(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2280"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In qs_relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In qs_relevance, input context will be set to __record__.app.retriever.get_context.rets[:] .\n",
      "âœ… In Groundedness, input source will be set to __record__.app.retriever.get_context.rets[:].collect() .\n",
      "âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.feedback.provider import OpenAI\n",
    "from trulens_eval.feedback.feedback import Feedback\n",
    "\n",
    "# Initialize provider class\n",
    "openai = OpenAI()\n",
    "\n",
    "# Select context to be used in feedback. We select the return values of the\n",
    "# virtual `get_context` call in the virtual `retriever` component. Names are\n",
    "# arbitrary except for `rets`.\n",
    "context = context_call.rets[:]\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(openai.qs_relevance)\n",
    "    .on_input()\n",
    "    .on(context)\n",
    ")\n",
    "\n",
    "from trulens_eval.feedback import Groundedness\n",
    "grounded = Groundedness(groundedness_provider=openai)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name = \"Groundedness\")\n",
    "    .on(context.collect())\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = (\n",
    "    Feedback(openai.relevance_with_cot_reasons, name = \"Answer Relevance\")\n",
    "    .on_input_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.tru_virtual import TruVirtual\n",
    "\n",
    "virtual_recorder = TruVirtual(\n",
    "    app_id=\"Mayil\",\n",
    "    app=virtual_app,\n",
    "    feedbacks=[f_context_relevance, f_groundedness, f_qa_relevance],\n",
    "    feedback_mode = \"deferred\" # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in data:\n",
    "    virtual_recorder.add_record(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "\n",
    "# tru.run_dashboard(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will keep max of 32 feedback(s) running.\n",
      "Tasks are spread among max of 128 thread(s).\n",
      "Will rerun running feedbacks after a minute.\n",
      "Will rerun failed feedbacks after 5 minutes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Thread(Thread-4 (_future_target_wrapper), started daemon 11688505344)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b2df8dd28046dfb99ceec0a5c7ab9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Feedback Status:   0%|          | 0/6840 [00:00<?, ?feedbacks/s, NONE=6840]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7924b8deb04f949fd76c11305ea9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Done Runs: 0runs [00:00, ?runs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf0c65ea53a4449aa448746fd954fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Waiting for Runs: 0runs [00:00, ?runs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a487015e8d1646558403901f7745c343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efd1df0069b4a789f768e3e352c6b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd431b1dce3c434eae90d7180b553714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9018c1c02182432683c08bdc637d2049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "affbb46d598544d2b507cf7d6baa3087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e279df2fc84604ad242d689d35e3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559412b6cbe240819c6fb2efd147225b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe827ba6e804ada997f1be97a99d5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e24c67bd5594783abf8008f6bdc4fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9545410b4c764ae9b5104432e8a00cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350bd194346e49f095e794fb1c70bbc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab027305b0a94a3ea6318d74b0871dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca9a6fe72084b05916354aa679bf704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e5a62c287545bf930d8bdf6c406402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19098cda88464f4985a24af874c937fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58841, Requested 2753. Please try again in 1.594s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 57948, Requested 3701. Please try again in 1.649s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59282, Requested 1292. Please try again in 574ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 57309, Requested 3348. Please try again in 657ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59158, Requested 2463. Please try again in 1.621s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58532, Requested 4233. Please try again in 2.765s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58066, Requested 5467. Please try again in 3.533s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59101, Requested 969. Please try again in 70ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58999, Requested 5484. Please try again in 4.483s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58949, Requested 1896. Please try again in 845ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58834, Requested 5256. Please try again in 4.09s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58821, Requested 5111. Please try again in 3.932s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 57976, Requested 2753. Please try again in 729ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59093, Requested 943. Please try again in 36ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_43d2548123fe82bc3165f3dbef60cb7b in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_89d6b8e624b228557bdf01cef42da4cb in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59388, Requested 4930. Please try again in 4.318s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59190, Requested 6434. Please try again in 5.624s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58954, Requested 2463. Please try again in 1.417s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59535, Requested 1896. Please try again in 1.431s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_b5be3cbc6b8f2db3018fbbf3eea3acac in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59142, Requested 3701. Please try again in 2.843s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58620, Requested 2405. Please try again in 1.025s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59145, Requested 943. Please try again in 88ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59210, Requested 3348. Please try again in 2.558s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_3e943000f3b1507f142effdf83e677c9 in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58751, Requested 1292. Please try again in 43ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58316, Requested 4233. Please try again in 2.549s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_33a5b694d067f12b123ac5596aa32546 in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_463ed76db37a2fcbd0445dd7699b2e02 in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58869, Requested 7837. Please try again in 6.706s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59902, Requested 2753. Please try again in 2.655s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59264, Requested 1896. Please try again in 1.16s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59236, Requested 2405. Please try again in 1.641s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58835, Requested 4041. Please try again in 2.876s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "\n",
      "Pace has a long delay of 34.989989 seconds. There might have been a burst of\n",
      "requests which may become a problem for the receiver of whatever is being paced.\n",
      "Consider reducing the `seconds_per_period` (currently 60.0 [seconds]) over which to\n",
      "maintain pace to reduce burstiness. \" Alternatively reduce `marks_per_second`\n",
      "(currently 1.0 [1/second]) to reduce the number of marks\n",
      "per second in that period. \n",
      "\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59611, Requested 5256. Please try again in 4.867s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_4fcbec54b2bfa1cb490fe9972efdb21b in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59464, Requested 5467. Please try again in 4.931s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59457, Requested 5111. Please try again in 4.568s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59859, Requested 943. Please try again in 802ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59186, Requested 5484. Please try again in 4.67s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59055, Requested 2463. Please try again in 1.518s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_acf0a73eb88f84e5a78906e3c014d34b in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59222, Requested 4930. Please try again in 4.152s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58630, Requested 3701. Please try again in 2.331s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_f7142d22d0724a191106ab29438cc69b in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989fec9a8ad64fe3b461d494cbd83ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 56858, Requested 3348. Please try again in 206ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 57611, Requested 6434. Please try again in 4.045s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_191860120789e056177d028b243c26ab in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_34a694314697191e6779e4ef06cd57e8 in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_8ac952d7b0e4ac0c422d6214ce44342c in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8608aba59c444d682aef6273f59bbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_cb66d8cca0c49edefe1dc9db154eb9eb in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_66ec30fab200a519d791d395adffab0e in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d5a8e3b5a740b08bde4015ccfd1a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cf1cfc8cfa41579d405abc2437bbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58e9452bebc4ad99b9299079abc71e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63b1f5a797b4fb2af6cb9e65dd88ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8da9d967f54fe19452951416703bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc4ef3e867a4809a2d1019acbd7b8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9af77e587641e4a6f10f55122983d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e74bbc3ecd849d7b261430692673145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b041161b1d6f47aabebf23707b91b6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4139152d85c54734a5708950c09a50f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87607c8eebbf4d8591f0df532bae9a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc564d2377754c64add3bd2b381e6506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d15855cfb074f8a93a002b5736dae3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59115, Requested 1896. Please try again in 1.011s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d865fd6e14f142fcad9887fb3f5c31d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17dabaeffb7749af842107d1683ca220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59159, Requested 1134. Please try again in 293ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59049, Requested 1112. Please try again in 161ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59888, Requested 943. Please try again in 831ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=0.\n",
      "Feedback Function exception caught: Traceback (most recent call last):\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 817, in run\n",
      "    result_and_meta, part_cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 496, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 477, in track_all_costs\n",
      "    return Endpoint._track_costs(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 574, in _track_costs\n",
      "    result: T = __func(*args, **kwargs)\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 336, in qs_relevance\n",
      "    return self.context_relevance(question, context)\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 318, in context_relevance\n",
      "    return self.generate_score(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 203, in generate_score\n",
      "    response = self.endpoint.run_in_pace(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 308, in run_in_pace\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Endpoint openai request failed 4 time(s): \n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59093, Requested 943. Please try again in 36ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59145, Requested 943. Please try again in 88ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59859, Requested 943. Please try again in 802ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59888, Requested 943. Please try again in 831ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 823, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Evaluation of qs_relevance failed on inputs: \n",
      "{'context': 'Filename: django/core/management/commands/loaddata.py | (Lines: '\n",
      "            '139 to 412)\\n'\n",
      "            'Code Sni.\n",
      "\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59915, Requested 1010. Please try again in 925ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59757, Requested 848. Please try again in 605ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_0e71156358952a44e57664be0362039e in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=0.\n",
      "Feedback Function exception caught: Traceback (most recent call last):\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 817, in run\n",
      "    result_and_meta, part_cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 496, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 477, in track_all_costs\n",
      "    return Endpoint._track_costs(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 574, in _track_costs\n",
      "    result: T = __func(*args, **kwargs)\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 336, in qs_relevance\n",
      "    return self.context_relevance(question, context)\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 318, in context_relevance\n",
      "    return self.generate_score(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 203, in generate_score\n",
      "    response = self.endpoint.run_in_pace(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 308, in run_in_pace\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Endpoint openai request failed 4 time(s): \n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59101, Requested 969. Please try again in 70ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_b5be3cbc6b8f2db3018fbbf3eea3acac in your email.)', 'type': 'server_error', 'param': None, 'code': None}}\n",
      "\tError code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_4fcbec54b2bfa1cb490fe9972efdb21b in your email.)', 'type': 'server_error', 'param': None, 'code': None}}\n",
      "\tError code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_0e71156358952a44e57664be0362039e in your email.)', 'type': 'server_error', 'param': None, 'code': None}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 823, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Evaluation of qs_relevance failed on inputs: \n",
      "{'context': 'Filename: sklearn/utils/estimator_checks.py | (Lines: 731 to '\n",
      "            '777)\\n'\n",
      "            'Code Snippet:\\n'\n",
      " .\n",
      "\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59750, Requested 830. Please try again in 580ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59707, Requested 1153. Please try again in 860ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59563, Requested 1536. Please try again in 1.099s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59124, Requested 1452. Please try again in 576ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59109, Requested 1508. Please try again in 617ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59053, Requested 1501. Please try again in 554ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58922, Requested 1896. Please try again in 818ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=0.\n",
      "Feedback Function exception caught: Traceback (most recent call last):\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 817, in run\n",
      "    result_and_meta, part_cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 496, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 477, in track_all_costs\n",
      "    return Endpoint._track_costs(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 574, in _track_costs\n",
      "    result: T = __func(*args, **kwargs)\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 472, in relevance_with_cot_reasons\n",
      "    return self.generate_score_and_reasons(system_prompt, user_prompt)\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 230, in generate_score_and_reasons\n",
      "    response = self.endpoint.run_in_pace(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 308, in run_in_pace\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Endpoint openai request failed 4 time(s): \n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58949, Requested 1896. Please try again in 845ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59535, Requested 1896. Please try again in 1.431s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59264, Requested 1896. Please try again in 1.16s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58922, Requested 1896. Please try again in 818ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 823, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Evaluation of Answer Relevance failed on inputs: \n",
      "{'prompt': \"has_key, has_keys, and has_any_keys JSONField() lookups don't \"\n",
      "           'handle numeric keys on SQLite, MySQL, an.\n",
      "\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58859, Requested 1359. Please try again in 218ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59897, Requested 440. Please try again in 337ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_02fa2aff9996bbe0082f7a95c86af11c in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=0.\n",
      "Feedback Function exception caught: Traceback (most recent call last):\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 817, in run\n",
      "    result_and_meta, part_cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 496, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 477, in track_all_costs\n",
      "    return Endpoint._track_costs(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 574, in _track_costs\n",
      "    result: T = __func(*args, **kwargs)\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 472, in relevance_with_cot_reasons\n",
      "    return self.generate_score_and_reasons(system_prompt, user_prompt)\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 230, in generate_score_and_reasons\n",
      "    response = self.endpoint.run_in_pace(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 308, in run_in_pace\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Endpoint openai request failed 4 time(s): \n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59282, Requested 1292. Please try again in 574ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_43d2548123fe82bc3165f3dbef60cb7b in your email.)', 'type': 'server_error', 'param': None, 'code': None}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58751, Requested 1292. Please try again in 43ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_02fa2aff9996bbe0082f7a95c86af11c in your email.)', 'type': 'server_error', 'param': None, 'code': None}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 823, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Evaluation of Answer Relevance failed on inputs: \n",
      "{'prompt': 'Fix handling empty string for If-Modified-Since header\\n'\n",
      "           'Description\\n'\n",
      "           '\\t\\n'\n",
      "           'E.\n",
      "\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59448, Requested 4233. Please try again in 3.681s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59212, Requested 2405. Please try again in 1.617s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59205, Requested 2463. Please try again in 1.668s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=0.\n",
      "Feedback Function exception caught: Traceback (most recent call last):\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 817, in run\n",
      "    result_and_meta, part_cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 496, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 477, in track_all_costs\n",
      "    return Endpoint._track_costs(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 574, in _track_costs\n",
      "    result: T = __func(*args, **kwargs)\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 472, in relevance_with_cot_reasons\n",
      "    return self.generate_score_and_reasons(system_prompt, user_prompt)\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 230, in generate_score_and_reasons\n",
      "    response = self.endpoint.run_in_pace(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 308, in run_in_pace\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Endpoint openai request failed 4 time(s): \n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59158, Requested 2463. Please try again in 1.621s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58954, Requested 2463. Please try again in 1.417s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59055, Requested 2463. Please try again in 1.518s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59205, Requested 2463. Please try again in 1.668s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 823, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Evaluation of Answer Relevance failed on inputs: \n",
      "{'prompt': 'Pytest 7 not ignoring warnings as instructed on `pytest.ini`\\n'\n",
      "           '<!--\\r\\n'\n",
      "           'Thanks for submitt.\n",
      "\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58177, Requested 4041. Please try again in 2.218s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58100, Requested 2753. Please try again in 853ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=0.\n",
      "Feedback Function exception caught: Traceback (most recent call last):\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 817, in run\n",
      "    result_and_meta, part_cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 496, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 477, in track_all_costs\n",
      "    return Endpoint._track_costs(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 574, in _track_costs\n",
      "    result: T = __func(*args, **kwargs)\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/groundedness.py\", line 120, in groundedness_measure_with_cot_reasons\n",
      "    reason = self.groundedness_provider._groundedness_doc_in_out(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 176, in _groundedness_doc_in_out\n",
      "    return self.endpoint.run_in_pace(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 308, in run_in_pace\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Endpoint openai request failed 4 time(s): \n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58841, Requested 2753. Please try again in 1.594s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 57976, Requested 2753. Please try again in 729ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59902, Requested 2753. Please try again in 2.655s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58100, Requested 2753. Please try again in 853ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 823, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Evaluation of Groundedness failed on inputs: \n",
      "{'source': ['Filename: django/forms/fields.py | (Lines: 1204 to 1255)\\n'\n",
      "            'Code Snippet:\\n'\n",
      "            'class FilePa.\n",
      "\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58896, Requested 3701. Please try again in 2.597s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=0.\n",
      "Feedback Function exception caught: Traceback (most recent call last):\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 817, in run\n",
      "    result_and_meta, part_cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 496, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 477, in track_all_costs\n",
      "    return Endpoint._track_costs(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 574, in _track_costs\n",
      "    result: T = __func(*args, **kwargs)\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/groundedness.py\", line 120, in groundedness_measure_with_cot_reasons\n",
      "    reason = self.groundedness_provider._groundedness_doc_in_out(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 176, in _groundedness_doc_in_out\n",
      "    return self.endpoint.run_in_pace(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 308, in run_in_pace\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Endpoint openai request failed 4 time(s): \n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 57948, Requested 3701. Please try again in 1.649s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59142, Requested 3701. Please try again in 2.843s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58630, Requested 3701. Please try again in 2.331s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58896, Requested 3701. Please try again in 2.597s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 823, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Evaluation of Groundedness failed on inputs: \n",
      "{'source': ['Filename: xarray/core/groupby.py | (Lines: 50 to 911)\\n'\n",
      "            'Code Snippet:\\n'\n",
      "            'from xarray.cor.\n",
      "\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58752, Requested 3348. Please try again in 2.1s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=0.\n",
      "Feedback Function exception caught: Traceback (most recent call last):\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 817, in run\n",
      "    result_and_meta, part_cost = Endpoint.track_all_costs_tally(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 496, in track_all_costs_tally\n",
      "    result, cbs = Endpoint.track_all_costs(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 477, in track_all_costs\n",
      "    return Endpoint._track_costs(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 574, in _track_costs\n",
      "    result: T = __func(*args, **kwargs)\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/groundedness.py\", line 120, in groundedness_measure_with_cot_reasons\n",
      "    reason = self.groundedness_provider._groundedness_doc_in_out(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/base.py\", line 176, in _groundedness_doc_in_out\n",
      "    return self.endpoint.run_in_pace(\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/provider/endpoint/base.py\", line 308, in run_in_pace\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Endpoint openai request failed 4 time(s): \n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 57309, Requested 3348. Please try again in 657ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59210, Requested 3348. Please try again in 2.558s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 56858, Requested 3348. Please try again in 206ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\tError code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58752, Requested 3348. Please try again in 2.1s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/noble/opt/anaconda3/envs/mayil_ai/lib/python3.10/site-packages/trulens_eval/feedback/feedback.py\", line 823, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Evaluation of Groundedness failed on inputs: \n",
      "{'source': ['Filename: .github/config.yml | (Lines: 1 to 2)\\n'\n",
      "            'Code Snippet:\\n'\n",
      "            'rtd:\\n'\n",
      "            ' .\n",
      "\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_5142eda74eab1deec600944102cc6414 in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59226, Requested 7837. Please try again in 7.063s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58634, Requested 4396. Please try again in 3.03s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_cf87045f7c1800d0ca41f40ad306e53c in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58618, Requested 5467. Please try again in 4.084s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58526, Requested 5111. Please try again in 3.637s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58499, Requested 5256. Please try again in 3.755s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58449, Requested 5484. Please try again in 3.933s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58376, Requested 4930. Please try again in 3.306s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58387, Requested 1887. Please try again in 274ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2aef0ab29b74f01a03f47e3f7ac92da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbc270c23894f16816197532eb2bc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640b746274684e8f8fe90620a075a95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956aa295fab048daa294879c3f4ec07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c198d4dc5e44948b9535d1c63b92a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f3b4f2cd6c4b21b11c52ccef5be39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f131697bcad542b0af1247708c626767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb0454e54c045bfaa0e2a89d3240522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76490c0cc884cedb439d93770354f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ced072b44ba420892d0143c37e13147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949a660f65ee4e7fbea57e06bac90e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52eca48183c455099e9723fb9fe528e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Groundedness per statement in source:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59733, Requested 2849. Please try again in 2.582s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_1bf53162f63329d946b49f029af40238 in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59063, Requested 6434. Please try again in 5.497s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=1.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58352, Requested 6844. Please try again in 5.196s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_ee0556a720b5d05d3dbbac86cc47ec10 in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59141, Requested 7718. Please try again in 6.859s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58917, Requested 1198. Please try again in 115ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59101, Requested 6943. Please try again in 6.044s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59109, Requested 1247. Please try again in 356ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59048, Requested 1445. Please try again in 493ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58630, Requested 1497. Please try again in 127ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59763, Requested 1311. Please try again in 1.074s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59791, Requested 1786. Please try again in 1.577s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59620, Requested 1612. Please try again in 1.232s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_9eb4679379e2bd66a0d19f9de5bfb82e in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59944, Requested 1485. Please try again in 1.429s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59802, Requested 1382. Please try again in 1.184s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_a63a54aa27ac122305020e78e3e9bd3b in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=2.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59144, Requested 1459. Please try again in 603ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58798, Requested 1738. Please try again in 536ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58807, Requested 1543. Please try again in 350ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59680, Requested 1282. Please try again in 962ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59362, Requested 2168. Please try again in 1.53s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "\n",
      "Pace has a long delay of 34.98537 seconds. There might have been a burst of\n",
      "requests which may become a problem for the receiver of whatever is being paced.\n",
      "Consider reducing the `seconds_per_period` (currently 60.0 [seconds]) over which to\n",
      "maintain pace to reduce burstiness. \" Alternatively reduce `marks_per_second`\n",
      "(currently 1.0 [1/second]) to reduce the number of marks\n",
      "per second in that period. \n",
      "\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_c7401cf3f5c21d6b1a8163c7303e163f in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58990, Requested 1175. Please try again in 165ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_085e3a9c871a60fab2dec4bf80c96e57 in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 59578, Requested 5016. Please try again in 4.594s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.InternalServerError'>=Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_f549b1b61d656a4ca8e0aed9d5ae8914 in your email.)', 'type': 'server_error', 'param': None, 'code': None}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 58608, Requested 5434. Please try again in 4.042s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n",
      "openai request failed <class 'openai.RateLimitError'>=Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-Fr9UDIYK44Xr4pHLhECIGN9O on tokens per min (TPM): Limit 60000, Used 56876, Requested 5062. Please try again in 1.938s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Retries remaining=3.\n"
     ]
    }
   ],
   "source": [
    "tru.start_evaluator()\n",
    "\n",
    "# tru.stop_evaluator() # stop if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mayil_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
