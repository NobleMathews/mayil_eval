{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET DETAILS\n",
    "\n",
    "\n",
    "- `instance_id: (str)` - A formatted instance identifier, usually as repo_owner__repo_name-PR-number.\n",
    "\n",
    "- `patch: (str)` - The gold patch, the patch generated by the PR (minus test-related code), that resolved the issue.\n",
    "\n",
    "- `repo: (str)` - The repository owner/name identifier from GitHub.\n",
    "base_commit: (str) - The commit hash of the repository representing the HEAD of the repository before the solution PR is applied.\n",
    "\n",
    "- `hints_text: (str)` - Comments made on the issue prior to the creation of the solution PR’s first commit creation date.\n",
    "\n",
    "- `created_at: (str)` - The creation date of the pull request.\n",
    "\n",
    "- `test_patch: (str)` - A test-file patch that was contributed by the solution PR.\n",
    "\n",
    "- `problem_statement: (str)` - The issue title and body.\n",
    "\n",
    "- `version: (str)` - Installation version to use for running evaluation.\n",
    "environment_setup_commit: (str) - commit hash to use for environment setup and installation.\n",
    "\n",
    "- `FAIL_TO_PASS: (str)` - A json list of strings that represent the set of tests resolved by the PR and tied to the issue resolution.\n",
    "\n",
    "- `PASS_TO_PASS: (str)` - A json list of strings that represent tests that should pass before and after the PR application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study Design\n",
    "\n",
    "- SWE Bench doesn’t make use of hints, these are Discussion so it makes sense for Mayil to not look at it as well\n",
    "\n",
    "- For the Unassisted leaderboard, we only consider systems that have no assistance finding the relevant files in the repository. (Assisted setting list of files modified are supplied)\n",
    "\n",
    "- Test patch is taken as is in the SWE-Bench dataset, it is used as the oracle for correctness after predicted patch is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from constants import swe_bench_tasks, KEY_INSTANCE_ID\n",
    "\n",
    "\n",
    "tasks = None\n",
    "\n",
    "temp = load_dataset('princeton-nlp/SWE-bench', split=swe_bench_tasks)\n",
    "temp = temp.to_dict()\n",
    "tasks = []\n",
    "for idx in range(len(temp[KEY_INSTANCE_ID])):\n",
    "    task_instance = {}\n",
    "    for k in temp.keys():\n",
    "        task_instance[k] = temp[k][idx]\n",
    "    tasks.append(task_instance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"data/{swe_bench_tasks}.json\", \"w\") as f:\n",
    "    json.dump(tasks, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mayil_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
